Unnamed: 0,Text,Text_1
0,"Aerodynamics, from Greek ἀήρ aero (air) + δυναμική (dynamics), is the study of motion of air, particularly when affected by a solid object, such as an airplane wing. It is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.  The term aerodynamics is often used synonymously with gas dynamics, the difference being that ""gas dynamics"" applies to the study of the motion of all gases, and is not limited to air. 
The formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier.  Most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by Otto Lilienthal in 1891.  Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies.  Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.


== History ==

Modern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the Ancient Greek legend of Icarus and Daedalus.  Fundamental concepts of continuum, drag, and pressure gradients appear in the work of Aristotle and Archimedes.In 1726, Sir Isaac Newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. Dutch-Swiss mathematician Daniel Bernoulli followed in 1738 with Hydrodynamica in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as Bernoulli's principle, which provides one method for calculating aerodynamic lift. In 1757, Leonhard Euler published the more general Euler equations which could be applied to both compressible and incompressible flows. The Euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the Navier–Stokes equations.  The Navier-Stokes equations are the most general governing equations of fluid flow and but are difficult to solve for the flow around all but the simplest of shapes.

In 1799, Sir George Cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century.  In 1871, Francis Herbert Wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces.  Drag theories were developed by Jean le Rond d'Alembert, Gustav Kirchhoff, and Lord Rayleigh. In 1889, Charles Renard, a French aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. Otto Lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag.  Building on these developments as well as research carried out in their own wind tunnel, the Wright brothers flew the first powered airplane on December 17, 1903.
During the time of the first flights, Frederick W. Lanchester, Martin Kutta, and Nikolai Zhukovsky independently created theories that connected circulation of a fluid flow to lift. Kutta and Zhukovsky went on to develop a two-dimensional wing theory. Expanding upon the work of Lanchester, Ludwig Prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers.
As aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near the speed of sound. The differences in airflow under such conditions lead to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. The ratio of the flow speed to the speed of sound was named the Mach number after Ernst Mach who was one of the first to investigate the properties of the supersonic flow. Macquorn Rankine and Pierre Henri Hugoniot independently developed the theory for flow properties before and after a shock wave, while Jakob Ackeret led the initial work of calculating the lift and drag of supersonic airfoils. Theodore von Kármán and Hugh Latimer Dryden introduced the term transonic to describe flow speeds between the critical Mach number and Mach 1 where drag increases rapidly. This rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken in 1947 using the Bell X-1 aircraft.
By the time the sound barrier was broken, aerodynamicists' understanding of the subsonic and low supersonic flow had matured. The Cold War prompted the design of an ever-evolving line of high-performance aircraft. Computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions.  Understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behaviour of fluid flow to the engineering of a vehicle such that it interacts predictably with the fluid flow. Designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the Navier-Stokes equations.


== Fundamental concepts ==

Understanding the motion of air around an object (often called a flow field)  enables the calculation of forces and moments acting on the object.  In many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight.  Of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body.  Calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. Continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. These properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows.  Density, flow velocity, and an additional property, viscosity, are used to classify flow fields. 


=== Flow classification ===
Flow velocity is used to classify flows according to speed regime. Subsonic flows are flow fields in which the air speed field is always below the local speed of sound. Transonic flows include both regions of subsonic flow and regions in which the local flow speed is greater than the local speed of sound. Supersonic flows are defined to be flows in which the flow speed is greater than the speed of sound everywhere. A fourth classification, hypersonic flow, refers to flows where the flow speed is much greater than the speed of sound.  Aerodynamicists disagree on the precise definition of hypersonic flow.
Compressible flow accounts for varying density within the flow. Subsonic flows are often idealized as incompressible, i.e. the density is assumed to be constant. Transonic and supersonic flows are compressible, and calculations that neglect the changes of density in these flow fields will yield inaccurate results.
Viscosity is associated with the frictional forces in a flow. In some flow fields, viscous effects are very small, and approximate solutions may safely neglect viscous effects. These approximations are called inviscid flows. Flows for which viscosity is not neglected are called viscous flows. Finally, aerodynamic problems may also be classified by the flow environment. External aerodynamics is the study of flow around solid objects of various shapes (e.g. around an airplane wing), while internal aerodynamics is the study of flow through passages inside solid objects (e.g. through a jet engine).


==== Continuum assumption ====
Unlike liquids and solids, gases are composed of discrete molecules which occupy only a small fraction of the volume filled by the gas. On a molecular level, flow fields are made up of the collisions of many individual of gas molecules between themselves and with solid surfaces. However, in most aerodynamics applications, the discrete molecular nature of gases is ignored, and the flow field is assumed to behave as a continuum. This assumption allows fluid properties such as density and flow velocity to be defined everywhere within the flow.
The validity of the continuum assumption is dependent on the density of the gas and the application in question. For the continuum assumption to be valid, the mean free path length must be much smaller than the length scale of the application in question. For example, many aerodynamics applications deal with aircraft flying in atmospheric conditions, where the mean free path length is on the order of micrometers and where the body is orders of magnitude larger. In these cases, the length scale of the aircraft ranges from a few meters to a few tens of meters, which is much larger than the mean free path length. For such applications, the continuum assumption is reasonable. The continuum assumption is less valid for extremely low-density flows, such as those encountered by vehicles at very high altitudes (e.g. 300,000 ft/90 km) or satellites in Low Earth orbit. In those cases, statistical mechanics is a more accurate method of solving the problem than is continuum aerodynamics. The Knudsen number can be used to guide the choice between statistical mechanics and the continuous formulation of aerodynamics.


=== Conservation laws ===
The assumption of  a fluid continuum allows problems in aerodynamics to be solved using fluid dynamics conservation laws. Three conservation principles are used: 

Conservation of mass
Conservation of mass requires that mass is neither created nor destroyed within a flow; the mathematical formulation of this principle is known as the mass continuity equation.
Conservation of momentum
The mathematical formulation of this principle can be considered an application of Newton's Second Law. Momentum within a flow is only changed by external forces, which may include both surface forces, such as viscous (frictional) forces, and body forces, such as weight.  The momentum conservation principle may be expressed as either a vector equation or separated into a set of three scalar equations (x,y,z components).
Conservation of energy
The energy conservation equation states that energy is neither created nor destroyed within a flow, and that any addition or subtraction of energy to a volume in the flow is caused by heat transfer, or by work into and out of the region of interest.Together, these  equations are known as the Navier-Stokes equations, although some authors define the term to only include the momentum equation(s).  The Navier-Stokes equations have no known analytical solution and are solved in modern aerodynamics using computational techniques. Because computational methods using high speed computers were not historically available and the high computational cost of solving these complex equations now that they are available, simplifications of the Navier-Stokes equations have been and continue to be employed. The Euler equations are a set of similar conservation equations which neglect viscosity and may be used in cases where the effect of viscosity is expected to be small. Further simplifications lead to Laplace's equation and potential flow theory.  Additionally, Bernoulli's equation is a solution in one dimension to both the momentum and energy conservation equations.
The ideal gas law or another such equation of state is often used in conjunction with these equations to form a determined system that allows the solution for the unknown variables.


== Branches of aerodynamics ==

Aerodynamic problems are classified by the flow environment or properties of the flow, including flow speed, compressibility, and viscosity. External aerodynamics is the study of flow around solid objects of various shapes. Evaluating the lift and drag on an airplane or the shock waves that form in front of the nose of a rocket are examples of external aerodynamics. Internal aerodynamics is the study of flow through passages in solid objects. For instance, internal aerodynamics encompasses the study of the airflow through a jet engine or through an air conditioning pipe.
Aerodynamic problems can also be classified according to whether the flow speed is below, near or above the speed of sound. A problem is called subsonic if all the speeds in the problem are less than the speed of sound, transonic if speeds both below and above the speed of sound are present (normally when the characteristic speed is approximately the speed of sound), supersonic when the characteristic flow speed is greater than the speed of sound, and hypersonic when the flow speed is much greater than the speed of sound. Aerodynamicists disagree over the precise definition of hypersonic flow; a rough definition considers flows with Mach numbers above 5 to be hypersonic.The influence of viscosity on the flow dictates a third classification. Some problems may encounter only very small viscous effects, in which case viscosity can be considered to be negligible. The approximations to these problems are called inviscid flows. Flows for which viscosity cannot be neglected are called viscous flows.


=== Incompressible aerodynamics ===

An incompressible flow is a flow in which density is constant in both time and space.  Although all real fluids are compressible, a flow is often approximated as incompressible if the effect of the density changes cause only small changes to the calculated results. This is more likely to be true when the flow speeds are significantly lower than the speed of sound. Effects of compressibility are more significant at speeds close to or above the speed of sound.  The Mach number is used to evaluate whether the incompressibility can be assumed, otherwise the effects of compressibility must be included.


==== Subsonic flow ====
Subsonic (or low-speed) aerodynamics describes fluid motion in flows which are much lower than the speed of sound everywhere in the flow. There are several branches of subsonic flow but one special case arises when the flow is inviscid, incompressible and irrotational. This case is called potential flow and allows the differential equations that describe the flow to be a simplified version of the equations of fluid dynamics, thus making available to the aerodynamicist a range of quick and easy solutions.In solving a subsonic problem, one decision to be made by the aerodynamicist is whether to incorporate the effects of compressibility. Compressibility is a description of the amount of change of density in the flow. When the effects of compressibility on the solution are small, the assumption that density is constant may be made. The problem is then an incompressible low-speed aerodynamics problem. When the density is allowed to vary, the flow is called compressible. In air, compressibility effects are usually ignored when the Mach number in the flow does not exceed 0.3 (about 335 feet (102 m) per second or 228 miles (366 km) per hour at 60 °F (16 °C)). Above Mach 0.3, the problem flow should be described using compressible aerodynamics.


=== Compressible aerodynamics ===

According to the theory of aerodynamics, a flow is considered to be compressible if the density changes along a streamline. This means that – unlike incompressible flow – changes in density are considered. In general, this is the case where the Mach number in part or all of the flow exceeds 0.3. The Mach 0.3 value is rather arbitrary, but it is used because gas flows with a Mach number below that value demonstrate changes in density of less than 5%. Furthermore, that maximum 5% density change occurs at the stagnation point (the point on the object where flow speed is zero), while the density changes around the rest of the object will be significantly lower. Transonic, supersonic, and hypersonic flows are all compressible flows.


==== Transonic flow ====

The term Transonic refers to a range of flow velocities just below and above the local speed of sound (generally taken as Mach 0.8–1.2). It is defined as the range of speeds between the critical Mach number, when some parts of the airflow over an aircraft become supersonic, and a higher speed, typically near Mach 1.2, when all of the airflow is supersonic. Between these speeds, some of the airflow is supersonic, while some of the airflow is not supersonic.


==== Supersonic flow ====

Supersonic aerodynamic problems are those involving flow speeds greater than the speed of sound. Calculating the lift on the Concorde during cruise can be an example of a supersonic aerodynamic problem.
Supersonic flow behaves very differently from subsonic flow. Fluids react to differences in pressure; pressure changes are how a fluid is ""told"" to respond to its environment. Therefore, since sound is, in fact, an infinitesimal pressure difference propagating through a fluid, the speed of sound in that fluid can be considered the fastest speed that ""information"" can travel in the flow. This difference most obviously manifests itself in the case of a fluid striking an object. In front of that object, the fluid builds up a stagnation pressure as impact with the object brings the moving fluid to rest. In fluid traveling at subsonic speed, this pressure disturbance can propagate upstream, changing the flow pattern ahead of the object and giving the impression that the fluid ""knows"" the object is there by seemingly adjusting its movement and is flowing around it. In a supersonic flow, however, the pressure disturbance cannot propagate upstream. Thus, when the fluid finally reaches the object it strikes it and the fluid is forced to change its properties – temperature, density, pressure, and Mach number—in an extremely violent and irreversible fashion called a shock wave. The presence of shock waves, along with the compressibility effects of high-flow velocity (see Reynolds number) fluids, is the central difference between the supersonic and subsonic aerodynamics regimes.


==== Hypersonic flow ====

In aerodynamics, hypersonic speeds are speeds that are highly supersonic. In the 1970s, the term generally came to refer to speeds of Mach 5 (5 times the speed of sound) and above. The hypersonic regime is a subset of the supersonic regime. Hypersonic flow is characterized by high temperature flow behind a shock wave, viscous interaction, and chemical dissociation of gas.


== Associated terminology ==

The incompressible and compressible flow regimes produce many associated phenomena, such as boundary layers and turbulence.


=== Boundary layers ===

The concept of a boundary layer is important in many problems in aerodynamics. The viscosity and fluid friction in the air is approximated as being significant only in this thin layer. This assumption makes the description of such aerodynamics much more tractable mathematically.


=== Turbulence ===

In aerodynamics, turbulence is characterized by chaotic property changes in the flow. These include low momentum diffusion, high momentum convection, and rapid variation of pressure and flow velocity in space and time. Flow that is not turbulent is called laminar flow.


== Aerodynamics in other fields ==


=== Engineering design ===

Aerodynamics is a significant element of vehicle design, including road cars and trucks where the main goal is to reduce the vehicle drag coefficient, and racing cars, where in addition to reducing drag the goal is also to increase the overall level of downforce.  Aerodynamics is also important in the prediction of forces and moments acting on sailing vessels. It is used in the design of mechanical components such as hard drive heads. Structural engineers resort to aerodynamics, and particularly aeroelasticity, when calculating wind loads in the design of large buildings, bridges, and wind turbines 
The aerodynamics of internal passages is important in heating/ventilation, gas piping, and in automotive engines where detailed flow patterns strongly affect the performance of the engine.


=== Environmental design ===
Urban aerodynamics are studied by town planners and designers seeking to improve amenity in outdoor spaces, or in creating urban microclimates to reduce the effects of urban pollution. The field of environmental aerodynamics describes ways in which atmospheric circulation and flight mechanics affect ecosystems. 
Aerodynamic equations are used in numerical weather prediction.


=== Ball-control in sports ===
Sports in which aerodynamics are of crucial importance include soccer, table tennis, cricket, baseball,  and golf, in which expert players can control the trajectory of the ball using the ""Magnus effect"".


== See also ==
Aeronautics
Aerostatics
Aviation
Insect flight – how bugs fly
List of aerospace engineering topics
List of engineering topics
Nose cone design


== References ==


== Further reading ==


== External links ==
NASA Beginner's Guide to Aerodynamics
Smithsonian National Air and Space Museum's How Things Fly website
Aerodynamics for Students
Aerodynamics for Pilots
Aerodynamics and Race Car Tuning
Aerodynamic Related Projects
eFluids Bicycle Aerodynamics
Application of Aerodynamics in Formula One (F1)
Aerodynamics in Car Racing
Aerodynamics of Birds","pandas(index=0, _1=0, text='aerodynamics, from greek ἀήρ aero (air)δυναμική (dynamics), is the study of motion of air, particularly when affected by a solid object, such as an airplane wing. it is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.  the term aerodynamics is often used synonymously with gas dynamics, the difference being that ""gas dynamics"" applies to the study of the motion of all gases, and is not limited to air. the formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier.  most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by otto lilienthal in 1891.  since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies.  recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.   == history ==  modern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the ancient greek legend of icarus and daedalus.  fundamental concepts of continuum, drag, and pressure gradients appear in the work of aristotle and archimedes.in 1726, sir isaac newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. dutch-swiss mathematician daniel bernoulli followed in 1738 with hydrodynamica in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as bernoulli\'s principle, which provides one method for calculating aerodynamic lift. in 1757, leonhard euler published the more general euler equations which could be applied to both compressible and incompressible flows. the euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the navier–stokes equations.  the navier-stokes equations are the most general governing equations of fluid flow and but are difficult to solve for the flow around all but the simplest of shapes.  in 1799, sir george cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century.  in 1871, francis herbert wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces.  drag theories were developed by jean le rond d\'alembert, gustav kirchhoff, and lord rayleigh. in 1889, charles renard, a french aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. otto lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag.  building on these developments as well as research carried out in their own wind tunnel, the wright brothers flew the first powered airplane on december 17, 1903. during the time of the first flights, frederick w. lanchester, martin kutta, and nikolai zhukovsky independently created theories that connected circulation of a fluid flow to lift. kutta and zhukovsky went on to develop a two-dimensional wing theory. expanding upon the work of lanchester, ludwig prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers. as aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near the speed of sound. the differences in airflow under such conditions lead to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. the ratio of the flow speed to the speed of sound was named the mach number after ernst mach who was one of the first to investigate the properties of the supersonic flow. macquorn rankine and pierre henri hugoniot independently developed the theory for flow properties before and after a shock wave, while jakob ackeret led the initial work of calculating the lift and drag of supersonic airfoils. theodore von kármán and hugh latimer dryden introduced the term transonic to describe flow speeds between the critical mach number and mach 1 where drag increases rapidly. this rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken in 1947 using the bell x-1 aircraft. by the time the sound barrier was broken, aerodynamicists\' understanding of the subsonic and low supersonic flow had matured. the cold war prompted the design of an ever-evolving line of high-performance aircraft. computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions.  understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behaviour of fluid flow to the engineering of a vehicle such that it interacts predictably with the fluid flow. designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the navier-stokes equations.   == fundamental concepts ==  understanding the motion of air around an object (often called a flow field)  enables the calculation of forces and moments acting on the object.  in many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight.  of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body.  calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. these properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows.  density, flow velocity, and an additional property, viscosity, are used to classify flow fields. sports in which aerodynamics are of crucial importance include soccer, table tennis, cricket, baseball,  and golf, in which expert players can control the trajectory of the ball using the ""magnus effect"".   == see also == aeronautics aerostatics aviation insect flight – how bugs fly list of aerospace engineering topics list of engineering topics nose cone design   == references ==   == further reading ==   == external links == nasa beginner\'s guide to aerodynamics smithsonian national air and space museum\'s how things fly website aerodynamics for students aerodynamics for pilots aerodynamics and race car tuning aerodynamic related projects efluids bicycle aerodynamics application of aerodynamics in formula one (f1) aerodynamics in car racing aerodynamics of birds')"
1,"Atmospheric entry is the movement of an object from outer space into and through the gases of an atmosphere of a planet, dwarf planet, or natural satellite. There are two main types of atmospheric entry: uncontrolled entry, such as the entry of astronomical objects, space debris, or bolides; and controlled entry (or reentry) of a spacecraft capable of being navigated or following a predetermined course. Technologies and procedures allowing the controlled atmospheric entry, descent, and landing of spacecraft are collectively termed as EDL.

Objects entering an atmosphere experience atmospheric drag, which puts mechanical stress on the object, and aerodynamic heating—caused mostly by compression of the air in front of the object, but also by drag. These forces can cause loss of mass (ablation) or even complete disintegration of smaller objects, and objects with lower compressive strength can explode.
Crewed space vehicles must be slowed to subsonic speeds before parachutes or air brakes may be deployed. Such vehicles have kinetic energies typically between 50 and 1,800 megajoules, and atmospheric dissipation is the only way of expending the kinetic energy. The amount of rocket fuel required to slow the vehicle would be nearly equal to the amount used to accelerate it initially, and it is thus highly impractical to use retro rockets for the entire Earth reentry procedure. While the high temperature generated at the surface of the heat shield is due to adiabatic compression, the vehicle's kinetic energy is ultimately lost to gas friction (viscosity) after the vehicle has passed by. Other smaller energy losses include black-body radiation directly from the hot gases and chemical reactions between ionized gases.
Ballistic warheads and expendable vehicles do not require slowing at reentry, and in fact, are made streamlined so as to maintain their speed. Furthermore, slow-speed returns to Earth from near-space such as parachute jumps from balloons do not require heat shielding because the gravitational acceleration of an object starting at relative rest from within the atmosphere itself (or not far above it) cannot create enough velocity to cause significant atmospheric heating.
For Earth, atmospheric entry occurs by convention at the Kármán line at an altitude of 100 km (62 miles; 54 nautical miles) above the surface, while at Venus atmospheric entry occurs at 250 km (160 mi; 130 nmi) and at Mars atmospheric entry at about 80 km (50 mi; 43 nmi). Uncontrolled objects reach high velocities while accelerating through space toward the Earth under the influence of Earth's gravity, and are slowed by friction upon encountering Earth's atmosphere. Meteors are also often travelling quite fast relative to the Earth simply because their own orbital path is different from that of the Earth before they encounter Earth's gravity well. Most controlled objects enter at hypersonic speeds due to their sub-orbital (e.g., intercontinental ballistic missile reentry vehicles), orbital (e.g., the Soyuz), or unbounded (e.g., meteors) trajectories. Various advanced technologies have been developed to enable atmospheric reentry and flight at extreme velocities. An alternative low-velocity method of controlled atmospheric entry is buoyancy which is suitable for planetary entry where thick atmospheres, strong gravity, or both factors complicate high-velocity hyperbolic entry, such as the atmospheres of Venus, Titan and the gas giants.


== History ==

The concept of the ablative heat shield was described as early as 1920 by Robert Goddard: ""In the case of meteors, which enter the atmosphere with speeds as high as 30 miles (48 km) per second, the interior of the meteors remains cold, and the erosion is due, to a large extent, to chipping or cracking of the suddenly heated surface. For this reason, if the outer surface of the apparatus were to consist of layers of a very infusible hard substance with layers of a poor heat conductor between, the surface would not be eroded to any considerable extent, especially as the velocity of the apparatus would not be nearly so great as that of the average meteor.""Practical development of reentry systems began as the range, and reentry velocity of ballistic missiles increased. For early short-range missiles, like the V-2, stabilization and aerodynamic stress were important issues (many V-2s broke apart during reentry), but heating was not a serious problem. Medium-range missiles like the Soviet R-5, with a 1,200-kilometer (650-nautical-mile) range, required ceramic composite heat shielding on separable reentry vehicles (it was no longer possible for the entire rocket structure to survive reentry). The first ICBMs, with ranges of 8,000 to 12,000 km (4,300 to 6,500 nmi), were only possible with the development of modern ablative heat shields and blunt-shaped vehicles.
In the United States, this technology was pioneered by H. Julian Allen and A. J. Eggers Jr. of the National Advisory Committee for Aeronautics (NACA) at Ames Research Center. In 1951, they made the counterintuitive discovery that a blunt shape (high drag) made the most effective heat shield. From simple engineering principles, Allen and Eggers showed that the heat load experienced by an entry vehicle was inversely proportional to the drag coefficient; i.e., the greater the drag, the less the heat load. If the reentry vehicle is made blunt, air cannot ""get out of the way"" quickly enough, and acts as an air cushion to push the shock wave and heated shock layer forward (away from the vehicle). Since most of the hot gases are no longer in direct contact with the vehicle, the heat energy would stay in the shocked gas and simply move around the vehicle to later dissipate into the atmosphere.
The Allen and Eggers discovery, though initially treated as a military secret, was eventually published in 1958.


== Terminology, definitions and jargon ==
Over the decades since the 1950s, a rich technical jargon has grown around the engineering of vehicles designed to enter planetary atmospheres. It is recommended that the reader review the jargon glossary before continuing with this article on atmospheric reentry.
When atmospheric entry is part of a spacecraft landing or recovery, particularly on a planetary body other than Earth, entry is part of a phase referred to as entry, descent, and landing, or EDL. When the atmospheric entry returns to the same body that the vehicle had launched from, the event is referred to as reentry (almost always referring to Earth entry).
The fundamental design objective in atmospheric entry of a spacecraft is to dissipate the energy of a spacecraft that is traveling at hypersonic speed as it enters an atmosphere such that equipment, cargo, and any passengers are slowed and land near a specific destination on the surface at zero velocity while keeping stresses on the spacecraft and any passengers within acceptable limits. This may be accomplished by propulsive or aerodynamic (vehicle characteristics or parachute) means, or by some combination.


== Entry vehicle shapes ==

There are several basic shapes used in designing entry vehicles:


=== Sphere or spherical section ===

The simplest axisymmetric shape is the sphere or spherical section. This can either be a complete sphere or a spherical section forebody with a converging conical afterbody. The aerodynamics of a sphere or spherical section are easy to model analytically using Newtonian impact theory. Likewise, the spherical section's heat flux can be accurately modeled with the Fay-Riddell equation. The static stability of a spherical section is assured if the vehicle's center of mass is upstream from the center of curvature (dynamic stability is more problematic). Pure spheres have no lift. However, by flying at an angle of attack, a spherical section has modest aerodynamic lift thus providing some cross-range capability and widening its entry corridor. In the late 1950s and early 1960s, high-speed computers were not yet available and computational fluid dynamics was still embryonic. Because the spherical section was amenable to closed-form analysis, that geometry became the default for conservative design. Consequently, manned capsules of that era were based upon the spherical section.
Pure spherical entry vehicles were used in the early Soviet Vostok and Voskhod capsules and in Soviet Mars and Venera descent vehicles. The Apollo command module used a spherical section forebody heat shield with a converging conical afterbody. It flew a lifting entry with a hypersonic trim angle of attack of −27° (0° is blunt-end first) to yield an average L/D (lift-to-drag ratio) of 0.368. The resultant lift achieved a measure of cross-range control by offsetting the vehicle's center of mass from its axis of symmetry, allowing the lift force to be directed left or right by rolling the capsule on its longitudinal axis. Other examples of the spherical section geometry in manned capsules are Soyuz/Zond, Gemini, and Mercury. Even these small amounts of lift allow trajectories that have very significant effects on peak g-force, reducing it from 8–9 g for a purely ballistic (slowed only by drag) trajectory to 4–5 g, as well as greatly reducing the peak reentry heat.


=== Sphere-cone ===
The sphere-cone is a spherical section with a frustum or blunted cone attached. The sphere-cone's dynamic stability is typically better than that of a spherical section. The vehicle enters sphere-first. With a sufficiently small half-angle and properly placed center of mass, a sphere-cone can provide aerodynamic stability from Keplerian entry to surface impact. (The half-angle is the angle between the cone's axis of rotational symmetry and its outer surface, and thus half the angle made by the cone's surface edges.)

The original American sphere-cone aeroshell was the Mk-2 RV (reentry vehicle), which was developed in 1955 by the General Electric Corp. The Mk-2's design was derived from blunt-body theory and used a radiatively cooled thermal protection system (TPS) based upon a metallic heat shield (the different TPS types are later described in this article). The Mk-2 had significant defects as a weapon delivery system, i.e., it loitered too long in the upper atmosphere due to its lower ballistic coefficient and also trailed a stream of vaporized metal making it very visible to radar. These defects made the Mk-2 overly susceptible to anti-ballistic missile (ABM) systems. Consequently, an alternative sphere-cone RV to the Mk-2 was developed by General Electric.

This new RV was the Mk-6 which used a non-metallic ablative TPS, a nylon phenolic. This new TPS was so effective as a reentry heat shield that significantly reduced bluntness was possible. However, the Mk-6 was a huge RV with an entry mass of 3,360 kg, a length of 3.1 m and a half-angle of 12.5°. Subsequent advances in nuclear weapon and ablative TPS design allowed RVs to become significantly smaller with a further reduced bluntness ratio compared to the Mk-6. Since the 1960s, the sphere-cone has become the preferred geometry for modern ICBM RVs with typical half-angles being between 10° to 11°.

Reconnaissance satellite RVs (recovery vehicles) also used a sphere-cone shape and were the first American example of a non-munition entry vehicle (Discoverer-I, launched on 28 February 1959). The sphere-cone was later used for space exploration missions to other celestial bodies or for return from open space; e.g., Stardust probe. Unlike with military RVs, the advantage of the blunt body's lower TPS mass remained with space exploration entry vehicles like the Galileo Probe with a half-angle of 45° or the Viking aeroshell with a half-angle of 70°. Space exploration sphere-cone entry vehicles have landed on the surface or entered the atmospheres of Mars, Venus, Jupiter, and Titan.


=== Biconic ===

The biconic is a sphere-cone with an additional frustum attached. The biconic offers a significantly improved L/D ratio. A biconic designed for Mars aerocapture typically has an L/D of approximately 1.0 compared to an L/D of 0.368 for the Apollo-CM. The higher L/D makes a biconic shape better suited for transporting people to Mars due to the lower peak deceleration. Arguably, the most significant biconic ever flown was the Advanced Maneuverable Reentry Vehicle (AMaRV). Four AMaRVs were made by the McDonnell Douglas Corp. and represented a significant leap in RV sophistication. Three AMaRVs were launched by Minuteman-1 ICBMs on 20 December 1979, 8 October 1980 and 4 October 1981. AMaRV had an entry mass of approximately 470 kg, a nose radius of 2.34 cm, a forward-frustum half-angle of 10.4°, an inter-frustum radius of 14.6 cm, aft-frustum half-angle of 6°, and an axial length of 2.079 meters. No accurate diagram or picture of AMaRV has ever appeared in the open literature. However, a schematic sketch of an AMaRV-like vehicle along with trajectory plots showing hairpin turns has been published.AMaRV's attitude was controlled through a split body flap (also called a split-windward flap) along with two yaw flaps mounted on the vehicle's sides. Hydraulic actuation was used for controlling the flaps. AMaRV was guided by a fully autonomous navigation system designed for evading anti-ballistic missile (ABM) interception. The McDonnell Douglas DC-X (also a biconic) was essentially a scaled-up version of AMaRV. AMaRV and the DC-X also served as the basis for an unsuccessful proposal for what eventually became the Lockheed Martin X-33.


=== Non-axisymmetric shapes ===
Non-axisymmetric shapes have been used for manned entry vehicles. One example is the winged orbit vehicle that uses a delta wing for maneuvering during descent much like a conventional glider. This approach has been used by the American Space Shuttle and the Soviet Buran. The lifting body is another entry vehicle geometry and was used with the X-23 PRIME (Precision Recovery Including Maneuvering Entry) vehicle.


== Reentry heating ==

Objects entering an atmosphere from space at high velocities relative to the atmosphere will cause very high levels of heating. Reentry heating comes principally from two sources:
convective heating, of two types:
hot gas flow past the surface of the body and
catalytic chemical recombination reactions between the object surface and the atmospheric gases
radiative heating, from the energetic shock layer that forms in front and to the sides of the objectAs velocity increases, both convective and radiative heating increase. At very high speeds, radiative heating will come to quickly dominate the convective heat fluxes, as convective heating is proportional to the velocity cubed, while radiative heating is proportional to the eighth power of velocity. Radiative heating—which is highly wavelength dependent—thus predominates very early in atmospheric entry while convection predominates in the later phases.


=== Shock layer gas physics ===
At typical reentry temperatures, the air in the shock layer is both ionized and dissociated. This chemical dissociation necessitates various physical models to describe the shock layer's thermal and chemical properties. There are four basic physical models of a gas that are important to aeronautical engineers who design heat shields:


==== Perfect gas model ====
Almost all aeronautical engineers are taught the perfect (ideal) gas model during their undergraduate education. Most of the important perfect gas equations along with their corresponding tables and graphs are shown in NACA Report 1135. Excerpts from NACA Report 1135 often appear in the appendices of thermodynamics textbooks and are familiar to most aeronautical engineers who design supersonic aircraft.
The perfect gas theory is elegant and extremely useful for designing aircraft but assumes that the gas is chemically inert. From the standpoint of aircraft design, air can be assumed to be inert for temperatures less than 550 K at one atmosphere pressure. The perfect gas theory begins to break down at 550 K and is not usable at temperatures greater than 2,000 K. For temperatures greater than 2,000 K, a heat shield designer must use a real gas model.


==== Real (equilibrium) gas model ====
An entry vehicle's pitching moment can be significantly influenced by real-gas effects. Both the Apollo command module and the Space Shuttle were designed using incorrect pitching moments determined through inaccurate real-gas modelling. The Apollo-CM's trim-angle angle of attack was higher than originally estimated, resulting in a narrower lunar return entry corridor. The actual aerodynamic center of the Columbia was upstream from the calculated value due to real-gas effects. On Columbia's maiden flight (STS-1), astronauts John W. Young and Robert Crippen had some anxious moments during reentry when there was concern about losing control of the vehicle.An equilibrium real-gas model assumes that a gas is chemically reactive, but also assumes all chemical reactions have had time to complete and all components of the gas have the same temperature (this is called thermodynamic equilibrium). When air is processed by a shock wave, it is superheated by compression and chemically dissociates through many different reactions. Direct friction upon the reentry object is not the main cause of shock-layer heating. It is caused mainly from isentropic heating of the air molecules within the compression wave. Friction based entropy increases of the molecules within the wave also account for some heating. The distance from the shock wave to the stagnation point on the entry vehicle's leading edge is called shock wave stand off. An approximate rule of thumb for shock wave standoff distance is 0.14 times the nose radius. One can estimate the time of travel for a gas molecule from the shock wave to the stagnation point by assuming a free stream velocity of 7.8 km/s and a nose radius of 1 meter, i.e., time of travel is about 18 microseconds. This is roughly the time required for shock-wave-initiated chemical dissociation to approach chemical equilibrium in a shock layer for a 7.8 km/s entry into air during peak heat flux. Consequently, as air approaches the entry vehicle's stagnation point, the air effectively reaches chemical equilibrium thus enabling an equilibrium model to be usable. For this case, most of the shock layer between the shock wave and leading edge of an entry vehicle is chemically reacting and not in a state of equilibrium. The Fay-Riddell equation, which is of extreme importance towards modeling heat flux, owes its validity to the stagnation point being in chemical equilibrium. The time required for the shock layer gas to reach equilibrium is strongly dependent upon the shock layer's pressure. For example, in the case of the Galileo probe's entry into Jupiter's atmosphere, the shock layer was mostly in equilibrium during peak heat flux due to the very high pressures experienced (this is counterintuitive given the free stream velocity was 39 km/s during peak heat flux).
Determining the thermodynamic state of the stagnation point is more difficult under an equilibrium gas model than a perfect gas model. Under a perfect gas model, the ratio of specific heats (also called isentropic exponent, adiabatic index, gamma, or kappa) is assumed to be constant along with the gas constant. For a real gas, the ratio of specific heats can wildly oscillate as a function of temperature. Under a perfect gas model there is an elegant set of equations for determining thermodynamic state along a constant entropy stream line called the isentropic chain. For a real gas, the isentropic chain is unusable and a Mollier diagram would be used instead for manual calculation. However, graphical solution with a Mollier diagram is now considered obsolete with modern heat shield designers using computer programs based upon a digital lookup table (another form of Mollier diagram) or a chemistry based thermodynamics program. The chemical composition of a gas in equilibrium with fixed pressure and temperature can be determined through the Gibbs free energy method. Gibbs free energy is simply the total enthalpy of the gas minus its total entropy times temperature. A chemical equilibrium program normally does not require chemical formulas or reaction-rate equations. The program works by preserving the original elemental abundances specified for the gas and varying the different molecular combinations of the elements through numerical iteration until the lowest possible Gibbs free energy is calculated (a Newton-Raphson method is the usual numerical scheme). The data base for a Gibbs free energy program comes from spectroscopic data used in defining partition functions. Among the best equilibrium codes in existence is the program Chemical Equilibrium with Applications (CEA) which was written by Bonnie J. McBride and Sanford Gordon at NASA Lewis (now renamed ""NASA Glenn Research Center""). Other names for CEA are the ""Gordon and McBride Code"" and the ""Lewis Code"". CEA is quite accurate up to 10,000 K for planetary atmospheric gases, but unusable beyond 20,000 K (double ionization is not modelled). CEA can be downloaded from the Internet along with full documentation and will compile on Linux under the G77 Fortran compiler.


==== Real (non-equilibrium) gas model ====
A non-equilibrium real gas model is the most accurate model of a shock layer's gas physics, but is more difficult to solve than an equilibrium model. As of 1958, the simplest non-equilibrium model was the Lighthill-Freeman model. The Lighthill-Freeman model initially assumes a gas made up of a single diatomic species susceptible to only one chemical formula and its reverse; e.g., N2 ? N + N and N + N ? N2 (dissociation and recombination). Because of its simplicity, the Lighthill-Freeman model is a useful pedagogical tool, but is unfortunately too simple for modelling non-equilibrium air. Air is typically assumed to have a mole fraction composition of 0.7812 molecular nitrogen, 0.2095 molecular oxygen and 0.0093 argon. The simplest real gas model for air is the five species model, which is based upon N2, O2, NO, N, and O. The five species model assumes no ionization and ignores trace species like carbon dioxide.
When running a Gibbs free energy equilibrium program, the iterative process from the originally specified molecular composition to the final calculated equilibrium composition is essentially random and not time accurate. With a non-equilibrium program, the computation process is time accurate and follows a solution path dictated by chemical and reaction rate formulas. The five species model has 17 chemical formulas (34 when counting reverse formulas). The Lighthill-Freeman model is based upon a single ordinary differential equation and one algebraic equation. The five species model is based upon 5 ordinary differential equations and 17 algebraic equations. Because the 5 ordinary differential equations are tightly coupled, the system is numerically ""stiff"" and difficult to solve. The five species model is only usable for entry from low Earth orbit where entry velocity is approximately 7.8 km/s (28,000 km/h; 17,000 mph). For lunar return entry of 11 km/s, the shock layer contains a significant amount of ionized nitrogen and oxygen. The five-species model is no longer accurate and a twelve-species model must be used instead.
Atmospheric entry interface velocities on a Mars–Earth trajectory are on the order of 12 km/s (43,000 km/h; 27,000 mph).
Modeling high-speed Mars atmospheric entry—which involves a carbon dioxide, nitrogen and argon atmosphere—is even more complex requiring a 19-species model.An important aspect of modelling non-equilibrium real gas effects is radiative heat flux. If a vehicle is entering an atmosphere at very high speed (hyperbolic trajectory, lunar return) and has a large nose radius then radiative heat flux can dominate TPS heating. Radiative heat flux during entry into an air or carbon dioxide atmosphere typically comes from asymmetric diatomic molecules; e.g., cyanogen (CN), carbon monoxide, nitric oxide (NO), single ionized molecular nitrogen etc. These molecules are formed by the shock wave dissociating ambient atmospheric gas followed by recombination within the shock layer into new molecular species. The newly formed diatomic molecules initially have a very high vibrational temperature that efficiently transforms the vibrational energy into radiant energy; i.e., radiative heat flux. The whole process takes place in less than a millisecond which makes modelling a challenge. The experimental measurement of radiative heat flux (typically done with shock tubes) along with theoretical calculation through the unsteady Schrödinger equation are among the more esoteric aspects of aerospace engineering. Most of the aerospace research work related to understanding radiative heat flux was done in the 1960s, but largely discontinued after conclusion of the Apollo Program. Radiative heat flux in air was just sufficiently understood to ensure Apollo's success. However, radiative heat flux in carbon dioxide (Mars entry) is still barely understood and will require major research.


==== Frozen gas model ====
The frozen gas model describes a special case of a gas that is not in equilibrium. The name ""frozen gas"" can be misleading. A frozen gas is not ""frozen"" like ice is frozen water. Rather a frozen gas is ""frozen"" in time (all chemical reactions are assumed to have stopped). Chemical reactions are normally driven by collisions between molecules. If gas pressure is slowly reduced such that chemical reactions can continue then the gas can remain in equilibrium. However, it is possible for gas pressure to be so suddenly reduced that almost all chemical reactions stop. For that situation the gas is considered frozen.The distinction between equilibrium and frozen is important because it is possible for a gas such as air to have significantly different properties (speed-of-sound, viscosity etc.) for the same thermodynamic state; e.g., pressure and temperature. Frozen gas can be a significant issue in the wake behind an entry vehicle. During reentry, free stream air is compressed to high temperature and pressure by the entry vehicle's shock wave. Non-equilibrium air in the shock layer is then transported past the entry vehicle's leading side into a region of rapidly expanding flow that causes freezing. The frozen air can then be entrained into a trailing vortex behind the entry vehicle. Correctly modelling the flow in the wake of an entry vehicle is very difficult. Thermal protection shield (TPS) heating in the vehicle's afterbody is usually not very high, but the geometry and unsteadiness of the vehicle's wake can significantly influence aerodynamics (pitching moment) and particularly dynamic stability.


== Thermal protection systems ==

A thermal protection system, or TPS, is the barrier that protects a spacecraft during the searing heat of atmospheric reentry. A secondary goal may be to protect the spacecraft from the heat and cold of space while in orbit. Multiple approaches for the thermal protection of spacecraft are in use, among them ablative heat shields, passive cooling, and active cooling of spacecraft surfaces.


=== Ablative ===

The ablative heat shield functions by lifting the hot shock layer gas away from the heat shield's outer wall (creating a cooler boundary layer). The boundary layer comes from blowing of gaseous reaction products from the heat shield material and provides protection against all forms of heat flux. The overall process of reducing the heat flux experienced by the heat shield's outer wall by way of a boundary layer is called blockage. Ablation occurs at two levels in an ablative TPS: the outer surface of the TPS material chars, melts, and sublimes, while the bulk of the TPS material undergoes pyrolysis and expels product gases. The gas produced by pyrolysis is what drives blowing and causes blockage of convective and catalytic heat flux. Pyrolysis can be measured in real time using thermogravimetric analysis, so that the ablative performance can be evaluated. Ablation can also provide blockage against radiative heat flux by introducing carbon into the shock layer thus making it optically opaque. Radiative heat flux blockage was the primary thermal protection mechanism of the Galileo Probe TPS material (carbon phenolic). Carbon phenolic was originally developed as a rocket nozzle throat material (used in the Space Shuttle Solid Rocket Booster) and for reentry-vehicle nose tips.
Early research on ablation technology in the USA was centered at NASA's Ames Research Center located at Moffett Field, California. Ames Research Center was ideal, since it had numerous wind tunnels capable of generating varying wind velocities. Initial experiments typically mounted a mock-up of the ablative material to be analyzed within a hypersonic wind tunnel. Testing of ablative materials occurs at the Ames Arc Jet Complex. Many spacecraft thermal protection systems have been tested in this facility, including the Apollo, space shuttle, and Orion heat shield materials.

The thermal conductivity of a particular TPS material is usually proportional to the material's density. Carbon phenolic is a very effective ablative material, but also has high density which is undesirable. If the heat flux experienced by an entry vehicle is insufficient to cause pyrolysis then the TPS material's conductivity could allow heat flux conduction into the TPS bondline material thus leading to TPS failure. Consequently, for entry trajectories causing lower heat flux, carbon phenolic is sometimes inappropriate and lower-density TPS materials such as the following examples can be better design choices:


==== Super light-weight ablator ====
SLA in SLA-561V stands for super light-weight ablator. SLA-561V is a proprietary ablative made by Lockheed Martin that has been used as the primary TPS material on all of the 70° sphere-cone entry vehicles sent by NASA to Mars other than the Mars Science Laboratory (MSL). SLA-561V begins significant ablation at a heat flux of approximately 110 W/cm2, but will fail for heat fluxes greater than 300 W/cm2. The MSL aeroshell TPS is currently designed to withstand a peak heat flux of 234 W/cm2. The peak heat flux experienced by the Viking 1 aeroshell which landed on Mars was 21 W/cm2. For Viking 1, the TPS acted as a charred thermal insulator and never experienced significant ablation. Viking 1 was the first Mars lander and based upon a very conservative design. The Viking aeroshell had a base diameter of 3.54 meters (the largest used on Mars until Mars Science Laboratory). SLA-561V is applied by packing the ablative material into a honeycomb core that is pre-bonded to the aeroshell's structure thus enabling construction of a large heat shield.


==== Phenolic-impregnated carbon ablator ====

Phenolic-impregnated carbon ablator (PICA), a carbon fiber preform impregnated in phenolic resin, is a modern TPS material and has the advantages of low density (much lighter than carbon phenolic) coupled with efficient ablative ability at high heat flux. It is a good choice for ablative applications such as high-peak-heating conditions found on sample-return missions or lunar-return missions. PICA's thermal conductivity is lower than other high-heat-flux-ablative materials, such as conventional carbon phenolics.PICA was patented by NASA Ames Research Center in the 1990s and was the primary TPS material for the Stardust aeroshell. The Stardust sample-return capsule was the fastest man-made object ever to reenter Earth's atmosphere (12.4 km/s (28,000 mph) at 135 km altitude). This was faster than the Apollo mission capsules and 70% faster than the Shuttle. PICA was critical for the viability of the Stardust mission, which returned to Earth in 2006. Stardust's heat shield (0.81 m base diameter) was made of one monolithic piece sized to withstand a nominal peak heating rate of 1.2 kW/cm2. A PICA heat shield was also used for the Mars Science Laboratory entry into the Martian atmosphere.


===== PICA-X =====
An improved and easier to produce version called PICA-X was developed by SpaceX in 2006–2010 for the Dragon space capsule. The first reentry test of a PICA-X heat shield was on the Dragon C1 mission on 8 December 2010. The PICA-X heat shield was designed, developed and fully qualified by a small team of a dozen engineers and technicians in less than four years.
PICA-X is ten times less expensive to manufacture than the NASA PICA heat shield material.


===== PICA-3 =====
A second enhanced version of PICA—called PICA-3—was developed by SpaceX during the mid-2010s.  It was first flight tested on the Crew Dragon spacecraft in 2019 during the flight demonstration mission, in April 2019, and put into regular service on that spacecraft in 2020.


==== SIRCA ====

Silicone-impregnated reusable ceramic ablator (SIRCA) was also developed at NASA Ames Research Center and was used on the Backshell Interface Plate (BIP) of the Mars Pathfinder and Mars Exploration Rover (MER) aeroshells. The BIP was at the attachment points between the aeroshell's backshell (also called the afterbody or aft cover) and the cruise ring (also called the cruise stage). SIRCA was also the primary TPS material for the unsuccessful Deep Space 2 (DS/2) Mars impactor probes with their 0.35-meter-base-diameter (1.1 ft) aeroshells. SIRCA is a monolithic, insulating material that can provide thermal protection through ablation. It is the only TPS material that can be machined to custom shapes and then applied directly to the spacecraft. There is no post-processing, heat treating, or additional coatings required (unlike Space Shuttle tiles). Since SIRCA can be machined to precise shapes, it can be applied as tiles, leading edge sections, full nose caps, or in any number of custom shapes or sizes. As of 1996, SIRCA had been demonstrated in backshell interface applications, but not yet as a forebody TPS material.


==== AVCOAT ====
AVCOAT is a NASA-specified ablative heat shield, a glass-filled epoxy–novolac system.NASA originally used it for the Apollo capsule in the 1960s, and then utilized the material for its next-generation beyond low-Earth-orbit Orion spacecraft, slated to fly in the late 2010s. The Avcoat to be used on Orion has been reformulated to meet environmental legislation that has been passed since the end of Apollo.


=== Thermal soak ===

Thermal soak is a part of almost all TPS schemes. For example, an ablative heat shield loses most of its thermal protection effectiveness when the outer wall temperature drops below the minimum necessary for pyrolysis. From that time to the end of the heat pulse, heat from the shock layer convects into the heat shield's outer wall and would eventually conduct to the payload. This outcome is prevented by ejecting the heat shield (with its heat soak) prior to the heat conducting to the inner wall.
Typical Space Shuttle TPS tiles (LI-900) have remarkable thermal protection properties. An LI-900 tile exposed to a temperature of 1,000 K on one side will remain merely warm to the touch on the other side. However, they are relatively brittle and break easily, and cannot survive in-flight rain.


=== Passively cooled ===
In some early ballistic missile RVs (e.g., the Mk-2 and the sub-orbital Mercury spacecraft), radiatively cooled TPS were used to initially absorb heat flux during the heat pulse, and, then, after the heat pulse, radiate and convect the stored heat back into the atmosphere. However, the earlier version of this technique required a considerable quantity of metal TPS (e.g., titanium, beryllium, copper, etc.). Modern designers prefer to avoid this added mass by using ablative and thermal-soak TPS instead.
Thermal protection systems relying on emissivity use high emissivity coatings (HECs) to facilitate radiative cooling, while an underlying porous ceramic layer serves to protect the structure from high surface temperatures. High thermally stable emissivity values coupled with low thermal conductivity are key to the functionality of such systems.Radiatively cooled TPS can be found on modern entry vehicles, but reinforced carbon–carbon (RCC) (also called carbon–carbon) is normally used instead of metal. RCC was the TPS material on the Space Shuttle's nose cone and wing leading edges, and was also proposed as the leading-edge material for the X-33. Carbon is the most refractory material known, with a one-atmosphere sublimation temperature of 3,825 °C (6,917 °F) for graphite. This high temperature made carbon an obvious choice as a radiatively cooled TPS material. Disadvantages of RCC are that it is currently expensive to manufacture, is heavy, and lacks robust impact resistance.Some high-velocity aircraft, such as the SR-71 Blackbird and Concorde, deal with heating similar to that experienced by spacecraft, but at much lower intensity, and for hours at a time. Studies of the SR-71's titanium skin revealed that the metal structure was restored to its original strength through annealing due to aerodynamic heating. In the case of the Concorde, the aluminium nose was permitted to reach a maximum operating temperature of 127 °C (261 °F) (approximately 180 °C (324 °F) warmer than the normally sub-zero, ambient air); the metallurgical implications (loss of temper) that would be associated with a higher peak temperature were the most significant factors determining the top speed of the aircraft.
A radiatively cooled TPS for an entry vehicle is often called a hot-metal TPS. Early TPS designs for the Space Shuttle called for a hot-metal TPS based upon a nickel superalloy (dubbed René 41) and titanium shingles. This Shuttle TPS concept was rejected, because it was believed a silica tile-based TPS would involve lower development and manufacturing costs. A nickel superalloy-shingle TPS was again proposed for the unsuccessful X-33 single-stage-to-orbit (SSTO) prototype.Recently, newer radiatively cooled TPS materials have been developed that could be superior to RCC. Known as Ultra-High Temperature Ceramics, they were developed for the prototype vehicle Slender Hypervelocity Aerothermodynamic Research Probe (SHARP). These TPS materials are based on zirconium diboride and hafnium diboride. SHARP TPS have suggested performance improvements allowing for sustained Mach 7 flight at sea level, Mach 11 flight at 100,000-foot (30,000 m) altitudes, and significant improvements for vehicles designed for continuous hypersonic flight. SHARP TPS materials enable sharp leading edges and nose cones to greatly reduce drag for airbreathing combined-cycle-propelled spaceplanes and lifting bodies. SHARP materials have exhibited effective TPS characteristics from zero to more than 2,000 °C (3,630 °F), with melting points over 3,500 °C (6,330 °F). They are structurally stronger than RCC, and, thus, do not require structural reinforcement with materials such as Inconel. SHARP materials are extremely efficient at reradiating absorbed heat, thus eliminating the need for additional TPS behind and between the SHARP materials and conventional vehicle structure. NASA initially funded (and discontinued) a multi-phase R&D program through the University of Montana in 2001 to test SHARP materials on test vehicles.


=== Actively cooled ===
Various advanced reusable spacecraft and hypersonic aircraft designs have been proposed to employ heat shields made from temperature-resistant metal alloys that incorporate a refrigerant or cryogenic fuel circulating through them, and one such spacecraft design is currently under development.
Such a TPS concept was proposed for the X-30 National Aerospace Plane (NASP). The NASP was supposed to have been a scramjet powered hypersonic aircraft, but failed in development.
SpaceX is currently developing an actively cooled heat shield for its Starship spacecraft where a part of the thermal protection system will be a transpirationally cooled outer-skin design for the reentering spaceship.In the early 1960s various TPS systems were proposed to use water or other cooling liquid sprayed into the shock layer, or passed through channels in the heat shield. Advantages included the possibility of more all-metal designs which would be cheaper to develop, be more rugged, and eliminate the need for classified technology. The disadvantages are increased weight and complexity, and lower reliability. The concept has never been flown, but a similar technology (the plug nozzle) did undergo extensive ground testing.


== Feathered reentry ==
In 2004, aircraft designer Burt Rutan demonstrated the feasibility of a shape-changing airfoil for reentry with the sub-orbital SpaceShipOne. The wings on this craft rotate upward into the feathered configuration that provides a shuttlecock effect. Thus SpaceShipOne achieves much more aerodynamic drag on reentry while not experiencing significant thermal loads.
The configuration increases drag, as the craft is now less streamlined and results in more atmospheric gas particles hitting the spacecraft at higher altitudes than otherwise. The aircraft thus slows down more in higher atmospheric layers which is the key to efficient reentry. Secondly, the aircraft will automatically orient itself in this state to a high drag attitude.However, the velocity attained by SpaceShipOne prior to reentry is much lower than that of an orbital spacecraft, and engineers, including Rutan, recognize that a feathered reentry technique is not suitable for return from orbit.
On 4 May 2011, the first test on the SpaceShipTwo of the feathering mechanism was made during a glideflight after release
from the White Knight Two. Premature deployment of the feathering system was responsible for the 2014 VSS Enterprise crash, in which the aircraft disintegrated, killing the co-pilot.
The feathered reentry was first described by Dean Chapman of NACA in 1958. In the section of his report on Composite Entry, Chapman described a solution to the problem using a high-drag device:

It may be desirable to combine lifting and nonlifting entry in order to achieve some advantages... For landing maneuverability it obviously is advantageous to employ a lifting vehicle. The total heat absorbed by a lifting vehicle, however, is much higher than for a nonlifting vehicle... Nonlifting vehicles can more easily be constructed... by employing, for example, a large, light drag device... The larger the device, the smaller is the heating rate.
Nonlifting vehicles with shuttlecock stability are advantageous also from the viewpoint of minimum control requirements during entry.
... an evident composite type of entry, which combines some of the desirable features of lifting and nonlifting trajectories, would be to enter first without lift but with a... drag device; then, when the velocity is reduced to a certain value... the device is jettisoned or retracted, leaving a lifting vehicle... for the remainder of the descent.

The North American X-15 used a similar mechanism.


== Inflatable heat shield reentry ==
Deceleration for atmospheric reentry, especially for higher-speed Mars-return missions, benefits from maximizing ""the drag area of the entry system. The larger the diameter of the aeroshell, the bigger the payload can be."" An inflatable aeroshell provides one alternative for enlarging the drag area with a low-mass design.


=== Non-US ===
Such an inflatable shield/aerobrake was designed for the penetrators of Mars 96 mission. Since the mission failed due to the launcher malfunction, the NPO Lavochkin and DASA/ESA have designed a mission for Earth orbit. The Inflatable Reentry and Descent Technology (IRDT) demonstrator was launched on Soyuz-Fregat on 8 February 2000. The inflatable shield was designed as a cone with two stages of inflation. Although the second stage of the shield failed to inflate, the demonstrator survived the orbital reentry and was recovered. The subsequent missions flown on the Volna rocket failed due to launcher failure.


=== NASA IRVE ===
NASA launched an inflatable heat shield experimental spacecraft on 17 August 2009 with the successful first test flight of the Inflatable Re-entry Vehicle Experiment (IRVE). The heat shield had been vacuum-packed into a 15-inch-diameter (38 cm) payload shroud and launched on a Black Brant 9 sounding rocket from NASA's Wallops Flight Facility on Wallops Island, Virginia. ""Nitrogen inflated the 10-foot-diameter (3.0 m) heat shield, made of several layers of silicone-coated [Kevlar] fabric, to a mushroom shape in space several minutes after liftoff."" The rocket apogee was at an altitude of 131 miles (211 km) where it began its descent to supersonic speed. Less than a minute later the shield was released from its cover to inflate at an altitude of 124 miles (200 km). The inflation of the shield took less than 90 seconds.


=== NASA HIAD ===
Following the success of the initial IRVE experiments, NASA developed the concept into the more ambitious Hypersonic Inflatable Aerodynamic Decelerator (HIAD). The current design is shaped like a shallow cone, with the structure built up as a stack of circular inflated tubes of gradually increasing major diameter. The forward (convex) face of the cone is covered with a flexible thermal protection system robust enough to withstand the stresses of atmospheric entry (or reentry).In 2012, a HIAD was tested as Inflatable Reentry Vehicle Experiment 3 (IRVE-3) using a sub-orbital sounding rocket, and worked.In 2020 there were plans to launch in 2022 a 6 m inflatable as Low-Earth Orbit Flight Test of an Inflatable Decelerator (LOFTID).See also Low-Density Supersonic Decelerator, a NASA project with tests in 2014 & 2015.


== Entry vehicle design considerations ==
There are four critical parameters considered when designing a vehicle for atmospheric entry:
Peak heat flux
Heat load
Peak deceleration
Peak dynamic pressurePeak heat flux and dynamic pressure selects the TPS material. Heat load selects the thickness of the TPS material stack. Peak deceleration is of major importance for manned missions. The upper limit for manned return to Earth from low Earth orbit (LEO) or lunar return is 10g. For Martian atmospheric entry after long exposure to zero gravity, the upper limit is 4g. Peak dynamic pressure can also influence the selection of the outermost TPS material if spallation is an issue.
Starting from the principle of conservative design, the engineer typically considers two worst-case trajectories, the undershoot and overshoot trajectories. The overshoot trajectory is typically defined as the shallowest-allowable entry velocity angle prior to atmospheric skip-off. The overshoot trajectory has the highest heat load and sets the TPS thickness. The undershoot trajectory is defined by the steepest allowable trajectory. For manned missions the steepest entry angle is limited by the peak deceleration. The undershoot trajectory also has the highest peak heat flux and dynamic pressure. Consequently, the undershoot trajectory is the basis for selecting the TPS material. There is no ""one size fits all"" TPS material. A TPS material that is ideal for high heat flux may be too conductive (too dense) for a long duration heat load. A low-density TPS material might lack the tensile strength to resist spallation if the dynamic pressure is too high. A TPS material can perform well for a specific peak heat flux, but fail catastrophically for the same peak heat flux if the wall pressure is significantly increased (this happened with NASA's R-4 test spacecraft). Older TPS materials tend to be more labor-intensive and expensive to manufacture compared to modern materials. However, modern TPS materials often lack the flight history of the older materials (an important consideration for a risk-averse designer).
Based upon Allen and Eggers discovery, maximum aeroshell bluntness (maximum drag) yields minimum TPS mass. Maximum bluntness (minimum ballistic coefficient) also yields a minimal terminal velocity at maximum altitude (very important for Mars EDL, but detrimental for military RVs). However, there is an upper limit to bluntness imposed by aerodynamic stability considerations based upon shock wave detachment. A shock wave will remain attached to the tip of a sharp cone if the cone's half-angle is below a critical value. This critical half-angle can be estimated using perfect gas theory (this specific aerodynamic instability occurs below hypersonic speeds). For a nitrogen atmosphere (Earth or Titan), the maximum allowed half-angle is approximately 60°. For a carbon dioxide atmosphere (Mars or Venus), the maximum-allowed half-angle is approximately 70°. After shock wave detachment, an entry vehicle must carry significantly more shocklayer gas around the leading edge stagnation point (the subsonic cap). Consequently, the aerodynamic center moves upstream thus causing aerodynamic instability. It is incorrect to reapply an aeroshell design intended for Titan entry (Huygens probe in a nitrogen atmosphere) for Mars entry (Beagle 2 in a carbon dioxide atmosphere). Prior to being abandoned, the Soviet Mars lander program achieved one successful landing (Mars 3), on the second of three entry attempts (the others were Mars 2 and Mars 6). The Soviet Mars landers were based upon a 60° half-angle aeroshell design.
A 45° half-angle sphere-cone is typically used for atmospheric probes (surface landing not intended) even though TPS mass is not minimized. The rationale for a 45° half-angle is to have either aerodynamic stability from entry-to-impact (the heat shield is not jettisoned) or a short-and-sharp heat pulse followed by prompt heat shield jettison. A 45° sphere-cone design was used with the DS/2 Mars impactor and Pioneer Venus probes.


== Notable atmospheric entry accidents ==

Not all atmospheric reentries have been successful and some have resulted in significant disasters.

Voskhod 2 – The service module failed to detach for some time, but the crew survived.
Soyuz 1 – The attitude control system failed while still in orbit and later parachutes got entangled during the emergency landing sequence (entry, descent, and landing (EDL) failure). Lone cosmonaut Vladimir Mikhailovich Komarov died.
Soyuz 5 – The service module failed to detach, but the crew survived.
Soyuz 11 – After tri-module separation, a valve was weakened by the blast and failed on reentry. The cabin depressurized killing all three crew members.
Mars Polar Lander – Failed during EDL. The failure was believed to be the consequence of a software error. The precise cause is unknown for lack of real-time telemetry.
Space Shuttle Columbia
STS-1 – a combination of launch damage, protruding gap filler, and tile installation error resulted in serious damage to the orbiter, only some of which the crew was privy to. Had the crew known the true extent of the damage before attempting reentry, they would have flown the shuttle to a safe altitude and then bailed out. Nevertheless, reentry was successful, and the orbiter proceeded to a normal landing.
STS-107 – The failure of an RCC panel on a wing leading edge caused by debris impact at launch led to breakup of the orbiter on reentry resulting in the deaths of all seven crew members.
Genesis – The parachute failed to deploy due to a G-switch having been installed backwards (a similar error delayed parachute deployment for the Galileo Probe). Consequently, the Genesis entry vehicle crashed into the desert floor. The payload was damaged, but most scientific data were recoverable.
Soyuz TMA-11 – The Soyuz propulsion module failed to separate properly; fallback ballistic reentry was executed that subjected the crew to accelerations of about 8 standard gravities (78 m/s2). The crew survived.


== Uncontrolled and unprotected reentries ==
Of satellites that reenter, approximately 10–40% of the mass of the object is likely to reach the surface of the Earth. On average, about one catalogued object reenters per day.Due to the Earth's surface being primarily water, most objects that survive reentry land in one of the world's oceans. The estimated chances that a given person will get hit and injured during his/her lifetime is around 1 in a trillion.On January 24, 1978, the Soviet Kosmos 954 (3,800 kilograms [8,400 lb]) reentered and crashed near Great Slave Lake in the Northwest Territories of Canada. The satellite was nuclear-powered and left radioactive debris near its impact site.On July 11, 1979, the US Skylab space station (77,100 kilograms [170,000 lb]) reentered and spread debris across the Australian Outback. The reentry was a major media event largely due to the Cosmos 954 incident, but not viewed as much as a potential disaster since it did not carry toxic nuclear or hydrazine fuel. NASA had originally hoped to use a Space Shuttle mission to either extend its life or enable a controlled reentry, but delays in the Shuttle program, plus unexpectedly high solar activity, made this impossible.On February 7, 1991, the Soviet Salyut 7 space station (19,820 kilograms [43,700 lb]), with the Kosmos 1686 module (20,000 kilograms [44,000 lb]) attached, reentered and scattered debris over the town of Capitán Bermúdez, Argentina. The station had been boosted to a higher orbit in August 1986 in an attempt to keep it up until 1994, but in a scenario similar to Skylab, the planned Buran shuttle was cancelled and high solar activity caused it to come down sooner than expected.
On September 7, 2011, NASA announced the impending uncontrolled reentry of the Upper Atmosphere Research Satellite (6,540 kilograms [14,420 lb]) and noted that there was a small risk to the public. The decommissioned satellite reentered the atmosphere on September 24, 2011, and some pieces are presumed to have crashed into the South Pacific Ocean over a debris field 500 miles (800 km) long.On April 1, 2018, the Chinese Tiangong-1 space station (8,510 kilograms [18,760 lb]) reentered over the Pacific Ocean, halfway between Australia and South America. The China Manned Space Engineering Office had intended to control the reentry, but lost telemetry and control in March 2017.On May 11, 2020, the core stage of Chinese Long March 5B (COSPAR ID 2020-027C) weighing roughly 20,000 kilograms [44,000 lb]) made an uncontrolled reentry over the Atlantic Ocean, near West African coast. Few pieces of rocket debris reportedly survived reentry and fell over at least two villages in Ivory Coast.It is expected that the Cruise Mass Balance Devices (CMBDs) from the Mars 2020 mission, which are ejected prior to the spacecraft entering the atmosphere, will survive re-entry and impact the surface on Thursday 18 February, 2021. The CMBDs are 77 kg tungsten blocks used to adjust the spacecraft's trajectory prior to entry. The Science Team of another NASA mission, InSight, announced in early 2021 that they would attempt to detect the seismic waves from this impact event.  


=== Deorbit disposal ===
Salyut 1, the world's first space station, was deliberately de-orbited into the Pacific Ocean in 1971 following the Soyuz 11 accident. Its successor, Salyut 6, was de-orbited in a controlled manner as well.
On June 4, 2000 the Compton Gamma Ray Observatory was deliberately de-orbited after one of its gyroscopes failed. The debris that did not burn up fell harmlessly into the Pacific Ocean. The observatory was still operational, but the failure of another gyroscope would have made de-orbiting much more difficult and dangerous. With some controversy, NASA decided in the interest of public safety that a controlled crash was preferable to letting the craft come down at random.
In 2001, the Russian Mir space station was deliberately de-orbited, and broke apart in the fashion expected by the command center during atmospheric reentry. Mir entered the Earth's atmosphere on March 23, 2001, near Nadi, Fiji, and fell into the South Pacific Ocean.
On February 21, 2008, a disabled U.S. spy satellite, USA-193, was hit at an altitude of approximately 246 kilometers (153 mi) with an SM-3 missile fired from the U.S. Navy cruiser Lake Erie off the coast of Hawaii. The satellite was inoperative, having failed to reach its intended orbit when it was launched in 2006. Due to its rapidly deteriorating orbit it was destined for uncontrolled reentry within a month. U.S. Department of Defense expressed concern that the 1,000-pound (450 kg) fuel tank containing highly toxic hydrazine might survive reentry to reach the Earth's surface intact. Several governments including those of Russia, China, and Belarus protested the action as a thinly-veiled demonstration of US anti-satellite capabilities. China had previously caused an international incident when it tested an anti-satellite missile in 2007.

		


== Successful atmospheric reentries from orbital velocities ==
Manned orbital reentry, by country/governmental entity

 China – Shenzhou
 Soviet Union/ Russia – Vostok, Voskhod, Soyuz
 United States – Mercury, Gemini, Apollo, Space ShuttleManned orbital reentry, by commercial entity

SpaceX – Dragon 2Unmanned orbital reentry, by country/governmental entity

 China
 European Space Agency
 India / Indian Space Research Organisation
 Japan
 Soviet Union/ Russia
 United StatesUnmanned orbital reentry, by commercial entity

SpaceX – Dragon


== Selected atmospheric reentries ==
This list includes some notable atmospheric entries in which the spacecraft was not intended to be recovered, but was destroyed in the atmosphere.


== See also ==


== Notes and references ==


== Further reading ==
Launius, Roger D.; Jenkins, Dennis R. (October 10, 2012). Coming Home: Reentry and Recovery from Space. NASA. ISBN 9780160910647. OCLC 802182873. Retrieved August 21, 2014.
Martin, John J. (1966). Atmospheric Entry – An Introduction to Its Science and Engineering. Old Tappan, New Jersey: Prentice-Hall.
Regan, Frank J. (1984). Re-Entry Vehicle Dynamics (AIAA Education Series). New York: American Institute of Aeronautics and Astronautics, Inc. ISBN 978-0-915928-78-1.
Etkin, Bernard (1972). Dynamics of Atmospheric Flight. New York: John Wiley & Sons, Inc. ISBN 978-0-471-24620-6.
Vincenti, Walter G.; Kruger Jr, Charles H. (1986). Introduction to Physical Gas Dynamics. Malabar, Florida: Robert E. Krieger Publishing Co. ISBN 978-0-88275-309-6.
Hansen, C. Frederick (1976). Molecular Physics of Equilibrium Gases, A Handbook for Engineers. NASA. Bibcode:1976mpeg.book.....H. NASA SP-3096.
Hayes, Wallace D.; Probstein, Ronald F. (1959). Hypersonic Flow Theory. New York and London: Academic Press. A revised version of this classic text has been reissued as an inexpensive paperback: Hayes, Wallace D. (1966). Hypersonic Inviscid Flow. Mineola, New York: Dover Publications. ISBN 978-0-486-43281-6. reissued in 2004
Anderson, John D. Jr. (1989). Hypersonic and High Temperature Gas Dynamics. New York: McGraw-Hill, Inc. ISBN 978-0-07-001671-2.


== External links ==
Aerocapture Mission Analysis Tool (AMAT) provides preliminary mission analysis and simulation capability for atmospheric entry vehicles at various Solar System destinations.
Center for Orbital and Reentry Debris Studies (The Aerospace Corporation)
Apollo Atmospheric Entry Phase, 1968, NASA Mission Planning and Analysis Division, Project Apollo. video (25:14).
Buran's heat shield
Encyclopedia Astronautica article on the history of space rescue crafts, including some reentry craft designs.","pandas(index=1, _1=1, text='atmospheric entry is the movement of an object from outer space into and through the gases of an atmosphere of a planet, dwarf planet, or natural satellite. there are two main types of atmospheric entry: uncontrolled entry, such as the entry of astronomical objects, space debris, or bolides; and controlled entry (or reentry) of a spacecraft capable of being navigated or following a predetermined course. technologies and procedures allowing the controlled atmospheric entry, descent, and landing of spacecraft are collectively termed as edl.  objects entering an atmosphere experience atmospheric drag, which puts mechanical stress on the object, and aerodynamic heating—caused mostly by compression of the air in front of the object, but also by drag. these forces can cause loss of mass (ablation) or even complete disintegration of smaller objects, and objects with lower compressive strength can explode. crewed space vehicles must be slowed to subsonic speeds before parachutes or air brakes may be deployed. such vehicles have kinetic energies typically between 50 and 1,800 megajoules, and atmospheric dissipation is the only way of expending the kinetic energy. the amount of rocket fuel required to slow the vehicle would be nearly equal to the amount used to accelerate it initially, and it is thus highly impractical to use retro rockets for the entire earth reentry procedure. while the high temperature generated at the surface of the heat shield is due to adiabatic compression, the vehicle\'s kinetic energy is ultimately lost to gas friction (viscosity) after the vehicle has passed by. other smaller energy losses include black-body radiation directly from the hot gases and chemical reactions between ionized gases. ballistic warheads and expendable vehicles do not require slowing at reentry, and in fact, are made streamlined so as to maintain their speed. furthermore, slow-speed returns to earth from near-space such as parachute jumps from balloons do not require heat shielding because the gravitational acceleration of an object starting at relative rest from within the atmosphere itself (or not far above it) cannot create enough velocity to cause significant atmospheric heating. for earth, atmospheric entry occurs by convention at the kármán line at an altitude of 100 km (62 miles; 54 nautical miles) above the surface, while at venus atmospheric entry occurs at 250 km (160 mi; 130 nmi) and at mars atmospheric entry at about 80 km (50 mi; 43 nmi). uncontrolled objects reach high velocities while accelerating through space toward the earth under the influence of earth\'s gravity, and are slowed by friction upon encountering earth\'s atmosphere. meteors are also often travelling quite fast relative to the earth simply because their own orbital path is different from that of the earth before they encounter earth\'s gravity well. most controlled objects enter at hypersonic speeds due to their sub-orbital (e.g., intercontinental ballistic missile reentry vehicles), orbital (e.g., the soyuz), or unbounded (e.g., meteors) trajectories. various advanced technologies have been developed to enable atmospheric reentry and flight at extreme velocities. an alternative low-velocity method of controlled atmospheric entry is buoyancy which is suitable for planetary entry where thick atmospheres, strong gravity, or both factors complicate high-velocity hyperbolic entry, such as the atmospheres of venus, titan and the gas giants.   == history ==  the concept of the ablative heat shield was described as early as 1920 by robert goddard: ""in the case of meteors, which enter the atmosphere with speeds as high as 30 miles (48 km) per second, the interior of the meteors remains cold, and the erosion is due, to a large extent, to chipping or cracking of the suddenly heated surface. for this reason, if the outer surface of the apparatus were to consist of layers of a very infusible hard substance with layers of a poor heat conductor between, the surface would not be eroded to any considerable extent, especially as the velocity of the apparatus would not be nearly so great as that of the average meteor.""practical development of reentry systems began as the range, and reentry velocity of ballistic missiles increased. for early short-range missiles, like the v-2, stabilization and aerodynamic stress were important issues (many v-2s broke apart during reentry), but heating was not a serious problem. medium-range missiles like the soviet r-5, with a 1,200-kilometer (650-nautical-mile) range, required ceramic composite heat shielding on separable reentry vehicles (it was no longer possible for the entire rocket structure to survive reentry). the first icbms, with ranges of 8,000 to 12,000 km (4,300 to 6,500 nmi), were only possible with the development of modern ablative heat shields and blunt-shaped vehicles. in the united states, this technology was pioneered by h. julian allen and a. j. eggers jr. of the national advisory committee for aeronautics (naca) at ames research center. in 1951, they made the counterintuitive discovery that a blunt shape (high drag) made the most effective heat shield. from simple engineering principles, allen and eggers showed that the heat load experienced by an entry vehicle was inversely proportional to the drag coefficient; i.e., the greater the drag, the less the heat load. if the reentry vehicle is made blunt, air cannot ""get out of the way"" quickly enough, and acts as an air cushion to push the shock wave and heated shock layer forward (away from the vehicle). since most of the hot gases are no longer in direct contact with the vehicle, the heat energy would stay in the shocked gas and simply move around the vehicle to later dissipate into the atmosphere. the allen and eggers discovery, though initially treated as a military secret, was eventually published in 1958.   == terminology, definitions and jargon == over the decades since the 1950s, a rich technical jargon has grown around the engineering of vehicles designed to enter planetary atmospheres. it is recommended that the reader review the jargon glossary before continuing with this article on atmospheric reentry. when atmospheric entry is part of a spacecraft landing or recovery, particularly on a planetary body other than earth, entry is part of a phase referred to as entry, descent, and landing, or edl. when the atmospheric entry returns to the same body that the vehicle had launched from, the event is referred to as reentry (almost always referring to earth entry). the fundamental design objective in atmospheric entry of a spacecraft is to dissipate the energy of a spacecraft that is traveling at hypersonic speed as it enters an atmosphere such that equipment, cargo, and any passengers are slowed and land near a specific destination on the surface at zero velocity while keeping stresses on the spacecraft and any passengers within acceptable limits. this may be accomplished by propulsive or aerodynamic (vehicle characteristics or parachute) means, or by some combination.   == entry vehicle shapes ==  there are several basic shapes used in designing entry vehicles: salyut 1, the world\'s first space station, was deliberately de-orbited into the pacific ocean in 1971 following the soyuz 11 accident. its successor, salyut 6, was de-orbited in a controlled manner as well. on june 4, 2000 the compton gamma ray observatory was deliberately de-orbited after one of its gyroscopes failed. the debris that did not burn up fell harmlessly into the pacific ocean. the observatory was still operational, but the failure of another gyroscope would have made de-orbiting much more difficult and dangerous. with some controversy, nasa decided in the interest of public safety that a controlled crash was preferable to letting the craft come down at random. in 2001, the russian mir space station was deliberately de-orbited, and broke apart in the fashion expected by the command center during atmospheric reentry. mir entered the earth\'s atmosphere on march 23, 2001, near nadi, fiji, and fell into the south pacific ocean. on february 21, 2008, a disabled u.s. spy satellite, usa-193, was hit at an altitude of approximately 246 kilometers (153 mi) with an sm-3 missile fired from the u.s. navy cruiser lake erie off the coast of hawaii. the satellite was inoperative, having failed to reach its intended orbit when it was launched in 2006. due to its rapidly deteriorating orbit it was destined for uncontrolled reentry within a month. u.s. department of defense expressed concern that the 1,000-pound (450 kg) fuel tank containing highly toxic hydrazine might survive reentry to reach the earth\'s surface intact. several governments including those of russia, china, and belarus protested the action as a thinly-veiled demonstration of us anti-satellite capabilities. china had previously caused an international incident when it tested an anti-satellite missile in 2007.       == successful atmospheric reentries from orbital velocities == manned orbital reentry, by country/governmental entity  china – shenzhou soviet union/ russia – vostok, voskhod, soyuz united states – mercury, gemini, apollo, space shuttlemanned orbital reentry, by commercial entity  spacex – dragon 2unmanned orbital reentry, by country/governmental entity  china european space agency india / indian space research organisation japan soviet union/ russia united statesunmanned orbital reentry, by commercial entity  spacex – dragon   == selected atmospheric reentries == this list includes some notable atmospheric entries in which the spacecraft was not intended to be recovered, but was destroyed in the atmosphere.   == see also ==   == notes and references ==   == further reading == launius, roger d.; jenkins, dennis r. (october 10, 2012). coming home: reentry and recovery from space. nasa. isbn 9780160910647. oclc 802182873. retrieved august 21, 2014. martin, john j. (1966). atmospheric entry – an introduction to its science and engineering. old tappan, new jersey: prentice-hall. regan, frank j. (1984). re-entry vehicle dynamics (aiaa education series). new york: american institute of aeronautics and astronautics, inc. isbn 978-0-915928-78-1. etkin, bernard (1972). dynamics of atmospheric flight. new york: john wiley & sons, inc. isbn 978-0-471-24620-6. vincenti, walter g.; kruger jr, charles h. (1986). introduction to physical gas dynamics. malabar, florida: robert e. krieger publishing co. isbn 978-0-88275-309-6. hansen, c. frederick (1976). molecular physics of equilibrium gases, a handbook for engineers. nasa. bibcode:1976mpeg.book.....h. nasa sp-3096. hayes, wallace d.; probstein, ronald f. (1959). hypersonic flow theory. new york and london: academic press. a revised version of this classic text has been reissued as an inexpensive paperback: hayes, wallace d. (1966). hypersonic inviscid flow. mineola, new york: dover publications. isbn 978-0-486-43281-6. reissued in 2004 anderson, john d. jr. (1989). hypersonic and high temperature gas dynamics. new york: mcgraw-hill, inc. isbn 978-0-07-001671-2.   == external links == aerocapture mission analysis tool (amat) provides preliminary mission analysis and simulation capability for atmospheric entry vehicles at various solar system destinations. center for orbital and reentry debris studies (the aerospace corporation) apollo atmospheric entry phase, 1968, nasa mission planning and analysis division, project apollo. video (25:14). buran\'s heat shield encyclopedia astronautica article on the history of space rescue crafts, including some reentry craft designs.')"
2,"In aerodynamics, a hypersonic speed is one that greatly exceeds the speed of sound, often stated as starting at speeds of Mach 5 and above.The precise Mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around Mach 5-10. The hypersonic regime can also be alternatively defined as speeds where specific heat capacity changes with the temperature of the flow as kinetic energy of the moving object is converted into heat.


== Characteristics of flow ==
While the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. The peculiarity in hypersonic flows are as follows:

Shock layer
Aerodynamic heating
Entropy layer
Real gas effects
Low density effects
Independence of aerodynamic coefficients with Mach number.


=== Small shock stand-off distance ===
As a body's Mach number increases, the density behind a bow shock generated by the body also increases, which corresponds to a decrease in volume behind the shock due to conservation of mass. Consequently, the distance between the bow shock and the body decreases at higher Mach numbers.


=== Entropy layer ===
As Mach numbers increase, the entropy change across the shock also increases, which results in a strong entropy gradient and highly vortical flow that mixes with the boundary layer.


=== Viscous interaction ===
A portion of the large kinetic energy associated with flow at high Mach numbers transforms into internal energy in the fluid due to viscous effects. The increase in internal energy is realized as an increase in temperature. Since the pressure gradient normal to the flow within a boundary layer is approximately zero for low to moderate hypersonic Mach numbers, the increase of temperature through the boundary layer coincides with a decrease in density. This causes the bottom of the boundary layer to expand, so that the boundary layer over the body grows thicker and can often merge with the shock wave near the body leading edge.


=== High-temperature flow ===
High temperatures due to a manifestation of viscous dissipation cause non-equilibrium chemical flow properties such as vibrational excitation and dissociation and ionization of molecules resulting in convective and radiative heat-flux.


== Classification of Mach regimes ==
Although ""subsonic"" and ""supersonic"" usually refer to speeds below and above the local speed of sound respectively, aerodynamicists often use these terms to refer to particular ranges of Mach values. This occurs because a ""transonic regime"" exists around M=1 where approximations of the Navier–Stokes equations used for subsonic design no longer apply, partly because the flow locally exceeds M=1 even when the freestream Mach number is below this value.
The ""supersonic regime"" usually refers to the set of Mach numbers for which linearised theory may be used; for example, where the (air) flow is not chemically reacting and where heat transfer between air and vehicle may be reasonably neglected in calculations. Generally, NASA defines ""high"" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Among the aircraft operating in this regime are the Space Shuttle and (theoretically) various developing spaceplanes.
In the following table, the ""regimes"" or ""ranges of Mach values"" are referenced instead of the usual meanings of ""subsonic"" and ""supersonic"".


== Similarity parameters ==
The categorization of airflow relies on a number of similarity parameters, which allow the simplification of a nearly infinite number of test cases into groups of similarity. For transonic and compressible flow, the Mach and Reynolds numbers alone allow good categorization of many flow cases.
Hypersonic flows, however, require other similarity parameters. First, the analytic equations for the oblique shock angle become nearly independent of Mach number at high (~>10) Mach numbers. Second, the formation of strong shocks around aerodynamic bodies means that the freestream Reynolds number is less useful as an estimate of the behavior of the boundary layer over a body (although it is still important). Finally, the increased temperature of hypersonic flows mean that real gas effects become important. For this reason, research in hypersonics is often referred to as aerothermodynamics, rather than aerodynamics.
The introduction of real gas effects means that more variables are required to describe the full state of a gas. Whereas a stationary gas can be described by three variables (pressure, temperature, adiabatic index), and a moving gas by four (flow velocity), a hot gas in chemical equilibrium also requires state equations for the chemical components of the gas, and a gas in nonequilibrium solves those state equations using time as an extra variable. This means that for a nonequilibrium flow, something between 10 and 100 variables may be required to describe the state of the gas at any given time. Additionally, rarefied hypersonic flows (usually defined as those with a Knudsen number above 0.1) do not follow the Navier–Stokes equations.
Hypersonic flows are typically categorized by their total energy, expressed as total enthalpy (MJ/kg), total pressure (kPa-MPa), stagnation pressure (kPa-MPa), stagnation temperature (K), or flow velocity (km/s).
Wallace D. Hayes developed a similarity parameter, similar to the Whitcomb area rule, which allowed similar configurations to be compared.


== Regimes ==
Hypersonic flow can be approximately separated into a number of regimes. The selection of these regimes is rough, due to the blurring of the boundaries where a particular effect can be found.


=== Perfect gas ===
In this regime, the gas can be regarded as an ideal gas. Flow in this regime is still Mach number dependent. Simulations start to depend on the use of a constant-temperature wall, rather than the adiabatic wall typically used at lower speeds. The lower border of this region is around Mach 5, where ramjets become inefficient, and the upper border around Mach 10-12.


=== Two-temperature ideal gas ===
This is a subset of the perfect gas regime, where the gas can be considered chemically perfect, but the rotational and vibrational temperatures of the gas must be considered separately, leading to two temperature models. See particularly the modeling of supersonic nozzles, where vibrational freezing becomes important.


=== Dissociated gas ===
In this regime, diatomic or polyatomic gases (the gases found in most atmospheres) begin to dissociate as they come into contact with the bow shock generated by the body. Surface catalysis plays a role in the calculation of surface heating, meaning that the type of surface material also has an effect on the flow. The lower border of this regime is where any component of a gas mixture first begins to dissociate in the stagnation point of a flow (which for nitrogen is around 2000 K). At the upper border of this regime, the effects of ionization start to have an effect on the flow.


=== Ionized gas ===
In this regime the ionized electron population of the stagnated flow becomes significant, and the electrons must be modeled separately. Often the electron temperature is handled separately from the temperature of the remaining gas components. This region occurs for freestream flow velocities around 3-4 km/s. Gases in this region are modeled as non-radiating plasmas.


=== Radiation-dominated regime ===
Above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. The modeling of gases in this regime is split into two classes:

Optically thin: where the gas does not re-absorb radiation emitted from other parts of the gas
Optically thick: where the radiation must be considered a separate source of energy.The modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.


== See also ==

EnginesRocket engine
Ramjet
Scramjet
Reaction Engines SABRE, LAPCAT (design studies)MissilesShaurya (missile) Ballistic Missile -  India (Entered Production)
BrahMos-II Cruise Missile -   (Under Development)
9K720 Iskander Short-range ballistic missile  Russia (Currently In Service)
3M22 Zircon Anti-ship hypersonic cruise missile  (in production)
R-37 (missile) Hypersonic air-to-air missile  (in service)
Kh-47M2 Kinzhal Hypersonic air-launched ballistic missile  (in service)Other flow regimesSubsonic flight
Transonic
Supersonic speed


== References ==


== External links ==
NASA's Guide to Hypersonics
Hypersonics Group at Imperial College
University of Queensland Centre for Hypersonics
High Speed Flow Group at University of New South Wales
Hypersonics Group at the University of Oxford","pandas(index=2, _1=2, text='in aerodynamics, a hypersonic speed is one that greatly exceeds the speed of sound, often stated as starting at speeds of mach 5 and above.the precise mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around mach 5-10. the hypersonic regime can also be alternatively defined as speeds where specific heat capacity changes with the temperature of the flow as kinetic energy of the moving object is converted into heat.   == characteristics of flow == while the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. the peculiarity in hypersonic flows are as follows:  shock layer aerodynamic heating entropy layer real gas effects low density effects independence of aerodynamic coefficients with mach number. above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. the modeling of gases in this regime is split into two classes:  optically thin: where the gas does not re-absorb radiation emitted from other parts of the gas optically thick: where the radiation must be considered a separate source of energy.the modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.   == see also ==  enginesrocket engine ramjet scramjet reaction engines sabre, lapcat (design studies)missilesshaurya (missile) ballistic missile -  india (entered production) brahmos-ii cruise missile -   (under development) 9k720 iskander short-range ballistic missile  russia (currently in service) 3m22 zircon anti-ship hypersonic cruise missile  (in production) r-37 (missile) hypersonic air-to-air missile  (in service) kh-47m2 kinzhal hypersonic air-launched ballistic missile  (in service)other flow regimessubsonic flight transonic supersonic speed   == references ==   == external links == nasa\'s guide to hypersonics hypersonics group at imperial college university of queensland centre for hypersonics high speed flow group at university of new south wales hypersonics group at the university of oxford')"
3,"Supersonic speed is the speed of an object that exceeds the speed of sound (Mach 1). For objects traveling in dry air of a temperature of 20 °C (68 °F) at sea level, this speed is approximately 343.2 m/s (1,126 ft/s; 768 mph; 667.1 kn; 1,236 km/h). Speeds greater than five times the speed of sound (Mach 5) are often referred to as hypersonic. Flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. This occurs typically somewhere between Mach 0.8 and Mach 1.2.
Sounds are traveling vibrations in the form of pressure waves in an elastic medium. In gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. Since air temperature and composition varies significantly with altitude, Mach numbers for aircraft may change despite a constant travel speed. In water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). In solids, sound waves can be polarized longitudinally or transversely and have even higher velocities.
Supersonic fracture is crack motion faster than the speed of sound in a brittle material.


== Early meaning ==
At the beginning of the 20th century, the term ""supersonic"" was used as an adjective to describe sound whose frequency is above the range of normal human hearing.  The modern term for this meaning is ""ultrasonic"".
Etymology: The word supersonic comes from two Latin derived words; 1) super: above and 2) sonus: sound, which together mean above sound or in other words faster than sound.


== Supersonic objects ==

The tip of a bullwhip is thought to be the first man-made object to break the sound barrier, resulting in the telltale ""crack"" (actually a small sonic boom). The wave motion traveling through the bullwhip is what makes it capable of achieving supersonic speeds.Most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely Concorde and the Tupolev Tu-144. Both these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. Due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, Concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. Since Concorde's final retirement flight on November 26, 2003, there are no supersonic passenger aircraft left in service. Some large bombers, such as the Tupolev Tu-160 and Rockwell B-1 Lancer are also supersonic-capable.
Most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding Mach 3.
Most spacecraft, most notably the Space Shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. During ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag.
Note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). At even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.When an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.


== Supersonic land vehicles ==
To date, only one land vehicle has officially travelled at supersonic speed. It is ThrustSSC, driven by Andy Green, which holds the world land speed record, having achieved an average speed on its bi-directional run of 1,228 km/h (763 mph) in the Black Rock Desert on 15 October 1997.
The Bloodhound LSR project is planning an attempt on the record in 2020 at Hakskeen Pan in South Africa with a combination jet and hybrid rocket propelled car. The aim is to break the existing record, then make further attempts during which [the members of] the team hope to reach speeds of up to 1,600 km/h (1,000 mph). The effort was originally run by Richard Noble who was the leader of the ThrustSSC project, however following funding issues in 2018, the team was bought by Ian Warhurst and renamed Bloodhound LSR. The new project retains many of the original Bloodhound SSC engineering staff, and Andy Green is still the driver for record attempt, with high speed trials expected to start in October 2019.


== Supersonic flight ==
Supersonic aerodynamics is simpler than subsonic aerodynamics because the airsheets at different points along the plane often cannot affect each other. Supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around Mach 0.85–1.2). At these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. Designers use the Supersonic area rule and the Whitcomb area rule to minimize sudden changes in size.

However, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex.
One problem with sustained supersonic flight is the generation of heat in flight.  At high speeds aerodynamic heating can occur, so an aircraft must be designed to operate and function under very high temperatures. Duralumin, a material traditionally used in aircraft manufacturing, starts to lose strength and deform at relatively low temperatures, and is unsuitable for continuous use at speeds above Mach 2.2 to 2.4. Materials such as titanium and stainless steel allow operations at much higher temperatures. For example, the Lockheed SR-71 Blackbird jet could fly continuously at Mach 3.1 which could lead to temperatures on some parts of the aircraft reaching above 315 °C (600 °F).
Another area of concern for sustained high-speed flight is engine operation. Jet engines create thrust by increasing the temperature of the air they ingest, and as the aircraft speeds up, the compression process in the intake causes a temperature rise before it reaches the engines. The maximum allowable temperature of the exhaust is determined by the materials in the turbine at the rear of the engine, so as the aircraft speeds up, the difference in intake and exhaust temperature that the engine can create, by burning fuel, decreases, as does the thrust. The higher thrust needed for supersonic speeds had to be regained by burning extra fuel in the exhaust.
Intake design was also a major issue. As much of the available energy in the incoming air has to be recovered, known as intake recovery, using shock waves in the supersonic compression process in the intake. At supersonic speeds the intake has to make sure that the air slows down without excessive pressure loss. It has to use the correct type of shock waves, oblique/plane, for the aircraft design speed to compress and slow the air to subsonic speed before it reaches the engine. The shock waves are positioned using a ramp or cone which may need to be adjustable depending on trade-offs between complexity and the required aircraft performance. 
An aircraft able to operate for extended periods at supersonic speeds has a potential range advantage over a similar design operating subsonically. Most of the drag an aircraft sees while speeding up to supersonic speeds occurs just below the speed of sound, due to an aerodynamic effect known as wave drag. An aircraft that can accelerate past this speed sees a significant drag decrease, and can fly supersonically with improved fuel economy. However, due to the way lift is generated supersonically, the lift-to-drag ratio of the aircraft as a whole drops, leading to lower range, offsetting or overturning this advantage.
The key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a ""perfect"" shape, the von Karman ogive or Sears-Haack body. This has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. SR-71, Concorde, etc. Although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use.


=== History of supersonic flight ===

Aviation research during World War II led to the creation of the first rocket- and jet-powered aircraft.  Several claims of breaking the sound barrier during the war subsequently emerged.  However, the first recognized flight exceeding the speed of sound by a manned aircraft in controlled level flight was performed on October 14, 1947 by the experimental Bell X-1 research rocket plane piloted by Charles ""Chuck"" Yeager. The first production plane to break the sound barrier was an F-86 Canadair Sabre with the first 'supersonic' woman pilot, Jacqueline Cochran, at the controls. According to David Masters, the DFS 346 prototype captured in Germany by the Soviets, after being released from a B-29 at 32800 ft (10000 m), reached 683 mph (1100 km/h) late in 1945, which would have exceeded Mach 1 at that height.  The pilot in these flights was the German Wolfgang Ziese.
On August 21, 1961, a Douglas DC-8-43 (registration N9604Z) exceeded Mach 1 in a controlled dive during a test flight at Edwards Air Force Base.  The crew were William Magruder (pilot), Paul Patten (copilot), Joseph Tomich (flight engineer), and Richard H. Edwards (flight test engineer).  This was the first supersonic flight by a civilian airliner other than the Concorde or Tu-144.


== See also ==
Area rule
Hypersonic speed
Transonic speed
Sonic boom
Supersonic aircraft
Supersonic airfoils
Vapor cone
Prandtl–Glauert singularity


== References ==


== External links ==
""Can We Ever Fly Faster Speed of Sound"", October 1944, Popular Science one of the earliest articles on shock waves and flying the speed of sound
""Britain Goes Supersonic"", January 1946, Popular Science 1946 article trying to explain supersonic flight to the general public
MathPages - The Speed of Sound
Supersonic sound pressure levels","pandas(index=3, _1=3, text='supersonic speed is the speed of an object that exceeds the speed of sound (mach 1). for objects traveling in dry air of a temperature of 20 °c (68 °f) at sea level, this speed is approximately 343.2 m/s (1,126 ft/s; 768 mph; 667.1 kn; 1,236 km/h). speeds greater than five times the speed of sound (mach 5) are often referred to as hypersonic. flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. this occurs typically somewhere between mach 0.8 and mach 1.2. sounds are traveling vibrations in the form of pressure waves in an elastic medium. in gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. since air temperature and composition varies significantly with altitude, mach numbers for aircraft may change despite a constant travel speed. in water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). in solids, sound waves can be polarized longitudinally or transversely and have even higher velocities. supersonic fracture is crack motion faster than the speed of sound in a brittle material.   == early meaning == at the beginning of the 20th century, the term ""supersonic"" was used as an adjective to describe sound whose frequency is above the range of normal human hearing.  the modern term for this meaning is ""ultrasonic"". etymology: the word supersonic comes from two latin derived words; 1) super: above and 2) sonus: sound, which together mean above sound or in other words faster than sound.   == supersonic objects ==  the tip of a bullwhip is thought to be the first man-made object to break the sound barrier, resulting in the telltale ""crack"" (actually a small sonic boom). the wave motion traveling through the bullwhip is what makes it capable of achieving supersonic speeds.most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely concorde and the tupolev tu-144. both these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. since concorde\'s final retirement flight on november 26, 2003, there are no supersonic passenger aircraft left in service. some large bombers, such as the tupolev tu-160 and rockwell b-1 lancer are also supersonic-capable. most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding mach 3. most spacecraft, most notably the space shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. during ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag. note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). at even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.when an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.   == supersonic land vehicles == to date, only one land vehicle has officially travelled at supersonic speed. it is thrustssc, driven by andy green, which holds the world land speed record, having achieved an average speed on its bi-directional run of 1,228 km/h (763 mph) in the black rock desert on 15 october 1997. the bloodhound lsr project is planning an attempt on the record in 2020 at hakskeen pan in south africa with a combination jet and hybrid rocket propelled car. the aim is to break the existing record, then make further attempts during which [the members of] the team hope to reach speeds of up to 1,600 km/h (1,000 mph). the effort was originally run by richard noble who was the leader of the thrustssc project, however following funding issues in 2018, the team was bought by ian warhurst and renamed bloodhound lsr. the new project retains many of the original bloodhound ssc engineering staff, and andy green is still the driver for record attempt, with high speed trials expected to start in october 2019.   == supersonic flight == supersonic aerodynamics is simpler than subsonic aerodynamics because the airsheets at different points along the plane often cannot affect each other. supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around mach 0.85–1.2). at these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. designers use the supersonic area rule and the whitcomb area rule to minimize sudden changes in size.  however, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex. one problem with sustained supersonic flight is the generation of heat in flight.  at high speeds aerodynamic heating can occur, so an aircraft must be designed to operate and function under very high temperatures. duralumin, a material traditionally used in aircraft manufacturing, starts to lose strength and deform at relatively low temperatures, and is unsuitable for continuous use at speeds above mach 2.2 to 2.4. materials such as titanium and stainless steel allow operations at much higher temperatures. for example, the lockheed sr-71 blackbird jet could fly continuously at mach 3.1 which could lead to temperatures on some parts of the aircraft reaching above 315 °c (600 °f). another area of concern for sustained high-speed flight is engine operation. jet engines create thrust by increasing the temperature of the air they ingest, and as the aircraft speeds up, the compression process in the intake causes a temperature rise before it reaches the engines. the maximum allowable temperature of the exhaust is determined by the materials in the turbine at the rear of the engine, so as the aircraft speeds up, the difference in intake and exhaust temperature that the engine can create, by burning fuel, decreases, as does the thrust. the higher thrust needed for supersonic speeds had to be regained by burning extra fuel in the exhaust. intake design was also a major issue. as much of the available energy in the incoming air has to be recovered, known as intake recovery, using shock waves in the supersonic compression process in the intake. at supersonic speeds the intake has to make sure that the air slows down without excessive pressure loss. it has to use the correct type of shock waves, oblique/plane, for the aircraft design speed to compress and slow the air to subsonic speed before it reaches the engine. the shock waves are positioned using a ramp or cone which may need to be adjustable depending on trade-offs between complexity and the required aircraft performance. an aircraft able to operate for extended periods at supersonic speeds has a potential range advantage over a similar design operating subsonically. most of the drag an aircraft sees while speeding up to supersonic speeds occurs just below the speed of sound, due to an aerodynamic effect known as wave drag. an aircraft that can accelerate past this speed sees a significant drag decrease, and can fly supersonically with improved fuel economy. however, due to the way lift is generated supersonically, the lift-to-drag ratio of the aircraft as a whole drops, leading to lower range, offsetting or overturning this advantage. the key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a ""perfect"" shape, the von karman ogive or sears-haack body. this has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. sr-71, concorde, etc. although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use. aviation research during world war ii led to the creation of the first rocket- and jet-powered aircraft.  several claims of breaking the sound barrier during the war subsequently emerged.  however, the first recognized flight exceeding the speed of sound by a manned aircraft in controlled level flight was performed on october 14, 1947 by the experimental bell x-1 research rocket plane piloted by charles ""chuck"" yeager. the first production plane to break the sound barrier was an f-86 canadair sabre with the first \'supersonic\' woman pilot, jacqueline cochran, at the controls. according to david masters, the dfs 346 prototype captured in germany by the soviets, after being released from a b-29 at 32800 ft (10000 m), reached 683 mph (1100 km/h) late in 1945, which would have exceeded mach 1 at that height.  the pilot in these flights was the german wolfgang ziese. on august 21, 1961, a douglas dc-8-43 (registration n9604z) exceeded mach 1 in a controlled dive during a test flight at edwards air force base.  the crew were william magruder (pilot), paul patten (copilot), joseph tomich (flight engineer), and richard h. edwards (flight test engineer).  this was the first supersonic flight by a civilian airliner other than the concorde or tu-144.   == see also == area rule hypersonic speed transonic speed sonic boom supersonic aircraft supersonic airfoils vapor cone prandtl–glauert singularity   == references ==   == external links == ""can we ever fly faster speed of sound"", october 1944, popular science one of the earliest articles on shock waves and flying the speed of sound ""britain goes supersonic"", january 1946, popular science 1946 article trying to explain supersonic flight to the general public mathpages - the speed of sound supersonic sound pressure levels')"
4,"Aeroelasticity is the branch of physics and engineering studying the interactions between the inertial, elastic, and aerodynamic forces occurring while an elastic body is exposed to a fluid flow. The study of aeroelasticity may be broadly classified into two fields: static aeroelasticity dealing with the static or steady state response of an elastic body to a fluid flow; and dynamic aeroelasticity dealing with the body's dynamic (typically vibrational) response.
Aircraft are prone to aeroelastic effects because they need to be lightweight and withstand large aerodynamic loads.  Aircraft are designed to avoid the following aeroelastic problems:

divergence where the aerodynamic forces increase the angle of attack of a wing which further increases the force;
control reversal where control activation produces an opposite aerodynamic moment that reduces, or in extreme cases, reverses the control effectiveness; and
flutter which is the uncontained vibration that can lead to the destruction of an aircraft.Aeroelasticity problems can be prevented by adjusting the mass, stiffness or aerodynamics of structures which can be determined and verified through the use of calculations, ground vibration tests and flight flutter trials. Flutter of control surfaces is usually eliminated by the careful placement of mass balances.
The synthesis of aeroelasticity with thermodynamics is known as aerothermoelasticity, and its synthesis with control theory is known as aeroservoelasticity.


== History ==
The second failure of Samuel Langley's prototype plane on the Potomac was attributed to aeroelastic effects (specifically, torsional divergence). An early scientific work on the subject was George Bryan's Theory of the Stability of a Rigid Aeroplane published in 1906. Problems with torsional divergence plagued aircraft in the First World War and were solved largely by trial-and-error and ad-hoc stiffening of the wing. The first recorded and documented case of flutter in an aircraft was that which occurred to a Handley Page O/400 bomber during a flight in 1916, when it suffered a violent tail oscillation, which caused extreme distortion of the rear fuselage and the elevators to move asymmetrically. Although the aircraft landed safely, in the subsequent investigation F. W. Lanchester was consulted. One of his recommendations was that left and right elevators should be rigidly connected by a stiff shaft, which was to subsequently become a design requirement. In addition, the National Physical Laboratory (NPL) was asked to investigate the phenomenon theoretically, which was subsequently carried out by Leonard Bairstow and Arthur Fage.In 1926, Hans Reissner published a theory of wing divergence, leading to much further theoretical research on the subject. The term aeroelasticity itself was coined by Harold Roxbee Cox and Alfred Pugsley at the Royal Aircraft Establishment (RAE), Farnborough in the early 1930s.In the development of aeronautical engineering at Caltech, Theodore von Kármán started a course ""Elasticity applied to Aeronautics"". After teaching the course for one term, Kármán passed it over to Ernest Edwin Sechler, who developed aeroelasticity in that course and in publication of textbooks on the subject.In 1947, Arthur Roderick Collar defined aeroelasticity as ""the study of the mutual interaction that takes place within the triangle of the inertial, elastic, and aerodynamic forces acting on structural members exposed to an airstream, and the influence of this study on design"".


== Static aeroelasticity ==
In an aeroplane, two significant static aeroelastic effects may occur. Divergence is a phenomenon in which the elastic twist of the wing suddenly becomes theoretically infinite, typically causing the wing to fail. Control reversal is a phenomenon occurring only in wings with ailerons or other control surfaces, in which these control surfaces reverse their usual functionality (e.g., the rolling direction associated with a given aileron moment is reversed).


=== Divergence ===
Divergence occurs when a lifting surface deflects under aerodynamic load in a direction which further increases lift in a positive feedback loop. The increased lift deflects the structure further, which eventually brings the structure to the point of divergence. 


=== Control reversal ===

Control surface reversal is the loss (or reversal) of the expected response of a control surface, due to deformation of the main lifting surface. For simple models (e.g. single aileron on an Euler-Bernoulli beam), control reversal speeds can be derived analytically as for torsional divergence. Control reversal can be used to aerodynamic advantage, and forms part of the Kaman servo-flap rotor design.


== Dynamic aeroelasticity ==
Dynamic aeroelasticity studies the interactions among aerodynamic, elastic, and inertial forces. Examples of dynamic aeroelastic phenomena are:


=== Flutter ===
Flutter is a dynamic instability of an elastic structure in a fluid flow, caused by positive feedback between the body's deflection and the force exerted by the fluid flow. In a linear system, ""flutter point"" is the point at which the structure is undergoing simple harmonic motion—zero net damping—and so any further decrease in net damping will result in a self-oscillation and eventual failure. ""Net damping"" can be understood as the sum of the structure's natural positive damping and the negative damping of the aerodynamic force. Flutter can be classified into two types: hard flutter, in which the net damping decreases very suddenly, very close to the flutter point; and soft flutter, in which the net damping decreases gradually.In water the mass ratio of the pitch inertia of the foil to that of the circumscribing cylinder of fluid is generally too low for binary flutter to occur, as shown by explicit solution of the simplest pitch and heave flutter stability determinant.

Structures exposed to aerodynamic forces—including wings and aerofoils, but also chimneys and bridges—are designed carefully within known parameters to avoid flutter. 
Blunt shapes, such as chimneys, can give off a continuous stream of vortices known as a Kármán vortex street, which can induce structural oscillations.  Strakes are typically wrapped around chimneys to stop the formation of these vortices. 
In complex structures where both the aerodynamics and the mechanical properties of the structure are not fully understood, flutter can be discounted only through detailed testing. Even changing the mass distribution of an aircraft or the stiffness of one component can induce flutter in an apparently unrelated aerodynamic component.  At its mildest, this can appear as a ""buzz"" in the aircraft structure, but at its most violent, it can develop uncontrollably with great speed and cause serious damage to or lead to the destruction of the aircraft, as in Braniff Flight 542, or the prototypes for the VL Myrsky fighter aircraft. Famously, the original Tacoma Narrows Bridge was destroyed as a result of aeroelastic fluttering.


==== Aeroservoelasticity ====
In some cases, automatic control systems have been demonstrated to help prevent or limit flutter-related structural vibration.


==== Propeller whirl flutter ====
Propeller whirl flutter is a special case of flutter involving the aerodynamic and inertial effects of a rotating propeller and the stiffness of the supporting nacelle structure. Dynamic instability can occur involving pitch and yaw degrees of freedom of the propeller and the engine supports leading to an unstable precession of the propeller. Failure of the engine supports led to whirl flutter occurring on two Lockheed L-188 Electra in 1959 on Braniff Flight 542 and again in 1960 on Northwest Orient Airlines Flight 710.


==== Transonic aeroelasticity ====
Flow is highly non-linear in the transonic regime, dominated by moving shock waves. It is mission-critical for aircraft that fly through transonic Mach numbers. The role of shock waves was first analyzed by Holt Ashley. A phenomenon that impacts stability of aircraft known as ""transonic dip"", in which the flutter speed can get close to flight speed, was reported in May 1976 by Farmer and Hanson of the Langley Research Center.


=== Buffeting ===

Buffeting is a high-frequency instability, caused by airflow separation or shock wave oscillations from one object striking another. It is caused by a sudden impulse of load increasing. It is a random forced vibration. Generally it affects the tail unit of the aircraft structure due to air flow downstream of the wing.The methods for buffet detection are:

Pressure coefficient diagram
Pressure divergence at trailing edge
Computing separation from trailing edge based on Mach number
Normal force fluctuating divergence


== Prediction and cure ==

In the period 1950–1970, AGARD developed the Manual on Aeroelasticity which details the processes used in solving and verifying aeroelastic problems along with standard examples that can be used to test numerical solutions.Aeroelasticity involves not just the external aerodynamic loads and the way they change but also the structural, damping and mass characteristics of the aircraft. Prediction involves making a mathematical model of the aircraft as a series of masses connected by springs and dampers which are tuned to represent the dynamic characteristics of the aircraft structure. The model also includes details of applied aerodynamic forces and how they vary.
The model can be used to predict the flutter margin and, if necessary, test fixes to potential problems. Small carefully chosen changes to mass distribution and local structural stiffness can be very effective in solving aeroelastic problems.
Methods of predicting flutter in linear structures include the p-method, the k-method and the p-k method.For nonlinear systems, flutter is usually interpreted as a limit cycle oscillation (LCO), and methods from the study of dynamical systems can be used to determine the speed at which flutter will occur.


== Media ==
These videos detail the Active Aeroelastic Wing two-phase NASA-Air Force flight research program to investigate the potential of aerodynamically twisting flexible wings to improve maneuverability of high-performance aircraft at transonic and supersonic speeds, with traditional control surfaces such as ailerons and leading-edge flaps used to induce the twist.

		


== Notable aeroelastic failures ==
The original Tacoma Narrows Bridge was destroyed as a result of aeroelastic fluttering.
Propeller whirl flutter of the Lockheed L-188 Electra on Braniff Flight 542.
1931 Transcontinental & Western Air Fokker F-10 crash.
Body freedom flutter of the GAF Jindivik drone.


== See also ==


== References ==


== Further reading ==
Bisplinghoff, R. L., Ashley, H. and Halfman, H., Aeroelasticity. Dover Science, 1996, ISBN 0-486-69189-6, 880 p.
Dowell, E. H., A Modern Course on Aeroelasticity. ISBN 90-286-0057-4.
Fung, Y. C., An Introduction to the Theory of Aeroelasticity. Dover, 1994, ISBN 978-0-486-67871-9.
Hodges, D. H. and Pierce, A., Introduction to Structural Dynamics and Aeroelasticity, Cambridge, 2002, ISBN 978-0-521-80698-5.
Wright, J. R. and Cooper, J. E., Introduction to Aircraft Aeroelasticity and Loads, Wiley 2007, ISBN 978-0-470-85840-0.
Hoque, M. E., ""Active Flutter Control"", LAP Lambert Academic Publishing, Germany, 2010, ISBN 978-3-8383-6851-1.
Collar, A. R., ""The first fifty years of aeroelasticity"", Aerospace, vol. 5, no. 2, pp. 12–20, 1978.
Garrick, I. E. and Reed W. H., ""Historical development of aircraft flutter"", Journal of Aircraft, vol. 18, pp. 897–912, Nov. 1981.
Patrick R. Veillette (Aug 23, 2018). ""Low-Speed Buffet: High-Altitude, Transonic Training Weakness Continues"". Business & Commercial Aviation. Aviation Week Network.


== External links ==
Aeroelasticity Branch – NASA Langley Research Center
DLR Institute of Aeroelasticity
National Aerospace Laboratory
The Aeroelasticity Group – Texas A&M University
NACA Technical Reports – NASA Langley Research Center
NASA Aeroelasticity Handbook","pandas(index=4, _1=4, text='aeroelasticity is the branch of physics and engineering studying the interactions between the inertial, elastic, and aerodynamic forces occurring while an elastic body is exposed to a fluid flow. the study of aeroelasticity may be broadly classified into two fields: static aeroelasticity dealing with the static or steady state response of an elastic body to a fluid flow; and dynamic aeroelasticity dealing with the body\'s dynamic (typically vibrational) response. aircraft are prone to aeroelastic effects because they need to be lightweight and withstand large aerodynamic loads.  aircraft are designed to avoid the following aeroelastic problems:  divergence where the aerodynamic forces increase the angle of attack of a wing which further increases the force; control reversal where control activation produces an opposite aerodynamic moment that reduces, or in extreme cases, reverses the control effectiveness; and flutter which is the uncontained vibration that can lead to the destruction of an aircraft.aeroelasticity problems can be prevented by adjusting the mass, stiffness or aerodynamics of structures which can be determined and verified through the use of calculations, ground vibration tests and flight flutter trials. flutter of control surfaces is usually eliminated by the careful placement of mass balances. the synthesis of aeroelasticity with thermodynamics is known as aerothermoelasticity, and its synthesis with control theory is known as aeroservoelasticity.   == history == the second failure of samuel langley\'s prototype plane on the potomac was attributed to aeroelastic effects (specifically, torsional divergence). an early scientific work on the subject was george bryan\'s theory of the stability of a rigid aeroplane published in 1906. problems with torsional divergence plagued aircraft in the first world war and were solved largely by trial-and-error and ad-hoc stiffening of the wing. the first recorded and documented case of flutter in an aircraft was that which occurred to a handley page o/400 bomber during a flight in 1916, when it suffered a violent tail oscillation, which caused extreme distortion of the rear fuselage and the elevators to move asymmetrically. although the aircraft landed safely, in the subsequent investigation f. w. lanchester was consulted. one of his recommendations was that left and right elevators should be rigidly connected by a stiff shaft, which was to subsequently become a design requirement. in addition, the national physical laboratory (npl) was asked to investigate the phenomenon theoretically, which was subsequently carried out by leonard bairstow and arthur fage.in 1926, hans reissner published a theory of wing divergence, leading to much further theoretical research on the subject. the term aeroelasticity itself was coined by harold roxbee cox and alfred pugsley at the royal aircraft establishment (rae), farnborough in the early 1930s.in the development of aeronautical engineering at caltech, theodore von kármán started a course ""elasticity applied to aeronautics"". after teaching the course for one term, kármán passed it over to ernest edwin sechler, who developed aeroelasticity in that course and in publication of textbooks on the subject.in 1947, arthur roderick collar defined aeroelasticity as ""the study of the mutual interaction that takes place within the triangle of the inertial, elastic, and aerodynamic forces acting on structural members exposed to an airstream, and the influence of this study on design"".   == static aeroelasticity == in an aeroplane, two significant static aeroelastic effects may occur. divergence is a phenomenon in which the elastic twist of the wing suddenly becomes theoretically infinite, typically causing the wing to fail. control reversal is a phenomenon occurring only in wings with ailerons or other control surfaces, in which these control surfaces reverse their usual functionality (e.g., the rolling direction associated with a given aileron moment is reversed). buffeting is a high-frequency instability, caused by airflow separation or shock wave oscillations from one object striking another. it is caused by a sudden impulse of load increasing. it is a random forced vibration. generally it affects the tail unit of the aircraft structure due to air flow downstream of the wing.the methods for buffet detection are:  pressure coefficient diagram pressure divergence at trailing edge computing separation from trailing edge based on mach number normal force fluctuating divergence   == prediction and cure ==  in the period 1950–1970, agard developed the manual on aeroelasticity which details the processes used in solving and verifying aeroelastic problems along with standard examples that can be used to test numerical solutions.aeroelasticity involves not just the external aerodynamic loads and the way they change but also the structural, damping and mass characteristics of the aircraft. prediction involves making a mathematical model of the aircraft as a series of masses connected by springs and dampers which are tuned to represent the dynamic characteristics of the aircraft structure. the model also includes details of applied aerodynamic forces and how they vary. the model can be used to predict the flutter margin and, if necessary, test fixes to potential problems. small carefully chosen changes to mass distribution and local structural stiffness can be very effective in solving aeroelastic problems. methods of predicting flutter in linear structures include the p-method, the k-method and the p-k method.for nonlinear systems, flutter is usually interpreted as a limit cycle oscillation (lco), and methods from the study of dynamical systems can be used to determine the speed at which flutter will occur.   == media == these videos detail the active aeroelastic wing two-phase nasa-air force flight research program to investigate the potential of aerodynamically twisting flexible wings to improve maneuverability of high-performance aircraft at transonic and supersonic speeds, with traditional control surfaces such as ailerons and leading-edge flaps used to induce the twist.       == notable aeroelastic failures == the original tacoma narrows bridge was destroyed as a result of aeroelastic fluttering. propeller whirl flutter of the lockheed l-188 electra on braniff flight 542. 1931 transcontinental & western air fokker f-10 crash. body freedom flutter of the gaf jindivik drone.   == see also ==   == references ==   == further reading == bisplinghoff, r. l., ashley, h. and halfman, h., aeroelasticity. dover science, 1996, isbn 0-486-69189-6, 880 p. dowell, e. h., a modern course on aeroelasticity. isbn 90-286-0057-4. fung, y. c., an introduction to the theory of aeroelasticity. dover, 1994, isbn 978-0-486-67871-9. hodges, d. h. and pierce, a., introduction to structural dynamics and aeroelasticity, cambridge, 2002, isbn 978-0-521-80698-5. wright, j. r. and cooper, j. e., introduction to aircraft aeroelasticity and loads, wiley 2007, isbn 978-0-470-85840-0. hoque, m. e., ""active flutter control"", lap lambert academic publishing, germany, 2010, isbn 978-3-8383-6851-1. collar, a. r., ""the first fifty years of aeroelasticity"", aerospace, vol. 5, no. 2, pp. 12–20, 1978. garrick, i. e. and reed w. h., ""historical development of aircraft flutter"", journal of aircraft, vol. 18, pp. 897–912, nov. 1981. patrick r. veillette (aug 23, 2018). ""low-speed buffet: high-altitude, transonic training weakness continues"". business & commercial aviation. aviation week network.   == external links == aeroelasticity branch – nasa langley research center dlr institute of aeroelasticity national aerospace laboratory the aeroelasticity group – texas a&m university naca technical reports – nasa langley research center nasa aeroelasticity handbook')"
5,"An aerospace manufacturer is a company or individual involved in the various aspects of designing, building, testing, selling, and maintaining aircraft, aircraft parts, missiles, rockets, or spacecraft. Aerospace is a high technology industry.
The aircraft industry is the industry supporting aviation by building aircraft and manufacturing aircraft parts for their maintenance.  This includes aircraft and parts used for civil aviation and military aviation. Most production is done pursuant to type certificates and Defense Standards issued by a government body. This term has been largely subsumed by the more encompassing term: ""aerospace industry"".


== Market ==
In 2015 the aircraft production was worth US$180.3 Billion: 61% airliners, 14% business and general aviation, 12% Military aircraft, 10% military rotary wing and 3% civil rotary wing; while their MRO was worth $135.1 Bn or $315.4 Bn combined.The global aerospace industry was worth $838 billion in 2017: Aircraft & Engine OEMs represented 28% ($235 Bn), Civil & Military MRO & Upgrades 27% ($226 Bn), Aircraft Systems & Component Manufacturing 26% ($218 Bn), Satellites & Space 7% ($59 Bn), Missiles & UAVs 5% ($42 Bn) and other activity, including flight simulators, defense electronics, public research accounted for 7% ($59 Bn).
The countries with the largest industry were led by the United States with $408.4 Bn (49%) followed by France with $69 Bn (8.2%) then China with $61.2 Bn (7.3%), United Kingdom with $48.8 Bn (5.8%), Germany with $46.2 Bn (5.5%), Russia with $27.1 Bn (3.2%), Canada with $24 Bn (2.9%), Japan with $21 Bn (2.5%), Spain with $14 Bn (1.7%) and India with $11Bn (1.3%). The top 10 countries represent $731 Bn or 87.2% of the whole industry.In 2018, the new commercial aircraft value is projected for $270.4 billion while business aircraft will amount for $18 billion and civil helicopters for $4 billion.


== Largest companies ==


== Geography ==
In September 2018, PwC ranked aerospace manufacturing attractiveness: the most attractive country was the United States, with $240 billion in sales in 2017, due to the sheer size of the industry (#1) and educated workforce (#1), low geopolitical risk (#4, #1 is Japan), strong transportation infrastructure (#5, #1 is Hong Kong), a healthy economy (#10, #1 is China), but high costs (#7, #1 is Denmark) and average tax policy (#36, #1 is Qatar).
Following were Canada, Singapore, Switzerland and United Kingdom.Within the US, the most attractive was Washington state, due to the best Industry (#1), leading Infrastructure (#4, New Jersey is #1) and Economy (#4, Texas is #1), good labor (#9, Massachusetts is #1), average tax policy (#17, Alaska is #1) but is costly (#33, Montana is #1).
Washington is tied to Boeing Commercial Airplanes, earning $10.3 billion, is home to 1,400 aerospace-related businesses, and has the highest aerospace jobs concentration.
Following are Texas, Georgia, Arizona and Colorado.In the European Union, aerospace companies such as Airbus, Safran, BAE Systems, Thales, Dassault, Saab AB, Terma A/S, Patria Plc and Leonardo are participants in the global aerospace industry and research effort.
In Russia, large aerospace companies like Oboronprom and the United Aircraft Corporation (encompassing Mikoyan, Sukhoi, Ilyushin, Tupolev, Yakovlev, and Irkut, which includes Beriev) are among the major global players in this industry.
In the US, the Department of Defense and NASA are the two biggest consumers of aerospace technology and products. The Bureau of Labor Statistics of the United States reported that the aerospace industry employed 444,000 wage and salary jobs in 2004, many of which were in Washington and California, this marked a steep decline from the peak years during the Reagan Administration when total employment exceeded 1,000,000 aerospace industry workers.During that period of recovery a special program to restore U.S. competitiveness across all U.S. industries, Project Socrates, contributed to employment growth as the U.S. aerospace industry captured 72 percent of world aerospace market.  By 1999 U.S. share of the world market fell to 52 percent.


=== Cities ===
Important locations of the civil aerospace industry worldwide include Seattle, Wichita, Kansas, Dayton, Ohio and St. Louis in the United States (Boeing), Montreal and Toronto in Canada (Bombardier, Pratt & Whitney Canada), Toulouse and Bordeaux in France (Airbus, Dassault, ATR), Seville in Spain and Hamburg in Germany (Airbus, EADS), the North-West of England and Bristol in Britain (BAE Systems, Airbus and AgustaWestland), Komsomolsk-on-Amur and Irkutsk in Russia (Sukhoi, Beriev), Kyiv and Kharkiv in Ukraine (Antonov), Nagoya in Japan (Mitsubishi Heavy Industries Aerospace and Kawasaki Heavy Industries Aerospace), as well as São José dos Campos in Brazil where Embraer is based.


== Consolidation ==
Several consolidations took place in the aerospace and defense industries over the last few decades.
BAE Systems is the successor company to numerous British aircraft manufacturers which merged throughout the second half of the 20th century. Many of these mergers followed the 1957 Defence White Paper.Airbus prominently illustrated the European airliner manufacturing consolidation in the late 1960s.Between 1988 and 2010, more than 5,452 mergers and acquisitions with a total known-value of US$579 billion were announced worldwide.In 1993, then United States Secretary of Defense Les Aspin and his deputy William J. Perry held the ""Last Supper"" at the Pentagon with contractors executives who were told that there were twice as many military suppliers as he wanted to see: $55 billion in military-industry mergers took place from 1992 to 1997, leaving mainly Boeing, Lockheed Martin, Northrop Grumman and Raytheon.Boeing bought McDonnell Douglas for US$13.3 billion in 1996.Raytheon acquired Hughes Aircraft Company for $9.5 billion in 1997.Marconi Electronic Systems, a subsidiary of the General Electric Company plc, was acquired by British Aerospace for US$12.3 billion in 1999 merger, to form BAE Systems.
In 2002, when Fairchild Dornier was bankrupt, Airbus, Boeing or Bombardier declined to take the 728JET/928JET large regional jet program as mainline and regional aircraft manufacturers were split and Airbus was digesting its ill-fated Fokker acquisition a decade earlier.On September 4, 2017, United Technologies acquired Rockwell Collins in cash and stock for $23 billion, $30 billion including Rockwell Collins' net debt, for $500+ million of synergies expected by year four.
The Oct. 16, 2017 announcement of the CSeries partnership between Airbus and Bombardier Aerospace could trigger a daisy chain of reactions towards a new order. 
Airbus gets a new, efficient model at the lower end of the narrowbody market which provides the bulk of airliner profits and can abandon the slow selling A319 while Bombardier benefits from the growth in this expanded market even if it holds a smaller residual stake.
Boeing could forge a similar alliance with either Embraer with its E-jet E2 or Mitsubishi Heavy Industries and its MRJ.On 21 December, Boeing and Embraer confirmed to be discussing a potential combination with a transaction subject to Brazilian government regulators, the companies' boards and shareholders approvals.
The weight of Airbus and Boeing could help E2 and CSeries sales but the 100-150 seats market seems slow.
As the CSeries, renamed A220, and E-jet E2 are more capable than their predecessors, they moved closer to the lower end of the narrowbodies.
In 2018, the four Western airframers combined into two within nine months as Boeing acquired 80% of Embraer's airliners for $3.8 billion on July 5.On April 3, 2020, Raytheon and United Technologies Corporation (except Otis Worldwide, leaving Rockwell Collins and engine maker Pratt and Whitney)  merged to form Raytheon Technologies Corporation, with combined sales of $79 billion in 2019.The most prominent unions between 1995 and 2020 include those of Boeing and McDonnell Douglas; the French, German and Spanish parts of EADS; and United Technologies with Rockwell Collins then Raytheon, but many mergers projects did not went through: Textron-Bombardier, EADS-BAE Systems, Hawker Beechcraft-Superior Aviation, GE-Honeywell, BAE Systems-Boeing (or Lockheed Martin), Dassault-Aerospatiale, Safran-Thales, BAE Systems-Rolls-Royce or Lockheed Martin–Northrop Grumman.


== Suppliers ==
The largest aerospace suppliers are United Technologies with $28.2 Billion of revenue, followed by GE Aviation with $24.7 Billion, Safran with $22.5 Billion, Rolls-Royce Holdings with $16.9 Billion, Honeywell Aerospace with $15.2 Billion and Rockwell Collins including B/E Aerospace with $8.1 Billion.
The electric aircraft development could generate large changes for the aerospace suppliers.On 26 November 2018, United Technologies announced the completion of its Rockwell Collins acquisition, renaming systems supplier UTC Aerospace Systems as Collins Aerospace, for $23 billion of sales in 2017 and 70,000 employees, and $39.0 billion of sales in 2017 combined with engine manufacturer Pratt & Whitney.


== Supply chain ==
Before the 1980s/1990s, aircraft and aeroengine manufacturers were vertically integrated.
Then Douglas aircraft outsourced large aerostructures and the Bombardier Global Express pioneered the ""Tier 1"" supply chain model inspired by automotive industry, with 10-12 risk-sharing limited partners funding around half of the development costs.
The Embraer E-Jet followed in the late 1990s with fewer than 40 primary suppliers. 
Tier 1 suppliers were led by Honeywell, Safran, Goodrich Corporation and Hamilton Sundstrand.In the 2000s Rolls-Royce reduced its supplier count after bringing in automotive supply chain executives.
On the Airbus A380, less than 100 major suppliers outsource 60% of its value, even 80% on the A350XWB.
Boeing embraced an aggressive Tier 1 model for the B787 but with its difficulties began to question why it was earning lower margins than its suppliers while it seemed to take all the risk, ensuing its 2011 Partnering for Success initiative, as Airbus initiated its own Scope+ initiative for the A320.
Tier 1 consolidation also affects engine manufacturers : GE Aviation acquired Avio in 2013 and Rolls-Royce plc is taking control of Industria de Turbo Propulsores.


== See also ==
Aerospace
Aviation accidents and incidents
List of aircraft manufacturers
List of spacecraft manufacturers
Military-industrial complex
Aircraft parts industry
Aerospace industry of Russia
Aviation


== References ==


== Further reading ==


== External links ==
""U.S. Aerospace Industries Association"".
""Aerospace, Defense & Government Services – Mergers & Acquisitions (January 1993 - December 2016)"" (PDF). Grundman Advisory. 6 Apr 2017.
Jens Flottau (Feb 22, 2018). ""Opinion: Airframers Should Watch Where They Squeeze Suppliers"". Aviation Week & Space Technology.
Aerospace Craft & Structural Components","pandas(index=5, _1=5, text='an aerospace manufacturer is a company or individual involved in the various aspects of designing, building, testing, selling, and maintaining aircraft, aircraft parts, missiles, rockets, or spacecraft. aerospace is a high technology industry. the aircraft industry is the industry supporting aviation by building aircraft and manufacturing aircraft parts for their maintenance.  this includes aircraft and parts used for civil aviation and military aviation. most production is done pursuant to type certificates and defense standards issued by a government body. this term has been largely subsumed by the more encompassing term: ""aerospace industry"".   == market == in 2015 the aircraft production was worth us$180.3 billion: 61% airliners, 14% business and general aviation, 12% military aircraft, 10% military rotary wing and 3% civil rotary wing; while their mro was worth $135.1 bn or $315.4 bn combined.the global aerospace industry was worth $838 billion in 2017: aircraft & engine oems represented 28% ($235 bn), civil & military mro & upgrades 27% ($226 bn), aircraft systems & component manufacturing 26% ($218 bn), satellites & space 7% ($59 bn), missiles & uavs 5% ($42 bn) and other activity, including flight simulators, defense electronics, public research accounted for 7% ($59 bn). the countries with the largest industry were led by the united states with $408.4 bn (49%) followed by france with $69 bn (8.2%) then china with $61.2 bn (7.3%), united kingdom with $48.8 bn (5.8%), germany with $46.2 bn (5.5%), russia with $27.1 bn (3.2%), canada with $24 bn (2.9%), japan with $21 bn (2.5%), spain with $14 bn (1.7%) and india with $11bn (1.3%). the top 10 countries represent $731 bn or 87.2% of the whole industry.in 2018, the new commercial aircraft value is projected for $270.4 billion while business aircraft will amount for $18 billion and civil helicopters for $4 billion.   == largest companies ==   == geography == in september 2018, pwc ranked aerospace manufacturing attractiveness: the most attractive country was the united states, with $240 billion in sales in 2017, due to the sheer size of the industry (#1) and educated workforce (#1), low geopolitical risk (#4, #1 is japan), strong transportation infrastructure (#5, #1 is hong kong), a healthy economy (#10, #1 is china), but high costs (#7, #1 is denmark) and average tax policy (#36, #1 is qatar). following were canada, singapore, switzerland and united kingdom.within the us, the most attractive was washington state, due to the best industry (#1), leading infrastructure (#4, new jersey is #1) and economy (#4, texas is #1), good labor (#9, massachusetts is #1), average tax policy (#17, alaska is #1) but is costly (#33, montana is #1). washington is tied to boeing commercial airplanes, earning $10.3 billion, is home to 1,400 aerospace-related businesses, and has the highest aerospace jobs concentration. following are texas, georgia, arizona and colorado.in the european union, aerospace companies such as airbus, safran, bae systems, thales, dassault, saab ab, terma a/s, patria plc and leonardo are participants in the global aerospace industry and research effort. in russia, large aerospace companies like oboronprom and the united aircraft corporation (encompassing mikoyan, sukhoi, ilyushin, tupolev, yakovlev, and irkut, which includes beriev) are among the major global players in this industry. in the us, the department of defense and nasa are the two biggest consumers of aerospace technology and products. the bureau of labor statistics of the united states reported that the aerospace industry employed 444,000 wage and salary jobs in 2004, many of which were in washington and california, this marked a steep decline from the peak years during the reagan administration when total employment exceeded 1,000,000 aerospace industry workers.during that period of recovery a special program to restore u.s. competitiveness across all u.s. industries, project socrates, contributed to employment growth as the u.s. aerospace industry captured 72 percent of world aerospace market.  by 1999 u.s. share of the world market fell to 52 percent. important locations of the civil aerospace industry worldwide include seattle, wichita, kansas, dayton, ohio and st. louis in the united states (boeing), montreal and toronto in canada (bombardier, pratt & whitney canada), toulouse and bordeaux in france (airbus, dassault, atr), seville in spain and hamburg in germany (airbus, eads), the north-west of england and bristol in britain (bae systems, airbus and agustawestland), komsomolsk-on-amur and irkutsk in russia (sukhoi, beriev), kyiv and kharkiv in ukraine (antonov), nagoya in japan (mitsubishi heavy industries aerospace and kawasaki heavy industries aerospace), as well as são josé dos campos in brazil where embraer is based.   == consolidation == several consolidations took place in the aerospace and defense industries over the last few decades. bae systems is the successor company to numerous british aircraft manufacturers which merged throughout the second half of the 20th century. many of these mergers followed the 1957 defence white paper.airbus prominently illustrated the european airliner manufacturing consolidation in the late 1960s.between 1988 and 2010, more than 5,452 mergers and acquisitions with a total known-value of us$579 billion were announced worldwide.in 1993, then united states secretary of defense les aspin and his deputy william j. perry held the ""last supper"" at the pentagon with contractors executives who were told that there were twice as many military suppliers as he wanted to see: $55 billion in military-industry mergers took place from 1992 to 1997, leaving mainly boeing, lockheed martin, northrop grumman and raytheon.boeing bought mcdonnell douglas for us$13.3 billion in 1996.raytheon acquired hughes aircraft company for $9.5 billion in 1997.marconi electronic systems, a subsidiary of the general electric company plc, was acquired by british aerospace for us$12.3 billion in 1999 merger, to form bae systems. in 2002, when fairchild dornier was bankrupt, airbus, boeing or bombardier declined to take the 728jet/928jet large regional jet program as mainline and regional aircraft manufacturers were split and airbus was digesting its ill-fated fokker acquisition a decade earlier.on september 4, 2017, united technologies acquired rockwell collins in cash and stock for $23 billion, $30 billion including rockwell collins\' net debt, for $500million of synergies expected by year four. the oct. 16, 2017 announcement of the cseries partnership between airbus and bombardier aerospace could trigger a daisy chain of reactions towards a new order. airbus gets a new, efficient model at the lower end of the narrowbody market which provides the bulk of airliner profits and can abandon the slow selling a319 while bombardier benefits from the growth in this expanded market even if it holds a smaller residual stake. boeing could forge a similar alliance with either embraer with its e-jet e2 or mitsubishi heavy industries and its mrj.on 21 december, boeing and embraer confirmed to be discussing a potential combination with a transaction subject to brazilian government regulators, the companies\' boards and shareholders approvals. the weight of airbus and boeing could help e2 and cseries sales but the 100-150 seats market seems slow. as the cseries, renamed a220, and e-jet e2 are more capable than their predecessors, they moved closer to the lower end of the narrowbodies. in 2018, the four western airframers combined into two within nine months as boeing acquired 80% of embraer\'s airliners for $3.8 billion on july 5.on april 3, 2020, raytheon and united technologies corporation (except otis worldwide, leaving rockwell collins and engine maker pratt and whitney)  merged to form raytheon technologies corporation, with combined sales of $79 billion in 2019.the most prominent unions between 1995 and 2020 include those of boeing and mcdonnell douglas; the french, german and spanish parts of eads; and united technologies with rockwell collins then raytheon, but many mergers projects did not went through: textron-bombardier, eads-bae systems, hawker beechcraft-superior aviation, ge-honeywell, bae systems-boeing (or lockheed martin), dassault-aerospatiale, safran-thales, bae systems-rolls-royce or lockheed martin–northrop grumman.   == suppliers == the largest aerospace suppliers are united technologies with $28.2 billion of revenue, followed by ge aviation with $24.7 billion, safran with $22.5 billion, rolls-royce holdings with $16.9 billion, honeywell aerospace with $15.2 billion and rockwell collins including b/e aerospace with $8.1 billion. the electric aircraft development could generate large changes for the aerospace suppliers.on 26 november 2018, united technologies announced the completion of its rockwell collins acquisition, renaming systems supplier utc aerospace systems as collins aerospace, for $23 billion of sales in 2017 and 70,000 employees, and $39.0 billion of sales in 2017 combined with engine manufacturer pratt & whitney.   == supply chain == before the 1980s/1990s, aircraft and aeroengine manufacturers were vertically integrated. then douglas aircraft outsourced large aerostructures and the bombardier global express pioneered the ""tier 1"" supply chain model inspired by automotive industry, with 10-12 risk-sharing limited partners funding around half of the development costs. the embraer e-jet followed in the late 1990s with fewer than 40 primary suppliers. tier 1 suppliers were led by honeywell, safran, goodrich corporation and hamilton sundstrand.in the 2000s rolls-royce reduced its supplier count after bringing in automotive supply chain executives. on the airbus a380, less than 100 major suppliers outsource 60% of its value, even 80% on the a350xwb. boeing embraced an aggressive tier 1 model for the b787 but with its difficulties began to question why it was earning lower margins than its suppliers while it seemed to take all the risk, ensuing its 2011 partnering for success initiative, as airbus initiated its own scopeinitiative for the a320. tier 1 consolidation also affects engine manufacturers : ge aviation acquired avio in 2013 and rolls-royce plc is taking control of industria de turbo propulsores.   == see also == aerospace aviation accidents and incidents list of aircraft manufacturers list of spacecraft manufacturers military-industrial complex aircraft parts industry aerospace industry of russia aviation   == references ==   == further reading ==   == external links == ""u.s. aerospace industries association"". ""aerospace, defense & government services – mergers & acquisitions (january 1993 - december 2016)"" (pdf). grundman advisory. 6 apr 2017. jens flottau (feb 22, 2018). ""opinion: airframers should watch where they squeeze suppliers"". aviation week & space technology. aerospace craft & structural components')"
6,"In fluid dynamics, a stall is a reduction in the lift coefficient generated by a foil as angle of attack increases. This occurs when the critical angle of attack of the foil is exceeded.  The critical angle of attack is typically about 15 degrees, but it may vary significantly depending on the fluid, foil, and Reynolds number.
Stalls in fixed-wing flight are often experienced as a sudden reduction in lift as the pilot increases the wing's angle of attack and exceeds its critical angle of attack (which may be due to slowing down below stall speed in level flight). A stall does not mean that the engine(s) have stopped working, or that the aircraft has stopped moving—the effect is the same even in an unpowered glider aircraft. Vectored thrust in manned and unmanned aircraft is used to maintain altitude or controlled flight with wings stalled by replacing lost wing lift with engine or propeller thrust, thereby giving rise to post-stall technology.Because stalls are most commonly discussed in connection with aviation, this article discusses stalls as they relate mainly to aircraft, in particular fixed-wing aircraft. The principles of stall discussed here translate to foils in other fluids as well.


== Formal definition ==

A stall is a condition in aerodynamics and aviation such that if the angle of attack increases beyond a certain point, then lift begins to decrease. The angle at which this occurs is called the critical angle of attack. This angle is dependent upon the airfoil section or profile of the wing, its planform, its aspect ratio, and other factors, but is typically in the range of 8 to 20 degrees relative to the incoming wind (""relative wind"") for most subsonic airfoils. The critical angle of attack is the angle of attack on the lift coefficient versus angle-of-attack (Cl~alpha) curve at which the maximum lift coefficient occurs.Stalling is caused by flow separation which, in turn, is caused by the air flowing against a rising pressure. Whitford describes three types of stall: trailing-edge, leading-edge and thin-aerofoil, each with distinctive Cl~alpha features. For the trailing-edge stall, separation begins at small angles of attack near the trailing edge of the wing while the rest of the flow over the wing remains attached. As angle of attack increases, the separated regions on the top of the wing increase in size as the flow separation moves forward, and this hinders the ability of the wing to create lift. This is shown by the reduction in lift-slope on a Cl~alpha curve as the lift nears its maximum value. The separated flow usually causes buffeting. Beyond the critical angle of attack, separated flow is so dominant that additional increases in angle of attack cause the lift to fall from its peak value.
Piston-engined and early jet transports had very good stall behaviour with pre-stall buffet warning and, if ignored, a straight nose-drop for a natural recovery. Wing developments that came with the introduction of turbo-prop engines introduced unacceptable stall behaviour. Leading-edge developments on high-lift wings, and the introduction of rear-mounted engines and high-set tailplanes on the next generation of jet transports, also introduced unacceptable stall behaviour. The probability of achieving the stall speed inadvertently, a potentially hazardous event, had been calculated, in 1965, at about once in every 100,000 flights, often enough to justify the cost of development of warning devices, such as stick shakers, and devices to automatically provide an adequate nose-down pitch, such as stick pushers.When the mean angle of attack of the wings is beyond the stall a spin, which is an autorotation of a stalled wing, may develop. A spin follows departures in roll, yaw and pitch from balanced flight. For example, a roll is naturally damped with an unstalled wing, but with wings stalled the damping moment is replaced with a propelling moment.


== Variation of lift with angle of attack ==

The graph shows that the greatest amount of lift is produced as the critical angle of attack is reached (which in early-20th century aviation was called the ""burble point""). This angle is 17.5 degrees in this case, but it varies from airfoil to airfoil. In particular, for aerodynamically thick airfoils (thickness to chord ratios of around 10%), the critical angle is higher than with a thin airfoil of the same camber. Symmetric airfoils have lower critical angles (but also work efficiently in inverted flight). The graph shows that, as the angle of attack exceeds the critical angle, the lift produced by the airfoil decreases.
The information in a graph of this kind is gathered using a model of the airfoil in a wind tunnel. Because aircraft models are normally used, rather than full-size machines, special care is needed to make sure that data is taken in the same Reynolds number regime (or scale speed) as in free flight. The separation of flow from the upper wing surface at high angles of attack is quite different at low Reynolds number from that at the high Reynolds numbers of real aircraft. In particular at high Reynolds numbers the flow tends to stay attached to the airfoil for longer because the inertial forces are dominant with respect to the viscous forces which are responsible for the flow separation ultimately leading to the aerodynamic stall. For this reason wind tunnel results carried out at lower speeds and on smaller scales models of the real life counterparts often tend to overestimate the aerodynamic stall angle of attack. High-pressure wind tunnels are one solution to this problem.
In general, steady operation of an aircraft at an angle of attack above the critical angle is not possible because, after exceeding the critical angle, the loss of lift from the wing causes the nose of the aircraft to fall, reducing the angle of attack again. This nose drop, independent of control inputs, indicates the pilot has actually stalled the aircraft.This graph shows the stall angle, yet in practice most pilot operating handbooks (POH) or generic flight manuals describe stalling in terms of airspeed. This is because all aircraft are equipped with an airspeed indicator, but fewer aircraft have an angle of attack indicator. An aircraft's stalling speed is published by the manufacturer (and is required for certification by flight testing) for a range of weights and flap positions, but the stalling angle of attack is not published.
As speed reduces, angle of attack has to increase to keep lift constant until the critical angle is reached. The airspeed at which this angle is reached is the (1g, unaccelerated) stalling speed of the aircraft in that particular configuration. Deploying flaps/slats decreases the stall speed to allow the aircraft to take off and land at a lower speed.


== Aerodynamic description ==


=== Fixed-wing aircraft ===
A fixed-wing aircraft can be made to stall in any pitch attitude or bank angle or at any airspeed but deliberate stalling is commonly practiced by reducing the speed to the unaccelerated stall speed, at a safe altitude. Unaccelerated (1g) stall speed varies on different fixed-wing aircraft and is represented by colour codes on the airspeed indicator. As the plane flies at this speed, the angle of attack must be increased to prevent any loss of altitude or gain in airspeed (which corresponds to the stall angle described above). The pilot will notice the flight controls have become less responsive and may also notice some buffeting, a result of the turbulent air separated from the wing hitting the tail of the aircraft.
In most light aircraft, as the stall is reached, the aircraft will start to descend (because the wing is no longer producing enough lift to support the aircraft's weight) and the nose will pitch down. Recovery from the stall involves lowering the aircraft nose, to decrease the angle of attack and increase the air speed, until smooth air-flow over the wing is restored. Normal flight can be resumed once recovery is complete. The maneuver is normally quite safe, and, if correctly handled, leads to only a small loss in altitude (20–30 m/50–100 ft). It is taught and practised in order for pilots to recognize, avoid, and recover from stalling the aircraft. A pilot is required to demonstrate competency in controlling an aircraft during and after a stall for certification in the United States, and it is a routine maneuver for pilots when getting to know the handling of an unfamiliar aircraft type. The only dangerous aspect of a stall is a lack of altitude for recovery.
A special form of asymmetric stall in which the aircraft also rotates about its yaw axis is called a spin. A spin can occur if an aircraft is stalled and there is an asymmetric yawing moment applied to it. This yawing moment can be aerodynamic (sideslip angle, rudder, adverse yaw from the ailerons), thrust related (p-factor, one engine inoperative on a multi-engine non-centreline thrust aircraft), or from less likely sources such as severe turbulence. The net effect is that one wing is stalled before the other and the aircraft descends rapidly while rotating, and some aircraft cannot recover from this condition without correct pilot control inputs (which must stop yaw) and loading. A new solution to the problem of difficult (or impossible) stall-spin recovery is provided by the ballistic parachute recovery system.
The most common stall-spin scenarios occur on takeoff (departure stall) and during landing (base to final turn) because of insufficient airspeed during these maneuvers. Stalls also occur during a go-around manoeuvre if the pilot does not properly respond to the out-of-trim situation resulting from the transition from low power setting to high power setting at low speed. Stall speed is increased when the wing surfaces are contaminated with ice or frost creating a rougher surface, and heavier airframe due to ice accumulation.
Stalls occur not only at slow airspeed, but at any speed when the wings exceed their critical angle of attack. Attempting to increase the angle of attack at 1g by moving the control column back normally causes the aircraft to climb. However, aircraft often experience higher g-forces, such as when turning steeply or pulling out of a dive. In these cases, the wings are already operating at a higher angle of attack to create the necessary force (derived from lift) to accelerate in the desired direction. Increasing the g-loading still further, by pulling back on the controls, can cause the stalling angle to be exceeded, even though the aircraft is flying at a high speed. These ""high-speed stalls"" produce the same buffeting characteristics as 1g stalls and can also initiate a spin if there is also any yawing.


=== Characteristics ===
Different aircraft types have different stalling characteristics but they only have to be good enough to satisfy their particular Airworthiness authority. For example, the Short Belfast heavy freighter  had a marginal nose drop which was acceptable to the Royal Air Force. When the aircraft were sold to a civil operator they had to be fitted with a stick pusher to meet the civil requirements. Some aircraft may naturally have very good behaviour well beyond what is required. For example, first generation jet transports have been described as having an immaculate nose drop at the stall. Loss of lift on one wing is acceptable as long as the roll, including during stall recovery, doesn't exceed about 20 degrees, or in turning flight the roll shall not exceed 90 degrees bank. If pre-stall warning followed by nose drop and limited wing drop are naturally not present or are deemed to be unacceptably marginal by an Airworthiness authority the stalling behaviour has to be made good enough with airframe modifications or devices such as a stick shaker and pusher. These are described in ""Warning and safety devices"".


== Stall speeds ==

Stalls depend only on angle of attack, not airspeed. However, the slower an aircraft flies, the greater the angle of attack it needs to produce lift equal to the aircraft's weight. As the speed decreases further, at some point this angle will be equal to the critical (stall) angle of attack. This speed is called the ""stall speed"". An aircraft flying at its stall speed cannot climb, and an aircraft flying below its stall speed cannot stop descending. Any attempt to do so by increasing angle of attack, without first increasing airspeed, will result in a stall.
The actual stall speed will vary depending on the airplane's weight, altitude, configuration, and vertical and lateral acceleration. Speed definitions vary and include:

VS: Stall speed: the speed at which the airplane exhibits those qualities accepted as defining the stall.
VS0: The stall speed or minimum steady flight speed in landing configuration. The zero-thrust stall speed at the most extended landing flap setting.
VS1: The stall speed or minimum steady flight speed obtained in a specified configuration. The zero thrust stall speed at a specified flap setting.An airspeed indicator, for the purpose of flight-testing, may have the following markings: the bottom of the white arc indicates VS0 at maximum weight, while the bottom of the green arc indicates VS1 at maximum weight. While an aircraft's VS speed is computed by design, its VS0 and VS1 speeds must be demonstrated empirically by flight testing.


== In accelerated and turning flight ==

The normal stall speed, specified by the VS values above, always refers to straight and level flight, where the load factor is equal to 1g. However, if the aircraft is turning or pulling up from a dive, additional lift is required to provide the vertical or lateral acceleration, and so the stall speed is higher. An accelerated stall is a stall that occurs under such conditions.In a banked turn, the lift required is equal to the weight of the aircraft plus extra lift to provide the centripetal force necessary to perform the turn:

  
    
      
        L
        =
        n
        W
      
    
    {\displaystyle L=nW}
  where:

  
    
      
        L
      
    
    {\displaystyle L}
   = lift

  
    
      
        n
      
    
    {\displaystyle n}
   = load factor (greater than 1 in a turn)

  
    
      
        W
      
    
    {\displaystyle W}
   = weight of the aircraftTo achieve the extra lift, the lift coefficient, and so the angle of attack, will have to be higher than it would be in straight and level flight at the same speed. Therefore, given that the stall always occurs at the same critical angle of attack, by increasing the load factor (e.g. by tightening the turn) the critical angle will be reached at a higher airspeed:

  
    
      
        
          V
          
            st
          
        
        =
        
          V
          
            s
          
        
        
          
            n
          
        
      
    
    {\displaystyle V_{\text{st}}=V_{\text{s}}{\sqrt {n}}}
  where:

  
    
      
        
          V
          
            st
          
        
      
    
    {\displaystyle V_{\text{st}}}
   = stall speed

  
    
      
        
          V
          
            s
          
        
      
    
    {\displaystyle V_{\text{s}}}
   = stall speed of the aircraft in straight, level flight

  
    
      
        n
      
    
    {\displaystyle n}
   = load factorThe table that follows gives some examples of the relation between the angle of bank and the square root of the load factor. It derives from the trigonometric relation (secant) between 
  
    
      
        L
      
    
    {\displaystyle L}
   and 
  
    
      
        W
      
    
    {\displaystyle W}
  .

For example, in a turn with bank angle of 45°, Vst is 19% higher than Vs.
According to Federal Aviation Administration (FAA) terminology, the above example illustrates a so-called turning flight stall, while the term accelerated is used to indicate an accelerated turning stall only, that is, a turning flight stall where the airspeed decreases at a given rate.Accelerated stalls also pose a risk in powerful propeller aircraft with a tendency to roll in reaction to engine torque. When such an aircraft is flying close to its stall speed in straight and level flight, the sudden application of full power may roll the aircraft and create the same aerodynamic conditions that induce an accelerated stall in turning flight. An aircraft that displays this rolling tendency is the Mitsubishi MU-2; pilots of this aircraft are trained to avoid sudden and drastic increases in power at low altitude and low airspeed, as an accelerated stall under these conditions is very difficult to safely recover from.A notable example of an air accident involving a low-altitude turning flight stall is the 1994 Fairchild Air Force Base B-52 crash.


== Types ==


=== Dynamic stall ===
Dynamic stall is a non-linear unsteady aerodynamic effect that occurs when airfoils rapidly change the angle of attack. The rapid change can cause a strong vortex to be shed from the leading edge of the aerofoil, and travel backwards above the wing. The vortex, containing high-velocity airflows, briefly increases the lift produced by the wing. As soon as it passes behind the trailing edge, however, the lift reduces dramatically, and the wing is in normal stall.Dynamic stall is an effect most associated with helicopters and flapping wings, though also occurs in wind turbines, and due to gusting airflow. During forward flight, some regions of a helicopter blade may incur flow that reverses (compared to the direction of blade movement), and thus includes rapidly changing angles of attack. Oscillating (flapping) wings, such as those of insects like the bumblebee—may rely almost entirely on dynamic stall for lift production, provided the oscillations are fast compared to the speed of flight, and the angle of the wing changes rapidly compared to airflow direction.Stall delay can occur on airfoils subject to a high angle of attack and a three-dimensional flow. When the angle of attack on an airfoil is increasing rapidly, the flow will remain substantially attached to the airfoil to a significantly higher angle of attack than can be achieved in steady-state conditions. As a result, the stall is delayed momentarily and a lift coefficient significantly higher than the steady-state maximum is achieved. The effect was first noticed on propellers.


=== Deep stall ===

A deep stall (or super-stall) is a dangerous type of stall that affects certain aircraft designs, notably jet aircraft with a T-tail configuration and rear-mounted engines. In these designs, the turbulent wake of a stalled main wing, nacelle-pylon wakes and the wake from the fuselage ""blanket"" the horizontal stabilizer, rendering the elevators ineffective and preventing the aircraft from recovering from the stall. Taylor states T-tail propeller aircraft, unlike jet aircraft, do not usually require a stall recovery system during stall flight testing due to increased airflow over the wing root from the prop wash. Nor do they have rear mounted nacelles which can contribute substantially to the problem. The A400M was fitted with a vertical tail booster for some flight tests in case of deep stall.Trubshaw gives a broad definition of deep stall as penetrating to such angles of attack 
  
    
      
        α
      
    
    {\textstyle \alpha }
   that pitch control effectiveness is reduced by the wing and nacelle wakes. He also gives a definition that relates deep stall to a locked-in condition where recovery is impossible. This is a single value of 
  
    
      
        α
      
    
    {\textstyle \alpha }
  , for a given aircraft configuration, where there is no pitching moment, i.e. a trim point.
Typical values both for the range of deep stall, as defined above, and the locked-in trim point are given for the Douglas DC-9 Series 10 by Schaufele. These values are from wind tunnel tests for an early design. The final design had no locked in trim point so recovery from the deep stall region was possible, as required to meet certification rules. Normal stall beginning at the 'g' break (sudden decrease of the vertical load factor) was at 18 degrees 
  
    
      
        α
      
    
    {\textstyle \alpha }
  , deep stall started at about 30 degrees and the locked-in unrecoverable trim point was at 47 degrees.
The very high 
  
    
      
        α
      
    
    {\textstyle \alpha }
   for a deep stall locked-in condition occurs well beyond the normal stall but can be attained very rapidly as the aircraft is unstable beyond the normal stall and requires immediate action to arrest it. The loss of lift causes high sink rates which, together with the low forward speed at the normal stall, give a high 
  
    
      
        α
      
    
    {\textstyle \alpha }
   with little or no rotation of the aircraft. BAC 1-11 G-ASHG, during stall flight tests before the type was modified to prevent a locked-in deep stall condition, descended at over 10,000 feet per minute (50 m/s) and struck the ground in a flat attitude moving only 70 feet (20 m) forward after initial impact. Sketches which show how the wing wake blankets the tail may be misleading if they imply that deep stall requires a high body angle. Taylor and Ray show how the aircraft attitude in the deep stall is relatively flat, even less than during the normal stall, with very high negative flight path angles.
Effects similar to deep stall had been known to occur on some aircraft designs before the term was coined. A prototype Gloster Javelin (serial WD808) was lost in a crash on 11 June 1953, to a ""locked in"" stall. However, Waterton states that the trimming tailplane was found to be the wrong way for recovery. Low speed handling tests were being done to assess a new wing. Handley Page Victor XL159 was lost to a ""stable stall"" on 23 March 1962. It had been clearing the fixed droop leading edge with the test being stall approach, landing configuration, C of G aft. The brake parachute had not been streamed as it may have hindered rear crew escape.The name ""deep stall"" first came into widespread use after the crash of the prototype BAC 1-11 G-ASHG on 22 October 1963, which killed its crew. This led to changes to the aircraft, including the installation of a stick shaker (see below) to clearly warn the pilot of an impending stall. Stick shakers are now a standard part of commercial airliners. Nevertheless, the problem continues to cause accidents; on 3 June 1966, a Hawker Siddeley Trident (G-ARPY), was lost to deep stall; deep stall is suspected to be cause of another Trident (the British European Airways Flight 548 G-ARPI) crash – known as the ""Staines Disaster"" – on 18 June 1972 when the crew failed to notice the conditions and had disabled the stall recovery system. On 3 April 1980, a prototype of the Canadair Challenger business jet crashed after initially entering a deep stall from 17,000 ft and having both engines flame-out. It recovered from the deep stall after deploying the anti-spin parachute but crashed after being unable to jettison the chute or relight the engines. One of the test pilots was unable to escape from the aircraft in time and was killed. On the 26 July 1993, a Canadair CRJ-100 was lost in flight testing due to a deep stall. It has been reported that a Boeing 727 entered a deep stall in a flight test, but the pilot was able to rock the airplane to increasingly higher bank angles until the nose finally fell through and normal control response was recovered. A 727 accident on 1 December 1974, has also been attributed to a deep stall. The crash of West Caribbean Airways Flight 708 in 2005 was also attributed to a deep stall.
Deep stalls can occur at apparently normal pitch attitudes, if the aircraft is descending quickly enough. The airflow is coming from below, so the angle of attack is increased. Early speculation on reasons for the crash of Air France Flight 447 blamed an unrecoverable deep stall since it descended in an almost flat attitude (15 degrees) at an angle of attack of 35 degrees or more. However it was held in a stalled glide by the pilots who held the nose up amid all the confusion of what was actually happening to the aircraft.Canard-configured aircraft are also at risk of getting into a deep stall. Two Velocity aircraft crashed due to locked-in deep stalls. Testing revealed that the addition of leading-edge cuffs to the outboard wing prevented the aircraft from getting into a deep stall. The Piper Advanced Technologies PAT-1, N15PT, another canard-configured aircraft, also crashed in an accident attributed to a deep stall. Wind tunnel testing of the design at the NASA Langley Research Center showed that it was vulnerable to a deep stall.In the early 1980s, a Schweizer SGS 1-36 sailplane was modified for NASA's controlled deep-stall flight program.


=== Tip stall ===
Wing sweep and taper cause stalling at the tip of a wing before the root. The position of a swept wing along the fuselage has to be such that the lift from the wing root, well forward of the aircraft center of gravity (c.g.), must be balanced by the wing tip, well aft of the c.g. If the tip stalls first the balance of the aircraft is upset causing dangerous nose pitch up. Swept wings have to incorporate features which prevent pitch-up caused by premature tip stall.
A swept wing has a higher lift coefficient on its outer panels than on the inner wing, causing them to reach their maximum lift capability first and to stall first. This is caused by the downwash pattern associated with swept/tapered wings. To delay tip stall the outboard wing is given washout to reduce its angle of attack. The root can also be modified with a suitable leading-edge and airfoil section to make sure it stalls before the tip. However, when taken beyond stalling incidence the tips may still become fully stalled before the inner wing despite initial separation occurring inboard. This causes pitch-up after the stall and entry to a super-stall on those aircraft with super-stall characteristics. Span-wise flow of the boundary layer is also present on swept wings and causes tip stall. The amount of boundary layer air flowing outboard can be reduced by generating vortices with a leading-edge device such as a fence, notch, saw tooth or a set of vortex generators behind the leading edge.


== Warning and safety devices ==
Fixed-wing aircraft can be equipped with devices to prevent or postpone a stall or to make it less (or in some cases more) severe, or to make recovery easier.

An aerodynamic twist can be introduced to the wing with the leading edge near the wing tip twisted downward. This is called washout and causes the wing root to stall before the wing tip. This makes the stall gentle and progressive. Since the stall is delayed at the wing tips, where the ailerons are, roll control is maintained when the stall begins.
A stall strip is a small sharp-edged device that, when attached to the leading edge of a wing, encourages the stall to start there in preference to any other location on the wing. If attached close to the wing root, it makes the stall gentle and progressive; if attached near the wing tip, it encourages the aircraft to drop a wing when stalling.
A stall fence is a flat plate in the direction of the chord to stop separated flow progressing out along the wing
Vortex generators, tiny strips of metal or plastic placed on top of the wing near the leading edge that protrude past the boundary layer into the free stream. As the name implies, they energize the boundary layer by mixing free stream airflow with boundary layer flow thereby creating vortices, this increases the momentum in the boundary layer. By increasing the momentum of the boundary layer, airflow separation and the resulting stall may be delayed.
An anti-stall strake is a leading edge extension that generates a vortex on the wing upper surface to postpone the stall.
A stick pusher is a mechanical device that prevents the pilot from stalling an aircraft. It pushes the elevator control forward as the stall is approached, causing a reduction in the angle of attack. In generic terms, a stick pusher is known as a stall identification device or stall identification system.
A stick shaker is a mechanical device that shakes the pilot's controls to warn of the onset of stall.
A stall warning is an electronic or mechanical device that sounds an audible warning as the stall speed is approached. The majority of aircraft contain some form of this device that warns the pilot of an impending stall. The simplest such device is a stall warning horn, which consists of either a pressure sensor or a movable metal tab that actuates a switch, and produces an audible warning in response.
An angle-of-attack indicator for light aircraft, the ""AlphaSystemsAOA"" and a nearly identical ""Lift Reserve Indicator"", are both pressure differential instruments that display margin above stall and/or angle of attack on an instantaneous, continuous readout. The General Technics CYA-100 displays true angle of attack via a magnetically coupled vane. An AOA indicator provides a visual display of the amount of available lift throughout its slow speed envelope regardless of the many variables that act upon an aircraft. This indicator is immediately responsive to changes in speed, angle of attack, and wind conditions, and automatically compensates for aircraft weight, altitude, and temperature.
An angle of attack limiter or an ""alpha"" limiter is a flight computer that automatically prevents pilot input from causing the plane to rise over the stall angle. Some alpha limiters can be disabled by the pilot.Stall warning systems often involve inputs from a broad range of sensors and systems to include a dedicated angle of attack sensor.
Blockage, damage, or inoperation of stall and angle of attack (AOA) probes can lead to unreliability of the stall warning, and cause the stick pusher, overspeed warning, autopilot, and yaw damper to malfunction.If a forward canard is used for pitch control, rather than an aft tail, the canard is designed to meet the airflow at a slightly greater angle of attack than the wing. Therefore, when the aircraft pitch increases abnormally, the canard will usually stall first, causing the nose to drop and so preventing the wing from reaching its critical AOA. Thus, the risk of main wing stalling is greatly reduced. However, if the main wing stalls, recovery becomes difficult, as the canard is more deeply stalled and angle of attack increases rapidly.If an aft tail is used, the wing is designed to stall before the tail. In this case, the wing can be flown at higher lift coefficient (closer to stall) to produce more overall lift.
Most military combat aircraft have an angle of attack indicator among the pilot's instruments, which lets the pilot know precisely how close to the stall point the aircraft is. Modern airliner instrumentation may also measure angle of attack, although this information may not be directly displayed on the pilot's display, instead driving a stall warning indicator or giving performance information to the flight computer (for fly by wire systems).


== Flight beyond the stall ==
As a wing stalls, aileron effectiveness is reduced, rendering the plane difficult to control and increasing the risk of a spin. Post stall, steady flight beyond the stalling angle (where the coefficient of lift is largest) requires engine thrust to replace lift as well as alternative controls to replace the loss of effectiveness of the ailerons. For high-powered aircraft, the loss of lift (and increase in drag) beyond the stall angle is less of a problem than maintaining control. Some aircraft may be subject to post-stall gyration (e.g. the F-4) or susceptible to entering a flat-spin (e.g. F-14). Control beyond-stall can be provided by reaction control systems (e.g. NF-104A), vectored thrust, as well as a rolling stabilator (or taileron). The enhanced manoeuvering capability by flights at very high angles of attack can provide a tactical advantage for military fighters such as the F-22 Raptor. Short-term stalls at 90–120° (e.g. Pugachev's Cobra) are sometimes performed at airshows. The highest angle of attack in sustained flight so far demonstrated was 70 degrees in the X-31 at the Dryden Flight Research Center.  Sustained post-stall flight is a type of supermaneuverability.


== Spoilers ==

Except for flight training, airplane testing, and aerobatics, a stall is usually an undesirable event. Spoilers (sometimes called lift dumpers), however, are devices that are intentionally deployed to create a carefully controlled flow separation over part of an aircraft's wing to reduce the lift it generates, increase the drag, and allow the aircraft to descend more rapidly without gaining speed. Spoilers are also deployed asymmetrically (one wing only) to enhance roll control. Spoilers can also be used on aborted take-offs and after main wheel contact on landing to increase the aircraft's weight on its wheels for better braking action.
Unlike powered airplanes, which can control descent by increasing or decreasing thrust, gliders have to increase drag to increase the rate of descent. In high-performance gliders, spoiler deployment is extensively used to control the approach to landing.
Spoilers can also be thought of as  ""lift reducers"" because they reduce the lift of the wing in which the spoiler resides. For example, an uncommanded roll to the left could be reversed by raising the right wing spoiler (or only a few of the spoilers present in large airliner wings). This has the advantage of avoiding the need to increase lift in the wing that is dropping (which may bring that wing closer to stalling).


== History ==
Otto Lilienthal died while flying in 1896 as the result of a stall. Wilbur Wright encountered stalls for the first time in 1901, while flying his second glider. Awareness of Lilienthal's accident and Wilbur's experience, motivated the Wright Brothers to design their plane in ""canard"" configuration. This made recoveries from stalls easier and more gentle. The design saved the brothers' lives more than once.The aircraft engineer Juan de la Cierva worked on his ""Autogiro"" project to develop a rotary wing aircraft which, he hoped, would be unable to stall and which therefore would be safer than aeroplanes. In developing the resulting ""autogyro"" aircraft, he solved many engineering problems which made the helicopter possible.


== See also ==
ArticlesAviation safety
Coffin corner (aerodynamics)
Compressor stall
Lift coefficient
Spin (flight)
Spoiler (aeronautics)
Wing twistNotable accidents1963 BAC One-Eleven test crash
1966 Felthorpe Trident crash
British European Airways Flight 548
China Airlines Flight 140
China Airlines Flight 676
Air France Flight 447
Colgan Air Flight 3407
Turkish Airlines Flight 1951
Indonesia AirAsia Flight 8501


== Notes ==


== References ==
USAF & NATO Report RTO-TR-015 AC/323/(HFM-015)/TP-1 (2001
Anderson, J.D., A History of Aerodynamics (1997). Cambridge University Press.  ISBN 0-521-66955-3
Chapter 4, ""Slow Flight, Stalls, and Spins,"" in the Airplane Flying Handbook. (FAA H-8083-3A)
L. J. Clancy (1975), Aerodynamics, Pitman Publishing Limited, London.  ISBN 0-273-01120-0
Stengel, R. (2004), Flight Dynamics, Princeton University Press, ISBN 0-691-11407-2
Alpha Systems AOA Website for information on AOA and Lift Reserve Indicators [1]
4239-01 Angle of Attack (AoA) Sensor Specifications [2]
Airplane flying Handbook. Federal Aviation Administration ISBN 1-60239-003-7  Pub. Skyhorse Publishing Inc.
Federal Aviation Administration (25 September 2000), Stall and Spin Awareness Training, AC No: 61-67C
Prof. Dr Mustafa Cavcar, ""Stall Speed"" [3]","pandas(index=6, _1=6, text='in fluid dynamics, a stall is a reduction in the lift coefficient generated by a foil as angle of attack increases. this occurs when the critical angle of attack of the foil is exceeded.  the critical angle of attack is typically about 15 degrees, but it may vary significantly depending on the fluid, foil, and reynolds number. stalls in fixed-wing flight are often experienced as a sudden reduction in lift as the pilot increases the wing\'s angle of attack and exceeds its critical angle of attack (which may be due to slowing down below stall speed in level flight). a stall does not mean that the engine(s) have stopped working, or that the aircraft has stopped moving—the effect is the same even in an unpowered glider aircraft. vectored thrust in manned and unmanned aircraft is used to maintain altitude or controlled flight with wings stalled by replacing lost wing lift with engine or propeller thrust, thereby giving rise to post-stall technology.because stalls are most commonly discussed in connection with aviation, this article discusses stalls as they relate mainly to aircraft, in particular fixed-wing aircraft. the principles of stall discussed here translate to foils in other fluids as well.   == formal definition ==  a stall is a condition in aerodynamics and aviation such that if the angle of attack increases beyond a certain point, then lift begins to decrease. the angle at which this occurs is called the critical angle of attack. this angle is dependent upon the airfoil section or profile of the wing, its planform, its aspect ratio, and other factors, but is typically in the range of 8 to 20 degrees relative to the incoming wind (""relative wind"") for most subsonic airfoils. the critical angle of attack is the angle of attack on the lift coefficient versus angle-of-attack (cl~alpha) curve at which the maximum lift coefficient occurs.stalling is caused by flow separation which, in turn, is caused by the air flowing against a rising pressure. whitford describes three types of stall: trailing-edge, leading-edge and thin-aerofoil, each with distinctive cl~alpha features. for the trailing-edge stall, separation begins at small angles of attack near the trailing edge of the wing while the rest of the flow over the wing remains attached. as angle of attack increases, the separated regions on the top of the wing increase in size as the flow separation moves forward, and this hinders the ability of the wing to create lift. this is shown by the reduction in lift-slope on a cl~alpha curve as the lift nears its maximum value. the separated flow usually causes buffeting. beyond the critical angle of attack, separated flow is so dominant that additional increases in angle of attack cause the lift to fall from its peak value. piston-engined and early jet transports had very good stall behaviour with pre-stall buffet warning and, if ignored, a straight nose-drop for a natural recovery. wing developments that came with the introduction of turbo-prop engines introduced unacceptable stall behaviour. leading-edge developments on high-lift wings, and the introduction of rear-mounted engines and high-set tailplanes on the next generation of jet transports, also introduced unacceptable stall behaviour. the probability of achieving the stall speed inadvertently, a potentially hazardous event, had been calculated, in 1965, at about once in every 100,000 flights, often enough to justify the cost of development of warning devices, such as stick shakers, and devices to automatically provide an adequate nose-down pitch, such as stick pushers.when the mean angle of attack of the wings is beyond the stall a spin, which is an autorotation of a stalled wing, may develop. a spin follows departures in roll, yaw and pitch from balanced flight. for example, a roll is naturally damped with an unstalled wing, but with wings stalled the damping moment is replaced with a propelling moment.   == variation of lift with angle of attack ==  the graph shows that the greatest amount of lift is produced as the critical angle of attack is reached (which in early-20th century aviation was called the ""burble point""). this angle is 17.5 degrees in this case, but it varies from airfoil to airfoil. in particular, for aerodynamically thick airfoils (thickness to chord ratios of around 10%), the critical angle is higher than with a thin airfoil of the same camber. symmetric airfoils have lower critical angles (but also work efficiently in inverted flight). the graph shows that, as the angle of attack exceeds the critical angle, the lift produced by the airfoil decreases. the information in a graph of this kind is gathered using a model of the airfoil in a wind tunnel. because aircraft models are normally used, rather than full-size machines, special care is needed to make sure that data is taken in the same reynolds number regime (or scale speed) as in free flight. the separation of flow from the upper wing surface at high angles of attack is quite different at low reynolds number from that at the high reynolds numbers of real aircraft. in particular at high reynolds numbers the flow tends to stay attached to the airfoil for longer because the inertial forces are dominant with respect to the viscous forces which are responsible for the flow separation ultimately leading to the aerodynamic stall. for this reason wind tunnel results carried out at lower speeds and on smaller scales models of the real life counterparts often tend to overestimate the aerodynamic stall angle of attack. high-pressure wind tunnels are one solution to this problem. in general, steady operation of an aircraft at an angle of attack above the critical angle is not possible because, after exceeding the critical angle, the loss of lift from the wing causes the nose of the aircraft to fall, reducing the angle of attack again. this nose drop, independent of control inputs, indicates the pilot has actually stalled the aircraft.this graph shows the stall angle, yet in practice most pilot operating handbooks (poh) or generic flight manuals describe stalling in terms of airspeed. this is because all aircraft are equipped with an airspeed indicator, but fewer aircraft have an angle of attack indicator. an aircraft\'s stalling speed is published by the manufacturer (and is required for certification by flight testing) for a range of weights and flap positions, but the stalling angle of attack is not published. as speed reduces, angle of attack has to increase to keep lift constant until the critical angle is reached. the airspeed at which this angle is reached is the (1g, unaccelerated) stalling speed of the aircraft in that particular configuration. deploying flaps/slats decreases the stall speed to allow the aircraft to take off and land at a lower speed.   == aerodynamic description == wing sweep and taper cause stalling at the tip of a wing before the root. the position of a swept wing along the fuselage has to be such that the lift from the wing root, well forward of the aircraft center of gravity (c.g.), must be balanced by the wing tip, well aft of the c.g. if the tip stalls first the balance of the aircraft is upset causing dangerous nose pitch up. swept wings have to incorporate features which prevent pitch-up caused by premature tip stall. a swept wing has a higher lift coefficient on its outer panels than on the inner wing, causing them to reach their maximum lift capability first and to stall first. this is caused by the downwash pattern associated with swept/tapered wings. to delay tip stall the outboard wing is given washout to reduce its angle of attack. the root can also be modified with a suitable leading-edge and airfoil section to make sure it stalls before the tip. however, when taken beyond stalling incidence the tips may still become fully stalled before the inner wing despite initial separation occurring inboard. this causes pitch-up after the stall and entry to a super-stall on those aircraft with super-stall characteristics. span-wise flow of the boundary layer is also present on swept wings and causes tip stall. the amount of boundary layer air flowing outboard can be reduced by generating vortices with a leading-edge device such as a fence, notch, saw tooth or a set of vortex generators behind the leading edge.   == warning and safety devices == fixed-wing aircraft can be equipped with devices to prevent or postpone a stall or to make it less (or in some cases more) severe, or to make recovery easier.  an aerodynamic twist can be introduced to the wing with the leading edge near the wing tip twisted downward. this is called washout and causes the wing root to stall before the wing tip. this makes the stall gentle and progressive. since the stall is delayed at the wing tips, where the ailerons are, roll control is maintained when the stall begins. a stall strip is a small sharp-edged device that, when attached to the leading edge of a wing, encourages the stall to start there in preference to any other location on the wing. if attached close to the wing root, it makes the stall gentle and progressive; if attached near the wing tip, it encourages the aircraft to drop a wing when stalling. a stall fence is a flat plate in the direction of the chord to stop separated flow progressing out along the wing vortex generators, tiny strips of metal or plastic placed on top of the wing near the leading edge that protrude past the boundary layer into the free stream. as the name implies, they energize the boundary layer by mixing free stream airflow with boundary layer flow thereby creating vortices, this increases the momentum in the boundary layer. by increasing the momentum of the boundary layer, airflow separation and the resulting stall may be delayed. an anti-stall strake is a leading edge extension that generates a vortex on the wing upper surface to postpone the stall. a stick pusher is a mechanical device that prevents the pilot from stalling an aircraft. it pushes the elevator control forward as the stall is approached, causing a reduction in the angle of attack. in generic terms, a stick pusher is known as a stall identification device or stall identification system. a stick shaker is a mechanical device that shakes the pilot\'s controls to warn of the onset of stall. a stall warning is an electronic or mechanical device that sounds an audible warning as the stall speed is approached. the majority of aircraft contain some form of this device that warns the pilot of an impending stall. the simplest such device is a stall warning horn, which consists of either a pressure sensor or a movable metal tab that actuates a switch, and produces an audible warning in response. an angle-of-attack indicator for light aircraft, the ""alphasystemsaoa"" and a nearly identical ""lift reserve indicator"", are both pressure differential instruments that display margin above stall and/or angle of attack on an instantaneous, continuous readout. the general technics cya-100 displays true angle of attack via a magnetically coupled vane. an aoa indicator provides a visual display of the amount of available lift throughout its slow speed envelope regardless of the many variables that act upon an aircraft. this indicator is immediately responsive to changes in speed, angle of attack, and wind conditions, and automatically compensates for aircraft weight, altitude, and temperature. an angle of attack limiter or an ""alpha"" limiter is a flight computer that automatically prevents pilot input from causing the plane to rise over the stall angle. some alpha limiters can be disabled by the pilot.stall warning systems often involve inputs from a broad range of sensors and systems to include a dedicated angle of attack sensor. blockage, damage, or inoperation of stall and angle of attack (aoa) probes can lead to unreliability of the stall warning, and cause the stick pusher, overspeed warning, autopilot, and yaw damper to malfunction.if a forward canard is used for pitch control, rather than an aft tail, the canard is designed to meet the airflow at a slightly greater angle of attack than the wing. therefore, when the aircraft pitch increases abnormally, the canard will usually stall first, causing the nose to drop and so preventing the wing from reaching its critical aoa. thus, the risk of main wing stalling is greatly reduced. however, if the main wing stalls, recovery becomes difficult, as the canard is more deeply stalled and angle of attack increases rapidly.if an aft tail is used, the wing is designed to stall before the tail. in this case, the wing can be flown at higher lift coefficient (closer to stall) to produce more overall lift. most military combat aircraft have an angle of attack indicator among the pilot\'s instruments, which lets the pilot know precisely how close to the stall point the aircraft is. modern airliner instrumentation may also measure angle of attack, although this information may not be directly displayed on the pilot\'s display, instead driving a stall warning indicator or giving performance information to the flight computer (for fly by wire systems).   == flight beyond the stall == as a wing stalls, aileron effectiveness is reduced, rendering the plane difficult to control and increasing the risk of a spin. post stall, steady flight beyond the stalling angle (where the coefficient of lift is largest) requires engine thrust to replace lift as well as alternative controls to replace the loss of effectiveness of the ailerons. for high-powered aircraft, the loss of lift (and increase in drag) beyond the stall angle is less of a problem than maintaining control. some aircraft may be subject to post-stall gyration (e.g. the f-4) or susceptible to entering a flat-spin (e.g. f-14). control beyond-stall can be provided by reaction control systems (e.g. nf-104a), vectored thrust, as well as a rolling stabilator (or taileron). the enhanced manoeuvering capability by flights at very high angles of attack can provide a tactical advantage for military fighters such as the f-22 raptor. short-term stalls at 90–120° (e.g. pugachev\'s cobra) are sometimes performed at airshows. the highest angle of attack in sustained flight so far demonstrated was 70 degrees in the x-31 at the dryden flight research center.  sustained post-stall flight is a type of supermaneuverability.   == spoilers ==  except for flight training, airplane testing, and aerobatics, a stall is usually an undesirable event. spoilers (sometimes called lift dumpers), however, are devices that are intentionally deployed to create a carefully controlled flow separation over part of an aircraft\'s wing to reduce the lift it generates, increase the drag, and allow the aircraft to descend more rapidly without gaining speed. spoilers are also deployed asymmetrically (one wing only) to enhance roll control. spoilers can also be used on aborted take-offs and after main wheel contact on landing to increase the aircraft\'s weight on its wheels for better braking action. unlike powered airplanes, which can control descent by increasing or decreasing thrust, gliders have to increase drag to increase the rate of descent. in high-performance gliders, spoiler deployment is extensively used to control the approach to landing. spoilers can also be thought of as  ""lift reducers"" because they reduce the lift of the wing in which the spoiler resides. for example, an uncommanded roll to the left could be reversed by raising the right wing spoiler (or only a few of the spoilers present in large airliner wings). this has the advantage of avoiding the need to increase lift in the wing that is dropping (which may bring that wing closer to stalling).   == history == otto lilienthal died while flying in 1896 as the result of a stall. wilbur wright encountered stalls for the first time in 1901, while flying his second glider. awareness of lilienthal\'s accident and wilbur\'s experience, motivated the wright brothers to design their plane in ""canard"" configuration. this made recoveries from stalls easier and more gentle. the design saved the brothers\' lives more than once.the aircraft engineer juan de la cierva worked on his ""autogiro"" project to develop a rotary wing aircraft which, he hoped, would be unable to stall and which therefore would be safer than aeroplanes. in developing the resulting ""autogyro"" aircraft, he solved many engineering problems which made the helicopter possible.   == see also == articlesaviation safety coffin corner (aerodynamics) compressor stall lift coefficient spin (flight) spoiler (aeronautics) wing twistnotable accidents1963 bac one-eleven test crash 1966 felthorpe trident crash british european airways flight 548 china airlines flight 140 china airlines flight 676 air france flight 447 colgan air flight 3407 turkish airlines flight 1951 indonesia airasia flight 8501   == notes ==   == references == usaf & nato report rto-tr-015 ac/323/(hfm-015)/tp-1 (2001 anderson, j.d., a history of aerodynamics (1997). cambridge university press.  isbn 0-521-66955-3 chapter 4, ""slow flight, stalls, and spins,"" in the airplane flying handbook. (faa h-8083-3a) l. j. clancy (1975), aerodynamics, pitman publishing limited, london.  isbn 0-273-01120-0 stengel, r. (2004), flight dynamics, princeton university press, isbn 0-691-11407-2 alpha systems aoa website for information on aoa and lift reserve indicators [1] 4239-01 angle of attack (aoa) sensor specifications [2] airplane flying handbook. federal aviation administration isbn 1-60239-003-7  pub. skyhorse publishing inc. federal aviation administration (25 september 2000), stall and spin awareness training, ac no: 61-67c prof. dr mustafa cavcar, ""stall speed"" [3]')"
7,"The wingspan (or just span) of a bird or an airplane is the distance from one wingtip to the other wingtip. For example, the Boeing 777-200 has a wingspan of 60.93 metres (199 ft 11 in), and a wandering albatross (Diomedea exulans) caught in 1965 had a wingspan of 3.63 metres (11 ft 11 in), the official record for a living bird.
The term wingspan, more technically extent, is also used for other winged animals such as pterosaurs, bats, insects, etc., and other aircraft such as ornithopters.
In humans, the term wingspan also refers to the arm span, which is distance between the length from one end of an individual's arms (measured at the fingertips) to the other when raised parallel to the ground at shoulder height at a 90º angle. Former professional basketball player Manute Bol stands at 7 ft 7 in (2.31 m) and owns one of the largest wingspans at 8 ft 6 in (2.59 m).


== Wingspan of aircraft ==
The wingspan of an aircraft is always measured in a straight line, from wingtip to wingtip, independently of wing shape or sweep.


=== Implications for aircraft design and animal evolution ===
The lift from wings is proportional to their area, so the heavier the animal or aircraft the bigger that area must be.  The area is the product of the span times the width (mean chord) of the wing, so either a long, narrow wing or a shorter, broader wing will support the same mass.  For efficient steady flight, the ratio of span to chord, the aspect ratio, should be as high as possible (the constraints are usually structural) because this lowers the lift-induced drag associated with the inevitable wingtip vortices. Long-ranging birds, like albatrosses, and most commercial aircraft maximize aspect ratio.  Alternatively, animals and aircraft which depend on maneuverability (fighters, predators and the preyed upon, and those who live amongst trees and bushes, insect catchers, etc.) need to be able to roll fast to turn, and the high moment of inertia of long narrow wings, as well as the high angular drag and quick balancing of aileron lift with wing lift at a low rotation rate, produces lower roll rates.  For them, short-span, broad wings are preferred. Additionally, ground handling in aircraft is a significant problem for very high aspect ratios and flying animals may encounter similar issues.
The highest aspect ratio man-made wings are aircraft propellers, in their most extreme form as helicopter rotors.


== Wingspan of flying animals ==
To measure the wingspan of a bird, a live or freshly-dead specimen is placed flat on its back, the wings are grasped at the wrist joints, ankles and the distance is measured between the tips of the longest primary feathers on each wing.The wingspan of an insect refers to the wingspan of pinned specimens, and may refer to the distance between the centre of the thorax to the apex of the wing doubled or to the width between the apices with the wings set with the trailing wing edge perpendicular to the body.


== Wingspan in sports ==
In basketball and gridiron football, a fingertip-to-fingertip measurement is used to determine the player's wingspan, also called armspan. This is called reach in boxing terminology.  The wingspan of 16-year-old BeeJay Anya, a top basketball Junior Class of 2013 prospect who played for the NC State Wolfpack, was officially measured at 7 feet 9 inches (2.36 m) across, one of the longest of all National Basketball Association draft prospects, and the longest ever for a non-7-foot player, though Anya went undrafted in 2017. The wingspan of Manute Bol, at 8 feet 6 inches (2.59 m), is (as of 2013) the longest in NBA history, and his vertical reach was 10 feet 5 inches (3.18 m).


== Wingspan records ==


=== Largest wingspan ===
Aircraft (current):  Scaled Composites Stratolaunch — 117 m (385 ft) 
Bat: Large flying fox – 1.5 m (4 ft 11 in)
Bird: Wandering albatross – 3.63 m (11 ft 11 in)
Bird (extinct): Argentavis – Estimated 7 m (23 ft 0 in)
Reptile (extinct): Quetzalcoatlus pterosaur – 10–11 m (33–36 ft)
Insect: White witch moth – 28 cm (11.0 in)
Insect (extinct): Meganeuropsis (relative of dragonflies) – estimated up to 71 cm (28.0 in)


=== Smallest wingspan ===
Aircraft (biplane): Starr Bumble Bee II – 1.68 m (5 ft 6 in)
Aircraft (jet): Bede BD-5 – 4.27 m (14 ft 0 in)
Aircraft (twin engine): Colomban Cri-cri – 4.9 m (16 ft 1 in)
Bat: Bumblebee bat – 16 cm (6.3 in)  
Bird: Bee hummingbird – 6.5 cm (2.6 in)
Insect: Tanzanian parasitic wasp (Fairyfly) – 0.2 mm (0.0079 in)


== References ==","pandas(index=7, _1=7, text=""the wingspan (or just span) of a bird or an airplane is the distance from one wingtip to the other wingtip. for example, the boeing 777-200 has a wingspan of 60.93 metres (199 ft 11 in), and a wandering albatross (diomedea exulans) caught in 1965 had a wingspan of 3.63 metres (11 ft 11 in), the official record for a living bird. the term wingspan, more technically extent, is also used for other winged animals such as pterosaurs, bats, insects, etc., and other aircraft such as ornithopters. in humans, the term wingspan also refers to the arm span, which is distance between the length from one end of an individual's arms (measured at the fingertips) to the other when raised parallel to the ground at shoulder height at a 90º angle. former professional basketball player manute bol stands at 7 ft 7 in (2.31 m) and owns one of the largest wingspans at 8 ft 6 in (2.59 m).   == wingspan of aircraft == the wingspan of an aircraft is always measured in a straight line, from wingtip to wingtip, independently of wing shape or sweep. aircraft (biplane): starr bumble bee ii – 1.68 m (5 ft 6 in) aircraft (jet): bede bd-5 – 4.27 m (14 ft 0 in) aircraft (twin engine): colomban cri-cri – 4.9 m (16 ft 1 in) bat: bumblebee bat – 16 cm (6.3 in) bird: bee hummingbird – 6.5 cm (2.6 in) insect: tanzanian parasitic wasp (fairyfly) – 0.2 mm (0.0079 in)   == references =="")"
8,"Working mass, also referred to as reaction mass, is a mass against which a system operates in order to produce acceleration.
In the case of a rocket, for example, the reaction mass is the fuel shot backwards to provide propulsion. All acceleration requires an exchange of momentum, which can be thought of as the ""unit of movement"". Momentum is related to mass and velocity, as given by the formula P = mv, where P is the momentum, m the mass, and v the velocity. The velocity of a body is easily changeable, but in most cases the mass is not, which makes it important.


== Rockets and rocket-like reaction engines ==
In rockets, the total velocity change can be calculated (using the Tsiolkovsky rocket equation) as follows:

  
    
      
        Δ
        
        v
        =
        u
        
        ln
        ⁡
        
          (
          
            
              
                m
                +
                M
              
              M
            
          
          )
        
      
    
    {\displaystyle \Delta \,v=u\,\ln \left({\frac {m+M}{M}}\right)}
  
Where:

v = ship velocity.
u = exhaust velocity.
M = ship mass, not including the working mass.
m = total mass ejected from the ship (working mass).The term working mass is used primarily in the aerospace field. In more ""down to earth"" examples the working mass is typically provided by the Earth, which contains so much momentum in comparison to most vehicles that the amount it gains or loses can be ignored. However, in the case of an aircraft the working mass is the air, and in the case of a rocket, it is the rocket fuel itself. Most rocket engines use light-weight fuels (liquid hydrogen, oxygen, or kerosene) accelerated to super-sonic speeds. However, ion engines often use heavier elements like xenon as the reaction mass, accelerated to much higher speeds using electric fields.
In many cases the working mass is separate from the energy used to accelerate it. In a car the engine provides power to the wheels, which then accelerates the Earth backward to make the car move forward. This is not the case for most rockets however, where the rocket propellant is the working mass, as well as the energy source. This means that rockets stop accelerating as soon as they run out of fuel, regardless of other power sources they may have. This can be a problem for satellites that need to be repositioned often, as it limits their useful life. In general, the exhaust velocity should be close to the ship velocity for optimum energy efficiency.  This limitation of rocket propulsion is one of the main motivations for the ongoing interest in field propulsion technology.


== See also ==
Rocket equation","pandas(index=8, _1=8, text='working mass, also referred to as reaction mass, is a mass against which a system operates in order to produce acceleration. in the case of a rocket, for example, the reaction mass is the fuel shot backwards to provide propulsion. all acceleration requires an exchange of momentum, which can be thought of as the ""unit of movement"". momentum is related to mass and velocity, as given by the formula p = mv, where p is the momentum, m the mass, and v the velocity. the velocity of a body is easily changeable, but in most cases the mass is not, which makes it important.   == rockets and rocket-like reaction engines == in rockets, the total velocity change can be calculated (using the tsiolkovsky rocket equation) as follows:     δ  v = u  ln \u2061  (    mm  m   )      where:  v = ship velocity. u = exhaust velocity. m = ship mass, not including the working mass. m = total mass ejected from the ship (working mass).the term working mass is used primarily in the aerospace field. in more ""down to earth"" examples the working mass is typically provided by the earth, which contains so much momentum in comparison to most vehicles that the amount it gains or loses can be ignored. however, in the case of an aircraft the working mass is the air, and in the case of a rocket, it is the rocket fuel itself. most rocket engines use light-weight fuels (liquid hydrogen, oxygen, or kerosene) accelerated to super-sonic speeds. however, ion engines often use heavier elements like xenon as the reaction mass, accelerated to much higher speeds using electric fields. in many cases the working mass is separate from the energy used to accelerate it. in a car the engine provides power to the wheels, which then accelerates the earth backward to make the car move forward. this is not the case for most rockets however, where the rocket propellant is the working mass, as well as the energy source. this means that rockets stop accelerating as soon as they run out of fuel, regardless of other power sources they may have. this can be a problem for satellites that need to be repositioned often, as it limits their useful life. in general, the exhaust velocity should be close to the ship velocity for optimum energy efficiency.  this limitation of rocket propulsion is one of the main motivations for the ongoing interest in field propulsion technology.   == see also == rocket equation')"
9,"In fluid dynamics, the drag coefficient (commonly denoted as: 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
  , 
  
    
      
        
          c
          
            x
          
        
      
    
    {\displaystyle c_{x}}
   or 
  
    
      
        
          c
          
            w
          
        
      
    
    {\displaystyle c_{w}}
  ) is a dimensionless quantity that is used to quantify the drag or resistance of an object in a fluid environment, such as air or water. It is used in the drag equation in which a lower drag coefficient indicates the object will have less aerodynamic or hydrodynamic drag. The drag coefficient is always associated with a particular surface area.The drag coefficient of any object comprises the effects of the two basic contributors to fluid dynamic drag: skin friction and form drag. The drag coefficient of a lifting airfoil or hydrofoil also includes the effects of lift-induced drag. The drag coefficient of a complete structure such as an aircraft also includes the effects of interference drag.


== Definition ==

The drag coefficient 
  
    
      
        
          c
          
            
              d
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }}
   is defined as

  
    
      
        
          c
          
            
              d
            
          
        
        =
        
          
            
              
                2
                
                  F
                  
                    
                      d
                    
                  
                
              
              
                ρ
                
                  u
                  
                    2
                  
                
                A
              
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }={\dfrac {2F_{\mathrm {d} }}{\rho u^{2}A}}}
  where:

  
    
      
        
          F
          
            
              d
            
          
        
      
    
    {\displaystyle F_{\mathrm {d} }}
   is the drag force, which is by definition the force component in the direction of the flow velocity,

  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the mass density of the fluid,

  
    
      
        u
      
    
    {\displaystyle u}
   is the flow speed of the object relative to the fluid,

  
    
      
        A
      
    
    {\displaystyle A}
   is the reference area.The reference area depends on what type of drag coefficient is being measured. For automobiles and many other objects, the reference area is the projected frontal area of the vehicle. This may not necessarily be the cross-sectional area of the vehicle, depending on where the cross-section is taken. For example, for a sphere 
  
    
      
        A
        =
        π
        
          r
          
            2
          
        
      
    
    {\displaystyle A=\pi r^{2}}
   (note this is not the surface area = 
  
    
      
        4
        π
        
          r
          
            2
          
        
      
    
    {\displaystyle 4\pi r^{2}}
  ).
For airfoils, the reference area is the nominal wing area. Since this tends to be large compared to the frontal area, the resulting drag coefficients tend to be low, much lower than for a car with the same drag, frontal area, and speed.
Airships and some bodies of revolution use the volumetric drag coefficient, in which the reference area is the square of the cube root of the airship volume (volume to the two-thirds power). Submerged streamlined bodies use the wetted surface area.
Two objects having the same reference area moving at the same speed through a fluid will experience a drag force proportional to their respective drag coefficients. Coefficients for unstreamlined objects can be 1 or more, for streamlined objects much less.
It has been demonstrated that drag coefficient 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   is a function of Bejan number (
  
    
      
        B
        e
      
    
    {\displaystyle Be}
  ), Reynolds number (
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  ) and the ratio between wet area 
  
    
      
        
          A
          
            w
          
        
      
    
    {\displaystyle A_{w}}
   and front area 
  
    
      
        
          A
          
            f
          
        
      
    
    {\displaystyle A_{f}}
  :
  
    
      
        
          c
          
            d
          
        
        =
        2
        
          
            
              A
              
                w
              
            
            
              A
              
                f
              
            
          
        
        
          
            
              B
              e
            
            
              R
              
                e
                
                  L
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle c_{d}=2{\frac {A_{w}}{A_{f}}}{\frac {Be}{Re_{L}^{2}}}}
  
where 
  
    
      
        R
        
          e
          
            L
          
        
      
    
    {\displaystyle Re_{L}}
   is the Reynolds Number related to fluid path length 
  
    
      
        L
      
    
    {\displaystyle L}
  .


== Background ==

The drag equation

  
    
      
        
          F
          
            d
          
        
        =
        
          
            
              1
              2
            
          
        
        ρ
        
          u
          
            2
          
        
        
          c
          
            d
          
        
        A
      
    
    {\displaystyle F_{d}={\tfrac {1}{2}}\rho u^{2}c_{d}A}
  is essentially a statement that the drag force on any object is proportional to the density of the fluid and proportional to the square of the relative flow speed between the object and the fluid.
Cd is not a constant but varies as a function of flow speed, flow direction, object position, object size, fluid density and fluid viscosity. Speed, kinematic viscosity and a characteristic length scale of the object are incorporated into a dimensionless quantity called the Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  . 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is thus a function of 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  . In a compressible flow, the speed of sound is relevant, and 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is also a function of Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
  .
For certain body shapes, the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   only depends on the Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  , Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
   and the direction of the flow. For low Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
  , the drag coefficient is independent of Mach number. Also, the variation with Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
   within a practical range of interest is usually small, while for cars at highway speed and aircraft at cruising speed, the incoming flow direction is also more-or-less the same. Therefore, the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   can often be treated as a constant.For a streamlined body to achieve a low drag coefficient, the boundary layer around the body must remain attached to the surface of the body for as long as possible, causing the wake to be narrow. A high form drag results in a broad wake. The boundary layer will transition from laminar to turbulent if Reynolds number of the flow around the body is sufficiently great. Larger velocities, larger objects, and lower viscosities contribute to larger Reynolds numbers.

For other objects, such as small particles, one can no longer consider that the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is constant, but certainly is a function of Reynolds number.
At a low Reynolds number, the flow around the object does not transition to turbulent but remains laminar, even up to the point at which it separates from the surface of the object. At very low Reynolds numbers, without flow separation, the drag force 
  
    
      
        
          
            F
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle F_{\mathrm {d} }}
   is proportional to 
  
    
      
        
          v
        
      
    
    {\displaystyle \scriptstyle v}
   instead of 
  
    
      
        
          
            v
            
              2
            
          
        
      
    
    {\displaystyle \scriptstyle v^{2}}
  ; for a sphere this is known as Stokes' law. The Reynolds number will be low for small objects, low velocities, and high viscosity fluids.A 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   equal to 1 would be obtained in a case where all of the fluid approaching the object is brought to rest, building up stagnation pressure over the whole front surface. The top figure shows a flat plate with the fluid coming from the right and stopping at the plate. The graph to the left of it shows equal pressure across the surface. In a real flat plate, the fluid must turn around the sides, and full stagnation pressure is found only at the center, dropping off toward the edges as in the lower figure and graph. Only considering the front side, the 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   of a real flat plate would be less than 1; except that there will be suction on the backside: a negative pressure (relative to ambient). The overall 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   of a real square flat plate perpendicular to the flow is often given as 1.17. Flow patterns and therefore 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   for some shapes can change with the Reynolds number and the roughness of the surfaces.


== Drag coefficient examples ==


=== General ===
In general, 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   is not an absolute constant for a given body shape. It varies with the speed of airflow (or more generally with Reynolds number 
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  ). A smooth sphere, for example, has a 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   that varies from high values for laminar flow to 0.47 for turbulent flow.  Although the drag coefficient decreases with increasing 
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  , the drag force increases.


=== Aircraft ===
As noted above, aircraft use their wing area as the reference area when computing 
  
    
      
        
          c
          
            
              d
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }}
  , while automobiles (and many other objects) use frontal cross-sectional area; thus, coefficients are not directly comparable between these classes of vehicles. In the aerospace industry, the drag coefficient is sometimes expressed in drag counts where 1 drag count = 0.0001 of a 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C_{d}}
  .


=== Automobile ===


== Blunt and streamlined body flows ==


=== Concept ===
Drag, in the context of fluid dynamics, refers to forces that act on a solid object in the direction of the relative flow velocity (note that the diagram below shows the drag in the opposite direction to the flow). The aerodynamic forces on a body come primarily from differences in pressure and viscous shearing stresses. Thereby, the drag force on a body could be divided into two components, namely frictional drag (viscous drag) and pressure drag (form drag). The net drag force could be decomposed as follows:

  
    
      
        
          
            
              
                
                  c
                  
                    
                      d
                    
                  
                
              
              
                
                =
                
                  
                    
                      
                        2
                        
                          F
                          
                            
                              d
                            
                          
                        
                      
                      
                        ρ
                        
                          v
                          
                            2
                          
                        
                        A
                      
                    
                  
                
              
            
            
              
              
                
                =
                
                  c
                  
                    
                      p
                    
                  
                
                +
                
                  c
                  
                    
                      f
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      
                        
                          
                            
                              1
                              
                                ρ
                                
                                  v
                                  
                                    2
                                  
                                
                                A
                              
                            
                          
                        
                        
                          
                            ∫
                            
                              S
                            
                          
                          
                            d
                          
                          A
                          (
                          p
                          −
                          
                            p
                            
                              o
                            
                          
                          )
                          
                            (
                            
                              
                                
                                  
                                    
                                      n
                                    
                                    ^
                                  
                                
                              
                              ⋅
                              
                                
                                  
                                    
                                      i
                                    
                                    ^
                                  
                                
                              
                            
                            )
                          
                        
                      
                      ⏟
                    
                  
                  
                    
                      c
                      
                        
                          p
                        
                      
                    
                  
                
                +
                
                  
                    
                      
                        
                          
                            
                              1
                              
                                ρ
                                
                                  v
                                  
                                    2
                                  
                                
                                A
                              
                            
                          
                        
                        
                          
                            ∫
                            
                              S
                            
                          
                          
                            d
                          
                          A
                          
                            (
                            
                              
                                
                                  
                                    
                                      t
                                    
                                    ^
                                  
                                
                              
                              ⋅
                              
                                
                                  
                                    
                                      i
                                    
                                    ^
                                  
                                
                              
                            
                            )
                          
                          
                            T
                            
                              w
                            
                          
                        
                      
                      ⏟
                    
                  
                  
                    
                      c
                      
                        
                          f
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}c_{\mathrm {d} }&={\dfrac {2F_{\mathrm {d} }}{\rho v^{2}A}}\\&=c_{\mathrm {p} }+c_{\mathrm {f} }\\&=\underbrace {{\dfrac {1}{\rho v^{2}A}}\displaystyle \int \limits _{S}\mathrm {d} A(p-p_{o})\left({\hat {\mathbf {n} }}\cdot {\hat {\mathbf {i} }}\right)} _{c_{\mathrm {p} }}+\underbrace {{\dfrac {1}{\rho v^{2}A}}\displaystyle \int \limits _{S}\mathrm {d} A\left({\hat {\mathbf {t} }}\cdot {\hat {\mathbf {i} }}\right)T_{w}} _{c_{\mathrm {f} }}\end{aligned}}}
  where:

  
    
      
        
          c
          
            
              p
            
          
        
      
    
    {\displaystyle c_{\mathrm {p} }}
   is the pressure drag coefficient,

  
    
      
        
          c
          
            
              f
            
          
        
      
    
    {\displaystyle c_{\mathrm {f} }}
   is the friction drag coefficient,

  
    
      
        
          
            
              
                t
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {t} }}}
   = Tangential direction to the surface with area dA,

  
    
      
        
          
            
              
                n
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {n} }}}
   = Normal direction to the surface with area dA,

  
    
      
        
          T
          
            
              w
            
          
        
      
    
    {\displaystyle T_{\mathrm {w} }}
   is the shear Stress acting on the surface dA,

  
    
      
        
          p
          
            
              o
            
          
        
      
    
    {\displaystyle p_{\mathrm {o} }}
   is the pressure far away from the surface dA,

  
    
      
        p
      
    
    {\displaystyle p}
   is pressure at surface dA,

  
    
      
        
          
            
              
                i
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {i} }}}
   is the unit vector in direction of free stream flowTherefore, when the drag is dominated by a frictional component, the body is called a streamlined body; whereas in the case of dominant pressure drag, the body is called a blunt or bluff body. Thus, the shape of the body and the angle of attack determine the type of drag. For example, an airfoil is considered as a body with a small angle of attack by the fluid flowing across it. This means that it has attached boundary layers, which produce much less pressure drag.

The wake produced is very small and drag is dominated by the friction component. Therefore, such a body (here an airfoil) is described as streamlined, whereas for bodies with fluid flow at high angles of attack, boundary layer separation takes place. This mainly occurs due to adverse pressure gradients at the top and rear parts of an airfoil.
Due to this, wake formation takes place, which consequently leads to eddy formation and pressure loss due to pressure drag. In such situations, the airfoil is stalled and has higher pressure drag than friction drag. In this case, the body is described as a blunt body.
A streamlined body looks like a fish (Tuna), Oropesa, etc. or an airfoil with small angle of attack, whereas a blunt body looks like a brick, a cylinder or an airfoil with high angle of attack. For a given frontal area and velocity, a streamlined body will have lower resistance than a blunt body. Cylinders and spheres are taken as blunt bodies because the drag is dominated by the pressure component in the wake region at high Reynolds number.
To reduce this drag, either the flow separation could be reduced or the surface area in contact with the fluid could be reduced (to reduce friction drag). This reduction is necessary in devices like cars, bicycle, etc. to avoid vibration and noise production.


==== Practical example ====
The aerodynamic design of cars has evolved from the 1920s to the end of the 20th century. This change in design from a blunt body to a more streamlined body reduced the drag coefficient from about 0.95 to 0.30.

Time history of cars' aerodynamic drag in comparison to change in geometry of streamlined bodies (blunt to streamline).


== See also ==
Automotive aerodynamics
Automobile drag coefficient
Ballistic coefficient
Drag crisis
Zero-lift drag coefficient


== Notes ==


== References ==
L. J. Clancy (1975): Aerodynamics. Pitman Publishing Limited, London, ISBN 0-273-01120-0
Abbott, Ira H., and Von Doenhoff, Albert E. (1959): Theory of Wing Sections. Dover Publications Inc., New York, Standard Book Number 486-60586-8
Hoerner, Dr. Sighard F., Fluid-Dynamic Drag, Hoerner Fluid Dynamics, Bricktown New Jersey, 1965.
Bluff Body: http://user.engineering.uiowa.edu/~me_160/lecture_notes/Bluff%20Body2.pdf
Drag of Blunt Bodies and Streamlined Bodies: http://www.princeton.edu/~asmits/Bicycle_web/blunt.html
Hucho, W.H., Janssen, L.J., Emmelmann, H.J. 6(1975): The optimization of body details-A method for reducing the aerodynamics drag. SAE 760185.","pandas(index=9, _1=9, text=""in fluid dynamics, the drag coefficient (commonly denoted as:     c  d      is the unit vector in direction of free stream flowtherefore, when the drag is dominated by a frictional component, the body is called a streamlined body; whereas in the case of dominant pressure drag, the body is called a blunt or bluff body. thus, the shape of the body and the angle of attack determine the type of drag. for example, an airfoil is considered as a body with a small angle of attack by the fluid flowing across it. this means that it has attached boundary layers, which produce much less pressure drag.  the wake produced is very small and drag is dominated by the friction component. therefore, such a body (here an airfoil) is described as streamlined, whereas for bodies with fluid flow at high angles of attack, boundary layer separation takes place. this mainly occurs due to adverse pressure gradients at the top and rear parts of an airfoil. due to this, wake formation takes place, which consequently leads to eddy formation and pressure loss due to pressure drag. in such situations, the airfoil is stalled and has higher pressure drag than friction drag. in this case, the body is described as a blunt body. a streamlined body looks like a fish (tuna), oropesa, etc. or an airfoil with small angle of attack, whereas a blunt body looks like a brick, a cylinder or an airfoil with high angle of attack. for a given frontal area and velocity, a streamlined body will have lower resistance than a blunt body. cylinders and spheres are taken as blunt bodies because the drag is dominated by the pressure component in the wake region at high reynolds number. to reduce this drag, either the flow separation could be reduced or the surface area in contact with the fluid could be reduced (to reduce friction drag). this reduction is necessary in devices like cars, bicycle, etc. to avoid vibration and noise production. the aerodynamic design of cars has evolved from the 1920s to the end of the 20th century. this change in design from a blunt body to a more streamlined body reduced the drag coefficient from about 0.95 to 0.30.  time history of cars' aerodynamic drag in comparison to change in geometry of streamlined bodies (blunt to streamline).   == see also == automotive aerodynamics automobile drag coefficient ballistic coefficient drag crisis zero-lift drag coefficient   == notes ==   == references == l. j. clancy (1975): aerodynamics. pitman publishing limited, london, isbn 0-273-01120-0 abbott, ira h., and von doenhoff, albert e. (1959): theory of wing sections. dover publications inc., new york, standard book number 486-60586-8 hoerner, dr. sighard f., fluid-dynamic drag, hoerner fluid dynamics, bricktown new jersey, 1965. bluff body: http://user.engineering.uiowa.edu/~me_160/lecture_notes/bluff%20body2.pdf drag of blunt bodies and streamlined bodies: http://www.princeton.edu/~asmits/bicycle_web/blunt.html hucho, w.h., janssen, l.j., emmelmann, h.j. 6(1975): the optimization of body details-a method for reducing the aerodynamics drag. sae 760185."")"
10,"The Boeing X-51 Waverider is an unmanned research scramjet experimental aircraft for hypersonic flight at Mach 5 (3,300 mph; 5,300 km/h) and an altitude of 70,000 feet (21,000 m). The aircraft was designated X-51 in 2005. It completed its first powered hypersonic flight on 26 May 2010.  After two unsuccessful test flights, the X-51 completed a flight of over six minutes and reached speeds of over Mach 5 for 210 seconds on 1 May 2013 for the longest duration powered hypersonic flight.
Waverider refers in general to aircraft that take advantage of compression lift produced by their own shock waves. The X-51 program was a cooperative effort by the United States Air Force, DARPA, NASA, Boeing, and Pratt & Whitney Rocketdyne. The program was managed by the Aerospace Systems Directorate within the U.S. Air Force Research Laboratory (AFRL).  X-51 technology is proposed for use in the High Speed Strike Weapon (HSSW), a Mach 5+ missile which could enter service in the mid-2020s.


== Design and development ==
In the 1990s, the Air Force Research Laboratory (AFRL) began the HyTECH program for hypersonic propulsion.  Pratt & Whitney received a contract from the AFRL to develop a hydrocarbon-fueled scramjet engine which led to the development of the SJX61 engine. The SJX61 engine was originally meant for the NASA X-43C, which was eventually canceled.  The engine was applied to the AFRL's Scramjet Engine Demonstrator program in late 2003.  The scramjet flight test vehicle was designated X-51 on 27 September 2005.

In flight demonstrations, the X-51 is carried by a B-52 to an altitude of about 50,000 feet (15 km; 9.5 mi) and then released over the Pacific Ocean.  The X-51 is initially propelled by an MGM-140 ATACMS solid rocket booster to approximately Mach 4.5 (3,000 mph; 4,800 km/h). The booster is then jettisoned and the vehicle's Pratt & Whitney Rocketdyne SJY61 scramjet accelerates it to a top flight speed near Mach 6 (4,000 mph; 6,400 km/h).  The X-51 uses JP-7 fuel for the SJY61 scramjet, carrying 270 lb (120 kg) on board.


=== Applications for hypersonic technology ===
DARPA once viewed X-51 as a stepping stone to Blackswift, a planned hypersonic demonstrator which was canceled in October 2008.In May 2013, the U.S. Air Force planned to apply X-51 technology to the High Speed Strike Weapon (HSSW), a missile similar in size to the X-51.  The HSSW could fly in 2020 and enter service in the mid-2020s.  It is envisioned to have a range of 500–600 nmi, fly at Mach 5–6, and fit on an F-35 or in the internal bay of a B-2 bomber.


== Testing ==


=== Ground and unpowered testing ===
Ground tests of the X-51A began in late 2005.  A preliminary version of the X-51, the ""Ground Demonstrator Engine No. 2"", completed wind tunnel tests at the NASA Langley Research Center on 27 July 2006.  Testing continued there until a simulated X-51 flight at Mach 5 was successfully completed on 30 April 2007. The testing is intended to observe acceleration between Mach 4 and Mach 6 and to demonstrate that hypersonic thrust ""isn't just luck"".  Four captive test flights were initially planned for 2009. However, the first captive flight of the X-51A on a B-52 was conducted on 9 December 2009, with further flights in early 2010.


=== Powered flight testing ===
The first powered flight of the X-51 was planned for 25 May 2010, but the presence of a cargo ship traveling through a portion of the Naval Air Station Point Mugu Sea Range caused a 24-hour delay. The X-51 completed its first powered flight successfully on 26 May 2010.  It reached a speed of Mach 5 (3,300 mph; 5,300 km/h), an altitude of 70,000 feet (21,000 m) and flew for over 200 seconds; it did not meet the planned 300 second flight duration, however.  The test had the longest hypersonic flight time of 140 seconds while under its scramjet power. The X-43 had the previous longest flight burn time of 12 seconds, while setting a new speed record of Mach 9.68.
Three more test flights were planned and used the same flight trajectory.  Boeing proposed to the Air Force Research Laboratory (AFRL) that two test flights be added to increase the total to six, with flights taking place at four to six week intervals, provided there are no failures.The second test flight was initially scheduled for 24 March 2011, but was not conducted due to unfavorable test conditions.  The flight took place on 13 June 2011.  However, the flight over the Pacific Ocean ended early due to an inlet unstart event after being boosted to Mach 5 speed.  The flight data from the test was being investigated.  A B-52 released the X-51 at an approximate altitude of 50,000 feet (15,000 m). The X-51's scramjet engine lit on ethylene, but did not properly transition to JP-7 fuel operation.The third test flight took place on 14 August 2012.  The X-51 was to make a 300-second (5 minutes) experimental flight at speeds of Mach 5 (3,300 mph; 5,312 km/h). After separating from its rocket booster, the craft lost control and crashed into the Pacific.  The Air Force Research Laboratory (AFRL) determined the problem was the X-51's upper right aerodynamic fin unlocked during flight and became uncontrollable; all four fins are needed for aerodynamic control.  The aircraft lost control before the scramjet engine could ignite.On 1 May 2013, the X-51 performed its first fully successful flight test on its fourth test flight.  The X-51 and booster detached from a B-52H and was powered to Mach 4.8 (3,200 mph; 5,100 km/h) by the booster rocket.  It then separated cleanly from the booster and ignited its own engine.  The test aircraft then accelerated to Mach 5.1 (3,400 mph; 5,400 km/h) and flew for 210 seconds until running out of fuel and plunging into the Pacific Ocean off Point Mugu for over six minutes of total flight time; this test was the longest air-breathing hypersonic flight.  Researchers collected telemetry data for 370 seconds of flight.  The test signified the completion of the program.  The Air Force Research Laboratory believes the successful flight will serve as research for practical applications of hypersonic flight, such as a missile, reconnaissance, transport, and air-breathing first stage for a space system.


== Specifications ==
Data from Boeing, Air Force
Crew: None
Length: 25 ft 0 in (7.62 m)
Empty weight: 4,000 lb (1,814 kg)
Powerplant: 1 × MGM-140 ATACMS rocket booster
Powerplant: 1 × Pratt & Whitney Rocketdyne SJY61 scramjetPerformance

Maximum speed: 3,900 mph (6,200 km/h, 3,400 kn)
Maximum speed: Mach 5.1
Range: 460 mi (740 km, 400 nmi)
Service ceiling: 70,000 ft (21,300 m)


== See also ==
Boeing Small Launch Vehicle concept, includes a hypersonic waverider as the second stage
Flight airspeed record
Scramjet programs


== References ==


== External links ==
X-51 fact sheet on USAF site
""WaveRider page on Boeing.com"". Archived from the original on 14 November 2012.
""AFRL mulls adding scope to X-51A Waverider hypersonic tests"". Flight International, March 2009.
""Pratt & Whitney Rocketdyne Scramjet Excels in USAF Tests"". Aviation Week - subscription
YouTube video of FoxNews report, preceding test flight
YouTube video of test flight, shot from NASA chase plane","pandas(index=10, _1=10, text='the boeing x-51 waverider is an unmanned research scramjet experimental aircraft for hypersonic flight at mach 5 (3,300 mph; 5,300 km/h) and an altitude of 70,000 feet (21,000 m). the aircraft was designated x-51 in 2005. it completed its first powered hypersonic flight on 26 may 2010.  after two unsuccessful test flights, the x-51 completed a flight of over six minutes and reached speeds of over mach 5 for 210 seconds on 1 may 2013 for the longest duration powered hypersonic flight. waverider refers in general to aircraft that take advantage of compression lift produced by their own shock waves. the x-51 program was a cooperative effort by the united states air force, darpa, nasa, boeing, and pratt & whitney rocketdyne. the program was managed by the aerospace systems directorate within the u.s. air force research laboratory (afrl).  x-51 technology is proposed for use in the high speed strike weapon (hssw), a mach 5missile which could enter service in the mid-2020s.   == design and development == in the 1990s, the air force research laboratory (afrl) began the hytech program for hypersonic propulsion.  pratt & whitney received a contract from the afrl to develop a hydrocarbon-fueled scramjet engine which led to the development of the sjx61 engine. the sjx61 engine was originally meant for the nasa x-43c, which was eventually canceled.  the engine was applied to the afrl\'s scramjet engine demonstrator program in late 2003.  the scramjet flight test vehicle was designated x-51 on 27 september 2005.  in flight demonstrations, the x-51 is carried by a b-52 to an altitude of about 50,000 feet (15 km; 9.5 mi) and then released over the pacific ocean.  the x-51 is initially propelled by an mgm-140 atacms solid rocket booster to approximately mach 4.5 (3,000 mph; 4,800 km/h). the booster is then jettisoned and the vehicle\'s pratt & whitney rocketdyne sjy61 scramjet accelerates it to a top flight speed near mach 6 (4,000 mph; 6,400 km/h).  the x-51 uses jp-7 fuel for the sjy61 scramjet, carrying 270 lb (120 kg) on board. the first powered flight of the x-51 was planned for 25 may 2010, but the presence of a cargo ship traveling through a portion of the naval air station point mugu sea range caused a 24-hour delay. the x-51 completed its first powered flight successfully on 26 may 2010.  it reached a speed of mach 5 (3,300 mph; 5,300 km/h), an altitude of 70,000 feet (21,000 m) and flew for over 200 seconds; it did not meet the planned 300 second flight duration, however.  the test had the longest hypersonic flight time of 140 seconds while under its scramjet power. the x-43 had the previous longest flight burn time of 12 seconds, while setting a new speed record of mach 9.68. three more test flights were planned and used the same flight trajectory.  boeing proposed to the air force research laboratory (afrl) that two test flights be added to increase the total to six, with flights taking place at four to six week intervals, provided there are no failures.the second test flight was initially scheduled for 24 march 2011, but was not conducted due to unfavorable test conditions.  the flight took place on 13 june 2011.  however, the flight over the pacific ocean ended early due to an inlet unstart event after being boosted to mach 5 speed.  the flight data from the test was being investigated.  a b-52 released the x-51 at an approximate altitude of 50,000 feet (15,000 m). the x-51\'s scramjet engine lit on ethylene, but did not properly transition to jp-7 fuel operation.the third test flight took place on 14 august 2012.  the x-51 was to make a 300-second (5 minutes) experimental flight at speeds of mach 5 (3,300 mph; 5,312 km/h). after separating from its rocket booster, the craft lost control and crashed into the pacific.  the air force research laboratory (afrl) determined the problem was the x-51\'s upper right aerodynamic fin unlocked during flight and became uncontrollable; all four fins are needed for aerodynamic control.  the aircraft lost control before the scramjet engine could ignite.on 1 may 2013, the x-51 performed its first fully successful flight test on its fourth test flight.  the x-51 and booster detached from a b-52h and was powered to mach 4.8 (3,200 mph; 5,100 km/h) by the booster rocket.  it then separated cleanly from the booster and ignited its own engine.  the test aircraft then accelerated to mach 5.1 (3,400 mph; 5,400 km/h) and flew for 210 seconds until running out of fuel and plunging into the pacific ocean off point mugu for over six minutes of total flight time; this test was the longest air-breathing hypersonic flight.  researchers collected telemetry data for 370 seconds of flight.  the test signified the completion of the program.  the air force research laboratory believes the successful flight will serve as research for practical applications of hypersonic flight, such as a missile, reconnaissance, transport, and air-breathing first stage for a space system.   == specifications == data from boeing, air force crew: none length: 25 ft 0 in (7.62 m) empty weight: 4,000 lb (1,814 kg) powerplant: 1 × mgm-140 atacms rocket booster powerplant: 1 × pratt & whitney rocketdyne sjy61 scramjetperformance  maximum speed: 3,900 mph (6,200 km/h, 3,400 kn) maximum speed: mach 5.1 range: 460 mi (740 km, 400 nmi) service ceiling: 70,000 ft (21,300 m)   == see also == boeing small launch vehicle concept, includes a hypersonic waverider as the second stage flight airspeed record scramjet programs   == references ==   == external links == x-51 fact sheet on usaf site ""waverider page on boeing.com"". archived from the original on 14 november 2012. ""afrl mulls adding scope to x-51a waverider hypersonic tests"". flight international, march 2009. ""pratt & whitney rocketdyne scramjet excels in usaf tests"". aviation week - subscription youtube video of foxnews report, preceding test flight youtube video of test flight, shot from nasa chase plane')"
11,"A leading-edge extension (LEX) is a small extension to an aircraft wing surface, forward of the leading edge. The primary reason for adding an extension is to improve the airflow at high angles of attack and low airspeeds, to improve handling and delay the stall. A dog tooth can also improve airflow and reduce drag at higher speeds.


== Leading–edge slat ==

A leading-edge slat is an aerodynamic surface running spanwise just ahead of the wing leading edge. It creates a leading edge slot between the slat and wing which directs air over the wing surface, helping to maintain smooth airflow at low speeds and high angles of attack. This delays the stall, allowing the aircraft to fly at a higher angle of attack. Slats may be made fixed, or retractable in normal flight to minimize drag.


== Dogtooth extension ==

A dogtooth is a small, sharp zig-zag break in the leading edge of a wing. It is usually used on a swept wing, to generate a vortex flow field to prevent separated flow from progressing outboard at high angle of attack. The effect is the same as a wing fence. It can also be used on straight wings in a drooped leading edge arrangement.Many high-performance aircraft use the dogtooth design, which induces a vortex over the wing to control  boundary layer spanwise extension, increasing lift and improving resistance to stall. Some of the best-known uses of the dogtooth are in the stabilizer of the F-15 Eagle and the wings of the F-4 Phantom II, F/A-18 Super Hornet, CF-105 Arrow, F-8U Crusader, and the Ilyushin Il-62. Where the dogtooth is added as an afterthought, as for example on the Hawker Hunter and some variants of the Quest Kodiak, the dogtooth is created by adding an extension to the outer section of the leading edge.


== Leading-edge cuff ==

A leading edge cuff (or wing cuff) is a fixed aerodynamic device employed on fixed-wing aircraft to introduce a sharp discontinuity in the leading edge of the wing in the same way as a dogtooth. It also typically has a slightly drooped leading edge to improve low-speed characteristics.


== Leading-edge root extension ==

A leading-edge root extension (LERX) is a small fillet, typically roughly triangular in shape, running forward from the leading edge of the wing root to a point along the fuselage.  These are often called simply leading-edge extensions (LEX), although they are not the only kind. To avoid ambiguity, this article uses the term LERX.
On a modern fighter aircraft LERXes induce controlled airflow over the wing at high angles of attack, so delaying the stall and consequent loss of lift. In cruising flight the effect of the LERX is minimal. However at high angles of attack, as often encountered in a dog fight or during takeoff and landing, the LERX generates a high-speed vortex that attaches to the top of the wing. The vortex action maintains the attachment of the airflow to the upper-wing surface well past the normal stall point at which the airflow separates from the wing surface, thus sustaining lift at very high angles.
LERX were first used on the Northrop F-5 ""Freedom fighter"" which flew in 1959, and have since become commonplace on many combat aircraft. The F/A-18 Hornet has especially large examples, as does the Sukhoi Su-27 and the CAC/PAC JF-17 Thunder. The Su-27 LERX help make some advanced maneuvers possible, such as the Pugachev's Cobra, the Cobra Turn and the Kulbit.
A long, narrow sideways extension to the fuselage, attached in this position, is an example of a chine.


== Leading-edge vortex controller ==
Leading-edge vortex controller (LEVCON) systems are a continuation of leading-edge root extension (LERX) technology, but with actuation that allows the leading edge vortices to be modified without adjusting the aircraft's attitude. Otherwise they operate on the same principles as the LERX system to create lift augmenting leading edge vortices during high angle of attack flight.
This system has been incorporated in the Russian Sukhoi Su-57 and Indian HAL LCA Navy.The LEVCONs actuation ability also improves its performance over the LERX system in other areas.
When combined with the thrust vectoring controller (TVC), the aircraft controllability at extreme angles of attack is further increased, which assists in stunts which require supermaneuverability such as Pugachev's Cobra.  
Additionally, on the Sukhoi Su-57 the LEVCON system is used for increased departure-resistance in the event of TVC failure at a post-stall attitude. It can also be used for trimming the aircraft, and optimizing the lift to drag ratio during cruise.


== See also ==
Strake (aviation)
Vortex generator


== References ==","pandas(index=11, _1=11, text='a leading-edge extension (lex) is a small extension to an aircraft wing surface, forward of the leading edge. the primary reason for adding an extension is to improve the airflow at high angles of attack and low airspeeds, to improve handling and delay the stall. a dog tooth can also improve airflow and reduce drag at higher speeds.   == leading–edge slat ==  a leading-edge slat is an aerodynamic surface running spanwise just ahead of the wing leading edge. it creates a leading edge slot between the slat and wing which directs air over the wing surface, helping to maintain smooth airflow at low speeds and high angles of attack. this delays the stall, allowing the aircraft to fly at a higher angle of attack. slats may be made fixed, or retractable in normal flight to minimize drag.   == dogtooth extension ==  a dogtooth is a small, sharp zig-zag break in the leading edge of a wing. it is usually used on a swept wing, to generate a vortex flow field to prevent separated flow from progressing outboard at high angle of attack. the effect is the same as a wing fence. it can also be used on straight wings in a drooped leading edge arrangement.many high-performance aircraft use the dogtooth design, which induces a vortex over the wing to control  boundary layer spanwise extension, increasing lift and improving resistance to stall. some of the best-known uses of the dogtooth are in the stabilizer of the f-15 eagle and the wings of the f-4 phantom ii, f/a-18 super hornet, cf-105 arrow, f-8u crusader, and the ilyushin il-62. where the dogtooth is added as an afterthought, as for example on the hawker hunter and some variants of the quest kodiak, the dogtooth is created by adding an extension to the outer section of the leading edge.   == leading-edge cuff ==  a leading edge cuff (or wing cuff) is a fixed aerodynamic device employed on fixed-wing aircraft to introduce a sharp discontinuity in the leading edge of the wing in the same way as a dogtooth. it also typically has a slightly drooped leading edge to improve low-speed characteristics.   == leading-edge root extension ==  a leading-edge root extension (lerx) is a small fillet, typically roughly triangular in shape, running forward from the leading edge of the wing root to a point along the fuselage.  these are often called simply leading-edge extensions (lex), although they are not the only kind. to avoid ambiguity, this article uses the term lerx. on a modern fighter aircraft lerxes induce controlled airflow over the wing at high angles of attack, so delaying the stall and consequent loss of lift. in cruising flight the effect of the lerx is minimal. however at high angles of attack, as often encountered in a dog fight or during takeoff and landing, the lerx generates a high-speed vortex that attaches to the top of the wing. the vortex action maintains the attachment of the airflow to the upper-wing surface well past the normal stall point at which the airflow separates from the wing surface, thus sustaining lift at very high angles. lerx were first used on the northrop f-5 ""freedom fighter"" which flew in 1959, and have since become commonplace on many combat aircraft. the f/a-18 hornet has especially large examples, as does the sukhoi su-27 and the cac/pac jf-17 thunder. the su-27 lerx help make some advanced maneuvers possible, such as the pugachev\'s cobra, the cobra turn and the kulbit. a long, narrow sideways extension to the fuselage, attached in this position, is an example of a chine.   == leading-edge vortex controller == leading-edge vortex controller (levcon) systems are a continuation of leading-edge root extension (lerx) technology, but with actuation that allows the leading edge vortices to be modified without adjusting the aircraft\'s attitude. otherwise they operate on the same principles as the lerx system to create lift augmenting leading edge vortices during high angle of attack flight. this system has been incorporated in the russian sukhoi su-57 and indian hal lca navy.the levcons actuation ability also improves its performance over the lerx system in other areas. when combined with the thrust vectoring controller (tvc), the aircraft controllability at extreme angles of attack is further increased, which assists in stunts which require supermaneuverability such as pugachev\'s cobra. additionally, on the sukhoi su-57 the levcon system is used for increased departure-resistance in the event of tvc failure at a post-stall attitude. it can also be used for trimming the aircraft, and optimizing the lift to drag ratio during cruise.   == see also == strake (aviation) vortex generator   == references ==')"
12,"A vortex generator (VG) is an aerodynamic device, consisting of a small vane usually attached to a lifting surface (or airfoil, such as an aircraft wing) or a rotor blade of a wind turbine. VGs may also be attached to some part of an aerodynamic vehicle such as an aircraft fuselage or a car.  When the airfoil or the body is in motion relative to the air, the VG creates a vortex,  which, by removing some part of the slow-moving boundary layer in contact with the airfoil surface, delays local flow separation and aerodynamic stalling, thereby improving the effectiveness of wings and control surfaces, such as flaps, elevators, ailerons, and rudders.


== Method of operation ==
Vortex generators are most often used to delay flow separation.  To accomplish this they are often placed on the external surfaces of vehicles and wind turbine blades. On both aircraft and wind turbine blades they are usually installed quite close to the leading edge of the aerofoil in order to maintain steady airflow over the control surfaces at the trailing edge. VGs are typically rectangular or triangular, about as tall as the local boundary layer, and run in spanwise lines usually near the thickest part of the wing. They can be seen on the wings and vertical tails of many airliners.
Vortex generators are positioned obliquely so that they have an angle of attack with respect to the local airflow in order to create a tip vortex which draws energetic, rapidly moving outside air into the slow-moving boundary layer in contact with the surface. A turbulent boundary layer is less likely to separate than a laminar one, and is therefore desirable to ensure effectiveness of trailing-edge control surfaces. Vortex generators are used to trigger this transition. Other devices such as vortilons, leading-edge extensions, and leading-edge cuffs, also delay flow separation at high angles of attack by re-energizing the boundary layer.Examples of aircraft which use VGs include the ST Aerospace A-4SU Super Skyhawk and Symphony SA-160. For swept-wing transonic designs, VGs alleviate potential shock-stall problems (e.g., Harrier, Blackburn Buccaneer, Gloster Javelin).


== Aftermarket installation ==
Many aircraft carry vane vortex generators from time of manufacture, but there are also aftermarket suppliers who sell VG kits to improve the STOL performance of some light aircraft.  Aftermarket suppliers claim (i) that VGs lower stall speed and reduce take-off and landing speeds, and (ii) that VGs increase the effectiveness of ailerons, elevators and rudders, thereby improving controllability and safety at low speeds. For home-built and experimental kitplanes, VGs are cheap, cost-effective and can be installed quickly; but for certified aircraft installations, certification costs can be high, making the modification a relatively expensive process.Owners fit aftermarket VGs primarily to gain benefits at low speeds, but a downside is that such VGs may reduce cruise speed slightly. In tests performed on a Cessna 182 and a Piper PA-28-235 Cherokee, independent reviewers have documented a loss of cruise speed of 1.5 to 2.0 kn (2.8 to 3.7 km/h) .  However, these losses are relatively minor, since an aircraft wing at high speed has a small angle of attack, thereby reducing VG drag to a minimum.Owners have reported that on the ground, it can be harder to clear snow and ice from wing surfaces with VGs than from a smooth wing, but VGs are not generally prone to inflight icing as they reside within the boundary layer of airflow. VGs may also have sharp edges which can tear the fabric of airframe covers and may thus require special covers to be made.For twin-engined aircraft, manufacturers claim that VGs reduce single-engine control speed (Vmca), increase zero fuel and gross weight, improve the effectiveness of ailerons and rudder, provide a smoother ride in turbulence and make the aircraft a more stable instrument platform.


== Increase in maximum takeoff weight ==
Some VG kits available for light twin-engine airplanes may allow an increase in maximum takeoff weight.  The maximum takeoff weight of a twin-engine airplane is determined by structural requirements and single-engine climb performance requirements (which are lower for a lower stall speed).  For many light twin-engine airplanes, the single-engine climb performance requirements determine a lower maximum weight rather than the structural requirements.  Consequently, anything that can be done to improve the single-engine-inoperative climb performance will bring about an increase in maximum takeoff weight.In the US from 1945 until 1991,
the one-engine-inoperative climb requirement for multi-engine airplanes with a maximum takeoff weight of 6,000 lb (2,700 kg) or less was as follows:

All multi-engine airplanes having a stalling speed 
  
    
      
        
          V
          
            s
            0
          
        
      
    
    {\displaystyle V_{s0}}
   greater than 70 miles per hour shall have a steady rate of climb of at least 
  
    
      
        0.02
        (
        
          V
          
            s
            0
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle 0.02(V_{s0})^{2}}
   in feet per minute at an altitude of 5,000 feet with the critical engine inoperative and the remaining engines operating at not more than maximum continuous power, the inoperative propeller in the minimum drag position, landing gear retracted, wing flaps in the most favorable position …
where 
  
    
      
        
          V
          
            s
            0
          
        
      
    
    {\displaystyle V_{s0}}
   is the stalling speed in the landing configuration in miles per hour.
Installation of vortex generators can usually bring about a slight reduction in stalling speed of an airplane and therefore reduce the required one-engine-inoperative climb performance.  The reduced requirement for climb performance allows an increase in maximum takeoff weight, at least up to the maximum weight allowed by structural requirements.  An increase in maximum weight allowed by structural requirements can usually be achieved by specifying a maximum zero fuel weight or, if a maximum zero fuel weight is already specified as one of the airplane's limitations, by specifying a new higher maximum zero fuel weight.  For these reasons, vortex generator kits for many light twin-engine airplanes are accompanied by a reduction in maximum zero fuel weight and an increase in maximum takeoff weight.The one-engine-inoperative rate-of-climb requirement does not apply to single-engine airplanes, so gains in the maximum takeoff weight (based on stall speed or structural considerations) are less significant compared to those for 1945–1991 twins.
After 1991, the airworthiness certification requirements in the USA specify the one-engine-inoperative climb requirement as a gradient independent of stalling speed, so there is less opportunity for vortex generators to increase the maximum takeoff weight of multi-engine airplanes whose certification basis is FAR 23 at amendment 23-42 or later.


== Maximum landing weight ==
Because the landing weights of most light aircraft are determined by structural considerations and not by stall speed, most VG kits increase only the takeoff weight and not the landing weight.  Any increase in landing weight would require either structural modifications or re-testing the aircraft at the higher landing weight to demonstrate that the certification requirements are still met.  However, after a lengthy flight, sufficient fuel may have been used, thereby bringing the aircraft back below the permitted maximum landing weight.


== Aircraft noise reduction ==
Vortex generators have been used on the wing underside of Airbus A320 family aircraft to reduce noise generated by airflow over circular pressure equalisation vents for the fuel tanks. Lufthansa claims a noise reduction of up to 2 dB can thus be achieved.


== See also ==
Turbulator
Boundary layer suction
Boundary layer control
Circulation control wing


== References ==


== External links ==
Vortex Generators: 50 Years of Performance Benefits, a history of VGs","pandas(index=12, _1=12, text=""a vortex generator (vg) is an aerodynamic device, consisting of a small vane usually attached to a lifting surface (or airfoil, such as an aircraft wing) or a rotor blade of a wind turbine. vgs may also be attached to some part of an aerodynamic vehicle such as an aircraft fuselage or a car.  when the airfoil or the body is in motion relative to the air, the vg creates a vortex,  which, by removing some part of the slow-moving boundary layer in contact with the airfoil surface, delays local flow separation and aerodynamic stalling, thereby improving the effectiveness of wings and control surfaces, such as flaps, elevators, ailerons, and rudders.   == method of operation == vortex generators are most often used to delay flow separation.  to accomplish this they are often placed on the external surfaces of vehicles and wind turbine blades. on both aircraft and wind turbine blades they are usually installed quite close to the leading edge of the aerofoil in order to maintain steady airflow over the control surfaces at the trailing edge. vgs are typically rectangular or triangular, about as tall as the local boundary layer, and run in spanwise lines usually near the thickest part of the wing. they can be seen on the wings and vertical tails of many airliners. vortex generators are positioned obliquely so that they have an angle of attack with respect to the local airflow in order to create a tip vortex which draws energetic, rapidly moving outside air into the slow-moving boundary layer in contact with the surface. a turbulent boundary layer is less likely to separate than a laminar one, and is therefore desirable to ensure effectiveness of trailing-edge control surfaces. vortex generators are used to trigger this transition. other devices such as vortilons, leading-edge extensions, and leading-edge cuffs, also delay flow separation at high angles of attack by re-energizing the boundary layer.examples of aircraft which use vgs include the st aerospace a-4su super skyhawk and symphony sa-160. for swept-wing transonic designs, vgs alleviate potential shock-stall problems (e.g., harrier, blackburn buccaneer, gloster javelin).   == aftermarket installation == many aircraft carry vane vortex generators from time of manufacture, but there are also aftermarket suppliers who sell vg kits to improve the stol performance of some light aircraft.  aftermarket suppliers claim (i) that vgs lower stall speed and reduce take-off and landing speeds, and (ii) that vgs increase the effectiveness of ailerons, elevators and rudders, thereby improving controllability and safety at low speeds. for home-built and experimental kitplanes, vgs are cheap, cost-effective and can be installed quickly; but for certified aircraft installations, certification costs can be high, making the modification a relatively expensive process.owners fit aftermarket vgs primarily to gain benefits at low speeds, but a downside is that such vgs may reduce cruise speed slightly. in tests performed on a cessna 182 and a piper pa-28-235 cherokee, independent reviewers have documented a loss of cruise speed of 1.5 to 2.0 kn (2.8 to 3.7 km/h) .  however, these losses are relatively minor, since an aircraft wing at high speed has a small angle of attack, thereby reducing vg drag to a minimum.owners have reported that on the ground, it can be harder to clear snow and ice from wing surfaces with vgs than from a smooth wing, but vgs are not generally prone to inflight icing as they reside within the boundary layer of airflow. vgs may also have sharp edges which can tear the fabric of airframe covers and may thus require special covers to be made.for twin-engined aircraft, manufacturers claim that vgs reduce single-engine control speed (vmca), increase zero fuel and gross weight, improve the effectiveness of ailerons and rudder, provide a smoother ride in turbulence and make the aircraft a more stable instrument platform.   == increase in maximum takeoff weight == some vg kits available for light twin-engine airplanes may allow an increase in maximum takeoff weight.  the maximum takeoff weight of a twin-engine airplane is determined by structural requirements and single-engine climb performance requirements (which are lower for a lower stall speed).  for many light twin-engine airplanes, the single-engine climb performance requirements determine a lower maximum weight rather than the structural requirements.  consequently, anything that can be done to improve the single-engine-inoperative climb performance will bring about an increase in maximum takeoff weight.in the us from 1945 until 1991, the one-engine-inoperative climb requirement for multi-engine airplanes with a maximum takeoff weight of 6,000 lb (2,700 kg) or less was as follows:  all multi-engine airplanes having a stalling speed     v  s 0      is the stalling speed in the landing configuration in miles per hour. installation of vortex generators can usually bring about a slight reduction in stalling speed of an airplane and therefore reduce the required one-engine-inoperative climb performance.  the reduced requirement for climb performance allows an increase in maximum takeoff weight, at least up to the maximum weight allowed by structural requirements.  an increase in maximum weight allowed by structural requirements can usually be achieved by specifying a maximum zero fuel weight or, if a maximum zero fuel weight is already specified as one of the airplane's limitations, by specifying a new higher maximum zero fuel weight.  for these reasons, vortex generator kits for many light twin-engine airplanes are accompanied by a reduction in maximum zero fuel weight and an increase in maximum takeoff weight.the one-engine-inoperative rate-of-climb requirement does not apply to single-engine airplanes, so gains in the maximum takeoff weight (based on stall speed or structural considerations) are less significant compared to those for 1945–1991 twins. after 1991, the airworthiness certification requirements in the usa specify the one-engine-inoperative climb requirement as a gradient independent of stalling speed, so there is less opportunity for vortex generators to increase the maximum takeoff weight of multi-engine airplanes whose certification basis is far 23 at amendment 23-42 or later.   == maximum landing weight == because the landing weights of most light aircraft are determined by structural considerations and not by stall speed, most vg kits increase only the takeoff weight and not the landing weight.  any increase in landing weight would require either structural modifications or re-testing the aircraft at the higher landing weight to demonstrate that the certification requirements are still met.  however, after a lengthy flight, sufficient fuel may have been used, thereby bringing the aircraft back below the permitted maximum landing weight.   == aircraft noise reduction == vortex generators have been used on the wing underside of airbus a320 family aircraft to reduce noise generated by airflow over circular pressure equalisation vents for the fuel tanks. lufthansa claims a noise reduction of up to 2 db can thus be achieved.   == see also == turbulator boundary layer suction boundary layer control circulation control wing   == references ==   == external links == vortex generators: 50 years of performance benefits, a history of vgs"")"
13,"In aerospace engineering, payload fraction is a common term used to characterize the efficiency of a particular design. Payload fraction is calculated by dividing the weight of the payload by the takeoff weight of aircraft. 
Fuel represents a considerable amount of the overall takeoff weight, and for shorter trips it is quite common to load less fuel in order to carry a lighter load. For this reason the useful load fraction calculates a similar number, but based on the combined weight of the payload and fuel together.
Propeller-driven airliners had useful load fractions on the order of 25-35%. Modern jet airliners have considerably higher useful load fractions, on the order of 45-55%.
For spacecraft the payload fraction is often less than 1%, while the useful load fraction is perhaps 90%. In this case the useful load fraction is not a useful term, because spacecraft typically cannot reach orbit without a full fuel load. For this reason the related term  propellant mass fraction, is used instead. However, if the latter is large, the payload can only be small.


== Examples ==
Note: the above table may incorrectly include the mass of the empty upper stage or stages.


== See also ==
Tsiolkovsky rocket equation


== References ==","pandas(index=13, _1=13, text='in aerospace engineering, payload fraction is a common term used to characterize the efficiency of a particular design. payload fraction is calculated by dividing the weight of the payload by the takeoff weight of aircraft. fuel represents a considerable amount of the overall takeoff weight, and for shorter trips it is quite common to load less fuel in order to carry a lighter load. for this reason the useful load fraction calculates a similar number, but based on the combined weight of the payload and fuel together. propeller-driven airliners had useful load fractions on the order of 25-35%. modern jet airliners have considerably higher useful load fractions, on the order of 45-55%. for spacecraft the payload fraction is often less than 1%, while the useful load fraction is perhaps 90%. in this case the useful load fraction is not a useful term, because spacecraft typically cannot reach orbit without a full fuel load. for this reason the related term  propellant mass fraction, is used instead. however, if the latter is large, the payload can only be small.   == examples == note: the above table may incorrectly include the mass of the empty upper stage or stages.   == see also == tsiolkovsky rocket equation   == references ==')"
14,"A tiger team is composed of specialists assembled to work on a specific goal or to solve a particular problem.


== Term ==
A 1964 paper entitled Program Management in Design and Development used the term tiger teams and defined it as ""a team of undomesticated and uninhibited technical specialists, selected for their experience, energy, and imagination, and assigned to track down relentlessly every possible source of failure in a spacecraft subsystem or simulation"". The paper consists of anecdotes and answers to questions from a panel on improving issues in program management concerning testing and quality assurance in aerospace vehicle development and production. One of the authors was Walter C. Williams, an engineer at the Manned Spacecraft Center and part of the Edwards Air Force Base National Advisory Committee for Aeronautics. Williams suggests that tiger teams are an effective and useful method for advancing the reliability of systems and subsystems in the context of actual flight environments. Jane Goodall, Liam Hunt and Kate Herron, among others, have noted that tigers are not naturally cooperative animals and have suggested referring to “chimpanzee teams” because of the intense cooperation that occurs in chimpanzee social groups.


== Examples ==
A tiger team was crucial to the Apollo 13 lunar landing mission in 1970. During the mission, part of the Apollo 13 Service Module malfunctioned and exploded. A team of specialists was formed to fix the issue and bring the astronauts back to Earth safely, led by NASA Flight and Mission Operations Director Gene Kranz. Kranz and the members of his ""White Team"", later designated the ""Tiger Team"", received the Presidential Medal of Freedom for their efforts in the Apollo 13 mission.
In security work, a tiger team is a group that tests an organization's ability to protect its assets by attempting to defeat its physical or information security. In this context, the tiger team is often a permanent team as security is typically an ongoing priority. For example, one implementation of an information security tiger team approach divides the team into two co-operating groups: one for vulnerability research, which finds and researches the technical aspects of a vulnerability, and one for vulnerability management, which manages communication and feedback between the team and the organization, as well as ensuring each discovered vulnerability is tracked throughout its life-cycle and ultimately resolved.
An initiative involving tiger teams was implemented by the United States Department of Energy (DOE) under then-Secretary James D. Watkins. From 1989 through 1992 the DOE formed tiger teams to assess 35 DOE facilities for compliance with environment, safety, and health requirements. Beginning in October 1991 smaller tiger teams were formed to perform more detailed follow up assessments to focus on the most pressing issues.
The NASA Engineering and Safety Center (NESC) puts together ""tiger teams"" of engineers and scientists from multiple NASA centers to assist solving complex problems when requested by a project or program.


== See also ==
Penetration test
Red team


== References ==


== External links ==
""All About Tiger Team""This article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.","pandas(index=14, _1=14, text='a tiger team is composed of specialists assembled to work on a specific goal or to solve a particular problem.   == term == a 1964 paper entitled program management in design and development used the term tiger teams and defined it as ""a team of undomesticated and uninhibited technical specialists, selected for their experience, energy, and imagination, and assigned to track down relentlessly every possible source of failure in a spacecraft subsystem or simulation"". the paper consists of anecdotes and answers to questions from a panel on improving issues in program management concerning testing and quality assurance in aerospace vehicle development and production. one of the authors was walter c. williams, an engineer at the manned spacecraft center and part of the edwards air force base national advisory committee for aeronautics. williams suggests that tiger teams are an effective and useful method for advancing the reliability of systems and subsystems in the context of actual flight environments. jane goodall, liam hunt and kate herron, among others, have noted that tigers are not naturally cooperative animals and have suggested referring to “chimpanzee teams” because of the intense cooperation that occurs in chimpanzee social groups.   == examples == a tiger team was crucial to the apollo 13 lunar landing mission in 1970. during the mission, part of the apollo 13 service module malfunctioned and exploded. a team of specialists was formed to fix the issue and bring the astronauts back to earth safely, led by nasa flight and mission operations director gene kranz. kranz and the members of his ""white team"", later designated the ""tiger team"", received the presidential medal of freedom for their efforts in the apollo 13 mission. in security work, a tiger team is a group that tests an organization\'s ability to protect its assets by attempting to defeat its physical or information security. in this context, the tiger team is often a permanent team as security is typically an ongoing priority. for example, one implementation of an information security tiger team approach divides the team into two co-operating groups: one for vulnerability research, which finds and researches the technical aspects of a vulnerability, and one for vulnerability management, which manages communication and feedback between the team and the organization, as well as ensuring each discovered vulnerability is tracked throughout its life-cycle and ultimately resolved. an initiative involving tiger teams was implemented by the united states department of energy (doe) under then-secretary james d. watkins. from 1989 through 1992 the doe formed tiger teams to assess 35 doe facilities for compliance with environment, safety, and health requirements. beginning in october 1991 smaller tiger teams were formed to perform more detailed follow up assessments to focus on the most pressing issues. the nasa engineering and safety center (nesc) puts together ""tiger teams"" of engineers and scientists from multiple nasa centers to assist solving complex problems when requested by a project or program.   == see also == penetration test red team   == references ==   == external links == ""all about tiger team""this article is based on material taken from  the free on-line dictionary of computing  prior to 1 november 2008 and incorporated under the ""relicensing"" terms of the gfdl, version 1.3 or later.')"
15,"A multistage rocket, or step rocket, is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. A tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. The result is effectively two or more rockets stacked on top of or attached next to each other. Two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched.
By jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. Each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. This staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height. 
In serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. In parallel staging schemes solid or liquid rocket boosters are used to assist with launch. These are sometimes referred to as ""stage 0"". In the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. When the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. The first stage then burns to completion and falls off. This leaves a smaller rocket, with the second stage on the bottom, which then fires. Known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. In some cases with serial staging, the upper stage ignites before the separation—the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles.
A multistage rocket is required to reach orbital speed. Single-stage-to-orbit designs are sought, but have not yet been demonstrated.


== Performance ==

The reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. This relation is given by the classical rocket equation:

  
    
      
        Δ
        v
        =
        
          v
          
            e
          
        
        ln
        ⁡
        
          (
          
            
              
                m
                
                  0
                
              
              
                m
                
                  f
                
              
            
          
          )
        
      
    
    {\displaystyle \Delta v=v_{\text{e}}\ln \left({\frac {m_{0}}{m_{f}}}\right)}
  where:

  
    
      
        Δ
        v
         
      
    
    {\displaystyle \Delta v\ }
   is delta-v  of the vehicle (change of velocity plus losses due to gravity and atmospheric drag);

  
    
      
        
          m
          
            0
          
        
      
    
    {\displaystyle m_{0}}
   is the initial total (wet) mass, equal to final (dry) mass plus propellant;

  
    
      
        
          m
          
            f
          
        
      
    
    {\displaystyle m_{f}}
   is the final (dry) mass, after the propellant is expended;

  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   is the effective exhaust velocity (determined by propellant, engine design and throttle condition);

  
    
      
        ln
      
    
    {\displaystyle \ln }
   is the natural logarithm function.The delta v required to reach low Earth orbit (or the required velocity of a sufficiently heavy suborbital payload) requires a wet to dry mass ratio larger than can realistically be achieved in a single rocket stage. The multistage rocket overcomes this limit by splitting the delta-v into fractions. As each lower stage drops off and the succeeding stage fires, the rest of the rocket is still traveling near the burnout speed. Each lower stage's dry mass includes the propellant in the upper stages, and each succeeding upper stage has reduced its dry mass by discarding the useless dry mass of the spent lower stages.
A further advantage is that each stage can use a different type of rocket engine, each tuned for its particular operating conditions. Thus the lower-stage engines are designed for use at atmospheric pressure, while the upper stages can use engines suited to near vacuum conditions. Lower stages tend to require more structure than upper as they need to bear their own weight plus that of the stages above them. Optimizing the structure of each stage decreases the weight of the total vehicle and provides further advantage.
The advantage of staging comes at the cost of the lower stages lifting engines which are not yet being used, as well as making the entire rocket more complex and harder to build than a single stage. In addition, each staging event is a possible point of launch failure, due to separation failure, ignition failure, or stage collision. Nevertheless, the savings are so great that every rocket ever used to deliver a payload into orbit has had staging of some sort.
One of the most common measures of rocket efficiency is its specific impulse, which is defined as the thrust per flow rate (per second) of propellant consumption:

  
    
      
        
          I
          
            
              s
              p
            
          
        
      
    
    {\displaystyle I_{\mathrm {sp} }}
   = 
  
    
      
         
        
          
            T
            
              
                
                  
                    d
                    m
                  
                  
                    d
                    t
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle \ {\frac {T}{{\frac {dm}{dt}}g_{\mathrm {0} }}}}
  When rearranging the equation such that thrust is calculated as a result of the other factors, we have:

  
    
      
        T
        =
        
          I
          
            
              s
              p
            
          
        
        
          g
          
            
              0
            
          
        
        
          
            
              d
              m
            
            
              d
              t
            
          
        
      
    
    {\displaystyle T=I_{\mathrm {sp} }g_{\mathrm {0} }{\frac {dm}{dt}}}
  These equations show that a higher specific impulse means a more efficient rocket engine, capable of burning for longer periods of time.  In terms of staging, the initial rocket stages usually have a lower specific impulse rating, trading efficiency for superior thrust in order to quickly push the rocket into higher altitudes.  Later stages of the rocket usually have a higher specific impulse rating because the vehicle is further outside the atmosphere and the exhaust gas does not need to expand against as much atmospheric pressure.
When selecting the ideal rocket engine to use as an initial stage for a launch vehicle, a useful performance metric to examine is the thrust-to-weight ratio, and is calculated by the equation:

  
    
      
        T
        W
        R
        =
        
          
            T
            
              m
              
                g
                
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle TWR={\frac {T}{mg_{\mathrm {0} }}}}
  The common thrust-to-weight ratio of a launch vehicle is within the range of 1.3 to 2.0.
Another performance metric to keep in mind when designing each rocket stage in a mission is the burn time, which is the amount of time the rocket engine will last before it has exhausted all of its propellant.  For most non-final stages, thrust and specific impulse can be assumed constant, which allows the equation for burn time to be written as:

  
    
      
        Δ
        
          t
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
            T
          
        
        ×
        (
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle \Delta {t}={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }}{T}}\times (m_{\mathrm {0} }-m_{\mathrm {f} })}
  Where 
  
    
      
        
          m
          
            
              0
            
          
        
      
    
    {\displaystyle m_{\mathrm {0} }}
   and 
  
    
      
        
          m
          
            
              f
            
          
        
      
    
    {\displaystyle m_{\mathrm {f} }}
   are the initial and final masses of the rocket stage respectively.  In conjunction with the burnout time, the burnout height and velocity are obtained using the same values, and are found by these two equations:

  
    
      
        
          h
          
            
              b
              o
            
          
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
            
              m
              
                
                  e
                
              
            
          
        
        ×
        (
        
          m
          
            
              f
            
          
        
         
        
          l
          n
        
        (
        
          m
          
            
              f
            
          
        
        
          /
        
        
          m
          
            
              0
            
          
        
        )
        +
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle h_{\mathrm {bo} }={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }}{m_{\mathrm {e} }}}\times (m_{\mathrm {f} }~\mathrm {ln} (m_{\mathrm {f} }/m_{\mathrm {0} })+m_{\mathrm {0} }-m_{\mathrm {f} })}
  

  
    
      
        
          v
          
            
              b
              o
            
          
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
              
                m
                
                  
                    0
                  
                
              
            
            
              m
              
                
                  f
                
              
            
          
        
        −
        
          
            
              g
              
                
                  0
                
              
            
            
              m
              
                
                  e
                
              
            
          
        
        (
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle v_{\mathrm {bo} }={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }m_{\mathrm {0} }}{m_{\mathrm {f} }}}-{\frac {g_{\mathrm {0} }}{m_{\mathrm {e} }}}(m_{\mathrm {0} }-m_{\mathrm {f} })}
  When dealing with the problem of calculating the total burnout velocity or time for the entire rocket system, the general procedure for doing so is as follows:
Partition the problem calculations into however many stages the rocket system comprises.
Calculate the initial and final mass for each individual stage.
Calculate the burnout velocity, and sum it with the initial velocity for each individual stage.  Assuming each stage occurs immediately after the previous, the burnout velocity becomes the initial velocity for the following stage.
Repeat the previous two steps until the burnout time and/or velocity has been calculated for the final stage.It is important to note that the burnout time does not define the end of the rocket stage's motion, as the vehicle will still have a velocity that will allow it to coast upward for a brief amount of time until the acceleration of the planet's gravity gradually changes it to a downward direction.  The velocity and altitude of the rocket after burnout can be easily modeled using the basic physics equations of motion.
When comparing one rocket with another, it is impractical to directly compare the rocket's certain trait with the same trait of another because their individual attributes are often not independent of one another.  For this reason, dimensionless ratios have been designed to enable a more meaningful comparison between rockets.  The first is the initial to final mass ratio, which is the ratio between the rocket stage's full initial mass and the rocket stage's final mass once all of its fuel has been consumed.  The equation for this ratio is:

  
    
      
        η
        =
        
          
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    p
                  
                
              
              +
              
                m
                
                  
                    P
                    L
                  
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                    L
                  
                
              
            
          
        
      
    
    {\displaystyle \eta ={\frac {m_{\mathrm {E} }+m_{\mathrm {p} }+m_{\mathrm {PL} }}{m_{\mathrm {E} }+m_{\mathrm {PL} }}}}
  Where 
  
    
      
        
          m
          
            
              E
            
          
        
      
    
    {\displaystyle m_{\mathrm {E} }}
   is the empty mass of the stage, 
  
    
      
        
          m
          
            
              p
            
          
        
      
    
    {\displaystyle m_{\mathrm {p} }}
   is the mass of the propellant, and 
  
    
      
        
          m
          
            
              P
              L
            
          
        
      
    
    {\displaystyle m_{\mathrm {PL} }}
   is the mass of the payload.  
The second dimensionless performance quantity is the structural ratio, which is the ratio between the empty mass of the stage, and the combined empty mass and propellant mass as shown in this equation:

  
    
      
        ϵ
        =
        
          
            
              m
              
                
                  E
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                  
                
              
            
          
        
      
    
    {\displaystyle \epsilon ={\frac {m_{\mathrm {E} }}{m_{\mathrm {E} }+m_{\mathrm {P} }}}}
  The last major dimensionless performance quantity is the payload ratio, which is the ratio between the payload mass and the combined mass of the empty rocket stage and the propellant:

  
    
      
        λ
        =
        
          
            
              m
              
                
                  P
                  L
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                  
                
              
            
          
        
      
    
    {\displaystyle \lambda ={\frac {m_{\mathrm {PL} }}{m_{\mathrm {E} }+m_{\mathrm {P} }}}}
  After comparing the three equations for the dimensionless quantities,  it is easy to see that they are not independent of each other, and in fact, the initial to final mass ratio can be rewritten in terms of structural ratio and payload ratio:

  
    
      
        η
        =
        
          
            
              1
              +
              λ
            
            
              ϵ
              +
              λ
            
          
        
      
    
    {\displaystyle \eta ={\frac {1+\lambda }{\epsilon +\lambda }}}
  These performance ratios can also be used as references for how efficient a rocket system will be when performing optimizations and comparing varying configurations for a mission.


== Component selection and sizing ==

For initial sizing, the rocket equations can be used to derive the amount of propellant needed for the rocket based on the specific impulse of the engine and the total impulse required in N*s.  The equation is:

  
    
      
        
          m
          
            
              p
            
          
        
        =
        
          I
          
            
              t
              o
              t
            
          
        
        
          /
        
        (
        g
        ∗
        
          I
          
            
              s
              p
            
          
        
        )
      
    
    {\displaystyle m_{\mathrm {p} }=I_{\mathrm {tot} }/(g*I_{\mathrm {sp} })}
  where g is the gravity constant of Earth.  This also enables the volume of storage required for the fuel to be calculated if the density of the fuel is known, which is almost always the case when designing the rocket stage.  The volume is yielded when dividing the mass of the propellant by its density.  Asides from the fuel required, the mass of the rocket structure itself must also be determined, which requires taking into account the mass of the required thrusters, electronics, instruments, power equipment, etc. These are known quantities for typical off the shelf hardware that should be considered in the mid to late stages of the design, but for preliminary and conceptual design, a simpler approach can be taken.  Assuming one engine for a rocket stage provides all of the total impulse for that particular segment, a mass fraction can be used to determine the mass of the system.  The mass of the stage transfer hardware such as initiators and safe-and-arm devices are very small by comparison and can be considered negligible.  
For modern day solid rocket motors, it is a safe and reasonable assumption to say that 91 to 94 percent of the total mass is fuel.  It is also important to note there is a small percentage of ""residual"" propellant that will be left stuck and unusable inside the tank, and should also be taken into consideration when determining amount of fuel for the rocket.  A common initial estimate for this residual propellant is five percent.  With this ratio and the mass of the propellant calculated, the mass of the empty rocket weight can be determined.  
Sizing rockets using a liquid bipropellant requires a slightly more involved approach because of the fact that there are two separate tanks that are required:  One for the fuel, and one for the oxidizer.  The ratio of these two quantities is known as the mixture ratio, and is defined by the equation:

  
    
      
        O
        
          /
        
        F
        =
        
          m
          
            
              o
              x
            
          
        
        
          /
        
        
          m
          
            
              f
              u
              e
              l
            
          
        
      
    
    {\displaystyle O/F=m_{\mathrm {ox} }/m_{\mathrm {fuel} }}
  Where 
  
    
      
        
          m
          
            
              o
              x
            
          
        
      
    
    {\displaystyle m_{\mathrm {ox} }}
   is the mass of the oxidizer and 
  
    
      
        
          m
          
            
              f
              u
              e
              l
            
          
        
      
    
    {\displaystyle m_{\mathrm {fuel} }}
   is the mass of the fuel.  This mixture ratio not only governs the size of each tank, but also the specific impulse of the rocket.  Determining the ideal mixture ratio is a balance of compromises between various aspects of the rocket being designed, and can vary depending on the type of fuel and oxidizer combination being used.  For example, a mixture ratio of a bipropellant could be adjusted such that it may not have the optimal specific impulse, but will result in fuel tanks of equal size.  This would yield simpler and cheaper manufacturing, packing, configuring, and integrating of the fuel systems with the rest of the rocket, and can become a benefit that could outweigh the drawbacks of a less efficient specific impulse rating.  But suppose the defining constraint for the launch system is volume, and a low density fuel is required such as hydrogen.  This example would be solved by using an oxidizer-rich mixture ratio, reducing efficiency and specific impulse rating, but will meet a smaller tank volume requirement.


== Optimal staging and restricted staging ==


=== Optimal ===
The ultimate goal of optimal staging is to maximize the payload ratio (see ratios under performance), meaning the largest amount of payload is carried up to the required burnout velocity using the least amount of non-payload mass, which comprises everything else. Here are a few quick rules and guidelines to follow in order to reach optimal staging:
Initial stages should have lower 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  , and later/final stages should have higher 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  .
The stages with the lower 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
   should contribute more ΔV.
The next stage is always a smaller size than the previous stage.
Similar stages should provide similar ΔV.The payload ratio can be calculated for each individual stage, and when multiplied together in sequence, will yield the overall payload ratio of the entire system.  It is important to note that when computing payload ratio for individual stages, the payload includes the mass of all the stages after the current one.  The overall payload ratio is:

  
    
      
        λ
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda =\prod _{i=1}^{n}\lambda _{i}}
  Where n is the number of stages the rocket system comprises.  Similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and ΔV requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above.  Two common methods of determining this perfect ΔV partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error.  For the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage.  From there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system.


=== Restricted ===
Restricted rocket staging is based on the simplified assumption that each of the stages of the rocket system have the same specific impulse, structural ratio, and payload ratio, the only difference being the total mass of each increasing stage is less than that of the previous stage.  Although this assumption may not be the ideal approach to yielding an efficient or optimal system, it greatly simplifies the equations for determining the burnout velocities, burnout times, burnout altitudes, and mass of each stage.  This would make for a better approach to a conceptual design in a situation where a basic understanding of the system behavior is preferential to a detailed, accurate design.  
One important concept to understand when undergoing restricted rocket staging, is how the burnout velocity is affected by the number of stages that split up the rocket system.  Increasing the number of stages for a rocket while keeping the specific impulse, payload ratios and structural ratios constant will always yield a higher burnout velocity than the same systems that use fewer stages.  However, the law of diminishing returns is evident in that each increment in number of stages gives less of an improvement in burnout velocity than the previous increment.  The burnout velocity gradually converges towards an asymptotic value as the number of stages increases towards a very high number.  In addition to diminishing returns in burnout velocity improvement, the main reason why real world rockets seldom use more than three stages is because of increase of weight and complexity in the system for each added stage, ultimately yielding a higher cost for deployment.


== Tandem vs parallel staging design ==
A rocket system that implements tandem staging means that each individual stage runs in order one after the other.  The rocket breaks free from the previous stage, then begins burning through the next stage in straight succession.  On the other hand, a rocket that implements parallel staging has two or more different stages that are active at the same time.  For example, the Space Shuttle has two Solid Rocket Boosters that burn simultaneously.  Upon launch, the boosters ignite, and at the end of the stage, the two boosters are discarded while the external fuel tank is kept for another stage.  
Most quantitative approaches to the design of the rocket system's performance are focused on tandem staging, but the approach can be easily modified to include parallel staging.  To begin with, the different stages of the rocket should be clearly defined.  Continuing with the previous example, the end of the first stage which is sometimes referred to as 'stage 0', can be defined as when the side boosters separate from the main rocket.  From there, the final mass of stage one can be considered the sum of the empty mass of stage one, the mass of stage two (the main rocket and the remaining unburned fuel) and the mass of the payload.


== Upper stages ==
High-altitude and space-bound upper stages are designed to operate with little or no atmospheric pressure. This allows the use of lower pressure combustion chambers and engine nozzles with optimal vacuum expansion ratios. Some upper stages, especially those using hypergolic propellants like Delta-K or Ariane 5 ES second stage, are pressure fed, which eliminates the need for complex turbopumps. Other upper stages, such as the Centaur or DCSS, use liquid hydrogen expander cycle engines, or gas generator cycle engines like the Ariane 5 ECA's HM7B or the S-IVB's J-2. These stages are usually tasked with completing orbital injection and accelerating payloads into higher energy orbits such as GTO or to escape velocity. Upper stages, such as Fregat, used primarily to bring payloads from low Earth orbit to GTO or beyond are sometimes referred to as space tugs.


== Assembly ==
Each individual stage is generally assembled at its manufacturing site and shipped to the launch site; the term vehicle assembly refers to the mating of all rocket stage(s) and the spacecraft payload into a single assembly known as a space vehicle. Single-stage vehicles (suborbital), and multistage vehicles on the smaller end of the size range, can usually be assembled directly on the launch pad by lifting the stage(s) and spacecraft vertically in place by means of a crane.
This is generally not practical for larger space vehicles, which are assembled off the pad and moved into place on the launch site by various methods. NASA's Apollo/Saturn V manned Moon landing vehicle, and  Space Shuttle, were assembled vertically onto mobile launcher platforms with attached launch umbilical towers, in a Vehicle Assembly Building, and then a special crawler-transporter moved the entire vehicle stack to the launch pad in an upright position. 
In contrast, vehicles such as the Russian Soyuz rocket and the SpaceX Falcon 9 are assembled horizontally in a processing hangar, transported horizontally, and then brought upright at the pad.


== Passivation and space debris ==
Spent upper stages of launch vehicles are a significant source of space debris remaining in orbit in a non-operational state for many years after use, and occasionally, large debris fields created from the breakup of a single upper stage while in orbit.After the 1990s, spent upper stages are generally passivated after their use as a launch vehicle is complete in order to minimize risks while the stage remains derelict in orbit.  Passivation means removing any sources of stored energy remaining on the vehicle, as by dumping fuel or discharging batteries.
Many early upper stages, in both the Soviet and U.S. space programs, were not passivated after mission completion.  During the initial attempts to characterize the space debris problem, it became evident that a good proportion of all debris was due to the breaking up of rocket upper stages, particularly unpassivated upper-stage propulsion units.


== History and development ==
An illustration and description in the 14th century Chinese Huolongjing by Jiao Yu and Liu Bowen shows the oldest known multistage rocket; this was the ""fire-dragon issuing from the water"" (火龙出水, huǒ lóng chū shuǐ), used mostly by the Chinese navy. It was a two-stage rocket that had booster rockets that would eventually burn out, yet before they did they automatically ignited a number of smaller rocket arrows that were shot out of the front end of the missile, which was shaped like a dragon's head with an open mouth. This multi-stage rocket may be considered the ancestor to the modern YingJi-62 ASCM. The British scientist and historian Joseph Needham points out that the written material and depicted illustration of this rocket come from the oldest stratum of the Huolongjing, which can be dated roughly 1300–1350 AD (from the book's part 1, chapter 3, page 23).Another example of an early multistaged rocket is the Juhwa (走火) of Korean development. It was proposed by medieval Korean engineer, scientist and inventor Choe Museon and developed by the Firearms Bureau (火㷁道監) during the 14th century. The rocket had the length of 15 cm and 13 cm; the diameter was 2.2 cm. It was attached to an arrow 110 cm long; experimental records show that the first results were around 200m in range. There are records that show Korea kept developing this technology until it came to produce the Singijeon, or 'magical machine arrows' in the 16th century. 
The earliest experiments with multistage rockets in Europe were made in 1551 by Austrian Conrad Haas (1509–1576), the arsenal master of the town of Hermannstadt, Transylvania (now Sibiu/Hermannstadt, Romania). This concept was developed independently by at least four individuals:

Kazimieras Simonavičius of the Polish–Lithuanian Commonwealth (1600–1651)
the Russian Konstantin Tsiolkovsky (1857–1935)
the American Robert Goddard (1882–1945)
the German Hermann Oberth (1894–1989), born in Hermannstadt, TransylvaniaThe first high-speed multistage rockets were the RTV-G-4 Bumper rockets tested at the White Sands Proving Ground and later at Cape Canaveral from 1948 to 1950. These consisted of a V-2 rocket and a WAC Corporal sounding rocket. The greatest altitude ever reached was 393 km, attained on February 24, 1949, at White Sands.
In 1947, the Soviet rocket engineer and scientist Mikhail Tikhonravov developed a theory of parallel stages, which he called ""packet rockets"". In his scheme, three parallel stages were fired from liftoff, but all three engines were fueled from the outer two stages, until they are empty and could be ejected. This is more efficient than sequential staging, because the second-stage engine is never just dead weight. In 1951, Soviet engineer and scientist Dmitry Okhotsimsky carried out a pioneering engineering study of general sequential and parallel staging, with and without the pumping of fuel between stages. The design of the R-7 Semyorka emerged from that study.  The trio of rocket engines used in the first stage of the American Atlas I and Atlas II launch vehicles, arranged in a row, used parallel staging in a similar way: the outer pair of booster engines existed as a jettisonable pair which would, after they shut down, drop away with the lowermost outer skirt structure, leaving the central sustainer engine to complete the first stage's engine burn towards apogee or orbit.


== Separation events ==
Separation of each portion of a multistage rocket introduces additional risk into the success of the launch mission.  Reducing the number of separation events results in a reduction in complexity. 
Separation events occur when stages or strap-on boosters separate after use, when the payload fairing separates prior to orbital insertion, or when used, a launch escape system which separates after the early phase of a launch. Pyrotechnic fasteners or pneumatic systems like on the Falcon 9 Full Thrust are typically used to separate rocket stages.


== Three stage to orbit ==
The three-stage-to-orbit launch system is a commonly used rocket system to attain Earth orbit.
The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity.
It is intermediate between a four-stage-to-orbit launcher and a two-stage-to-orbit launcher.


=== Examples of three stage to orbit systems ===
Saturn V
Vanguard
Ariane 4 (optional boosters)
Ariane 2
Ariane 1 (four stages)
GSLV (three stages and boosters)
PSLV (four stages)
Proton (optional fourth stage)
Long March 5 (optional boosters and optional third stage)
Long March 1, Long March 1D
Zenit-3SL
Minotaur IV (four stages)
Minotaur V (five stages)
Unha-3
KSLV-2 ""Nuri""


=== Examples of two stages with boosters ===
Other designs (in fact, most modern medium- to heavy-lift designs) do not have all three stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with two core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.

US Space Shuttle — SRB first stage ; External Tank + SSME second stage ; OMS on internal tanks third stage ;
Angara A5
Ariane 5
Atlas V 551
Delta II  third stage)
Delta III
Delta IV-Medium+ and -Heavy
Falcon Heavy
Geosynchronous Satellite Launch Vehicle Mk III (However, like the Titan IIIC, the GSLV MkIII is launched solely by the side boosters. The main core only ignites a few minutes into flight, shortly before the boosters are jettisoned.)
H-IIA, H-IIB
Soyuz
Space Launch System
Titan IV
Long March 2E, Long March 2F, Long March 3B


== Four stage to orbit ==
The four-stage-to-orbit launch system is a rocket system used to attain Earth orbit.
The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity.
It is intermediate between a five-stage-to-orbit launcher and a three-stage-to-orbit launcher.


=== Examples of four stage to orbit systems ===
Ariane 1
PSLV
Minotaur IV
Proton (optional fourth stage)
Minotaur V (five stages)
ASLV (five stages)


=== Examples of three stages with boosters ===
Other designs do not have all four stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with three core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.

Long March 5 (optional boosters and optional third stage)


== See also ==
Multistage rocket
Three-stage-to-orbit
Two-stage-to-orbit
Single-stage-to-orbit
Adapter
Reusable launch system
Space tug
Apogee kick motor
Conrad Haas
Modular rocket


== References ==","pandas(index=15, _1=15, text='a multistage rocket, or step rocket, is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. a tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. the result is effectively two or more rockets stacked on top of or attached next to each other. two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched. by jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. this staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height. in serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. in parallel staging schemes solid or liquid rocket boosters are used to assist with launch. these are sometimes referred to as ""stage 0"". in the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. when the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. the first stage then burns to completion and falls off. this leaves a smaller rocket, with the second stage on the bottom, which then fires. known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. in some cases with serial staging, the upper stage ignites before the separation—the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles. a multistage rocket is required to reach orbital speed. single-stage-to-orbit designs are sought, but have not yet been demonstrated.   == performance ==  the reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. this relation is given by the classical rocket equation:     δ v =  v  e   ln \u2061  (    m  0    m  f     )     where n is the number of stages the rocket system comprises.  similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and δv requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above.  two common methods of determining this perfect δv partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error.  for the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage.  from there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system. other designs do not have all four stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with three core stages. in these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. the boosters are jettisoned a few minutes into flight to reduce weight.  long march 5 (optional boosters and optional third stage)   == see also == multistage rocket three-stage-to-orbit two-stage-to-orbit single-stage-to-orbit adapter reusable launch system space tug apogee kick motor conrad haas modular rocket   == references ==')"
16,"The advanced multi-mission operations system (AMMOS) is a common set of services and tools created by the Interplanetary Network Directorate, a division of the Jet Propulsion Laboratory, for use in JPL's operation of spacecraft. These tools include a means by which mission planning and analysis can be undertaken, as well as developing pre-planned command sequences for the spacecraft. AMMOS also provides a means by which downlinked data can be displayed and manipulated, including key mission telemetry such as readings of temperature, pressure, power, and other critical indicators. This common toolset allows space missions to minimize the cost of developing operations infrastructure, which is very important in light of recent restricted spending by space agencies.


== References ==
JPL AMMOS Landing Page","pandas(index=16, _1=16, text=""the advanced multi-mission operations system (ammos) is a common set of services and tools created by the interplanetary network directorate, a division of the jet propulsion laboratory, for use in jpl's operation of spacecraft. these tools include a means by which mission planning and analysis can be undertaken, as well as developing pre-planned command sequences for the spacecraft. ammos also provides a means by which downlinked data can be displayed and manipulated, including key mission telemetry such as readings of temperature, pressure, power, and other critical indicators. this common toolset allows space missions to minimize the cost of developing operations infrastructure, which is very important in light of recent restricted spending by space agencies.   == references == jpl ammos landing page"")"
17,"The maiden flight, also known as first flight, of an aircraft is the first occasion on which an aircraft leaves the ground under its own power. The same term is also used for the first launch of rockets.
The maiden flight of a new aircraft type is always a historic occasion for the type and can be quite emotional for those involved. In the early days of aviation it could be dangerous, because the exact handling characteristics of the aircraft were generally unknown. The maiden flight of a new type is almost invariably flown by a highly experienced test pilot. Maiden flights are usually accompanied by a chase plane, to verify items like altitude, airspeed, and general airworthiness.
A maiden flight is only one stage in the development of an aircraft type. Unless the type is a pure research aircraft (such as the X-15), the aircraft must be tested extensively to ensure that it delivers the desired performance with an acceptable margin of safety. In the case of civilian aircraft, a new type must be certified by a governing agency (such as the Federal Aviation Administration in the United States) before it can enter operation.


== Notable maiden flights (aircraft) ==

An incomplete list of maiden flights of notable aircraft types, organized by date, follows.

June, 1875 – Thomas Moy's Aerial Steamer, London, England (pilotless, tethered)
October 9, 1890 – Clément Ader – took off from Gretz-Armainvilliers, Ouest of Paris, France.
August 14, 1901 – Gustave Whitehead from Leutershausen, Bavaria.
May 15, 1902 – Lyman Gilmore – took off from Grass Valley, California.
March 31, 1903 – Richard Pearse – took off from Waitohi Flat, Temuka, South Island, New Zealand.
December 17, 1903 – Wright brothers Wright Flyer – first heavier-than-air powered aircraft. Took off four miles south of Kitty Hawk, North Carolina.
March 18, 1906 – Traian Vuia, a Romanian inventor and engineer, who flew 11 meters in his self-named monoplane at Montesson near Paris, France.
October 23, 1906 – Alberto Santos-Dumont 14-bis made a manned powered flight in Bagatelle Park, Paris, France, that was the first to be publicly witnessed by a crowd.
July 4, 1908 - Glenn Curtiss flew the first pre-announced public flight in the United States of America of a heavier-than-air flying machine. He flew 5,080 feet, to win the Scientific American Trophy and its $2,500 purse (equivalent to $71,000 in 2019).
December 22, 1916 - Sopwith Camel - this iconic biplane first took off from Brooklands, Weybridge, Surrey.
July 28, 1935 – Boeing B-17 Flying Fortress – World War II American heavy bomber.
December 17, 1935 – Douglas DC-3 – propeller-driven passenger and cargo aircraft of which more than 10,000 were produced.
December 29, 1939 – Consolidated B-24 – World War II American heavy bomber.
November 2, 1947 – Hughes H-4 Hercules – only flight of this oversized flying boat whose common name is Spruce Goose.
July 27, 1949 – de Havilland Comet – first jet airliner.
August 23, 1954 – Lockheed C-130 Hercules – military transport plane.
May 27, 1955 – Sud Aviation Caravelle – first jet airliner with engines mounted in the tail.
March 25, 1958 - Avro Canada CF-105 Arrow - Canadian supersonic fighter interceptor. First non-experimental aircraft designed and equipped with a fly-by-wire flight control system.
April 25, 1962 – Lockheed A-12 – supersonic reconnaissance aircraft.
June 29, 1962 – Vickers VC10 – first airliner with 4 engines mounted in the tail.
April 9, 1967 – Boeing 737 – short-to-medium-range airliner.
October 4, 1968 – Tupolev Tu-154 – Soviet/Russian airliner, still in operation.
December 31, 1968 – Tupolev Tu-144 – Soviet supersonic airliner.
February 9, 1969 – Boeing 747 – first widebody airliner.
March 2, 1969 – Anglo-French Concorde – supersonic airliner.
September 19, 1969 – Mil Mi-24 – Russian/Soviet-made helicopter used by many countries to this day.
October 28, 1972 – Airbus A300 – first Airbus aircraft, short- to medium-range wide-body jet airliner.
February 22, 1987 – Airbus A320 airliner – first civilian aircraft to have an all-digital fly-by-wire system.
December 21, 1988 – Antonov An-225 Mriya – jet with the longest fuselage and wingspan and overall heaviest aircraft.
June 12, 1994 – Boeing 777 – long-range airliner with the most powerful jet engines ever made.
April 27, 2005 – Airbus A380 – double-decker jet airliner, currently largest capacity in the world, took off from Toulouse–Blagnac Airport.
December 11, 2009 – Airbus A400M – military cargo plane, Airbus' first propeller plane.
December 15, 2009 – Boeing 787 Dreamliner – first major widebody airliner to use non-metal composite materials for most of its construction.
November 11, 2015 - Mitsubishi Regional Jet - Japanese twin-engine regional jet, the first designed and built in Japan, took off from Mitsubishi Heavy Industries, Tokyo.
May 5, 2017 - Comac C919 - Chinese commercial aircraft.
January 25, 2020 - Boeing 777X - The world's longest and largest twin-engine airliner


== Notable maiden flights (rockets) ==
October 3, 1942 - V-2 Rocket made its first successful test flight. The nose cone crossed the Karman line, widely considered the end of Earth's atmosphere, making it the first human-made object to reach space.
August 3, 1953 - PGM-11 Redstone, designed by Wernher von Braun, was the US's first large ballistic missile. Launched from Cape Canaveral Air Force Station Launch Complex 4, it flew for 80 seconds until an engine failure caused it to crash into the sea.
October 4, 1957 - Sputnik, first orbital rocket.
December 22, 1960 - Vostok-K, first human-rated rocket (first manned flight April 12, 1961).
November 9, 1967 - Saturn V, most powerful rocket launched so far, was used to launch humans to the Moon.
April 12, 1981 - Space Shuttle, first partially reusable launch system, largest payload at the time of its maiden flight.
December 21, 2004 - Delta IV Heavy, largest payload at the time of its maiden flight.
February 6, 2018 - Falcon Heavy, largest payload at the time of its maiden flight, partially reusable.


== See also ==
Flight test
Maiden voyage


== References ==","pandas(index=17, _1=17, text=""the maiden flight, also known as first flight, of an aircraft is the first occasion on which an aircraft leaves the ground under its own power. the same term is also used for the first launch of rockets. the maiden flight of a new aircraft type is always a historic occasion for the type and can be quite emotional for those involved. in the early days of aviation it could be dangerous, because the exact handling characteristics of the aircraft were generally unknown. the maiden flight of a new type is almost invariably flown by a highly experienced test pilot. maiden flights are usually accompanied by a chase plane, to verify items like altitude, airspeed, and general airworthiness. a maiden flight is only one stage in the development of an aircraft type. unless the type is a pure research aircraft (such as the x-15), the aircraft must be tested extensively to ensure that it delivers the desired performance with an acceptable margin of safety. in the case of civilian aircraft, a new type must be certified by a governing agency (such as the federal aviation administration in the united states) before it can enter operation.   == notable maiden flights (aircraft) ==  an incomplete list of maiden flights of notable aircraft types, organized by date, follows.  june, 1875 – thomas moy's aerial steamer, london, england (pilotless, tethered) october 9, 1890 – clément ader – took off from gretz-armainvilliers, ouest of paris, france. august 14, 1901 – gustave whitehead from leutershausen, bavaria. may 15, 1902 – lyman gilmore – took off from grass valley, california. march 31, 1903 – richard pearse – took off from waitohi flat, temuka, south island, new zealand. december 17, 1903 – wright brothers wright flyer – first heavier-than-air powered aircraft. took off four miles south of kitty hawk, north carolina. march 18, 1906 – traian vuia, a romanian inventor and engineer, who flew 11 meters in his self-named monoplane at montesson near paris, france. october 23, 1906 – alberto santos-dumont 14-bis made a manned powered flight in bagatelle park, paris, france, that was the first to be publicly witnessed by a crowd. july 4, 1908 - glenn curtiss flew the first pre-announced public flight in the united states of america of a heavier-than-air flying machine. he flew 5,080 feet, to win the scientific american trophy and its $2,500 purse (equivalent to $71,000 in 2019). december 22, 1916 - sopwith camel - this iconic biplane first took off from brooklands, weybridge, surrey. july 28, 1935 – boeing b-17 flying fortress – world war ii american heavy bomber. december 17, 1935 – douglas dc-3 – propeller-driven passenger and cargo aircraft of which more than 10,000 were produced. december 29, 1939 – consolidated b-24 – world war ii american heavy bomber. november 2, 1947 – hughes h-4 hercules – only flight of this oversized flying boat whose common name is spruce goose. july 27, 1949 – de havilland comet – first jet airliner. august 23, 1954 – lockheed c-130 hercules – military transport plane. may 27, 1955 – sud aviation caravelle – first jet airliner with engines mounted in the tail. march 25, 1958 - avro canada cf-105 arrow - canadian supersonic fighter interceptor. first non-experimental aircraft designed and equipped with a fly-by-wire flight control system. april 25, 1962 – lockheed a-12 – supersonic reconnaissance aircraft. june 29, 1962 – vickers vc10 – first airliner with 4 engines mounted in the tail. april 9, 1967 – boeing 737 – short-to-medium-range airliner. october 4, 1968 – tupolev tu-154 – soviet/russian airliner, still in operation. december 31, 1968 – tupolev tu-144 – soviet supersonic airliner. february 9, 1969 – boeing 747 – first widebody airliner. march 2, 1969 – anglo-french concorde – supersonic airliner. september 19, 1969 – mil mi-24 – russian/soviet-made helicopter used by many countries to this day. october 28, 1972 – airbus a300 – first airbus aircraft, short- to medium-range wide-body jet airliner. february 22, 1987 – airbus a320 airliner – first civilian aircraft to have an all-digital fly-by-wire system. december 21, 1988 – antonov an-225 mriya – jet with the longest fuselage and wingspan and overall heaviest aircraft. june 12, 1994 – boeing 777 – long-range airliner with the most powerful jet engines ever made. april 27, 2005 – airbus a380 – double-decker jet airliner, currently largest capacity in the world, took off from toulouse–blagnac airport. december 11, 2009 – airbus a400m – military cargo plane, airbus' first propeller plane. december 15, 2009 – boeing 787 dreamliner – first major widebody airliner to use non-metal composite materials for most of its construction. november 11, 2015 - mitsubishi regional jet - japanese twin-engine regional jet, the first designed and built in japan, took off from mitsubishi heavy industries, tokyo. may 5, 2017 - comac c919 - chinese commercial aircraft. january 25, 2020 - boeing 777x - the world's longest and largest twin-engine airliner   == notable maiden flights (rockets) == october 3, 1942 - v-2 rocket made its first successful test flight. the nose cone crossed the karman line, widely considered the end of earth's atmosphere, making it the first human-made object to reach space. august 3, 1953 - pgm-11 redstone, designed by wernher von braun, was the us's first large ballistic missile. launched from cape canaveral air force station launch complex 4, it flew for 80 seconds until an engine failure caused it to crash into the sea. october 4, 1957 - sputnik, first orbital rocket. december 22, 1960 - vostok-k, first human-rated rocket (first manned flight april 12, 1961). november 9, 1967 - saturn v, most powerful rocket launched so far, was used to launch humans to the moon. april 12, 1981 - space shuttle, first partially reusable launch system, largest payload at the time of its maiden flight. december 21, 2004 - delta iv heavy, largest payload at the time of its maiden flight. february 6, 2018 - falcon heavy, largest payload at the time of its maiden flight, partially reusable.   == see also == flight test maiden voyage   == references =="")"
18,"A navigation light, also known as a running or position light, is a source of illumination on a vessel, aircraft or spacecraft.  Navigation lights give information on a craft's position, heading, and status.  Their placement is mandated by international conventions or civil authorities.  Navigation lights are not intended to provide illumination for the craft making the passage, only for other craft to be aware of it.


== Marine navigation lights ==
In 1838 the United States passed an act requiring steamboats running between sunset and sunrise to carry one or more signal lights; colour, visibility and location were not specified.
In 1846 the United Kingdom passed legislation enabling the Lord High Admiral  to publish regulations requiring all sea-going steam vessels to carry lights.  The admiralty exercised these powers in 1848 and required steam vessels to display red and green sidelights as well as a white masthead light whilst under way and a single white light when at anchor.
In 1849 the U.S. Congress extended the light requirements to sailing vessels.
In 1889 the United States convened the first International Maritime Conference to consider regulations for preventing collisions. The resulting Washington Conference Rules were adopted by the U.S. in 1890 and became effective internationally in 1897. Within these rules was the requirement for steamships to carry a second mast head light.
The international 1948 Safety of Life at Sea Conference recommended a mandatory second masthead light solely for power driven vessels over 150 feet in length and a fixed sternlight for almost all vessels. The regulations have changed little since then.The International Regulations for Preventing Collisions at Sea established in 1972 stipulates the requirements for the navigation lights required on a vessel.


=== Basic lighting ===

To avoid collisions, vessels mount navigation lights that permit other vessels to determine the type and relative angle of a vessel, and thus decide if there is a danger of collision. In general sailing vessels are required to carry a green light that shines from dead ahead to 2 points (​22 1⁄2°) abaft the beam on the starboard side (the right side from the perspective of someone on board facing forward), a red light from dead ahead to two points abaft the beam on the port side (left side) and a white light that shines from astern to two points abaft the beam on both sides. Power driven vessels in addition  to these lights, must carry either one or two (depending on length) white masthead lights that shine from ahead to two points abaft the beam on both sides. If two masthead lights are carried then the aft one must be higher than the forward one.
Small power driven vessels (under 12 metres (39 ft)) may carry a single all-round white light in place of the two or three white lights carried by larger vessels, they must also carry red and green navigation lights.  Vessels under 7 metres (23 ft) with a maximum speed of less than 7 knots are not required to carry navigation lights, but must be capable of showing a white light. Hovercraft at all times and some boats operating in crowded areas may also carry a yellow flashing beacon for added visibility during day or night.


=== Lights of special significance ===
In addition to red, white and green running lights, a combination of red, white and green Mast Lights placed on a mast higher than all the running lights, and viewable from all directions, may be used to indicate the type of craft or the service it is performing. See ""User Guide"" in external links.

Ships at anchor display one or two white anchor lights (depending on the vessel's length) that can be seen from all directions. If two lights are shown then the forward light is higher than the aft one.
Boats classed as ""small"" are not compelled to carry navigation lights and may make use of a handheld torch.


== Aviation navigation lights ==

Aircraft external lights are any light fitted to the exterior of an aircraft. They are usually used to increase visibility to others, and to signal actions such as entering an active runway or starting up an engine. Historically, incandescent bulbs have been used to provide light, however recently Light-emitting diodes have been used.

Aircraft navigation lights follow the convention of marine vessels established a half-century earlier, with a red navigation light located on the left wingtip leading edge and a green light on the right wingtip leading edge. A white navigation light is as far aft as possible on the tail or each wing tip. High-intensity strobe lights are located on the aircraft to aid in collision avoidance. Anti-collision lights are flashing lights on the top and bottom of the fuselage, wingtips and tail tip. Their purpose is to alert others when something is happening that ground crew and other aircraft need to be aware of, such as running engines or entering active runways.
In civil aviation, pilots must keep navigation lights on from sunset to sunrise. High-intensity white strobe lights are part of the anti-collision light system, as well as the red rotating beacon.
All aircraft built after 11 March 1996 must have an anti-collision light system (strobe lights or rotating beacon) turned on for all flight activities in poor visibility. The anti-collision system is recommended in good visibility, where only strobes and beacon are required. For example, just before pushback, the pilot must keep the beacon lights on to notify ground crews that the engines are about to start. These beacon lights stay on for the duration of the flight. While taxiing, the taxi lights are on. When coming onto the runway, the taxi lights go off and the landing lights and strobes go on. When passing 10,000 feet, the landing lights are no longer required, and the pilot can elect to turn them off. The same cycle in reverse order applies when landing. Landing lights are bright white, forward and downward facing lights on the front of an aircraft. Their purpose is to allow the pilot to see the landing area, and to allow ground crew to see the approaching aircraft.
Civilian commercial airliners also have other non-navigational lights. These include logo lights, which illuminate the company logo on the tail fin. These lights are optional to turn on, though most pilots switch them on at night to increase visibility from other aircraft. Modern airliners also have a wing light. These are positioned on the outer side just in front of the engine cowlings on the fuselage. These are not required to be on, but in some cases pilots turn these lights on for engine checks and also while passengers board the aircraft for better visibility of the ground near the aircraft.  While seldom seen, the International Code of Signals allows for the exclusive use of a flashing blue lights (60 to 100 flashes/minute) and visible from as many directions as possible, by medical aircraft to signal their identity.


== Spacecraft navigation lights ==

In 2011, ORBITEC developed the first Light-emitting diode (LED) lighting system for use around spacecraft.  Currently, Cygnus spacecraft, which are unmanned transport vessels designed for cargo transport to the International Space Station, utilize a navigational lighting system consisting of five flashing high power LED lights.  The Cygnus displays a flashing red light on the port side of the vessel, a flashing green on the starboard side of the vessel, two flashing white lights on the top and one flashing yellow on the bottom side of the fuselage.The SpaceX Dragon and Dragon 2 spacecraft also feature a flashing strobe along with red and green lights.


== See also ==
Formation light
Landing lights


== Notes ==


== References ==


== External links ==
Navigation Lights User Guide","pandas(index=18, _1=18, text='a navigation light, also known as a running or position light, is a source of illumination on a vessel, aircraft or spacecraft.  navigation lights give information on a craft\'s position, heading, and status.  their placement is mandated by international conventions or civil authorities.  navigation lights are not intended to provide illumination for the craft making the passage, only for other craft to be aware of it.   == marine navigation lights == in 1838 the united states passed an act requiring steamboats running between sunset and sunrise to carry one or more signal lights; colour, visibility and location were not specified. in 1846 the united kingdom passed legislation enabling the lord high admiral  to publish regulations requiring all sea-going steam vessels to carry lights.  the admiralty exercised these powers in 1848 and required steam vessels to display red and green sidelights as well as a white masthead light whilst under way and a single white light when at anchor. in 1849 the u.s. congress extended the light requirements to sailing vessels. in 1889 the united states convened the first international maritime conference to consider regulations for preventing collisions. the resulting washington conference rules were adopted by the u.s. in 1890 and became effective internationally in 1897. within these rules was the requirement for steamships to carry a second mast head light. the international 1948 safety of life at sea conference recommended a mandatory second masthead light solely for power driven vessels over 150 feet in length and a fixed sternlight for almost all vessels. the regulations have changed little since then.the international regulations for preventing collisions at sea established in 1972 stipulates the requirements for the navigation lights required on a vessel. in addition to red, white and green running lights, a combination of red, white and green mast lights placed on a mast higher than all the running lights, and viewable from all directions, may be used to indicate the type of craft or the service it is performing. see ""user guide"" in external links.  ships at anchor display one or two white anchor lights (depending on the vessel\'s length) that can be seen from all directions. if two lights are shown then the forward light is higher than the aft one. boats classed as ""small"" are not compelled to carry navigation lights and may make use of a handheld torch.   == aviation navigation lights ==  aircraft external lights are any light fitted to the exterior of an aircraft. they are usually used to increase visibility to others, and to signal actions such as entering an active runway or starting up an engine. historically, incandescent bulbs have been used to provide light, however recently light-emitting diodes have been used.  aircraft navigation lights follow the convention of marine vessels established a half-century earlier, with a red navigation light located on the left wingtip leading edge and a green light on the right wingtip leading edge. a white navigation light is as far aft as possible on the tail or each wing tip. high-intensity strobe lights are located on the aircraft to aid in collision avoidance. anti-collision lights are flashing lights on the top and bottom of the fuselage, wingtips and tail tip. their purpose is to alert others when something is happening that ground crew and other aircraft need to be aware of, such as running engines or entering active runways. in civil aviation, pilots must keep navigation lights on from sunset to sunrise. high-intensity white strobe lights are part of the anti-collision light system, as well as the red rotating beacon. all aircraft built after 11 march 1996 must have an anti-collision light system (strobe lights or rotating beacon) turned on for all flight activities in poor visibility. the anti-collision system is recommended in good visibility, where only strobes and beacon are required. for example, just before pushback, the pilot must keep the beacon lights on to notify ground crews that the engines are about to start. these beacon lights stay on for the duration of the flight. while taxiing, the taxi lights are on. when coming onto the runway, the taxi lights go off and the landing lights and strobes go on. when passing 10,000 feet, the landing lights are no longer required, and the pilot can elect to turn them off. the same cycle in reverse order applies when landing. landing lights are bright white, forward and downward facing lights on the front of an aircraft. their purpose is to allow the pilot to see the landing area, and to allow ground crew to see the approaching aircraft. civilian commercial airliners also have other non-navigational lights. these include logo lights, which illuminate the company logo on the tail fin. these lights are optional to turn on, though most pilots switch them on at night to increase visibility from other aircraft. modern airliners also have a wing light. these are positioned on the outer side just in front of the engine cowlings on the fuselage. these are not required to be on, but in some cases pilots turn these lights on for engine checks and also while passengers board the aircraft for better visibility of the ground near the aircraft.  while seldom seen, the international code of signals allows for the exclusive use of a flashing blue lights (60 to 100 flashes/minute) and visible from as many directions as possible, by medical aircraft to signal their identity.   == spacecraft navigation lights ==  in 2011, orbitec developed the first light-emitting diode (led) lighting system for use around spacecraft.  currently, cygnus spacecraft, which are unmanned transport vessels designed for cargo transport to the international space station, utilize a navigational lighting system consisting of five flashing high power led lights.  the cygnus displays a flashing red light on the port side of the vessel, a flashing green on the starboard side of the vessel, two flashing white lights on the top and one flashing yellow on the bottom side of the fuselage.the spacex dragon and dragon 2 spacecraft also feature a flashing strobe along with red and green lights.   == see also == formation light landing lights   == notes ==   == references ==   == external links == navigation lights user guide')"
19,"Bleed air is compressed air taken from the compressor stage of a gas turbine upstream of its fuel-burning sections.  Automatic air supply and cabin pressure controller (ASCPCs) valves bleed air from high or low stage engine compressor sections. Low stage air is used during high power setting operation, and high during descent and other low power setting operations. Bleed air from that system can be utilized for internal cooling of the engine, cross-starting another engine, engine and airframe anti-icing, cabin pressurization, pneumatic actuators, air-driven motors, pressurizing the hydraulic reservoir, and waste and water storage tanks. Some engine maintenance manuals refer to such systems as ""customer bleed air"". Bleed air is valuable in an aircraft for two properties: high temperature and high pressure (typical values are 200–250 °C and 275 kPa (40 PSI), for regulated bleed air exiting the engine pylon for use throughout the aircraft).


== Uses ==

In civil aircraft, bleed air's primary use is to provide pressure for the aircraft cabin by supplying air to the environmental control system. Additionally, bleed air is used to keep critical parts of the plane (such as the wing leading edges) ice-free.Bleed air is used on many aircraft systems because it is easily available, reliable, and a potent source of power. For example, bleed air from an airplane engine is used to start the remaining engines. Lavatory water storage tanks are pressurized by bleed air that is fed through a pressure regulator.When used for cabin pressurization, the bleed air from the engine must first be cooled (as it exits the compressor stage at temperatures as high as 250 °C) by passing it through an air-to-air heat exchanger cooled by cold outside air. It is then fed to an air cycle machine unit that regulates the temperature and flow of air into the cabin, keeping the environment comfortable.Bleed air is also used to heat the engine intakes. This prevents ice from forming, accumulating, breaking loose, and being ingested by the engine, possibly damaging it.On aircraft powered by jet engines, a similar system is used for wing anti-icing by the 'hot-wing' method. In icing conditions, water droplets condensing on a wing's leading edge can freeze. If that happens, the ice build-up adds weight and changes the shape of the wing, causing a degradation in performance, and possibly a critical loss of control or lift. To prevent this, hot bleed air is pumped through the inside of the wing's leading edge, heating it to a temperature above freezing, which prevents the formation of ice. The air then exits through small holes in the wing edge.
On propeller-driven aircraft, it is common to use bleed air to inflate a rubber boot on the leading edge, breaking the ice loose after it has already formed.Bleed air from the high-pressure compressor of the engine is used to supply reaction control valves as used for part of the flight control system in the Harrier jump jet family of military aircraft.


== Contamination ==

On about 1 in 5,000 flights, bleed air used for air conditioning and pressurization can be contaminated by chemicals such as oil or hydraulic fluid. This is known as a fume event. While those chemicals can be irritating, such events have not been established to cause long-term harm.Certain neurological and respiratory ill health effects have been linked anecdotally to exposure to bleed air that has been alleged to have been contaminated with toxic levels on commercial and military aircraft. This alleged long-term illness is referred to as aerotoxic syndrome, but it is not a medically recognized syndrome. One potential contaminant is tricresyl phosphate.Many lobbying groups have been set up to advocate for research into this hazard, including the Aviation Organophosphate Information Site (AOPIS) (2001), the Global Cabin Air Quality Executive (2006) and the UK-based Aerotoxic Association (2007). Cabin Environment Research is one of many functions of the ACER Group, but their researchers have not yet established any causal relationship.Although a study made for the EU in 2014 confirmed that contamination of cabin air could be a problem, that study also stated:

""A lot of reported fume events caused comfort limitations for the occupants but posed no danger. A verification of cabin air contamination with toxic substances (e.g. TCP/TOCP) was not possible with the fume events the BFU investigated.""While no scientific evidence to date has found that airliner cabin air has been contaminated to toxic levels (exceeding known safe levels, in ppm, of any dangerous chemical), a court in Australia in March 2010 found in favor of a former airline flight attendant who claimed she suffered chronic respiratory problems after being exposed to oil fumes on a trip in March 1992. Such testing is infrequent due to Boeing's refusal to install air quality sensors in its planes, fearing lawsuits from crew or passengers over fume events, and airlines refused to allow flight attendants to carry air samplers after Congress mandated chemical measurements.The FAA has revoked the medical certificates of several pilots who developed neurological issues after fume events. A judge who awarded workers' compensation to a pilot who had suffered toxic encephalopathy (brain damage) from a fume event condemned the airline industry's obstructionism around fume events.In July 2015, pilots on a Spirit Airlines flight were partially incapacitated by fumes in bleed air.


== Bleedless aircraft ==
Bleed air systems have been in use for several decades in passenger jets. Recent improvements in solid-state electronics have enabled pneumatic power systems to be replaced by electric power systems. In a bleedless aircraft such as the Boeing 787, each engine has two variable-frequency electrical generators to compensate for not providing compressed air to external systems. Eliminating bleed air and replacing it with extra electric generation is believed to provide a net improvement in engine efficiency, lower weight, and ease of maintenance.


=== Benefits ===
A bleedless aircraft achieves fuel efficiency by eliminating the process of compressing and decompressing air, and by reducing the aircraft's mass due to the removal of ducts, valves, heat exchangers, and other heavy equipment.The APU (auxiliary power unit) does not need to supply bleed air when the main engines are not operating. Aerodynamics are improved due to the lack of bleed air vent holes on the wings. By driving cabin air supply compressors at the minimum required speed, no energy wasting modulating valves are required. High-temperature, high-pressure air cycle machine (ACM) packs can be replaced with low temperature, low-pressure packs to increase efficiency. At cruise altitude, where most aircraft spend the majority of their time and burn the majority of their fuel, the ACM packs can be bypassed entirely, saving even more energy. Since no bleed air is taken from the engines for the cabin, the potential of engine oil contamination of the cabin air supply is eliminated.Lastly, advocates of the design say it improves safety as heated air is confined to the engine pod, as opposed to being pumped through pipes and heat exchangers in the wing and near the cabin, where a leak could damage surrounding systems.


== See also ==
Aerotoxic syndrome
Cabin pressurization
Environmental control system (aircraft)
Ice protection system


== References ==","pandas(index=19, _1=19, text='bleed air is compressed air taken from the compressor stage of a gas turbine upstream of its fuel-burning sections.  automatic air supply and cabin pressure controller (ascpcs) valves bleed air from high or low stage engine compressor sections. low stage air is used during high power setting operation, and high during descent and other low power setting operations. bleed air from that system can be utilized for internal cooling of the engine, cross-starting another engine, engine and airframe anti-icing, cabin pressurization, pneumatic actuators, air-driven motors, pressurizing the hydraulic reservoir, and waste and water storage tanks. some engine maintenance manuals refer to such systems as ""customer bleed air"". bleed air is valuable in an aircraft for two properties: high temperature and high pressure (typical values are 200–250 °c and 275 kpa (40 psi), for regulated bleed air exiting the engine pylon for use throughout the aircraft).   == uses ==  in civil aircraft, bleed air\'s primary use is to provide pressure for the aircraft cabin by supplying air to the environmental control system. additionally, bleed air is used to keep critical parts of the plane (such as the wing leading edges) ice-free.bleed air is used on many aircraft systems because it is easily available, reliable, and a potent source of power. for example, bleed air from an airplane engine is used to start the remaining engines. lavatory water storage tanks are pressurized by bleed air that is fed through a pressure regulator.when used for cabin pressurization, the bleed air from the engine must first be cooled (as it exits the compressor stage at temperatures as high as 250 °c) by passing it through an air-to-air heat exchanger cooled by cold outside air. it is then fed to an air cycle machine unit that regulates the temperature and flow of air into the cabin, keeping the environment comfortable.bleed air is also used to heat the engine intakes. this prevents ice from forming, accumulating, breaking loose, and being ingested by the engine, possibly damaging it.on aircraft powered by jet engines, a similar system is used for wing anti-icing by the \'hot-wing\' method. in icing conditions, water droplets condensing on a wing\'s leading edge can freeze. if that happens, the ice build-up adds weight and changes the shape of the wing, causing a degradation in performance, and possibly a critical loss of control or lift. to prevent this, hot bleed air is pumped through the inside of the wing\'s leading edge, heating it to a temperature above freezing, which prevents the formation of ice. the air then exits through small holes in the wing edge. on propeller-driven aircraft, it is common to use bleed air to inflate a rubber boot on the leading edge, breaking the ice loose after it has already formed.bleed air from the high-pressure compressor of the engine is used to supply reaction control valves as used for part of the flight control system in the harrier jump jet family of military aircraft.   == contamination ==  on about 1 in 5,000 flights, bleed air used for air conditioning and pressurization can be contaminated by chemicals such as oil or hydraulic fluid. this is known as a fume event. while those chemicals can be irritating, such events have not been established to cause long-term harm.certain neurological and respiratory ill health effects have been linked anecdotally to exposure to bleed air that has been alleged to have been contaminated with toxic levels on commercial and military aircraft. this alleged long-term illness is referred to as aerotoxic syndrome, but it is not a medically recognized syndrome. one potential contaminant is tricresyl phosphate.many lobbying groups have been set up to advocate for research into this hazard, including the aviation organophosphate information site (aopis) (2001), the global cabin air quality executive (2006) and the uk-based aerotoxic association (2007). cabin environment research is one of many functions of the acer group, but their researchers have not yet established any causal relationship.although a study made for the eu in 2014 confirmed that contamination of cabin air could be a problem, that study also stated:  ""a lot of reported fume events caused comfort limitations for the occupants but posed no danger. a verification of cabin air contamination with toxic substances (e.g. tcp/tocp) was not possible with the fume events the bfu investigated.""while no scientific evidence to date has found that airliner cabin air has been contaminated to toxic levels (exceeding known safe levels, in ppm, of any dangerous chemical), a court in australia in march 2010 found in favor of a former airline flight attendant who claimed she suffered chronic respiratory problems after being exposed to oil fumes on a trip in march 1992. such testing is infrequent due to boeing\'s refusal to install air quality sensors in its planes, fearing lawsuits from crew or passengers over fume events, and airlines refused to allow flight attendants to carry air samplers after congress mandated chemical measurements.the faa has revoked the medical certificates of several pilots who developed neurological issues after fume events. a judge who awarded workers\' compensation to a pilot who had suffered toxic encephalopathy (brain damage) from a fume event condemned the airline industry\'s obstructionism around fume events.in july 2015, pilots on a spirit airlines flight were partially incapacitated by fumes in bleed air.   == bleedless aircraft == bleed air systems have been in use for several decades in passenger jets. recent improvements in solid-state electronics have enabled pneumatic power systems to be replaced by electric power systems. in a bleedless aircraft such as the boeing 787, each engine has two variable-frequency electrical generators to compensate for not providing compressed air to external systems. eliminating bleed air and replacing it with extra electric generation is believed to provide a net improvement in engine efficiency, lower weight, and ease of maintenance. a bleedless aircraft achieves fuel efficiency by eliminating the process of compressing and decompressing air, and by reducing the aircraft\'s mass due to the removal of ducts, valves, heat exchangers, and other heavy equipment.the apu (auxiliary power unit) does not need to supply bleed air when the main engines are not operating. aerodynamics are improved due to the lack of bleed air vent holes on the wings. by driving cabin air supply compressors at the minimum required speed, no energy wasting modulating valves are required. high-temperature, high-pressure air cycle machine (acm) packs can be replaced with low temperature, low-pressure packs to increase efficiency. at cruise altitude, where most aircraft spend the majority of their time and burn the majority of their fuel, the acm packs can be bypassed entirely, saving even more energy. since no bleed air is taken from the engines for the cabin, the potential of engine oil contamination of the cabin air supply is eliminated.lastly, advocates of the design say it improves safety as heated air is confined to the engine pod, as opposed to being pumped through pipes and heat exchangers in the wing and near the cabin, where a leak could damage surrounding systems.   == see also == aerotoxic syndrome cabin pressurization environmental control system (aircraft) ice protection system   == references ==')"
20,"The pressure coefficient is a dimensionless number which describes the relative pressures throughout a flow field in fluid dynamics.  The pressure coefficient is used in aerodynamics and hydrodynamics.  Every point in a fluid flow field has its own unique pressure coefficient, 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
  .
In many situations in aerodynamics and hydrodynamics, the pressure coefficient at a point near a body is independent of body size.  Consequently, an engineering model can be tested in a wind tunnel or water tunnel, pressure coefficients can be determined at critical locations around the model, and these pressure coefficients can be used with confidence to predict the fluid pressure at those critical locations around a full-size aircraft or boat.


== Definition ==
The pressure coefficient is a parameter for studying both incompressible/compressible fluids such as water and air.  The relationship between the dimensionless coefficient and the dimensional numbers is

  
    
      
        
          C
          
            p
          
        
        =
        
          
            
              p
              −
              
                p
                
                  ∞
                
              
            
            
              
                
                  1
                  2
                
              
              
                ρ
                
                  ∞
                
              
              
                V
                
                  ∞
                
                
                  2
                
              
            
          
        
        =
        
          
            
              p
              −
              
                p
                
                  ∞
                
              
            
            
              
                p
                
                  0
                
              
              −
              
                p
                
                  ∞
                
              
            
          
        
      
    
    {\displaystyle C_{p}={p-p_{\infty } \over {\frac {1}{2}}\rho _{\infty }V_{\infty }^{2}}={p-p_{\infty } \over p_{0}-p_{\infty }}}
  where:

  
    
      
        p
      
    
    {\displaystyle p}
   is the static pressure at the point at which pressure coefficient is being evaluated

  
    
      
        
          p
          
            ∞
          
        
      
    
    {\displaystyle p_{\infty }}
   is the static pressure in the freestream (i.e. remote from any disturbance)

  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
   is the stagnation pressure in the freestream (i.e. remote from any disturbance)

  
    
      
        
          ρ
          
            ∞
          
        
      
    
    {\displaystyle \rho _{\infty }}
   is the freestream fluid density (Air at sea level and 15 °C is 1.225 
  
    
      
        
          
            k
            g
            
              /
            
            
              m
              
                3
              
            
          
        
      
    
    {\displaystyle {\rm {kg/m^{3}}}}
  )

  
    
      
        
          V
          
            ∞
          
        
      
    
    {\displaystyle V_{\infty }}
   is the freestream velocity of the fluid, or the velocity of the body through the fluid


== Incompressible flow ==

Using Bernoulli's Equation, the pressure coefficient can be further simplified for potential flows (inviscid, and steady):

  
    
      
        
          C
          
            p
          
        
        0
        =
        
          C
          
            p
          
        
        
          
            |
          
          
            M
            a
            
            ≈
            
            0
          
        
        =
        
          1
          −
          
            
              (
            
          
          
            
              u
              
                u
                
                  ∞
                
              
            
          
          
            
              
                )
              
            
            
              2
            
          
        
      
    
    {\displaystyle C_{p}0=C_{p}|_{Ma\,\approx \,0}={1-{\bigg (}{\frac {u}{u_{\infty }}}{\bigg )}^{2}}}
  where u is the flow speed at the point at which pressure coefficient is being evaluated, and Ma is the Mach number: the flow speed is negligible in comparison with the speed of sound. For a case of an incompressible but viscous fluid, this represents the profile pressure coefficient, since it is associated with the pressure hydrodynamic forces rather than the viscous ones.
This relationship is valid for the flow of incompressible fluids where variations in speed and pressure are sufficiently small that variations in fluid density can be neglected. This is a reasonable assumption when the Mach Number is less than about 0.3.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of zero indicates the pressure is the same as the free stream pressure.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of one corresponds to the stagnation pressure and indicates a stagnation point.
the most negative values of 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   in a liquid flow can be summed to the cavitation number to give the cavitation margin. If this margin is positive, the flow is locally fully liquid, while if it is zero or negative the flow is cavitating or gas.
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of minus one is significant in the design of gliders because this indicates a perfect location for a ""Total energy"" port for supply of signal pressure to the Variometer, a special Vertical Speed Indicator which reacts to vertical movements of the atmosphere but does not react to vertical maneuvering of the glider.
In the fluid flow field around a body there will be points having positive pressure coefficients up to one, and negative pressure coefficients including coefficients less than minus one, but nowhere will the coefficient exceed plus one because the highest pressure that can be achieved is the stagnation pressure.


== Compressible flow ==

In the flow of compressible fluids such as air, and particularly the high-speed flow of compressible fluids, 
  
    
      
        
          ρ
          
            v
            
              2
            
          
        
        
          /
        
        2
      
    
    {\displaystyle {\rho v^{2}}/2}
   (the dynamic pressure) is no longer an accurate measure of the difference between stagnation pressure and static pressure.  Also, the familiar relationship that stagnation pressure is equal to total pressure does not always hold true.  (It is always true in isentropic flow but the presence of shock waves can cause the flow to depart from isentropic.)  As a result, pressure coefficients can be greater than one in compressible flow.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   greater than one indicates the freestream flow is compressible.


=== Perturbation theory ===
The pressure coefficient 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
    can be estimated for irrotational and isentropic flow by introducing the potential  
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   and the perturbation potential 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  , normalized by the free-stream velocity 
  
    
      
        
          u
          
            ∞
          
        
      
    
    {\displaystyle u_{\infty }}
  

  
    
      
        Φ
        =
        
          u
          
            ∞
          
        
        x
        +
        ϕ
        (
        x
        ,
        y
        ,
        z
        )
      
    
    {\displaystyle \Phi =u_{\infty }x+\phi (x,y,z)}
  
Using Bernoulli's Equation,

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      Φ
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  
                    
                      ∇
                      Φ
                      ⋅
                      ∇
                      Φ
                    
                    2
                  
                
                +
                
                  
                    γ
                    
                      γ
                      −
                      1
                    
                  
                
                
                  
                    p
                    ρ
                  
                
                =
                
                  constant
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial \Phi }{\partial t}}+{\frac {\nabla \Phi \cdot \nabla \Phi }{2}}+{\frac {\gamma }{\gamma -1}}{\frac {p}{\rho }}={\text{constant}}\end{aligned}}}
  
which can be rewritten as 

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      Φ
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  
                    
                      ∇
                      Φ
                      ⋅
                      ∇
                      Φ
                    
                    2
                  
                
                +
                
                  
                    
                      a
                      
                        2
                      
                    
                    
                      γ
                      −
                      1
                    
                  
                
                =
                
                  constant
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial \Phi }{\partial t}}+{\frac {\nabla \Phi \cdot \nabla \Phi }{2}}+{\frac {a^{2}}{\gamma -1}}={\text{constant}}\end{aligned}}}
  
here 
  
    
      
        a
      
    
    {\displaystyle a}
   is the sound speed.
The pressure coefficient becomes

  
    
      
        
          
            
              
                
                  C
                  
                    p
                  
                
              
              
                
                =
                
                  
                    
                      p
                      −
                      
                        p
                        
                          ∞
                        
                      
                    
                    
                      
                        
                          γ
                          2
                        
                      
                      
                        p
                        
                          ∞
                        
                      
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          
                            a
                            
                              a
                              
                                ∞
                              
                            
                          
                        
                        )
                      
                      
                        
                          
                            2
                            γ
                          
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          
                            
                              
                                γ
                                −
                                1
                              
                              
                                a
                                
                                  ∞
                                
                                
                                  2
                                
                              
                            
                          
                          (
                          
                            
                              
                                u
                                
                                  ∞
                                
                                
                                  2
                                
                              
                              2
                            
                          
                          −
                          
                            Φ
                            
                              t
                            
                          
                          −
                          
                            
                              
                                ∇
                                Φ
                                ⋅
                                ∇
                                Φ
                              
                              2
                            
                          
                          )
                          +
                          1
                        
                        )
                      
                      
                        
                          γ
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                ≈
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          1
                          −
                          
                            
                              
                                γ
                                −
                                1
                              
                              
                                a
                                
                                  ∞
                                
                                
                                  2
                                
                              
                            
                          
                          (
                          
                            ϕ
                            
                              t
                            
                          
                          +
                          
                            u
                            
                              ∞
                            
                          
                          
                            ϕ
                            
                              x
                            
                          
                          )
                        
                        )
                      
                      
                        
                          γ
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                ≈
                −
                
                  
                    
                      2
                      
                        ϕ
                        
                          t
                        
                      
                    
                    
                      u
                      
                        ∞
                      
                      
                        2
                      
                    
                  
                
                −
                
                  
                    
                      2
                      
                        ϕ
                        
                          x
                        
                      
                    
                    
                      u
                      
                        ∞
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}C_{p}&={\frac {p-p_{\infty }}{{\frac {\gamma }{2}}p_{\infty }M^{2}}}={\frac {2}{\gamma M^{2}}}\left[\left({\frac {a}{a_{\infty }}}\right)^{\frac {2\gamma }{\gamma -1}}-1\right]\\&={\frac {2}{\gamma M^{2}}}\left[\left({\frac {\gamma -1}{a_{\infty }^{2}}}({\frac {u_{\infty }^{2}}{2}}-\Phi _{t}-{\frac {\nabla \Phi \cdot \nabla \Phi }{2}})+1\right)^{\frac {\gamma }{\gamma -1}}-1\right]\\&\approx {\frac {2}{\gamma M^{2}}}\left[\left(1-{\frac {\gamma -1}{a_{\infty }^{2}}}(\phi _{t}+u_{\infty }\phi _{x})\right)^{\frac {\gamma }{\gamma -1}}-1\right]\\&\approx -{\frac {2\phi _{t}}{u_{\infty }^{2}}}-{\frac {2\phi _{x}}{u_{\infty }}}\end{aligned}}}
  
here 
  
    
      
        
          a
          
            ∞
          
        
      
    
    {\displaystyle a_{\infty }}
   is the far-field sound speed.


=== Local piston theory ===
The classical piston theory is a powerful aerodynamic tool. From the use of the momentum equation and the assumption of isentropic perturbations, one obtains the following basic piston theory formula for the surface pressure:

  
    
      
        p
        =
        
          p
          
            ∞
          
        
        
          
            (
            
              1
              +
              
                
                  
                    γ
                    −
                    1
                  
                  2
                
              
              
                
                  w
                  a
                
              
            
            )
          
          
            
              
                2
                γ
              
              
                γ
                −
                1
              
            
          
        
      
    
    {\displaystyle p=p_{\infty }\left(1+{\frac {\gamma -1}{2}}{\frac {w}{a}}\right)^{\frac {2\gamma }{\gamma -1}}}
  
here 
  
    
      
        w
      
    
    {\displaystyle w}
   is the downwash speed and 
  
    
      
        a
      
    
    {\displaystyle a}
   is the sound speed.

  
    
      
        
          
            
              
                
                  C
                  
                    p
                  
                
                =
                
                  
                    
                      p
                      −
                      
                        p
                        
                          ∞
                        
                      
                    
                    
                      
                        
                          γ
                          2
                        
                      
                      
                        p
                        
                          ∞
                        
                      
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          1
                          +
                          
                            
                              
                                γ
                                −
                                1
                              
                              2
                            
                          
                          
                            
                              w
                              a
                            
                          
                        
                        )
                      
                      
                        
                          
                            2
                            γ
                          
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}C_{p}={\frac {p-p_{\infty }}{{\frac {\gamma }{2}}p_{\infty }M^{2}}}={\frac {2}{\gamma M^{2}}}\left[\left(1+{\frac {\gamma -1}{2}}{\frac {w}{a}}\right)^{\frac {2\gamma }{\gamma -1}}-1\right]\end{aligned}}}
  
The surface is defined as 

  
    
      
        
          
            
              
                F
                (
                x
                ,
                y
                ,
                z
                ,
                t
                )
                =
                z
                −
                f
                (
                x
                ,
                y
                ,
                t
                )
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}F(x,y,z,t)=z-f(x,y,t)=0\end{aligned}}}
  
The slip velocity boundary condition leads to 

  
    
      
        
          
            
              
                
                  
                    
                      ∇
                      F
                    
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
                (
                
                  u
                  
                    ∞
                  
                
                +
                
                  ϕ
                  
                    x
                  
                
                ,
                
                  ϕ
                  
                    y
                  
                
                ,
                
                  ϕ
                  
                    z
                  
                
                )
                =
                
                  V
                  
                    wall
                  
                
                ⋅
                
                  
                    
                      ∇
                      F
                    
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
                =
                −
                
                  
                    
                      ∂
                      F
                    
                    
                      ∂
                      t
                    
                  
                
                
                  
                    1
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\nabla F}{|\nabla F|}}(u_{\infty }+\phi _{x},\phi _{y},\phi _{z})=V_{\text{wall}}\cdot {\frac {\nabla F}{|\nabla F|}}=-{\frac {\partial F}{\partial t}}{\frac {1}{|\nabla F|}}\end{aligned}}}
  
The downwash speed 
  
    
      
        w
      
    
    {\displaystyle w}
   is approximated as 

  
    
      
        
          
            
              
                w
                =
                
                  
                    
                      ∂
                      f
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  u
                  
                    ∞
                  
                
                
                  
                    
                      ∂
                      f
                    
                    
                      ∂
                      x
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}w={\frac {\partial f}{\partial t}}+u_{\infty }{\frac {\partial f}{\partial x}}\end{aligned}}}
  


== Pressure distribution ==
An airfoil at a given angle of attack will have what is called a pressure distribution.  This pressure distribution is simply the pressure at all points around an airfoil.  Typically, graphs of these distributions are drawn so that negative numbers are higher on the graph, as the 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   for the upper surface of the airfoil will usually be farther below zero and will hence be the top line on the graph.


== Relationship with aerodynamic coefficients ==
All the three aerodynamic coefficients are integrals of the pressure coefficient curve along the chord.
The coefficient of lift for a two-dimensional airfoil section with strictly horizontal surfaces can be calculated from the coefficient of pressure distribution by integration, or calculating the area between the lines on the distribution. This expression is not suitable for direct numeric integration using the panel method of lift approximation, as it does not take into account the direction of pressure-induced lift. This equation is true only for zero angle of attack.

  
    
      
        
          C
          
            l
          
        
        =
        
          
            1
            
              
                x
                
                  T
                  E
                
              
              −
              
                x
                
                  L
                  E
                
              
            
          
        
        
          ∫
          
            
              x
              
                L
                E
              
            
          
          
            
              x
              
                T
                E
              
            
          
        
        
          (
          
            
              C
              
                
                  p
                  
                    l
                  
                
              
            
            (
            x
            )
            −
            
              C
              
                
                  p
                  
                    u
                  
                
              
            
            (
            x
            )
          
          )
        
        d
        x
      
    
    {\displaystyle C_{l}={\frac {1}{x_{TE}-x_{LE}}}\int \limits _{x_{LE}}^{x_{TE}}\left(C_{p_{l}}(x)-C_{p_{u}}(x)\right)dx}
  where:

  
    
      
        
          C
          
            
              p
              
                l
              
            
          
        
      
    
    {\displaystyle C_{p_{l}}}
   is pressure coefficient on the lower surface

  
    
      
        
          C
          
            
              p
              
                u
              
            
          
        
      
    
    {\displaystyle C_{p_{u}}}
   is pressure coefficient on the upper surface

  
    
      
        
          x
          
            L
            E
          
        
      
    
    {\displaystyle x_{LE}}
   is the leading edge location

  
    
      
        
          x
          
            T
            E
          
        
      
    
    {\displaystyle x_{TE}}
   is the trailing edge locationWhen the lower surface 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   is higher (more negative) on the distribution it counts as a negative area as this will be producing down force rather than lift.


== See also ==
Lift coefficient
Drag coefficient
Pitching moment coefficient


== References ==

Abbott, I.H. and Von Doenhoff, A.E. (1959) Theory of Wing Sections, Dover Publications, Inc. New York, Standard Book No. 486-60586-8
Anderson, John D (2001) Fundamentals of Aerodynamic 3rd Edition, McGraw-Hill. ISBN 0-07-237335-0","pandas(index=20, _1=20, text='the pressure coefficient is a dimensionless number which describes the relative pressures throughout a flow field in fluid dynamics.  the pressure coefficient is used in aerodynamics and hydrodynamics.  every point in a fluid flow field has its own unique pressure coefficient,     c  p      is higher (more negative) on the distribution it counts as a negative area as this will be producing down force rather than lift.   == see also == lift coefficient drag coefficient pitching moment coefficient   == references ==  abbott, i.h. and von doenhoff, a.e. (1959) theory of wing sections, dover publications, inc. new york, standard book no. 486-60586-8 anderson, john d (2001) fundamentals of aerodynamic 3rd edition, mcgraw-hill. isbn 0-07-237335-0')"
21,"Energy–maneuverability theory is a model of aircraft performance. It was developed by Col. John Boyd, a fighter pilot, and Thomas P. Christie, a mathematician with the Air Force, and is useful in describing an aircraft's performance as the total of kinetic and potential energies or aircraft specific energy.  It relates the thrust, weight, aerodynamic drag, wing area, and other flight characteristics of an aircraft into a quantitative model. This allows combat capabilities of various aircraft or prospective design trade-offs to be predicted and compared.
All of these aspects of airplane performance are compressed into a single value by the following formula:

  
    
      
        
          
            
              
                
                  P
                  
                    S
                  
                
              
              
                =
              
              
                V
                
                  (
                  
                    
                      
                        T
                        −
                        D
                      
                      W
                    
                  
                  )
                
              
            
            
              
            
            
              
                V
              
              
                =
              
              
                
                  Speed
                
              
            
            
              
                T
              
              
                =
              
              
                
                  Thrust
                
              
            
            
              
                D
              
              
                =
              
              
                
                  Drag
                
              
            
            
              
                W
              
              
                =
              
              
                
                  Weight
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{rcl}P_{S}&=&V\left({\frac {T-D}{W}}\right)\\\\V&=&{\text{Speed}}\\T&=&{\text{Thrust}}\\D&=&{\text{Drag}}\\W&=&{\text{Weight}}\end{array}}}
  In words, the specific excess energy is proportional to the ratio of net motive forces compared to the weight of the plane and proportional to speed. (Note that dimensionally, 
  
    
      
        
          P
          
            S
          
        
      
    
    {\displaystyle P_{S}}
   has units of ""speed,"" not ""specific energy"" (energy per unit mass).)
The net motive force is found by calculating the engine's ability to move the plane after accounting for friction and other aerodynamic issues that slow down the plane.  The ratio (T-D)/W is similar to T/W, the Thrust-to-weight ratio, which is also used as a figure of merit for airplanes and rockets.  By normalizing the motive forces to the weight of the plane, it is clear how efficient the plane is.  A very large engine may be able to generate a huge thrust but could be so heavy that it would not even lift itself.  The ratio is unity (T-D)/W = 1 when the engine is powerful enough to keep the plane at constant speed in a 90 degree ascending trajectory.  Fighter jets, such as the F-16 have a T/W ratio close to 1, depending on fuel weight and armament.
The difference between T/W and (T-D)/W is that T/W does not include the effects of friction and other aerodynamic losses.  When a plane is moving very slowly, these losses are small and can be ignored.  However, T/W does not accurately describe the performance of the plane at its normal operating conditions.  By including drag in the formula, the aerodynamics of the plane are also summarized in the 
  
    
      
        
          P
          
            S
          
        
      
    
    {\displaystyle P_{S}}
   value.
The specific excess energy model is proportional to the speed of the plane.  This means that the faster the plane is capable of flying, the better its score.  The other parts of this model (thrust, drag, and weight) may say that a plane is excellent but a good fighter must also go fast.
Boyd, a U.S. jet fighter pilot in the Korean War, began developing the theory in the early 1960s.  He teamed with mathematician Thomas Christie at Eglin Air Force Base to use the base's high-speed computer to compare the performance envelopes of U.S. and Soviet aircraft from the Korean and Vietnam Wars.  They completed a two-volume report on their studies in 1964.  Energy Maneuverability came to be accepted within the U.S. Air Force and brought about improvements in the requirements for the F-15 Eagle and later the F-16 Fighting Falcon fighters.


== Notes ==


== See also ==
Lagrangian mechanics


== References ==","pandas(index=21, _1=21, text='energy–maneuverability theory is a model of aircraft performance. it was developed by col. john boyd, a fighter pilot, and thomas p. christie, a mathematician with the air force, and is useful in describing an aircraft\'s performance as the total of kinetic and potential energies or aircraft specific energy.  it relates the thrust, weight, aerodynamic drag, wing area, and other flight characteristics of an aircraft into a quantitative model. this allows combat capabilities of various aircraft or prospective design trade-offs to be predicted and compared. all of these aspects of airplane performance are compressed into a single value by the following formula:          p  s     =   v  (    t − d  w   )         v   =    speed      t   =    thrust      d   =    drag      w   =    weight         value. the specific excess energy model is proportional to the speed of the plane.  this means that the faster the plane is capable of flying, the better its score.  the other parts of this model (thrust, drag, and weight) may say that a plane is excellent but a good fighter must also go fast. boyd, a u.s. jet fighter pilot in the korean war, began developing the theory in the early 1960s.  he teamed with mathematician thomas christie at eglin air force base to use the base\'s high-speed computer to compare the performance envelopes of u.s. and soviet aircraft from the korean and vietnam wars.  they completed a two-volume report on their studies in 1964.  energy maneuverability came to be accepted within the u.s. air force and brought about improvements in the requirements for the f-15 eagle and later the f-16 fighting falcon fighters.   == notes ==   == see also == lagrangian mechanics   == references ==')"
22,"Damage tolerance is a property of a structure relating to its ability to sustain defects safely until repair can be effected. The approach to engineering design to account for damage tolerance is based on the assumption that flaws can exist in any structure and such flaws propagate with usage. This approach is commonly used in aerospace engineering, mechanical engineering, and civil engineering to manage the extension of cracks in structure through the application of the principles of fracture mechanics.  In engineering, a structure is considered to be damage tolerant if a maintenance program has been implemented that will result in the detection and repair of accidental damage, corrosion and fatigue cracking before such damage reduces the residual strength of the structure below an acceptable limit. 


== History ==
Structures upon which human life depends have long been recognized as needing an element of fail-safety.  When describing his flying machine, Leonardo da Vinci noted that ""In constructing wings one should make one chord to bear the strain and a looser one in the same position so that if one breaks under the strain, the other is in the position to serve the same function.""Prior to the 1970s, the prevailing engineering philosophy of aircraft structures was to ensure that airworthiness was maintained with a single part broken, a redundancy requirement known as fail-safety.  However, advances in fracture mechanics, along with infamous catastrophic fatigue failures such as those in the de Havilland Comet prompted a change in requirements for aircraft.  It was discovered that a phenomenon known as  multiple-site damage could cause many small cracks in the structure, which grow slowly by themselves, to join one another over time, creating a much larger crack, and significantly reducing the expected time until failure 


== Safe-life structure ==
Not all structure must demonstrate detectable crack propagation to ensure safety of operation.  Some structures operate under the safe-life design principle, where an extremely low level of risk is accepted through a combination of testing and analysis that the part will never form a detectable crack due to fatigue during the service life of the part.  This is achieved through a significant reduction of stresses below the typical fatigue capability of the part.  Safe-life structures are employed when the cost or infeasibility of inspections outweighs the weight penalty and development costs associated with safe-life structures.  An example of a safe-life component is the helicopter rotor blade.  Due to the extremely large numbers of cycles endured by the rotating component, an undetectable crack may grow to a critical length in a single flight and before the aircraft lands, result in a catastrophic failure that regular maintenance could not have prevented.


== Damage tolerance analysis ==
In ensuring the continued safe operation of the damage tolerant structure, inspection schedules are devised.  This schedule is based on many criteria, including:

assumed initial damaged condition of the structure
stresses in the structure (both fatigue and operational maximum stresses) that cause crack growth from the damaged condition
geometry of the material which intensifies or reduces the stresses on the crack tip
ability of the material to withstand cracking due to stresses in the expected environment
largest crack size that the structure can endure before catastrophic failure
likelihood that a particular inspection method will reveal a crack
acceptable level of risk that a certain structure will be completely failed
expected duration after manufacture until a detectable crack will form
assumption of failure in adjacent components which may have the effect of changing stresses in the structure of interestThese factors affect how long the structure may operate normally in the damaged condition before one or more inspection intervals has the opportunity to discover the damaged state and effect a repair.  The interval between inspections must be selected with a certain minimum safety, and also must balance the expense of the inspections, the weight penalty of lowering fatigue stresses, and the opportunity costs associated with a structure being out of service for maintenance.


== Non-destructive inspections ==
Manufacturers and operators of aircraft, trains, and civil engineering structures like bridges have a financial interest in ensuring that the inspection schedule is as cost-efficient as possible.  In the example of aircraft, because these structures are often revenue producing, there is an opportunity cost associated with the maintenance of the aircraft (lost ticket revenue), in addition to the cost of maintenance itself.  Thus, this maintenance is desired to be performed infrequently, even when such increased intervals cause increased complexity and cost to the overhaul.  Crack growth, as shown by fracture mechanics, is exponential in nature; meaning that the crack growth rate is a function of an exponent of the current crack size (see Paris' law). This means that only the largest cracks influence the overall strength of a structure; small internal damages do not necessarily decrease the strength. A desire for infrequent inspection intervals, combined with the exponential growth of cracks in structure has led to the development of non-destructive testing methods which allow inspectors to look for very tiny cracks which are often invisible to the naked eye. Examples of this technology include eddy current, ultrasonic, dye penetrant, and X-ray inspections.  By catching structural cracks when they are very small, and growing slowly, these non-destructive inspections can reduce the amount of maintenance checks, and allow damage to be caught when it is small, and still inexpensive to repair. As an example, such repair can be achieved by drilling a small hole at the crack tip, thus effectively turning the crack into a keyhole-notch.


== References ==


== Further reading ==
Peggy C. Miedlar; Alan P. Berens; Allan Gunderson & J. P. Gallagher, Damage Tolerant Design Handbook: Guidelines for the Analysis and Design of Damage Tolerant Aircraft Structures, University of Dayton Research Institute, archived from the original on July 1, 2016, retrieved June 1, 2016","pandas(index=22, _1=22, text='damage tolerance is a property of a structure relating to its ability to sustain defects safely until repair can be effected. the approach to engineering design to account for damage tolerance is based on the assumption that flaws can exist in any structure and such flaws propagate with usage. this approach is commonly used in aerospace engineering, mechanical engineering, and civil engineering to manage the extension of cracks in structure through the application of the principles of fracture mechanics.  in engineering, a structure is considered to be damage tolerant if a maintenance program has been implemented that will result in the detection and repair of accidental damage, corrosion and fatigue cracking before such damage reduces the residual strength of the structure below an acceptable limit.   == history == structures upon which human life depends have long been recognized as needing an element of fail-safety.  when describing his flying machine, leonardo da vinci noted that ""in constructing wings one should make one chord to bear the strain and a looser one in the same position so that if one breaks under the strain, the other is in the position to serve the same function.""prior to the 1970s, the prevailing engineering philosophy of aircraft structures was to ensure that airworthiness was maintained with a single part broken, a redundancy requirement known as fail-safety.  however, advances in fracture mechanics, along with infamous catastrophic fatigue failures such as those in the de havilland comet prompted a change in requirements for aircraft.  it was discovered that a phenomenon known as  multiple-site damage could cause many small cracks in the structure, which grow slowly by themselves, to join one another over time, creating a much larger crack, and significantly reducing the expected time until failure   == safe-life structure == not all structure must demonstrate detectable crack propagation to ensure safety of operation.  some structures operate under the safe-life design principle, where an extremely low level of risk is accepted through a combination of testing and analysis that the part will never form a detectable crack due to fatigue during the service life of the part.  this is achieved through a significant reduction of stresses below the typical fatigue capability of the part.  safe-life structures are employed when the cost or infeasibility of inspections outweighs the weight penalty and development costs associated with safe-life structures.  an example of a safe-life component is the helicopter rotor blade.  due to the extremely large numbers of cycles endured by the rotating component, an undetectable crack may grow to a critical length in a single flight and before the aircraft lands, result in a catastrophic failure that regular maintenance could not have prevented.   == damage tolerance analysis == in ensuring the continued safe operation of the damage tolerant structure, inspection schedules are devised.  this schedule is based on many criteria, including:  assumed initial damaged condition of the structure stresses in the structure (both fatigue and operational maximum stresses) that cause crack growth from the damaged condition geometry of the material which intensifies or reduces the stresses on the crack tip ability of the material to withstand cracking due to stresses in the expected environment largest crack size that the structure can endure before catastrophic failure likelihood that a particular inspection method will reveal a crack acceptable level of risk that a certain structure will be completely failed expected duration after manufacture until a detectable crack will form assumption of failure in adjacent components which may have the effect of changing stresses in the structure of interestthese factors affect how long the structure may operate normally in the damaged condition before one or more inspection intervals has the opportunity to discover the damaged state and effect a repair.  the interval between inspections must be selected with a certain minimum safety, and also must balance the expense of the inspections, the weight penalty of lowering fatigue stresses, and the opportunity costs associated with a structure being out of service for maintenance.   == non-destructive inspections == manufacturers and operators of aircraft, trains, and civil engineering structures like bridges have a financial interest in ensuring that the inspection schedule is as cost-efficient as possible.  in the example of aircraft, because these structures are often revenue producing, there is an opportunity cost associated with the maintenance of the aircraft (lost ticket revenue), in addition to the cost of maintenance itself.  thus, this maintenance is desired to be performed infrequently, even when such increased intervals cause increased complexity and cost to the overhaul.  crack growth, as shown by fracture mechanics, is exponential in nature; meaning that the crack growth rate is a function of an exponent of the current crack size (see paris\' law). this means that only the largest cracks influence the overall strength of a structure; small internal damages do not necessarily decrease the strength. a desire for infrequent inspection intervals, combined with the exponential growth of cracks in structure has led to the development of non-destructive testing methods which allow inspectors to look for very tiny cracks which are often invisible to the naked eye. examples of this technology include eddy current, ultrasonic, dye penetrant, and x-ray inspections.  by catching structural cracks when they are very small, and growing slowly, these non-destructive inspections can reduce the amount of maintenance checks, and allow damage to be caught when it is small, and still inexpensive to repair. as an example, such repair can be achieved by drilling a small hole at the crack tip, thus effectively turning the crack into a keyhole-notch.   == references ==   == further reading == peggy c. miedlar; alan p. berens; allan gunderson & j. p. gallagher, damage tolerant design handbook: guidelines for the analysis and design of damage tolerant aircraft structures, university of dayton research institute, archived from the original on july 1, 2016, retrieved june 1, 2016')"
23,"Araldite is a registered trademark of Huntsman Advanced Materials (previously part of Ciba-Geigy) referring to their range of engineering and structural epoxy, acrylic, and polyurethane adhesives. The name was first used in 1946 for a two-part epoxy adhesive.
Araldite adhesive sets by the interaction of a resin with a hardener. Heat is not necessary although warming will reduce the curing time and improve the strength of the bond. After curing, the joint is claimed to be impervious to boiling water and all common organic solvents. It is available in many different types of pack, the most common containing two different tubes, one each for the resin and the hardener. Other variations include double syringe-type packages which automatically measure equal parts. This type of dispensing is not exact, however, and also poses the problem of unintentional mixing of resin and hardener.


== History ==
Aero Research Limited (ARL), founded in the UK in 1934, developed a new  synthetic-resin adhesive for bonding metals, glass, porcelain, china and other materials. The name ""Araldite"" recalls the ARL brand: ARaLdite. 
De Trey Frères SA of Switzerland carried out the first production of epoxy resins. They licensed the process to  Ciba AG in the early 1940s and Ciba first demonstrated a product under the tradename ""Araldite"" at the Swiss Industries Fair in 1945. Ciba went on to become one of the three major epoxy-resin producers worldwide. Ciba's epoxy business was spun off and later sold in the late 1990s and became the advanced materials business unit of Huntsman Corporation of the US.


== Notable applications ==
Beginning in 1940, Araldite was used in the production of the De Havilland Mosquito aircraft.
Araldite adhesive is used to join together the two sections of carbon composite which make up the monocoque of the Lamborghini Aventador.
The use of Araldite adhesive in architecture to bond thin joints of pre-cast concrete units was pioneered by Ove Arup in Coventry cathedral and the Sydney Opera House. At Coventry cathedral, Araldite adhesive was used to bond its columns and fins, while at Sydney Opera House, Araldite adhesive was used to bond the rib sections of the shells, since a traditional concrete joint would have slowed construction, as it would need 24 hours to cure before stressing.
Highmark Manufacturing uses Araldite epoxy resin in the manufacture of advanced ballistic protection body armour.
Schlösser Metallbau, a manufacturer of metal parts for railway carriages, uses Araldite epoxy resin to bond aluminium profiles of cab doorframes on the DBAG Class 423 Siemens Bombardier train.
Fischer Composite Technology GmbH uses the Araldite RTM System to produce carbon composite side blades for the Audi R8.
Araldite epoxy resin is commonly used as an embedding medium for electron microscopy.
Some Flamenco guitarists (e.g. Paco Peña) use it to reinforce their fingernails.
Brian May used it to seal the pickups in his homemade Red Special guitar to prevent microphonic feedback.


== Advertising ==
In 1983, British advertising agency FCO Univas set up a visual stunt presentation of the strength of Araldite adhesive by gluing a yellow Ford Cortina to a billboard on Cromwell Road, London, with the tagline ""It also sticks handles to teapots"".  Later, to demonstrate more of its strength, a red Cortina was placed on top of the yellow Cortina, with the tagline ""The tension mounts"". Finally, the car was removed from the billboard, leaving a hole on the billboard and a tagline ""How did we pull it off?"".


== See also ==
Aerolite
J-B Weld
Redux


== References ==


== External links ==
Specifications for 'Araldite Super Strength'","pandas(index=23, _1=23, text='araldite is a registered trademark of huntsman advanced materials (previously part of ciba-geigy) referring to their range of engineering and structural epoxy, acrylic, and polyurethane adhesives. the name was first used in 1946 for a two-part epoxy adhesive. araldite adhesive sets by the interaction of a resin with a hardener. heat is not necessary although warming will reduce the curing time and improve the strength of the bond. after curing, the joint is claimed to be impervious to boiling water and all common organic solvents. it is available in many different types of pack, the most common containing two different tubes, one each for the resin and the hardener. other variations include double syringe-type packages which automatically measure equal parts. this type of dispensing is not exact, however, and also poses the problem of unintentional mixing of resin and hardener.   == history == aero research limited (arl), founded in the uk in 1934, developed a new  synthetic-resin adhesive for bonding metals, glass, porcelain, china and other materials. the name ""araldite"" recalls the arl brand: araldite. de trey frères sa of switzerland carried out the first production of epoxy resins. they licensed the process to  ciba ag in the early 1940s and ciba first demonstrated a product under the tradename ""araldite"" at the swiss industries fair in 1945. ciba went on to become one of the three major epoxy-resin producers worldwide. ciba\'s epoxy business was spun off and later sold in the late 1990s and became the advanced materials business unit of huntsman corporation of the us.   == notable applications == beginning in 1940, araldite was used in the production of the de havilland mosquito aircraft. araldite adhesive is used to join together the two sections of carbon composite which make up the monocoque of the lamborghini aventador. the use of araldite adhesive in architecture to bond thin joints of pre-cast concrete units was pioneered by ove arup in coventry cathedral and the sydney opera house. at coventry cathedral, araldite adhesive was used to bond its columns and fins, while at sydney opera house, araldite adhesive was used to bond the rib sections of the shells, since a traditional concrete joint would have slowed construction, as it would need 24 hours to cure before stressing. highmark manufacturing uses araldite epoxy resin in the manufacture of advanced ballistic protection body armour. schlösser metallbau, a manufacturer of metal parts for railway carriages, uses araldite epoxy resin to bond aluminium profiles of cab doorframes on the dbag class 423 siemens bombardier train. fischer composite technology gmbh uses the araldite rtm system to produce carbon composite side blades for the audi r8. araldite epoxy resin is commonly used as an embedding medium for electron microscopy. some flamenco guitarists (e.g. paco peña) use it to reinforce their fingernails. brian may used it to seal the pickups in his homemade red special guitar to prevent microphonic feedback.   == advertising == in 1983, british advertising agency fco univas set up a visual stunt presentation of the strength of araldite adhesive by gluing a yellow ford cortina to a billboard on cromwell road, london, with the tagline ""it also sticks handles to teapots"".  later, to demonstrate more of its strength, a red cortina was placed on top of the yellow cortina, with the tagline ""the tension mounts"". finally, the car was removed from the billboard, leaving a hole on the billboard and a tagline ""how did we pull it off?"".   == see also == aerolite j-b weld redux   == references ==   == external links == specifications for \'araldite super strength\'')"
24,"In aviation, hot and high is a condition of low air density due to high ambient temperature and high airport elevation. Air density decreases with increasing temperature and altitude. The reduced density reduces the performance of the aircraft's engine and also provides less lift, requiring a higher speed to lift the plane off the ground. Aviators gauge air density by calculating the density altitude.An airport may be especially hot or high, without the other condition being present. Temperature and pressure altitude can change from one hour to the next. The fact that temperature decreases as altitude increases mitigates the ""hot and high"" effect to a small extent.


== Negative effects of reduced engine power due to hot and high conditions ==
Airplanes require a longer takeoff run, potentially exceeding the amount of available runway.
Reduced take-off power hampers an aircraft's ability to climb. In some cases, an aircraft may be unable to climb rapidly enough to clear terrain surrounding a mountain airport.
Helicopters may be forced to operate in the shaded portion of the height-velocity diagram in order to become airborne at all. This creates the potential for an uncontrollable descent in the event of an engine failure.
In some cases, aircraft have landed at high-altitude airports by taking advantage of cold temperatures only to become stranded as temperatures warmed and air density decreased.
While unsafe at any altitude, an overloaded aircraft is much more dangerous under hot and high conditions.


== Improving hot and high performance ==
Some ways to increase aircraft performance in hot and high conditions include:

Reduce aircraft weight. Weight can be reduced by carrying only enough fuel to reach the (lower-altitude) destination rather than filling the tanks completely. In some cases, unnecessary equipment can be removed from the aircraft. In many cases, however, the only practical way to adequately reduce aircraft weight is to depart with a smaller passenger, cargo, or weapons load. Consequently, hot and high conditions at the originating airport may prevent a commercial aircraft from operating with a load large enough to be profitable, or may constrain the firepower that a combat aircraft can bring to bear when conducting a long-range airstrike.
Increase engine power. More powerful engines can improve an airplane's acceleration and reduce its takeoff run. More powerful engines are generally larger and heavier and use more fuel during cruise, however, increasing the fuel load needed to reach the same destination. The added weight of the fuel and engines may negate the potential performance gain, and the added cost of the extra fuel may constrain the profitability of a commercial aircraft. On the other hand, replacing an older, less efficient engine with a newer engine of more advanced design can increase both power output and efficiency while sometimes even decreasing weight. In this situation, the only real disadvantage is the cost of the upgrade.
Utilize assisted take off devices, such as rockets, to increase acceleration and rate of climb.
Inject distilled water into the engine compressor or combustor. The primary purpose of water injection into jet engines is to increase the mass being accelerated, thereby increasing the force created by the engine. A secondary purpose is to lower the combustion temperature so that higher power settings may be used without causing engine temperatures to exceed limits.


== Jet or rocket assisted take off ==

Auxiliary rockets and/or jet engines can help a fully loaded aircraft to take off within the length of the runway. The rockets are usually one-time units that are jettisoned after takeoff. This practice was common in the 1950s and 60s, when the lower levels of thrust from military turbojets was inadequate for takeoff from shorter runways or with very heavy payloads. It is now seldom used.
Auxiliary jets and rockets have rarely been used on civil aircraft due to the risk of aircraft damage and loss of control if something were to go wrong during their use. Boeing did, however, produce a version of its popular Boeing 727 with JATO primarily for ""hot and high"" operations out of Mexico City Airport (MMMX) and La Paz, Bolivia. The boosters were located adjacent to the main landing gear at the wing root on each side of the aircraft.


== Specialized aircraft ==
Several manufacturers of early jet airliners offered variants optimized for hot and high operations. Such aircraft generally offered the largest wings and/or the most powerful engines in the model lineup coupled with a small fuselage to reduce weight. Some such aircraft include:

The BAC One-Eleven 475 combined the short body of the series 400 with the more powerful engines and improved wings of the series 500.  This aircraft also featured stronger landing gear for rough field operations.
The Boeing 707-220, which was a 707-120 airframe fitted with more powerful Pratt & Whitney JT4A engines, civilian versions of the military J75. The 707-220 had extremely high fuel consumption, and only 5 were built, all for Braniff International Airways. The 707-220 was rendered redundant by the release of the turbofan-powered 707-120B, which had even greater power along with much lower fuel consumption.
The Convair 880. Although Convair only offered one configuration of this aircraft, it had more power and a smaller fuselage than its competitors from Boeing and Douglas. Convair essentially wagered the success of the entire 880 model line on the appeal of an aircraft optimized for hot and high operations. The wager failed; only sixty-five 880s were sold and Convair's nascent airliner business soon collapsed.
The De Havilland Canada Dash 8-200, which is a -100 airframe fitted with larger engines of the -300 for hot and high operations. They proved successful and eventually replaced the -100 production line.
The Lockheed L-1011-200, which was otherwise an L-1011-100 with more powerful RB.211-524B engines.
The McDonnell Douglas DC-9-20, which combined the smaller fuselage of the DC-9-10 with the larger wings and more powerful engines of the DC-9-30, and was significantly outsold by both.
The McDonnell Douglas DC-10-15, which combined the fuselage of the DC-10-10 with the larger engines of the DC-10-30. These were specifically designed for and sold to Aeromexico and Mexicana. Only seven were built.
The Vickers VC10, which was designed to meet BOAC requirements for a large airliner that could operate medium range flights from short runways in southern Asia and Africa. The rear-mounted engines gave a more efficient wing and made them less vulnerable to runway debris. The resulting high fuel consumption compared to the contemporary Boeing 707 prompted all other major airlines to dismiss the VC10.
The McDonnell Douglas MD-82 was a hot and high version of the MD-80, and sold well, which generally is extremely rare for a type of performance-specialised aircraftThe marketing failure of most of these airplanes demonstrated that airlines were generally unwilling to accept reduced efficiency at cruise and smaller ultimate load-carrying capacity in return for a slight performance gain at particular airports. Rather than accepting these drawbacks, it was easier for airlines to demand the construction of longer runways, operate with smaller loads as conditions dictated, or simply drop the unprofitable destinations.
Furthermore, as the second generation of jet airliners began to appear in the 1970s, some aircraft were designed to eliminate the need for a special ""hot and high"" variant – for instance, the Airbus A300 can perform a 15/0 takeoff, where the leading edge slats are adjusted to 15 degrees and the flaps kept retracted. This takeoff technique is only used at hot and high airports, for it enables a higher climb limit weight and improves second segment climb performance.
Most jetliner manufacturers have dropped the ""hot and high"" variants from their model lineups.


== Hot and high airports ==
Notable examples of hot and high airports include:
Addis Ababa, Ethiopia – Bole Airport
Albuquerque, New Mexico, United States - Albuquerque International Sunport, especially from late spring to early autumn
Brasília, Brazil – Brasília Airport
Bogotá, Colombia – El Dorado Airport
Calgary, Alberta, Canada – Calgary International Airport, especially from late spring to early autumn
Daulat Beg Oldi, Ladakh, India - Daulat Beg Oldi Advanced Landing Ground (The world's highest airstrip at 16,700 feet (5,100 m). Climate ranges from a maximum of 35 °C (95 °F) in summer to −35 °C (−31 °F) in winter )
Denver, Colorado – Denver International Airport, especially from late spring to early autumn
Edwards Air Force Base, California, United States
Guatemala City, Guatemala - La Aurora International Airport, the highest international airport in Central America (4,951 feet (1,509 m)). It is hot from late February to late October
Harare, Zimbabwe – Robert Gabriel Mugabe International Airport
Johannesburg, South Africa – O. R. Tambo International Airport
Kabul, Afghanistan – Kabul Airport
Kampala, Uganda - Entebbe International Airport
Kunming, Yunnan, China - Kunming Changshui International Airport
Kuwait City, Kuwait - Kuwait International Airport (while only at an elevation of 206 feet (63 m), it is widely considered one of the world's hottest airports, as temperatures can reach up to 114 °F (46 °C) on an average summer day) 
La Paz, Bolivia – El Alto International Airport (not generally a ""hot"" airport, as average high temperatures are never more than 15 °C (59 °F) throughout the year, but the world's highest commercial airport with regularly scheduled international flights at 13,325 feet (4,061 m))
Las Vegas, Nevada, United States – McCarran International Airport
Leh, Ladakh, India - Kushok Bakula Rimpochee Airport - (One of the highest commercial airports in the world at 10,700 feet (3,300 m). Surrounded by high mountain peaks and with temperatures ranging from −42 °C (-43.6 °F) in winter to 33 °C (91.4 °F) in summer, it is an extremely challenging airport to fly from)
Lhasa, Tibet, China - Lhasa Gonggar Airport
Medellín, Colombia - José María Córdoba International Airport ( hot and high tests for the Airbus A380 were done there).
Mexico City, Mexico – Mexico City International Airport
Nairobi, Kenya - Jomo Kenyatta International Airport
Phoenix, Arizona, United States – Phoenix Sky Harbor International Airport (altitude of 1,135 feet (346 m) is not extreme, but the area's hot desert climate gives it hot and high characteristics for most of the year)
Quito, Ecuador – Mariscal Sucre Airport
Salt Lake City, Utah, United States – Salt Lake City International Airport, especially from late spring to early autumn
Sanaa, Yemen – Sanaa International Airport
Siachen Glacier, India - Sonam Post, world's highest helipad (altitude of 21,000 feet (6,400 m) in the world's highest manned post.
Tehran, Iran - Tehran Imam Khomeini International Airport
Xining, Qinghai, China - Xining Caojiabao Airport
Yerevan, Armenia – Zvartnots International Airport
Windhoek, Namibia - Hosea Kutako International Airport


== References ==","pandas(index=24, _1=24, text='in aviation, hot and high is a condition of low air density due to high ambient temperature and high airport elevation. air density decreases with increasing temperature and altitude. the reduced density reduces the performance of the aircraft\'s engine and also provides less lift, requiring a higher speed to lift the plane off the ground. aviators gauge air density by calculating the density altitude.an airport may be especially hot or high, without the other condition being present. temperature and pressure altitude can change from one hour to the next. the fact that temperature decreases as altitude increases mitigates the ""hot and high"" effect to a small extent.   == negative effects of reduced engine power due to hot and high conditions == airplanes require a longer takeoff run, potentially exceeding the amount of available runway. reduced take-off power hampers an aircraft\'s ability to climb. in some cases, an aircraft may be unable to climb rapidly enough to clear terrain surrounding a mountain airport. helicopters may be forced to operate in the shaded portion of the height-velocity diagram in order to become airborne at all. this creates the potential for an uncontrollable descent in the event of an engine failure. in some cases, aircraft have landed at high-altitude airports by taking advantage of cold temperatures only to become stranded as temperatures warmed and air density decreased. while unsafe at any altitude, an overloaded aircraft is much more dangerous under hot and high conditions.   == improving hot and high performance == some ways to increase aircraft performance in hot and high conditions include:  reduce aircraft weight. weight can be reduced by carrying only enough fuel to reach the (lower-altitude) destination rather than filling the tanks completely. in some cases, unnecessary equipment can be removed from the aircraft. in many cases, however, the only practical way to adequately reduce aircraft weight is to depart with a smaller passenger, cargo, or weapons load. consequently, hot and high conditions at the originating airport may prevent a commercial aircraft from operating with a load large enough to be profitable, or may constrain the firepower that a combat aircraft can bring to bear when conducting a long-range airstrike. increase engine power. more powerful engines can improve an airplane\'s acceleration and reduce its takeoff run. more powerful engines are generally larger and heavier and use more fuel during cruise, however, increasing the fuel load needed to reach the same destination. the added weight of the fuel and engines may negate the potential performance gain, and the added cost of the extra fuel may constrain the profitability of a commercial aircraft. on the other hand, replacing an older, less efficient engine with a newer engine of more advanced design can increase both power output and efficiency while sometimes even decreasing weight. in this situation, the only real disadvantage is the cost of the upgrade. utilize assisted take off devices, such as rockets, to increase acceleration and rate of climb. inject distilled water into the engine compressor or combustor. the primary purpose of water injection into jet engines is to increase the mass being accelerated, thereby increasing the force created by the engine. a secondary purpose is to lower the combustion temperature so that higher power settings may be used without causing engine temperatures to exceed limits.   == jet or rocket assisted take off ==  auxiliary rockets and/or jet engines can help a fully loaded aircraft to take off within the length of the runway. the rockets are usually one-time units that are jettisoned after takeoff. this practice was common in the 1950s and 60s, when the lower levels of thrust from military turbojets was inadequate for takeoff from shorter runways or with very heavy payloads. it is now seldom used. auxiliary jets and rockets have rarely been used on civil aircraft due to the risk of aircraft damage and loss of control if something were to go wrong during their use. boeing did, however, produce a version of its popular boeing 727 with jato primarily for ""hot and high"" operations out of mexico city airport (mmmx) and la paz, bolivia. the boosters were located adjacent to the main landing gear at the wing root on each side of the aircraft.   == specialized aircraft == several manufacturers of early jet airliners offered variants optimized for hot and high operations. such aircraft generally offered the largest wings and/or the most powerful engines in the model lineup coupled with a small fuselage to reduce weight. some such aircraft include:  the bac one-eleven 475 combined the short body of the series 400 with the more powerful engines and improved wings of the series 500.  this aircraft also featured stronger landing gear for rough field operations. the boeing 707-220, which was a 707-120 airframe fitted with more powerful pratt & whitney jt4a engines, civilian versions of the military j75. the 707-220 had extremely high fuel consumption, and only 5 were built, all for braniff international airways. the 707-220 was rendered redundant by the release of the turbofan-powered 707-120b, which had even greater power along with much lower fuel consumption. the convair 880. although convair only offered one configuration of this aircraft, it had more power and a smaller fuselage than its competitors from boeing and douglas. convair essentially wagered the success of the entire 880 model line on the appeal of an aircraft optimized for hot and high operations. the wager failed; only sixty-five 880s were sold and convair\'s nascent airliner business soon collapsed. the de havilland canada dash 8-200, which is a -100 airframe fitted with larger engines of the -300 for hot and high operations. they proved successful and eventually replaced the -100 production line. the lockheed l-1011-200, which was otherwise an l-1011-100 with more powerful rb.211-524b engines. the mcdonnell douglas dc-9-20, which combined the smaller fuselage of the dc-9-10 with the larger wings and more powerful engines of the dc-9-30, and was significantly outsold by both. the mcdonnell douglas dc-10-15, which combined the fuselage of the dc-10-10 with the larger engines of the dc-10-30. these were specifically designed for and sold to aeromexico and mexicana. only seven were built. the vickers vc10, which was designed to meet boac requirements for a large airliner that could operate medium range flights from short runways in southern asia and africa. the rear-mounted engines gave a more efficient wing and made them less vulnerable to runway debris. the resulting high fuel consumption compared to the contemporary boeing 707 prompted all other major airlines to dismiss the vc10. the mcdonnell douglas md-82 was a hot and high version of the md-80, and sold well, which generally is extremely rare for a type of performance-specialised aircraftthe marketing failure of most of these airplanes demonstrated that airlines were generally unwilling to accept reduced efficiency at cruise and smaller ultimate load-carrying capacity in return for a slight performance gain at particular airports. rather than accepting these drawbacks, it was easier for airlines to demand the construction of longer runways, operate with smaller loads as conditions dictated, or simply drop the unprofitable destinations. furthermore, as the second generation of jet airliners began to appear in the 1970s, some aircraft were designed to eliminate the need for a special ""hot and high"" variant – for instance, the airbus a300 can perform a 15/0 takeoff, where the leading edge slats are adjusted to 15 degrees and the flaps kept retracted. this takeoff technique is only used at hot and high airports, for it enables a higher climb limit weight and improves second segment climb performance. most jetliner manufacturers have dropped the ""hot and high"" variants from their model lineups.   == hot and high airports == notable examples of hot and high airports include: addis ababa, ethiopia – bole airport albuquerque, new mexico, united states - albuquerque international sunport, especially from late spring to early autumn brasília, brazil – brasília airport bogotá, colombia – el dorado airport calgary, alberta, canada – calgary international airport, especially from late spring to early autumn daulat beg oldi, ladakh, india - daulat beg oldi advanced landing ground (the world\'s highest airstrip at 16,700 feet (5,100 m). climate ranges from a maximum of 35 °c (95 °f) in summer to −35 °c (−31 °f) in winter ) denver, colorado – denver international airport, especially from late spring to early autumn edwards air force base, california, united states guatemala city, guatemala - la aurora international airport, the highest international airport in central america (4,951 feet (1,509 m)). it is hot from late february to late october harare, zimbabwe – robert gabriel mugabe international airport johannesburg, south africa – o. r. tambo international airport kabul, afghanistan – kabul airport kampala, uganda - entebbe international airport kunming, yunnan, china - kunming changshui international airport kuwait city, kuwait - kuwait international airport (while only at an elevation of 206 feet (63 m), it is widely considered one of the world\'s hottest airports, as temperatures can reach up to 114 °f (46 °c) on an average summer day) la paz, bolivia – el alto international airport (not generally a ""hot"" airport, as average high temperatures are never more than 15 °c (59 °f) throughout the year, but the world\'s highest commercial airport with regularly scheduled international flights at 13,325 feet (4,061 m)) las vegas, nevada, united states – mccarran international airport leh, ladakh, india - kushok bakula rimpochee airport - (one of the highest commercial airports in the world at 10,700 feet (3,300 m). surrounded by high mountain peaks and with temperatures ranging from −42 °c (-43.6 °f) in winter to 33 °c (91.4 °f) in summer, it is an extremely challenging airport to fly from) lhasa, tibet, china - lhasa gonggar airport medellín, colombia - josé maría córdoba international airport ( hot and high tests for the airbus a380 were done there). mexico city, mexico – mexico city international airport nairobi, kenya - jomo kenyatta international airport phoenix, arizona, united states – phoenix sky harbor international airport (altitude of 1,135 feet (346 m) is not extreme, but the area\'s hot desert climate gives it hot and high characteristics for most of the year) quito, ecuador – mariscal sucre airport salt lake city, utah, united states – salt lake city international airport, especially from late spring to early autumn sanaa, yemen – sanaa international airport siachen glacier, india - sonam post, world\'s highest helipad (altitude of 21,000 feet (6,400 m) in the world\'s highest manned post. tehran, iran - tehran imam khomeini international airport xining, qinghai, china - xining caojiabao airport yerevan, armenia – zvartnots international airport windhoek, namibia - hosea kutako international airport   == references ==')"
25,"A geodetic airframe is a type of construction for the airframes of aircraft developed by British aeronautical engineer Barnes Wallis in the 1930s (who sometimes spelled it ""geodesic""). Earlier, it was used by Prof. Schütte for the Schütte Lanz Airship LS 1 in 1909. It makes use of a space frame formed from a spirally crossing basket-weave of load-bearing members. The principle is that two geodesic arcs can be drawn to intersect on a curving surface (the fuselage) in a manner that the torsional load on each cancels out that on the other.


== Early examples ==

The ""diagonal rider"" structural element was used by Joshua Humphreys in the first US Navy sail frigates in 1794. Diagonal riders are viewable in the interior hull structure of the preserved USS Constitution on display in Boston Harbor. The structure was a pioneering example of placing ""non-orthogonal"" structural components within an otherwise conventional structure for its time. The ""diagonal riders"" were included in these American naval vessels' construction as one of five elements to reduce the problem of hogging in the ship's hull, and did not make up the bulk of the vessel's structure, they do not constitute a completely ""geodetic"" space frame.Calling any diagonal wood brace (as used on gates, buildings, ships or other structures with cantilevered or diagonal loads) an example of geodesic design is a misnomer. In a geodetic structure, the strength and structural integrity, and indeed the shape, come from the diagonal ""braces"" - the structure does not need the ""bits in between"" for part of its strength (implicit in the name space frame) as does a more conventional wooden structure.


== Aeroplanes ==

The earliest-known use of a geodesic airframe design for any aircraft was for the pre-World War I Schütte-Lanz SL1 rigid airship's envelope structure of 1911, with the airship capable of up to a 38.3 km/h (23.8 mph) top airspeed.
The Latécoère 6 was a French four-engined biplane bomber of the early 1920s. It was of advanced all-metal construction and probably the first aircraft to use geodetic construction. Only one was built.
Barnes Wallis, inspired by his earlier experience with light alloy structures and the use of geodesically-arranged wiring to distribute the lifting loads of the gasbags in the design of the R100 airship, evolved the geodetic construction method (although it is commonly stated, there was no geodetic structure in R100). Wallis used the term ""geodetic"" to apply to the airframe and distinguish it from ""geodesic"" which is the proper term for a line on a curved surface, arising from geodesy.The system was later used by Wallis's employer, Vickers-Armstrongs in a series of bomber aircraft, the Wellesley, Wellington, Warwick and Windsor. In these aircraft, the fuselage was built up from a number of duralumin alloy channel-beams that were formed into a large framework. Wooden battens were screwed onto the metal, to which the doped linen skin of the aircraft was fixed.
The metal lattice-work gave a light structure with tremendous strength; any one of the stringers could support some of the load from the opposite side of the aircraft. Blowing out the structure from one side would still leave the load-bearing structure as a whole intact. As a result, Wellingtons with huge areas of framework missing continued to return home when other types would not have survived; the dramatic effect enhanced by the doped fabric skin burning off, leaving the naked frames exposed (see photo). The benefits of the geodesic construction were partly offset by the difficulty of modifying the physical structure of the aircraft to allow for a change in length, profile, wingspan etc.
Geodetic wing and fin structures—taken from the Wellington—were used on the post-war Vickers VC.1 Viking, though with a new fuselage and metal-skinned.


== See also ==
Design principle
Figure of the Earth
Geodesic dome
Geodesic (disambiguation)
Geodetic system


== References ==


=== Inline citations ===


=== Sources ===
Buttler, Tony (2004). British Secret Projects: Fighters & Bombers 1935-1950. Hinckley: Midland Publishing. p. 240 pages. ISBN 978-1-85780-179-8.
Murray, Iain (2009). Bouncing-Bomb Man: the Science of Sir Barnes Wallis. Haynes. ISBN 978-1-84425-588-7.","pandas(index=25, _1=25, text='a geodetic airframe is a type of construction for the airframes of aircraft developed by british aeronautical engineer barnes wallis in the 1930s (who sometimes spelled it ""geodesic""). earlier, it was used by prof. schütte for the schütte lanz airship ls 1 in 1909. it makes use of a space frame formed from a spirally crossing basket-weave of load-bearing members. the principle is that two geodesic arcs can be drawn to intersect on a curving surface (the fuselage) in a manner that the torsional load on each cancels out that on the other.   == early examples ==  the ""diagonal rider"" structural element was used by joshua humphreys in the first us navy sail frigates in 1794. diagonal riders are viewable in the interior hull structure of the preserved uss constitution on display in boston harbor. the structure was a pioneering example of placing ""non-orthogonal"" structural components within an otherwise conventional structure for its time. the ""diagonal riders"" were included in these american naval vessels\' construction as one of five elements to reduce the problem of hogging in the ship\'s hull, and did not make up the bulk of the vessel\'s structure, they do not constitute a completely ""geodetic"" space frame.calling any diagonal wood brace (as used on gates, buildings, ships or other structures with cantilevered or diagonal loads) an example of geodesic design is a misnomer. in a geodetic structure, the strength and structural integrity, and indeed the shape, come from the diagonal ""braces"" - the structure does not need the ""bits in between"" for part of its strength (implicit in the name space frame) as does a more conventional wooden structure.   == aeroplanes ==  the earliest-known use of a geodesic airframe design for any aircraft was for the pre-world war i schütte-lanz sl1 rigid airship\'s envelope structure of 1911, with the airship capable of up to a 38.3 km/h (23.8 mph) top airspeed. the latécoère 6 was a french four-engined biplane bomber of the early 1920s. it was of advanced all-metal construction and probably the first aircraft to use geodetic construction. only one was built. barnes wallis, inspired by his earlier experience with light alloy structures and the use of geodesically-arranged wiring to distribute the lifting loads of the gasbags in the design of the r100 airship, evolved the geodetic construction method (although it is commonly stated, there was no geodetic structure in r100). wallis used the term ""geodetic"" to apply to the airframe and distinguish it from ""geodesic"" which is the proper term for a line on a curved surface, arising from geodesy.the system was later used by wallis\'s employer, vickers-armstrongs in a series of bomber aircraft, the wellesley, wellington, warwick and windsor. in these aircraft, the fuselage was built up from a number of duralumin alloy channel-beams that were formed into a large framework. wooden battens were screwed onto the metal, to which the doped linen skin of the aircraft was fixed. the metal lattice-work gave a light structure with tremendous strength; any one of the stringers could support some of the load from the opposite side of the aircraft. blowing out the structure from one side would still leave the load-bearing structure as a whole intact. as a result, wellingtons with huge areas of framework missing continued to return home when other types would not have survived; the dramatic effect enhanced by the doped fabric skin burning off, leaving the naked frames exposed (see photo). the benefits of the geodesic construction were partly offset by the difficulty of modifying the physical structure of the aircraft to allow for a change in length, profile, wingspan etc. geodetic wing and fin structures—taken from the wellington—were used on the post-war vickers vc.1 viking, though with a new fuselage and metal-skinned.   == see also == design principle figure of the earth geodesic dome geodesic (disambiguation) geodetic system   == references == buttler, tony (2004). british secret projects: fighters & bombers 1935-1950. hinckley: midland publishing. p. 240 pages. isbn 978-1-85780-179-8. murray, iain (2009). bouncing-bomb man: the science of sir barnes wallis. haynes. isbn 978-1-84425-588-7.')"
26,"REFSMMAT is a term used by guidance, navigation, and control system flight controllers during the Apollo program, which carried over into the Space Shuttle program. REFSMMAT stands for ""Reference to Stable Member Matrix"". It is a numerical definition of a fixed orientation in space and is usually (but not always) defined with respect to the stars. It was used by the Apollo Primary Guidance, Navigation and Control System (PGNCS) as a reference to which the gimbal-mounted platform at its core should be oriented. Every operation within the spacecraft that required knowledge of direction was carried out with respect to the orientation of the guidance platform, itself aligned according to a particular REFSMMAT.
During an Apollo flight, the REFSMMAT being used, and therefore the orientation of the guidance platform, would change as operational needs required it, but never during a guidance process—that is, one REFSMMAT might be in use from launch through Trans-Lunar Injection, another from TLI to Midpoint, but would not change during the middle of a burn or set of maneuvers.For example, it was considered good practice to have the spacecraft displays show some meaningful attitude value that would be easy to monitor during an important engine burn. Flight controllers at mission control in Houston would calculate what attitude the spacecraft had to be at for that burn and would devise a REFSMMAT that matched it in some way. Then, when it came time for the burn, if the spacecraft was in its correct attitude, the crew would see their 8-ball display a simple attitude that would be easy to interpret, allowing errors to be easily tracked and corrected.In the hallowed halls of mission control, Captain Refsmmat was a Kilroy-type character, conceived as a joke spoken to a 'Flight Dynamics Branch' rookie by Flight Controller RETRO John Llewellyn, and first drawn by flight controller FIDO Ed Pavelka as the ""ideal mission controller"".  'Capt. Refsmmat' served during the Apollo and Skylab years as an aid to the esprit de corps within the mission control team.


== See also ==

Apollo program


== References ==


== External links ==","pandas(index=26, _1=26, text='refsmmat is a term used by guidance, navigation, and control system flight controllers during the apollo program, which carried over into the space shuttle program. refsmmat stands for ""reference to stable member matrix"". it is a numerical definition of a fixed orientation in space and is usually (but not always) defined with respect to the stars. it was used by the apollo primary guidance, navigation and control system (pgncs) as a reference to which the gimbal-mounted platform at its core should be oriented. every operation within the spacecraft that required knowledge of direction was carried out with respect to the orientation of the guidance platform, itself aligned according to a particular refsmmat. during an apollo flight, the refsmmat being used, and therefore the orientation of the guidance platform, would change as operational needs required it, but never during a guidance process—that is, one refsmmat might be in use from launch through trans-lunar injection, another from tli to midpoint, but would not change during the middle of a burn or set of maneuvers.for example, it was considered good practice to have the spacecraft displays show some meaningful attitude value that would be easy to monitor during an important engine burn. flight controllers at mission control in houston would calculate what attitude the spacecraft had to be at for that burn and would devise a refsmmat that matched it in some way. then, when it came time for the burn, if the spacecraft was in its correct attitude, the crew would see their 8-ball display a simple attitude that would be easy to interpret, allowing errors to be easily tracked and corrected.in the hallowed halls of mission control, captain refsmmat was a kilroy-type character, conceived as a joke spoken to a \'flight dynamics branch\' rookie by flight controller retro john llewellyn, and first drawn by flight controller fido ed pavelka as the ""ideal mission controller"".  \'capt. refsmmat\' served during the apollo and skylab years as an aid to the esprit de corps within the mission control team.   == see also ==  apollo program   == references ==   == external links ==')"
27,"Cabin pressurization is a process in which conditioned air is pumped into the cabin of an aircraft or spacecraft in order to create a safe and comfortable environment for passengers and crew flying at high altitudes. For aircraft, this air is usually bled off from the gas turbine engines at the compressor stage, and for spacecraft, it is carried in high-pressure, often cryogenic tanks. The air is cooled, humidified, and mixed with recirculated air if necessary before it is distributed to the cabin by one or more environmental control systems. The cabin pressure is regulated by the outflow valve.
While the first experimental pressurization systems saw use during the 1920s and 1930s, it was not until 1938 that the Boeing 307 Stratoliner, the first commercial aircraft to be equipped with a pressurized cabin, was introduced. The practice would become widespread a decade later, particularly with the introduction of the British de Havilland Comet in 1949, the world's first jetliner. While initially a success, two catastrophic failures in 1954 temporarily grounded the worldwide fleet; the cause was found to be a combination of progressive metal fatigue and aircraft skin stresses, both of which aeronautical engineers only had a limited understanding of at the time. The key engineering principles learned from the Comet were applied directly to the design of all subsequent jet airliners, such as the Boeing 707.
Certain aircraft have presented unusual pressurization scenarios. The supersonic airliner Concorde had a particularly high pressure differential due to flying at unusually high altitude (up to 60,000 feet (18,000 m) while maintaining a cabin altitude of 6,000 feet (1,800 m). This not only increased airframe weight, but also saw the use of smaller cabin windows than most other commercial passenger aircraft, intended to slow the decompression rate if a depressurization event occurred. The Aloha Airlines Flight 243 incident, involving a Boeing 737-200 that suffered catastrophic cabin failure mid-flight, was primarily caused by its continued operation despite having accumulated more than twice the number of flight cycles that the airframe was designed to endure. For increased passenger comfort, several modern airliners, such as the Boeing 787 Dreamliner and the Airbus A350 XWB, feature reduced operating cabin altitudes as well as greater humidity levels; the use of composite airframes has aided the adoption of such comfort-maximising practices.


== Need for cabin pressurization ==

Pressurization becomes increasingly necessary at altitudes above 10,000 feet (3,000 m) above sea level to protect crew and passengers from the risk of a number of physiological problems caused by the low outside air pressure above that altitude. For private aircraft operating in the US, crew members are required to use oxygen masks if the cabin altitude (a representation of the air pressure, see below) stays above 12,500 ft for more than 30 minutes, or if the cabin altitude reaches 14,000 ft at any time. At altitudes above 15,000 ft, passengers are required to be provided oxygen masks as well. On commercial aircraft, the cabin altitude must be maintained at 8,000 feet (2,400 m) or less. Pressurization of the cargo hold is also required to prevent damage to pressure-sensitive goods that might leak, expand, burst or be crushed on re-pressurization. The principal physiological problems are listed below.

Hypoxia
The lower partial pressure of oxygen at high altitude reduces the alveolar oxygen tension in the lungs and subsequently in the brain, leading to sluggish thinking, dimmed vision, loss of consciousness, and ultimately death. In some individuals, particularly those with heart or lung disease, symptoms may begin as low as 5,000 feet (1,500 m), although most passengers can tolerate altitudes of 8,000 feet (2,400 m) without ill effect. At this altitude, there is about 25% less oxygen than there is at sea level.
Hypoxia may be addressed by the administration of supplemental oxygen, either through an oxygen mask or through a nasal cannula. Without pressurization, sufficient oxygen can be delivered up to an altitude of about 40,000 feet (12,000 m). This is because a person who is used to living at sea level needs about 0.20 bar partial oxygen pressure to function normally and that pressure can be maintained up to about 40,000 feet (12,000 m) by increasing the mole fraction of oxygen in the air that is being breathed. At 40,000 feet (12,000 m), the ambient air pressure falls to about 0.2 bar, at which maintaining a minimum partial pressure of oxygen of 0.2 bar requires breathing 100% oxygen using an oxygen mask.
Emergency oxygen supply masks in the passenger compartment of airliners do not need to be pressure-demand masks because most flights stay below 40,000 feet (12,000 m). Above that altitude the partial pressure of oxygen will fall below 0.2 bar even at 100% oxygen and some degree of cabin pressurization or rapid descent will be essential to avoid the risk of hypoxia.
Altitude sickness
Hyperventilation, the body's most common response to hypoxia, does help to partially restore the partial pressure of oxygen in the blood, but it also causes carbon dioxide (CO2) to out-gas, raising the blood pH and inducing alkalosis. Passengers may experience fatigue, nausea, headaches, sleeplessness, and (on extended flights) even pulmonary oedema. These are the same symptoms that mountain climbers experience, but the limited duration of powered flight makes the development of pulmonary oedema unlikely. Altitude sickness may be controlled by a full pressure suit with helmet and faceplate, which completely envelops the body in a pressurized environment; however, this is impractical for commercial passengers.
Decompression sickness
The low partial pressure of gases, principally nitrogen (N2) but including all other gases, may cause dissolved gases in the bloodstream to precipitate out, resulting in gas embolism, or bubbles in the bloodstream. The mechanism is the same as that of compressed-air divers on ascent from depth. Symptoms may include the early symptoms of ""the bends""—tiredness, forgetfulness, headache, stroke, thrombosis, and subcutaneous itching—but rarely the full symptoms thereof. Decompression sickness may also be controlled by a full-pressure suit as for altitude sickness.
Barotrauma
As the aircraft climbs or descends, passengers may experience discomfort or acute pain as gases trapped within their bodies expand or contract. The most common problems occur with air trapped in the middle ear (aerotitis) or paranasal sinuses by a blocked Eustachian tube or sinuses. Pain may also be experienced in the gastrointestinal tract or even the teeth (barodontalgia). Usually these are not severe enough to cause actual trauma but can result in soreness in the ear that persists after the flight and can exacerbate or precipitate pre-existing medical conditions, such as pneumothorax.


== Cabin altitude ==

The pressure inside the cabin is technically referred to as the equivalent effective cabin altitude or more commonly as the cabin altitude. This is defined as the equivalent altitude above mean sea level having the same atmospheric pressure according to a standard atmospheric model such as the International Standard Atmosphere. Thus a cabin altitude of zero would have the pressure found at mean sea level, which is taken to be 101.325 kilopascals (14.696 psi).


=== Aircraft ===
In airliners, cabin altitude during flight is kept above sea level in order to reduce stress on the pressurized part of the fuselage; this stress is proportional to the difference in pressure inside and outside the cabin. In a typical commercial passenger flight, the cabin altitude is programmed to rise gradually from the altitude of the airport of origin to a regulatory maximum of 8,000 ft (2,400 m). This cabin altitude is maintained while the aircraft is cruising at its maximum altitude and then reduced gradually during descent until the cabin pressure matches the ambient air pressure at the destination.Keeping the cabin altitude below 8,000 ft (2,400 m) generally prevents significant hypoxia, altitude sickness, decompression sickness, and barotrauma. Federal Aviation Administration (FAA) regulations in the U.S. mandate that under normal operating conditions, the cabin altitude may not exceed this limit at the maximum operating altitude of the aircraft. This mandatory maximum cabin altitude does not eliminate all physiological problems; passengers with conditions such as pneumothorax are advised not to fly until fully healed, and people suffering from a cold or other infection may still experience pain in the ears and sinuses. The rate of change of cabin altitude strongly affects comfort as humans are sensitive to pressure changes in the inner ear and sinuses and this has to be managed carefully. Scuba divers flying within the ""no fly"" period after a dive are at risk of decompression sickness because the accumulated nitrogen in their bodies can form bubbles when exposed to reduced cabin pressure.
The cabin altitude of the Boeing 767 is typically about 7,000 feet (2,100 m) when cruising at 37,000 feet (11,000 m). This is typical for older jet airliners. A design goal for many, but not all, newer aircraft is to provide a lower cabin altitude than older designs. This can be beneficial for passenger comfort. For example, the Bombardier Global Express business jet can provide a cabin altitude of 4,500 ft (1,400 m) when cruising at 41,000 feet (12,000 m). The Emivest SJ30 business jet can provide a sea-level cabin altitude when cruising at 41,000 feet (12,000 m). One study of eight flights in Airbus A380 aircraft found a median cabin pressure altitude of 6,128 feet (1,868 m), and 65 flights in Boeing 747-400 aircraft found a median cabin pressure altitude of 5,159 feet (1,572 m).Before 1996, approximately 6,000 large commercial transport airplanes were assigned a type certificate to fly up to 45,000 ft (14,000 m) without having to meet high-altitude special conditions. In 1996, the FAA adopted Amendment 25-87, which imposed additional high-altitude cabin pressure specifications for new-type aircraft designs. Aircraft certified to operate above 25,000 ft (7,600 m) ""must be designed so that occupants will not be exposed to cabin pressure altitudes in excess of 15,000 ft (4,600 m) after any probable failure condition in the pressurization system"". In the event of a decompression that results from ""any failure condition not shown to be extremely improbable"", the plane must be designed such that occupants will not be exposed to a cabin altitude exceeding 25,000 ft (7,600 m) for more than 2 minutes, nor to an altitude exceeding 40,000 ft (12,000 m) at any time. In practice, that new Federal Aviation Regulations amendment imposes an operational ceiling of 40,000 ft (12,000 m) on the majority of newly designed commercial aircraft. Aircraft manufacturers can apply for a relaxation of this rule if the circumstances warrant it. In 2004, Airbus acquired an FAA exemption to allow the cabin altitude of the A380 to reach 43,000 ft (13,000 m) in the event of a decompression incident and to exceed 40,000 ft (12,000 m) for one minute. This allows the A380 to operate at a higher altitude than other newly designed civilian aircraft.


=== Spacecraft ===
Russian engineers used an air-like nitrogen/oxygen mixture, kept at a cabin altitude near zero at all times, in their 1961 Vostok, 1964 Voskhod, and 1967 to present Soyuz spacecraft. This requires a heavier space vehicle design, because the spacecraft cabin structure must withstand the stress of 14.7 pounds per square inch (1 bar) against the vacuum of space, and also because an inert nitrogen mass must be carried. Care must also be taken to avoid decompression sickness when cosmonauts perform extravehicular activity, as current soft space suits are pressurized with pure oxygen at relatively low pressure in order to provide reasonable flexibility.By contrast, the United States used a pure oxygen atmosphere for its 1961 Mercury, 1965 Gemini, and 1967 Apollo spacecraft, mainly in order to avoid decompression sickness. Mercury used a cabin altitude of 24,800 feet (7,600 m) (5.5 pounds per square inch (0.38 bar)); Gemini used an altitude of 25,700 feet (7,800 m) (5.3 psi (0.37 bar)); and Apollo used 27,000 feet (8,200 m) (5.0 psi (0.34 bar)) in space. This allowed for a lighter space vehicle design. This is possible because at 100% oxygen, enough oxygen gets to the bloodstream to allow astronauts to operate normally. Before launch, the pressure was kept at slightly higher than sea level at a constant 5.3 psi (0.37 bar) above ambient for Gemini, and 2 psi (0.14 bar) above sea level at launch for Apollo), and transitioned to the space cabin altitude during ascent. However, the high pressure pure oxygen atmosphere proved to be a fatal fire hazard in Apollo, contributing to the deaths of the entire crew of Apollo 1 during a 1967 ground test. After this, NASA revised its procedure to use a nitrogen/oxygen mix at zero cabin altitude at launch, but kept the low-pressure pure oxygen atmosphere at 5 psi (0.34 bar) in space.After the Apollo program, the United States used standard air-like cabin atmospheres for Skylab, the Space Shuttle orbiter, and the International Space Station.


== Mechanics ==
Pressurization is achieved by the design of an airtight fuselage engineered to be pressurized with a source of compressed air and controlled by an environmental control system (ECS). The most common source of compressed air for pressurization is bleed air extracted from the compressor stage of a gas turbine engine, from a low or intermediate stage and also from an additional high stage; the exact stage can vary depending on engine type. By the time the cold outside air has reached the bleed air valves, it is at a very high pressure and has been heated to around 200 °C (392 °F). The control and selection of high or low bleed sources is fully automatic and is governed by the needs of various pneumatic systems at various stages of flight.The part of the bleed air that is directed to the ECS is then expanded to bring it to cabin pressure, which cools it. A final, suitable temperature is then achieved by adding back heat from the hot compressed air via a heat exchanger and air cycle machine known as a PAC (Pressurization and Air Conditioning) system. In some larger airliners, hot trim air can be added downstream of air conditioned air coming from the packs if it is needed to warm a section of the cabin that is colder than others.

At least two engines provide compressed bleed air for all the plane's pneumatic systems, to provide full redundancy. Compressed air is also obtained from the auxiliary power unit (APU), if fitted, in the event of an emergency and for cabin air supply on the ground before the main engines are started. Most modern commercial aircraft today have fully redundant, duplicated electronic controllers for maintaining pressurization along with a manual back-up control system.
All exhaust air is dumped to atmosphere via an outflow valve, usually at the rear of the fuselage. This valve controls the cabin pressure and also acts as a safety relief valve, in addition to other safety relief valves. If the automatic pressure controllers fail, the pilot can manually control the cabin pressure valve, according to the backup emergency procedure checklist. The automatic controller normally maintains the proper cabin pressure altitude by constantly adjusting the outflow valve position so that the cabin altitude is as low as practical without exceeding the maximum pressure differential limit on the fuselage. The pressure differential varies between aircraft types, typical values are between 540 hPa (7.8 psi) and 650 hPa (9.4 psi). At 39,000 feet (12,000 m), the cabin pressure would be automatically maintained at about 6,900 feet (2,100 m) (450 feet (140 m) lower than Mexico City), which is about 790 hPa (11.5 psi) of atmosphere pressure.Some aircraft, such as the Boeing 787 Dreamliner, have re-introduced electric compressors previously used on piston-engined airliners to provide pressurization. The use of electric compressors increases the electrical generation load on the engines and introduces a number of stages of energy transfer; therefore, it is unclear whether this increases the overall efficiency of the aircraft air handling system. It does, however, remove the danger of chemical contamination of the cabin, simplify engine design, avert the need to run high pressure pipework around the aircraft, and provide greater design flexibility.


== Unplanned decompression ==

Unplanned loss of cabin pressure at altitude/in space is rare but has resulted in a number of fatal accidents. Failures range from sudden, catastrophic loss of airframe integrity (explosive decompression) to slow leaks or equipment malfunctions that allow cabin pressure to drop.
Any failure of cabin pressurization above 10,000 feet (3,000 m) requires an emergency descent to 8,000 feet (2,400 m) or the closest to that while maintaining the Minimum Sector  Altitude (MSA), and the deployment of an oxygen mask for each seat. The oxygen systems have sufficient oxygen for all on board and give the pilots adequate time to descend to below 8,000 ft (2,400 m). Without emergency oxygen, hypoxia may lead to loss of consciousness and a subsequent loss of control of the aircraft. Modern airliners include a pressurized pure oxygen tank in the cockpit, giving the pilots more time to bring the aircraft to a safe altitude. The time of useful consciousness varies according to altitude. As the pressure falls the cabin air temperature may also plummet to the ambient outside temperature with a danger of hypothermia or frostbite.
For airliners that need to fly over terrain that does not allow reaching the safe altitude within a minimum of 30 minutes, pressurized oxygen bottles are mandatory since the chemical oxygen generators fitted to most planes cannot supply sufficient oxygen.
In jet fighter aircraft, the small size of the cockpit means that any decompression will be very rapid and would not allow the pilot time to put on an oxygen mask. Therefore, fighter jet pilots and aircrew are required to wear oxygen masks at all times.On June 30, 1971, the crew of Soyuz 11, Soviet cosmonauts Georgy Dobrovolsky, Vladislav Volkov, and Viktor Patsayev were killed after the cabin vent valve accidentally opened before atmospheric re-entry.


== History ==
The aircraft that pioneered pressurized cabin systems include:

Packard-Le Père LUSAC-11, (1920, a modified French design, not actually pressurized but with an enclosed, oxygen enriched cockpit)
Engineering Division USD-9A, a modified Airco DH.9A (1921 – the first aircraft to fly with the addition of a pressurized cockpit module)
Junkers Ju 49 (1931 – a German experimental aircraft purpose-built to test the concept of cabin pressurization)
Farman F.1000 (1932 – a French record breaking pressurized cockpit, experimental aircraft)
Chizhevski BOK-1 (1936 – a Russian experimental aircraft)
Lockheed XC-35 (1937 – an American pressurized aircraft. Rather than a pressure capsule enclosing the cockpit, the monocoque fuselage skin was the pressure vessel.)
Renard R.35 (1938 – the first pressurized piston airliner, which crashed on first flight)
Boeing 307 (1938 – the first pressurized airliner to enter commercial service)
Lockheed Constellation (1943 – the first pressurized airliner in wide service)
Avro Tudor (1946 – first British pressurized airliner)
de Havilland Comet (British, Comet 1 1949 – the first jetliner, Comet 4 1958 – resolving the Comet 1 problems)
Tupolev Tu-144 and Concorde (1968 USSR and 1969 Anglo-French respectively – first to operate at very high altitude)
SyberJet SJ30 (2005) First civilian business jet to certify 12.0 psi pressurization system allowing for a sea level cabin at 41,000 ft (12,000 m).In the late 1910s, attempts were being made to achieve higher and higher altitudes. In 1920, flights well over 37,000 ft (11,000 m) were first achieved by test pilot Lt. John A. Macready in a Packard-Le Père LUSAC-11 biplane at McCook Field in Dayton, Ohio. The flight was possible by releasing stored oxygen into the cockpit, which was released directly into an enclosed cabin and not to an oxygen mask, which was developed later. With this system flights nearing 40,000 ft (12,000 m) were possible, but the lack of atmospheric pressure at that altitude caused the pilot's heart to enlarge visibly, and many pilots reported health problems from such high altitude flights. Some early airliners had oxygen masks for the passengers for routine flights.
In 1921, a Wright-Dayton USD-9A reconnaissance biplane was modified with the addition of a completely enclosed air-tight chamber that could be pressurized with air forced into it by small external turbines. The chamber had a hatch only 22 in (0.56 m) in diameter that would be sealed by the pilot at 3,000 ft (910 m). The chamber contained only one instrument, an altimeter, while the conventional cockpit instruments were all mounted outside the chamber, visible through five small portholes. The first attempt to operate the aircraft was again made by Lt. John A. McCready, who discovered that the turbine was forcing air into the chamber faster than the small release valve provided could release it. As a result, the chamber quickly over pressurized, and the flight was abandoned. A second attempt had to be abandoned when the pilot discovered at 3,000 ft (910 m) that he was too short to close the chamber hatch. The first successful flight was finally made by test pilot Lt. Harrold Harris, making it the world's first flight by a pressurized aircraft.The first airliner with a pressurized cabin was the Boeing 307 Stratoliner, built in 1938, prior to World War II, though only ten were produced. The 307's ""pressure compartment was from the nose of the aircraft to a pressure bulkhead in the aft just forward of the horizontal stabilizer.""

World War II was a catalyst for aircraft development. Initially, the piston aircraft of World War II, though they often flew at very high altitudes, were not pressurized and relied on oxygen masks. This became impractical with the development of larger bombers where crew were required to move about the cabin and this led to the first bomber with cabin pressurization (though restricted to crew areas), the Boeing B-29 Superfortress. The control system for this was designed by Garrett AiResearch Manufacturing Company, drawing in part on licensing of patents held by Boeing for the Stratoliner.Post-war piston airliners such as the Lockheed Constellation (1943) extended the technology to civilian service. The piston engined airliners generally relied on electrical compressors to provide pressurized cabin air. Engine supercharging and cabin pressurization enabled planes like the Douglas DC-6, the Douglas DC-7, and the Constellation to have certified service ceilings from 24,000 ft (7,300 m) to 28,400 ft (8,700 m). Designing a pressurized fuselage to cope with that altitude range was within the engineering and metallurgical knowledge of that time. The introduction of jet airliners required a significant increase in cruise altitudes to the 30,000–41,000 ft (9,100–12,500 m) range, where jet engines are more fuel efficient. That increase in cruise altitudes required far more rigorous engineering of the fuselage, and in the beginning not all the engineering problems were fully understood.
The world's first commercial jet airliner was the British de Havilland Comet (1949) designed with a service ceiling of 36,000 ft (11,000 m). It was the first time that a large diameter, pressurized fuselage with windows had been built and flown at this altitude. Initially, the design was very successful but two catastrophic airframe failures in 1954 resulting in the total loss of the aircraft, passengers and crew grounded what was then the entire world jet airliner fleet. Extensive investigation and groundbreaking engineering analysis of the wreckage led to a number of very significant engineering advances that solved the basic problems of pressurized fuselage design at altitude. The critical problem proved to be a combination of an inadequate understanding of the effect of progressive metal fatigue as the fuselage undergoes repeated stress cycles coupled with a misunderstanding of how aircraft skin stresses are redistributed around openings in the fuselage such as windows and rivet holes.
The critical engineering principles concerning metal fatigue learned from the Comet 1 program were applied directly to the design of the Boeing 707 (1957) and all subsequent jet airliners. For example, detailed routine inspection processes were introduced, in addition to thorough visual inspections of the outer skin, mandatory structural sampling was routinely conducted by operators; the need to inspect areas not easily viewable by the naked eye led to the introduction of widespread radiography examination in aviation; this also had the advantage of detecting cracks and flaws too small to be seen otherwise. Another visibly noticeable legacy of the Comet disasters is the oval windows on every jet airliner; the metal fatigue cracks that destroyed the Comets were initiated by the small radius corners on the Comet 1's almost square windows. The Comet fuselage was redesigned and the Comet 4 (1958) went on to become a successful airliner, pioneering the first transatlantic jet service, but the program never really recovered from these disasters and was overtaken by the Boeing 707.Even following the Comet disasters, there were several subsequent catastrophic fatigue failures attributed to cabin pressurisation. Perhaps the most prominent example was Aloha Airlines Flight 243, involving a Boeing 737-200. In this case, the principal cause was the continued operation of the specific aircraft despite having accumulated 35,496 flight hours prior to the accident, those hours included over 89,680 flight cycles (takeoffs and landings), owing to its use on short flights; this amounted to more than twice the number of flight cycles that the airframe was designed to endure. Aloha 243 was able to land despite the substantial damage inflicted by the decompression, which had resulted in the loss of one member of the cabin crew; the incident had far-reaching effects on aviation safety policies and led to changes in operating procedures.The supersonic airliner Concorde had to deal with particularly high pressure differentials because it flew at unusually high altitude (up to 60,000 feet (18,000 m)) and maintained a cabin altitude of 6,000 ft (1,800 m). Despite this, its cabin altitude was intentionally maintained at 6,000 feet (1,800 m). This combination, while providing for increasing comfort, necessitated making Concorde a significantly heavier aircraft, which in turn contributed to the relatively high cost of a flight. Unusually, Concorde was provisioned with smaller cabin windows than most other commercial passenger aircraft in order to slow the rate of decompression in the event of a window seal failing. The high cruising altitude also required the use of high pressure oxygen and demand valves at the emergency masks unlike the continuous-flow masks used in conventional airliners. The FAA, which enforces minimum emergency descent rates for aircraft, determined that, in relation to Concorde's higher operating altitude, the best response to a pressure loss incident would be to perform a rapid descent.The designed operating cabin altitude for new aircraft is falling and this is expected to reduce any remaining physiological problems. Both the Boeing 787 Dreamliner and the Airbus A350 XWB airliners have made such modifications for increased passenger comfort. The 787's internal cabin pressure is the equivalent of 6,000 feet (1,800 m) altitude resulting in a higher pressure than for the 8,000 feet (2,400 m) altitude of older conventional aircraft; according to a joint study performed by Boeing and Oklahoma State University, such a level significantly improves comfort levels. Airbus has stated that the A350 XWB provides for a typical cabin altitude at or below 6,000 ft (1,800 m), along with a cabin atmosphere of 20% humidity and an airflow management system that adapts cabin airflow to passenger load with draught-free air circulation. The adoption of composite fuselages eliminates the threat posed by metal fatigue that would have been exacerbated by the higher cabin pressures being adopted by modern airliners, it also eliminates the risk of corrosion from the use of greater humidity levels.


== See also ==
Aerotoxic syndrome
Air cycle machine
Atmosphere (unit)
Compressed air
Fume event
Rarefaction
Space suit
Time of useful consciousness


== Footnotes ==


== General references ==
Seymour L. Chapin (August 1966). ""Garrett and Pressurized Flight: A Business Built on Thin Air"". Pacific Historical Review. 35 (3): 329–43. doi:10.2307/3636792. JSTOR 3636792.
Seymour L. Chapin (July 1971). ""Patent Interferences and the History of Technology: A High-flying Example"". Technology and Culture. 12 (3): 414–46. doi:10.2307/3102997. JSTOR 3102997.
Cornelisse, Diana G. Splendid Vision, Unswerving Purpose; Developing Air Power for the United States Air Force During the First Century of Powered Flight. Wright-Patterson Air Force Base, Ohio: U.S. Air Force Publications, 2002. ISBN 0-16-067599-5. pp. 128–29.
Portions from the United States Naval Flight Surgeon's Manual
""121 Dead in Greek Air Crash"", CNN


== External links ==
Video with Cabin Pressurization Demo in Civil Aircraft on YouTube","pandas(index=27, _1=27, text='cabin pressurization is a process in which conditioned air is pumped into the cabin of an aircraft or spacecraft in order to create a safe and comfortable environment for passengers and crew flying at high altitudes. for aircraft, this air is usually bled off from the gas turbine engines at the compressor stage, and for spacecraft, it is carried in high-pressure, often cryogenic tanks. the air is cooled, humidified, and mixed with recirculated air if necessary before it is distributed to the cabin by one or more environmental control systems. the cabin pressure is regulated by the outflow valve. while the first experimental pressurization systems saw use during the 1920s and 1930s, it was not until 1938 that the boeing 307 stratoliner, the first commercial aircraft to be equipped with a pressurized cabin, was introduced. the practice would become widespread a decade later, particularly with the introduction of the british de havilland comet in 1949, the world\'s first jetliner. while initially a success, two catastrophic failures in 1954 temporarily grounded the worldwide fleet; the cause was found to be a combination of progressive metal fatigue and aircraft skin stresses, both of which aeronautical engineers only had a limited understanding of at the time. the key engineering principles learned from the comet were applied directly to the design of all subsequent jet airliners, such as the boeing 707. certain aircraft have presented unusual pressurization scenarios. the supersonic airliner concorde had a particularly high pressure differential due to flying at unusually high altitude (up to 60,000 feet (18,000 m) while maintaining a cabin altitude of 6,000 feet (1,800 m). this not only increased airframe weight, but also saw the use of smaller cabin windows than most other commercial passenger aircraft, intended to slow the decompression rate if a depressurization event occurred. the aloha airlines flight 243 incident, involving a boeing 737-200 that suffered catastrophic cabin failure mid-flight, was primarily caused by its continued operation despite having accumulated more than twice the number of flight cycles that the airframe was designed to endure. for increased passenger comfort, several modern airliners, such as the boeing 787 dreamliner and the airbus a350 xwb, feature reduced operating cabin altitudes as well as greater humidity levels; the use of composite airframes has aided the adoption of such comfort-maximising practices.   == need for cabin pressurization ==  pressurization becomes increasingly necessary at altitudes above 10,000 feet (3,000 m) above sea level to protect crew and passengers from the risk of a number of physiological problems caused by the low outside air pressure above that altitude. for private aircraft operating in the us, crew members are required to use oxygen masks if the cabin altitude (a representation of the air pressure, see below) stays above 12,500 ft for more than 30 minutes, or if the cabin altitude reaches 14,000 ft at any time. at altitudes above 15,000 ft, passengers are required to be provided oxygen masks as well. on commercial aircraft, the cabin altitude must be maintained at 8,000 feet (2,400 m) or less. pressurization of the cargo hold is also required to prevent damage to pressure-sensitive goods that might leak, expand, burst or be crushed on re-pressurization. the principal physiological problems are listed below.  hypoxia the lower partial pressure of oxygen at high altitude reduces the alveolar oxygen tension in the lungs and subsequently in the brain, leading to sluggish thinking, dimmed vision, loss of consciousness, and ultimately death. in some individuals, particularly those with heart or lung disease, symptoms may begin as low as 5,000 feet (1,500 m), although most passengers can tolerate altitudes of 8,000 feet (2,400 m) without ill effect. at this altitude, there is about 25% less oxygen than there is at sea level. hypoxia may be addressed by the administration of supplemental oxygen, either through an oxygen mask or through a nasal cannula. without pressurization, sufficient oxygen can be delivered up to an altitude of about 40,000 feet (12,000 m). this is because a person who is used to living at sea level needs about 0.20 bar partial oxygen pressure to function normally and that pressure can be maintained up to about 40,000 feet (12,000 m) by increasing the mole fraction of oxygen in the air that is being breathed. at 40,000 feet (12,000 m), the ambient air pressure falls to about 0.2 bar, at which maintaining a minimum partial pressure of oxygen of 0.2 bar requires breathing 100% oxygen using an oxygen mask. emergency oxygen supply masks in the passenger compartment of airliners do not need to be pressure-demand masks because most flights stay below 40,000 feet (12,000 m). above that altitude the partial pressure of oxygen will fall below 0.2 bar even at 100% oxygen and some degree of cabin pressurization or rapid descent will be essential to avoid the risk of hypoxia. altitude sickness hyperventilation, the body\'s most common response to hypoxia, does help to partially restore the partial pressure of oxygen in the blood, but it also causes carbon dioxide (co2) to out-gas, raising the blood ph and inducing alkalosis. passengers may experience fatigue, nausea, headaches, sleeplessness, and (on extended flights) even pulmonary oedema. these are the same symptoms that mountain climbers experience, but the limited duration of powered flight makes the development of pulmonary oedema unlikely. altitude sickness may be controlled by a full pressure suit with helmet and faceplate, which completely envelops the body in a pressurized environment; however, this is impractical for commercial passengers. decompression sickness the low partial pressure of gases, principally nitrogen (n2) but including all other gases, may cause dissolved gases in the bloodstream to precipitate out, resulting in gas embolism, or bubbles in the bloodstream. the mechanism is the same as that of compressed-air divers on ascent from depth. symptoms may include the early symptoms of ""the bends""—tiredness, forgetfulness, headache, stroke, thrombosis, and subcutaneous itching—but rarely the full symptoms thereof. decompression sickness may also be controlled by a full-pressure suit as for altitude sickness. barotrauma as the aircraft climbs or descends, passengers may experience discomfort or acute pain as gases trapped within their bodies expand or contract. the most common problems occur with air trapped in the middle ear (aerotitis) or paranasal sinuses by a blocked eustachian tube or sinuses. pain may also be experienced in the gastrointestinal tract or even the teeth (barodontalgia). usually these are not severe enough to cause actual trauma but can result in soreness in the ear that persists after the flight and can exacerbate or precipitate pre-existing medical conditions, such as pneumothorax.   == cabin altitude ==  the pressure inside the cabin is technically referred to as the equivalent effective cabin altitude or more commonly as the cabin altitude. this is defined as the equivalent altitude above mean sea level having the same atmospheric pressure according to a standard atmospheric model such as the international standard atmosphere. thus a cabin altitude of zero would have the pressure found at mean sea level, which is taken to be 101.325 kilopascals (14.696 psi). russian engineers used an air-like nitrogen/oxygen mixture, kept at a cabin altitude near zero at all times, in their 1961 vostok, 1964 voskhod, and 1967 to present soyuz spacecraft. this requires a heavier space vehicle design, because the spacecraft cabin structure must withstand the stress of 14.7 pounds per square inch (1 bar) against the vacuum of space, and also because an inert nitrogen mass must be carried. care must also be taken to avoid decompression sickness when cosmonauts perform extravehicular activity, as current soft space suits are pressurized with pure oxygen at relatively low pressure in order to provide reasonable flexibility.by contrast, the united states used a pure oxygen atmosphere for its 1961 mercury, 1965 gemini, and 1967 apollo spacecraft, mainly in order to avoid decompression sickness. mercury used a cabin altitude of 24,800 feet (7,600 m) (5.5 pounds per square inch (0.38 bar)); gemini used an altitude of 25,700 feet (7,800 m) (5.3 psi (0.37 bar)); and apollo used 27,000 feet (8,200 m) (5.0 psi (0.34 bar)) in space. this allowed for a lighter space vehicle design. this is possible because at 100% oxygen, enough oxygen gets to the bloodstream to allow astronauts to operate normally. before launch, the pressure was kept at slightly higher than sea level at a constant 5.3 psi (0.37 bar) above ambient for gemini, and 2 psi (0.14 bar) above sea level at launch for apollo), and transitioned to the space cabin altitude during ascent. however, the high pressure pure oxygen atmosphere proved to be a fatal fire hazard in apollo, contributing to the deaths of the entire crew of apollo 1 during a 1967 ground test. after this, nasa revised its procedure to use a nitrogen/oxygen mix at zero cabin altitude at launch, but kept the low-pressure pure oxygen atmosphere at 5 psi (0.34 bar) in space.after the apollo program, the united states used standard air-like cabin atmospheres for skylab, the space shuttle orbiter, and the international space station.   == mechanics == pressurization is achieved by the design of an airtight fuselage engineered to be pressurized with a source of compressed air and controlled by an environmental control system (ecs). the most common source of compressed air for pressurization is bleed air extracted from the compressor stage of a gas turbine engine, from a low or intermediate stage and also from an additional high stage; the exact stage can vary depending on engine type. by the time the cold outside air has reached the bleed air valves, it is at a very high pressure and has been heated to around 200 °c (392 °f). the control and selection of high or low bleed sources is fully automatic and is governed by the needs of various pneumatic systems at various stages of flight.the part of the bleed air that is directed to the ecs is then expanded to bring it to cabin pressure, which cools it. a final, suitable temperature is then achieved by adding back heat from the hot compressed air via a heat exchanger and air cycle machine known as a pac (pressurization and air conditioning) system. in some larger airliners, hot trim air can be added downstream of air conditioned air coming from the packs if it is needed to warm a section of the cabin that is colder than others.  at least two engines provide compressed bleed air for all the plane\'s pneumatic systems, to provide full redundancy. compressed air is also obtained from the auxiliary power unit (apu), if fitted, in the event of an emergency and for cabin air supply on the ground before the main engines are started. most modern commercial aircraft today have fully redundant, duplicated electronic controllers for maintaining pressurization along with a manual back-up control system. all exhaust air is dumped to atmosphere via an outflow valve, usually at the rear of the fuselage. this valve controls the cabin pressure and also acts as a safety relief valve, in addition to other safety relief valves. if the automatic pressure controllers fail, the pilot can manually control the cabin pressure valve, according to the backup emergency procedure checklist. the automatic controller normally maintains the proper cabin pressure altitude by constantly adjusting the outflow valve position so that the cabin altitude is as low as practical without exceeding the maximum pressure differential limit on the fuselage. the pressure differential varies between aircraft types, typical values are between 540 hpa (7.8 psi) and 650 hpa (9.4 psi). at 39,000 feet (12,000 m), the cabin pressure would be automatically maintained at about 6,900 feet (2,100 m) (450 feet (140 m) lower than mexico city), which is about 790 hpa (11.5 psi) of atmosphere pressure.some aircraft, such as the boeing 787 dreamliner, have re-introduced electric compressors previously used on piston-engined airliners to provide pressurization. the use of electric compressors increases the electrical generation load on the engines and introduces a number of stages of energy transfer; therefore, it is unclear whether this increases the overall efficiency of the aircraft air handling system. it does, however, remove the danger of chemical contamination of the cabin, simplify engine design, avert the need to run high pressure pipework around the aircraft, and provide greater design flexibility.   == unplanned decompression ==  unplanned loss of cabin pressure at altitude/in space is rare but has resulted in a number of fatal accidents. failures range from sudden, catastrophic loss of airframe integrity (explosive decompression) to slow leaks or equipment malfunctions that allow cabin pressure to drop. any failure of cabin pressurization above 10,000 feet (3,000 m) requires an emergency descent to 8,000 feet (2,400 m) or the closest to that while maintaining the minimum sector  altitude (msa), and the deployment of an oxygen mask for each seat. the oxygen systems have sufficient oxygen for all on board and give the pilots adequate time to descend to below 8,000 ft (2,400 m). without emergency oxygen, hypoxia may lead to loss of consciousness and a subsequent loss of control of the aircraft. modern airliners include a pressurized pure oxygen tank in the cockpit, giving the pilots more time to bring the aircraft to a safe altitude. the time of useful consciousness varies according to altitude. as the pressure falls the cabin air temperature may also plummet to the ambient outside temperature with a danger of hypothermia or frostbite. for airliners that need to fly over terrain that does not allow reaching the safe altitude within a minimum of 30 minutes, pressurized oxygen bottles are mandatory since the chemical oxygen generators fitted to most planes cannot supply sufficient oxygen. in jet fighter aircraft, the small size of the cockpit means that any decompression will be very rapid and would not allow the pilot time to put on an oxygen mask. therefore, fighter jet pilots and aircrew are required to wear oxygen masks at all times.on june 30, 1971, the crew of soyuz 11, soviet cosmonauts georgy dobrovolsky, vladislav volkov, and viktor patsayev were killed after the cabin vent valve accidentally opened before atmospheric re-entry.   == history == the aircraft that pioneered pressurized cabin systems include:  packard-le père lusac-11, (1920, a modified french design, not actually pressurized but with an enclosed, oxygen enriched cockpit) engineering division usd-9a, a modified airco dh.9a (1921 – the first aircraft to fly with the addition of a pressurized cockpit module) junkers ju 49 (1931 – a german experimental aircraft purpose-built to test the concept of cabin pressurization) farman f.1000 (1932 – a french record breaking pressurized cockpit, experimental aircraft) chizhevski bok-1 (1936 – a russian experimental aircraft) lockheed xc-35 (1937 – an american pressurized aircraft. rather than a pressure capsule enclosing the cockpit, the monocoque fuselage skin was the pressure vessel.) renard r.35 (1938 – the first pressurized piston airliner, which crashed on first flight) boeing 307 (1938 – the first pressurized airliner to enter commercial service) lockheed constellation (1943 – the first pressurized airliner in wide service) avro tudor (1946 – first british pressurized airliner) de havilland comet (british, comet 1 1949 – the first jetliner, comet 4 1958 – resolving the comet 1 problems) tupolev tu-144 and concorde (1968 ussr and 1969 anglo-french respectively – first to operate at very high altitude) syberjet sj30 (2005) first civilian business jet to certify 12.0 psi pressurization system allowing for a sea level cabin at 41,000 ft (12,000 m).in the late 1910s, attempts were being made to achieve higher and higher altitudes. in 1920, flights well over 37,000 ft (11,000 m) were first achieved by test pilot lt. john a. macready in a packard-le père lusac-11 biplane at mccook field in dayton, ohio. the flight was possible by releasing stored oxygen into the cockpit, which was released directly into an enclosed cabin and not to an oxygen mask, which was developed later. with this system flights nearing 40,000 ft (12,000 m) were possible, but the lack of atmospheric pressure at that altitude caused the pilot\'s heart to enlarge visibly, and many pilots reported health problems from such high altitude flights. some early airliners had oxygen masks for the passengers for routine flights. in 1921, a wright-dayton usd-9a reconnaissance biplane was modified with the addition of a completely enclosed air-tight chamber that could be pressurized with air forced into it by small external turbines. the chamber had a hatch only 22 in (0.56 m) in diameter that would be sealed by the pilot at 3,000 ft (910 m). the chamber contained only one instrument, an altimeter, while the conventional cockpit instruments were all mounted outside the chamber, visible through five small portholes. the first attempt to operate the aircraft was again made by lt. john a. mccready, who discovered that the turbine was forcing air into the chamber faster than the small release valve provided could release it. as a result, the chamber quickly over pressurized, and the flight was abandoned. a second attempt had to be abandoned when the pilot discovered at 3,000 ft (910 m) that he was too short to close the chamber hatch. the first successful flight was finally made by test pilot lt. harrold harris, making it the world\'s first flight by a pressurized aircraft.the first airliner with a pressurized cabin was the boeing 307 stratoliner, built in 1938, prior to world war ii, though only ten were produced. the 307\'s ""pressure compartment was from the nose of the aircraft to a pressure bulkhead in the aft just forward of the horizontal stabilizer.""  world war ii was a catalyst for aircraft development. initially, the piston aircraft of world war ii, though they often flew at very high altitudes, were not pressurized and relied on oxygen masks. this became impractical with the development of larger bombers where crew were required to move about the cabin and this led to the first bomber with cabin pressurization (though restricted to crew areas), the boeing b-29 superfortress. the control system for this was designed by garrett airesearch manufacturing company, drawing in part on licensing of patents held by boeing for the stratoliner.post-war piston airliners such as the lockheed constellation (1943) extended the technology to civilian service. the piston engined airliners generally relied on electrical compressors to provide pressurized cabin air. engine supercharging and cabin pressurization enabled planes like the douglas dc-6, the douglas dc-7, and the constellation to have certified service ceilings from 24,000 ft (7,300 m) to 28,400 ft (8,700 m). designing a pressurized fuselage to cope with that altitude range was within the engineering and metallurgical knowledge of that time. the introduction of jet airliners required a significant increase in cruise altitudes to the 30,000–41,000 ft (9,100–12,500 m) range, where jet engines are more fuel efficient. that increase in cruise altitudes required far more rigorous engineering of the fuselage, and in the beginning not all the engineering problems were fully understood. the world\'s first commercial jet airliner was the british de havilland comet (1949) designed with a service ceiling of 36,000 ft (11,000 m). it was the first time that a large diameter, pressurized fuselage with windows had been built and flown at this altitude. initially, the design was very successful but two catastrophic airframe failures in 1954 resulting in the total loss of the aircraft, passengers and crew grounded what was then the entire world jet airliner fleet. extensive investigation and groundbreaking engineering analysis of the wreckage led to a number of very significant engineering advances that solved the basic problems of pressurized fuselage design at altitude. the critical problem proved to be a combination of an inadequate understanding of the effect of progressive metal fatigue as the fuselage undergoes repeated stress cycles coupled with a misunderstanding of how aircraft skin stresses are redistributed around openings in the fuselage such as windows and rivet holes. the critical engineering principles concerning metal fatigue learned from the comet 1 program were applied directly to the design of the boeing 707 (1957) and all subsequent jet airliners. for example, detailed routine inspection processes were introduced, in addition to thorough visual inspections of the outer skin, mandatory structural sampling was routinely conducted by operators; the need to inspect areas not easily viewable by the naked eye led to the introduction of widespread radiography examination in aviation; this also had the advantage of detecting cracks and flaws too small to be seen otherwise. another visibly noticeable legacy of the comet disasters is the oval windows on every jet airliner; the metal fatigue cracks that destroyed the comets were initiated by the small radius corners on the comet 1\'s almost square windows. the comet fuselage was redesigned and the comet 4 (1958) went on to become a successful airliner, pioneering the first transatlantic jet service, but the program never really recovered from these disasters and was overtaken by the boeing 707.even following the comet disasters, there were several subsequent catastrophic fatigue failures attributed to cabin pressurisation. perhaps the most prominent example was aloha airlines flight 243, involving a boeing 737-200. in this case, the principal cause was the continued operation of the specific aircraft despite having accumulated 35,496 flight hours prior to the accident, those hours included over 89,680 flight cycles (takeoffs and landings), owing to its use on short flights; this amounted to more than twice the number of flight cycles that the airframe was designed to endure. aloha 243 was able to land despite the substantial damage inflicted by the decompression, which had resulted in the loss of one member of the cabin crew; the incident had far-reaching effects on aviation safety policies and led to changes in operating procedures.the supersonic airliner concorde had to deal with particularly high pressure differentials because it flew at unusually high altitude (up to 60,000 feet (18,000 m)) and maintained a cabin altitude of 6,000 ft (1,800 m). despite this, its cabin altitude was intentionally maintained at 6,000 feet (1,800 m). this combination, while providing for increasing comfort, necessitated making concorde a significantly heavier aircraft, which in turn contributed to the relatively high cost of a flight. unusually, concorde was provisioned with smaller cabin windows than most other commercial passenger aircraft in order to slow the rate of decompression in the event of a window seal failing. the high cruising altitude also required the use of high pressure oxygen and demand valves at the emergency masks unlike the continuous-flow masks used in conventional airliners. the faa, which enforces minimum emergency descent rates for aircraft, determined that, in relation to concorde\'s higher operating altitude, the best response to a pressure loss incident would be to perform a rapid descent.the designed operating cabin altitude for new aircraft is falling and this is expected to reduce any remaining physiological problems. both the boeing 787 dreamliner and the airbus a350 xwb airliners have made such modifications for increased passenger comfort. the 787\'s internal cabin pressure is the equivalent of 6,000 feet (1,800 m) altitude resulting in a higher pressure than for the 8,000 feet (2,400 m) altitude of older conventional aircraft; according to a joint study performed by boeing and oklahoma state university, such a level significantly improves comfort levels. airbus has stated that the a350 xwb provides for a typical cabin altitude at or below 6,000 ft (1,800 m), along with a cabin atmosphere of 20% humidity and an airflow management system that adapts cabin airflow to passenger load with draught-free air circulation. the adoption of composite fuselages eliminates the threat posed by metal fatigue that would have been exacerbated by the higher cabin pressures being adopted by modern airliners, it also eliminates the risk of corrosion from the use of greater humidity levels.   == see also == aerotoxic syndrome air cycle machine atmosphere (unit) compressed air fume event rarefaction space suit time of useful consciousness   == footnotes ==   == general references == seymour l. chapin (august 1966). ""garrett and pressurized flight: a business built on thin air"". pacific historical review. 35 (3): 329–43. doi:10.2307/3636792. jstor 3636792. seymour l. chapin (july 1971). ""patent interferences and the history of technology: a high-flying example"". technology and culture. 12 (3): 414–46. doi:10.2307/3102997. jstor 3102997. cornelisse, diana g. splendid vision, unswerving purpose; developing air power for the united states air force during the first century of powered flight. wright-patterson air force base, ohio: u.s. air force publications, 2002. isbn 0-16-067599-5. pp. 128–29. portions from the united states naval flight surgeon\'s manual ""121 dead in greek air crash"", cnn   == external links == video with cabin pressurization demo in civil aircraft on youtube')"
28,"""Shirt-sleeve environment"" is a term used in aircraft design to describe the interior of an aircraft in which no special clothing need be worn. Early aircraft had no internal pressurization, so the crews of those that reached the stratosphere had to be garbed to withstand the low temperature and pressure of the air outside. Respirator masks needed to cover the mouth and nose. Silk socks were worn to retain heat. Sometimes leather clothing, such as boots, were electrically heated. When jet fighter aircraft reached still higher altitudes, something similar to a space suit had to be worn, and pilots of the highest reconnaissance aircraft wore real space suits. 
Commercial jet airliners fly in the stratosphere, but because they are pressurized, they could be said to have a shirt-sleeve environment. Crews of the US Apollo spacecraft always began the flight phases of launch, docking, and re-entry in space suits, although they could remove them for many hours. The Soviets tried to perfect this to save weight. This worked well, until an accidental depressurization on entry resulted in the deaths of an entire Soyuz crew. Protocols were changed shortly thereafter to require at least partial spacesuits. Early Soyuz spacecraft had no provision for space suits in the re-entry module, although the orbital module was intended for use as an airlock. Thus these operated in a shirt-sleeve environment except for spacewalks.
This term is also used in science fiction to describe an alien planet with an atmosphere breathable by humans without special equipment.The Space Shuttle's Spacelab Habitable module was an area with expanded volume for astronauts to work in a shirt sleeve environment and had space for equipment racks and related support equipment for operations in Low Earth orbit.One of the goals for MOLAB rover was to achieve a shirt-sleeve environment (compared to a lunar rover which was open to space and required the use of space suits to operate). One of the considerations was the habitable volume that could be occupied.


== References ==","pandas(index=28, _1=28, text='""shirt-sleeve environment"" is a term used in aircraft design to describe the interior of an aircraft in which no special clothing need be worn. early aircraft had no internal pressurization, so the crews of those that reached the stratosphere had to be garbed to withstand the low temperature and pressure of the air outside. respirator masks needed to cover the mouth and nose. silk socks were worn to retain heat. sometimes leather clothing, such as boots, were electrically heated. when jet fighter aircraft reached still higher altitudes, something similar to a space suit had to be worn, and pilots of the highest reconnaissance aircraft wore real space suits. commercial jet airliners fly in the stratosphere, but because they are pressurized, they could be said to have a shirt-sleeve environment. crews of the us apollo spacecraft always began the flight phases of launch, docking, and re-entry in space suits, although they could remove them for many hours. the soviets tried to perfect this to save weight. this worked well, until an accidental depressurization on entry resulted in the deaths of an entire soyuz crew. protocols were changed shortly thereafter to require at least partial spacesuits. early soyuz spacecraft had no provision for space suits in the re-entry module, although the orbital module was intended for use as an airlock. thus these operated in a shirt-sleeve environment except for spacewalks. this term is also used in science fiction to describe an alien planet with an atmosphere breathable by humans without special equipment.the space shuttle\'s spacelab habitable module was an area with expanded volume for astronauts to work in a shirt sleeve environment and had space for equipment racks and related support equipment for operations in low earth orbit.one of the goals for molab rover was to achieve a shirt-sleeve environment (compared to a lunar rover which was open to space and required the use of space suits to operate). one of the considerations was the habitable volume that could be occupied.   == references ==')"
29,"NOTAR (no tail rotor) is a helicopter  system which avoids the use of a tail rotor. It was developed by McDonnell Douglas Helicopter Systems (through their acquisition of Hughes Helicopters). The system uses a fan inside the tail boom to build a high volume of low-pressure air, which exits through two slots and creates a boundary layer flow of air along the tailboom utilizing the Coandă effect. The boundary layer changes the direction of airflow around the tailboom, creating thrust opposite the motion imparted to the fuselage by the torque effect of the main rotor. Directional yaw control is gained through a vented, rotating drum at the end of the tailboom, called the direct jet thruster. Advocates of NOTAR believe the system offers quieter and safer operation over a traditional tail rotor.


== Development ==

The use of directed air to provide anti-torque control had been tested as early as 1945 in the British Cierva W.9. During 1957, a Spanish prototype designed and built by Aerotecnica flew using exhaust gases from the turbine instead of a tail rotor. This model was designated as Aerotecnica AC-14. 
Development of the NOTAR system dates back to 1975, when engineers at Hughes Helicopters began concept development work. On December 17 1981, Hughes flew an OH-6A fitted with NOTAR for the first time. The OH-6A helicopter (serial number 65-12917) was supplied by the U.S. Army for Hughes to develop the NOTAR technology and was the second OH-6 built by Hughes for the U.S. Army.  A more heavily modified version of the prototype demonstrator first flew in March 1986 (by which time McDonnell Douglas had acquired Hughes Helicopters).   The original prototype last flew in June 1986 and is now at the U.S. Army Aviation Museum in Fort Rucker, Alabama.
A production model NOTAR 520N (N520NT) was later produced and first flew on May 1, 1990.  It crashed on September 27, 1994 when it collided with an AH-64D while flying as a chase aircraft for the Apache.


== Concept ==
Although the concept took over three years to refine, the NOTAR system is simple in theory and works to provide some directional control using the Coandă effect. A variable pitch fan is enclosed in the aft fuselage section immediately forward of the tail boom and driven by the main rotor transmission. This fan forces low pressure air through two slots on the right side of the tailboom, causing the downwash from the main rotor to hug the tailboom, producing lift, and thus a measure of directional control. This is augmented by a direct jet thruster and vertical stabilisers.
Benefits of the NOTAR system include increased safety (the tail rotor being vulnerable), and greatly reduced external noise as tail rotors on helicopters produce a lot of noise. NOTAR-equipped helicopters are among the quietest certified helicopters.

		
		


== Applications ==

There are several production helicopters that utilize the NOTAR system, which are produced by MD Helicopters:

MD 520N: a NOTAR variant of the Hughes/MD500 series helicopter.
MD 600N: a larger version of the MD 520N.
MD Explorer: a twin-engine, eight-seat light helicopter.and other designs:

Youngcopter Neo


== See also ==
Cierva W.9
Fenestron
Tip jet rotor
Coaxial rotors
Tandem rotors
Synchropter


== Notes ==


== References ==","pandas(index=29, _1=29, text='notar (no tail rotor) is a helicopter  system which avoids the use of a tail rotor. it was developed by mcdonnell douglas helicopter systems (through their acquisition of hughes helicopters). the system uses a fan inside the tail boom to build a high volume of low-pressure air, which exits through two slots and creates a boundary layer flow of air along the tailboom utilizing the coandă effect. the boundary layer changes the direction of airflow around the tailboom, creating thrust opposite the motion imparted to the fuselage by the torque effect of the main rotor. directional yaw control is gained through a vented, rotating drum at the end of the tailboom, called the direct jet thruster. advocates of notar believe the system offers quieter and safer operation over a traditional tail rotor.   == development ==  the use of directed air to provide anti-torque control had been tested as early as 1945 in the british cierva w.9. during 1957, a spanish prototype designed and built by aerotecnica flew using exhaust gases from the turbine instead of a tail rotor. this model was designated as aerotecnica ac-14. development of the notar system dates back to 1975, when engineers at hughes helicopters began concept development work. on december 17 1981, hughes flew an oh-6a fitted with notar for the first time. the oh-6a helicopter (serial number 65-12917) was supplied by the u.s. army for hughes to develop the notar technology and was the second oh-6 built by hughes for the u.s. army.  a more heavily modified version of the prototype demonstrator first flew in march 1986 (by which time mcdonnell douglas had acquired hughes helicopters).   the original prototype last flew in june 1986 and is now at the u.s. army aviation museum in fort rucker, alabama. a production model notar 520n (n520nt) was later produced and first flew on may 1, 1990.  it crashed on september 27, 1994 when it collided with an ah-64d while flying as a chase aircraft for the apache.   == concept == although the concept took over three years to refine, the notar system is simple in theory and works to provide some directional control using the coandă effect. a variable pitch fan is enclosed in the aft fuselage section immediately forward of the tail boom and driven by the main rotor transmission. this fan forces low pressure air through two slots on the right side of the tailboom, causing the downwash from the main rotor to hug the tailboom, producing lift, and thus a measure of directional control. this is augmented by a direct jet thruster and vertical stabilisers. benefits of the notar system include increased safety (the tail rotor being vulnerable), and greatly reduced external noise as tail rotors on helicopters produce a lot of noise. notar-equipped helicopters are among the quietest certified helicopters.          == applications ==  there are several production helicopters that utilize the notar system, which are produced by md helicopters:  md 520n: a notar variant of the hughes/md500 series helicopter. md 600n: a larger version of the md 520n. md explorer: a twin-engine, eight-seat light helicopter.and other designs:  youngcopter neo   == see also == cierva w.9 fenestron tip jet rotor coaxial rotors tandem rotors synchropter   == notes ==   == references ==')"
30,"Space environment is a branch of astronautics, aerospace engineering and space physics that seeks to understand and address conditions existing in space that affect the design and operation of spacecraft. A related subject, space weather, deals with dynamic processes in the solar-terrestrial system that can give rise to effects on spacecraft, but that can also affect the atmosphere, ionosphere and geomagnetic field, giving rise to several other kinds of effects on human technologies.
Effects on spacecraft can arise from radiation, space debris and meteoroid impact, upper atmospheric drag and spacecraft electrostatic charging.
Radiation in space usually comes from three main sources:

The Van Allen radiation belts
Solar proton events and solar energetic particles; and
Galactic cosmic rays.For long-duration missions, the high doses of radiation can damage electronic components and solar cells. A major concern is also radiation-induced ""single-event effects"" such as single event upset. Crewed missions usually avoid the radiation belts and the International Space Station is at an altitude well below the most severe regions of the radiation belts. During solar energetic events (solar flares and coronal mass ejections) particles can be accelerated to very high energies and can reach the Earth in times as short as 30 minutes (but usually take some hours). These particles are mainly protons and heavier ions that can cause radiation damage, disruption to logic circuits, and even hazards to astronauts. Crewed missions to return to the Moon or to travel to Mars will have to deal with the major problems presented by solar particle events to radiation safety, in addition to the important contribution to doses from the low-level background cosmic rays. In near-Earth orbits, the Earth's geomagnetic field screens spacecraft from a large part of these hazards - a process called geomagnetic shielding.
Space debris and meteoroids can impact spacecraft at high speeds, causing mechanical or electrical damage.  The average speed of space debris is 10 km/s (22,000 mph; 36,000 km/h) while the average speed of meteoroids is much greater.  For example, the meteoroids associated with the Perseid meteor shower travel at an average speed of 58 km/s (130,000 mph; 210,000 km/h).  Mechanical damage from debris impacts have been studied through space missions including LDEF, which had over 20,000 documented impacts through its 5.7-year mission.  Electrical anomalies associated with impact events include ESA's Olympus spacecraft, which lost attitude control during the 1993 Perseid meteor shower.  A similar event occurred with the Landsat 5 spacecraft during the 2009 Perseid meteor shower.Spacecraft electrostatic charging is caused by the hot plasma environment around the Earth. The plasma encountered in the region of the geostationary orbit becomes heated during geomagnetic substorms caused by disturbances in the solar wind. ""Hot"" electrons (with energies in the kilo-electron volt range) collect on surfaces of spacecraft and can establish electrostatic potentials of the order of kilovolts. As a result, discharges can occur and are known to be the source of many spacecraft anomalies.
Solutions devised by scientists and engineers include, but are not limited to, spacecraft shielding, special ""hardening"" of electronic systems, various collision detection systems. Evaluation of effects during spacecraft design includes application of various models of the environment, including radiation belt models, spacecraft-plasma interaction models and atmospheric models to predict drag effects encountered in lower orbits and during reentry.
The field often overlaps with the disciplines of astrophysics, atmospheric science, space physics, and geophysics, albeit usually with an emphasis on application.
The United States government maintains a Space Weather Prediction Center at Boulder, Colorado.  The Space Weather Prediction Center (SWPC) is part of the National Oceanic and Atmospheric Administration (NOAA).  SWPC is one of the National Weather Service's (NWS) National Centers for Environmental Prediction (NCEP).
Space weather effects on Earth can include ionospheric storms, temporary decreases in ozone densities, disruption to radio communication, to GPS signals and submarine positioning. Some scientists also theorize links between sunspot activity and ice ages. [1]


== See also ==
Astronautics
ECSS standard E-ST-10-04C on Space environment
Karman line
Outer space
Space Environment Data System (SEDAT)
Space Environment Information System (SPENVIS)
Space climate
Space science
Space weather
Space weathering
Space Weather Prediction Center (SWPC)


== References ==


== External links ==
Space Environment Technologies (SET)
Space Weather Center (SWC)
ESA Space Environment and Effects Analysis Section 
International Space Environment Service (ISES)","pandas(index=30, _1=30, text='space environment is a branch of astronautics, aerospace engineering and space physics that seeks to understand and address conditions existing in space that affect the design and operation of spacecraft. a related subject, space weather, deals with dynamic processes in the solar-terrestrial system that can give rise to effects on spacecraft, but that can also affect the atmosphere, ionosphere and geomagnetic field, giving rise to several other kinds of effects on human technologies. effects on spacecraft can arise from radiation, space debris and meteoroid impact, upper atmospheric drag and spacecraft electrostatic charging. radiation in space usually comes from three main sources:  the van allen radiation belts solar proton events and solar energetic particles; and galactic cosmic rays.for long-duration missions, the high doses of radiation can damage electronic components and solar cells. a major concern is also radiation-induced ""single-event effects"" such as single event upset. crewed missions usually avoid the radiation belts and the international space station is at an altitude well below the most severe regions of the radiation belts. during solar energetic events (solar flares and coronal mass ejections) particles can be accelerated to very high energies and can reach the earth in times as short as 30 minutes (but usually take some hours). these particles are mainly protons and heavier ions that can cause radiation damage, disruption to logic circuits, and even hazards to astronauts. crewed missions to return to the moon or to travel to mars will have to deal with the major problems presented by solar particle events to radiation safety, in addition to the important contribution to doses from the low-level background cosmic rays. in near-earth orbits, the earth\'s geomagnetic field screens spacecraft from a large part of these hazards - a process called geomagnetic shielding. space debris and meteoroids can impact spacecraft at high speeds, causing mechanical or electrical damage.  the average speed of space debris is 10 km/s (22,000 mph; 36,000 km/h) while the average speed of meteoroids is much greater.  for example, the meteoroids associated with the perseid meteor shower travel at an average speed of 58 km/s (130,000 mph; 210,000 km/h).  mechanical damage from debris impacts have been studied through space missions including ldef, which had over 20,000 documented impacts through its 5.7-year mission.  electrical anomalies associated with impact events include esa\'s olympus spacecraft, which lost attitude control during the 1993 perseid meteor shower.  a similar event occurred with the landsat 5 spacecraft during the 2009 perseid meteor shower.spacecraft electrostatic charging is caused by the hot plasma environment around the earth. the plasma encountered in the region of the geostationary orbit becomes heated during geomagnetic substorms caused by disturbances in the solar wind. ""hot"" electrons (with energies in the kilo-electron volt range) collect on surfaces of spacecraft and can establish electrostatic potentials of the order of kilovolts. as a result, discharges can occur and are known to be the source of many spacecraft anomalies. solutions devised by scientists and engineers include, but are not limited to, spacecraft shielding, special ""hardening"" of electronic systems, various collision detection systems. evaluation of effects during spacecraft design includes application of various models of the environment, including radiation belt models, spacecraft-plasma interaction models and atmospheric models to predict drag effects encountered in lower orbits and during reentry. the field often overlaps with the disciplines of astrophysics, atmospheric science, space physics, and geophysics, albeit usually with an emphasis on application. the united states government maintains a space weather prediction center at boulder, colorado.  the space weather prediction center (swpc) is part of the national oceanic and atmospheric administration (noaa).  swpc is one of the national weather service\'s (nws) national centers for environmental prediction (ncep). space weather effects on earth can include ionospheric storms, temporary decreases in ozone densities, disruption to radio communication, to gps signals and submarine positioning. some scientists also theorize links between sunspot activity and ice ages. [1]   == see also == astronautics ecss standard e-st-10-04c on space environment karman line outer space space environment data system (sedat) space environment information system (spenvis) space climate space science space weather space weathering space weather prediction center (swpc)   == references ==   == external links == space environment technologies (set) space weather center (swc) esa space environment and effects analysis section international space environment service (ises)')"
31,"Aerodynamic heating is the heating of a solid body produced by its high-speed passage through air (or by the passage of air past a static body), whereby its kinetic energy is converted to heat by adiabatic heating, and by skin friction on the surface of the object at a rate that depends on the viscosity and speed of the air. In science and engineering, it is most frequently a concern regarding meteors, atmospheric reentry of spacecraft, and the design of high-speed aircraft.


== Physics ==
When moving through air at high speeds, an object's kinetic energy is converted to heat through compression of and friction with the air. At low speeds, the object also loses heat to the air if the air is cooler. The combined temperature effect of heat from the air and from passage through it is called the stagnation temperature; the actual temperature is called the recovery temperature. These viscous dissipative effects to neighboring sub-layers make the boundary layer slow down via a non-isentropic process.  Heat then conducts into the surface material from the higher temperature air. The result is an increase in the temperature of the material and a loss of energy from the flow. The forced convection ensures that other material replenishes the gases that have cooled to continue the process.The stagnation and the recovery temperature of a flow increases with the speed of the flow and are greater at high speeds. The total thermal loading of the object is a function of both the recovery temperature and the mass flow rate of the flow. Aerodynamic heating is greatest at high speeds and in the lower atmosphere where the density is greater.  In addition to the convective process described above, there is also thermal radiation from the flow to the body and vice versa, with the net direction governed by their temperatures relative to each other.Aerodynamic heating increases with the speed of the vehicle. Its effects are minimal at subsonic speeds, but are significant enough at supersonic speeds beyond about Mach 2.2 that they affect design and material considerations for the vehicle's structure and internal systems. The heating effects are greatest at leading edges, but the whole vehicle heats up to a stable temperature if its speed remains constant. Aerodynamic heating is dealt with by the use of alloys that can withstand high temperatures, insulation of the exterior of the vehicle, or the use of ablative material.


== Aircraft ==
Aerodynamic heating is a concern for supersonic and hypersonic aircraft.
One of the main concerns caused by aerodynamic heating arises in the design of the wing. For subsonic speeds, two main goals of wing design are minimizing weight and maximizing strength. Aerodynamic heating, which occurs at supersonic and hypersonic speeds, adds an additional consideration in wing structure analysis. An idealized wing structure is made up of spars, stringers, and skin segments. In a wing that normally experiences subsonic speeds, there must be a sufficient number of stringers to withstand the axial and bending stresses induced by the lift force acting on the wing. In addition, the distance between the stringers must be small enough that the skin panels do not buckle, and the panels must be thick enough to withstand the shear stress and shear flow present in the panels due to the lifting force on the wing. However, the weight of the wing must be made as small as possible, so the choice of material for the stringers and the skin is an important factor.At supersonic speeds, aerodynamic heating adds another element to this structural analysis. At normal speeds, spars and stringers experience a load called Delta P, which is a function of the lift force, first and second moments of inertia, and length of the spar. When there are more spars and stringers, the Delta P in each member is reduced, and the area of the stringer can be reduced to meet critical stress requirements. However, the increase in temperature caused by energy flowing from the air (heated by skin friction at these high speeds) adds another load factor, called a thermal load, to the spars. This thermal load increases the net force felt by the stringers, and thus the area of the stringers must be increased in order for the critical stress requirement to be met.Another issue that aerodynamic heating causes for aircraft design is the effect of high temperatures on common material properties. Common materials used in aircraft wing design, such as aluminum and steel, experience a decrease in strength as temperatures get extremely high. The Young's Modulus of the material, defined as the ratio between stress and strain experienced by the material, decreases as the temperature increases. Young's Modulus is critical in the selection of materials for wing, as a higher value lets the material resist the yield and shear stress caused by the lift and thermal loads. This is because Young's Modulus is an important factor in the equations for calculating the critical buckling load for axial members and the critical buckling shear stress for skin panels. If the Young's Modulus of the material decreases at high temperatures caused by aerodynamic heating, then the wing design will call for larger spars and thicker skin segments in order to account for this decrease in strength as the aircraft goes supersonic. There are some materials that retain their strength at the high temperatures that aerodynamic heating induces. For example, Inconel X-750 was used on parts of the airframe of the X-15, a North American aircraft that flew at hypersonic speeds in 1958. Titanium is another high-strength material, even at high temperatures, and is often used for wing frames of supersonic aircraft. The SR-71 used titanium skin panels painted black to reduce the temperature and corrugated to accommodate expansion. Another important design concept for early supersonic aircraft wings was using a small thickness-to-chord ratio, so that the speed of the flow over the airfoil does not increase too much from the free stream speed. As the flow is already supersonic, increasing the speed even more would not be beneficial for the wing structure. Reducing the thickness of the wing brings the top and bottom stringers closer together, reducing the total moment of inertia of the structure. This increases axial load in the stringers, and thus the area, and weight, of the stringers must be increased. Some designs for hypersonic missiles have used liquid cooling of the leading edges (usually the fuel en route to the engine). The Sprint missile's heat shield needed several design iterations for Mach 10 temperatures.


== Reentry vehicles ==
Heating caused by the very high reentry speeds (greater than Mach 20) is sufficient to destroy the vehicle unless special techniques are used. The early space capsules such as used on Mercury, Gemini, and Apollo were given blunt shapes to produce a stand-off bow shock, allowing most of the heat to dissipate into the surrounding air. Additionally, these vehicles had ablative material that sublimates into a gas at high temperature. The act of sublimation absorbs the thermal energy from the aerodynamic heating and erodes the material rather than heating the capsule. The surface of the heat shield for the Mercury spacecraft had a coating of aluminum with glassfiber in many layers. As the temperature rose to 1,100 °C (1,400 K) the layers would evaporate and take the heat with it. The spacecraft would become hot but not harmfully so. The Space Shuttle used insulating tiles on its lower surface to absorb and radiate heat while preventing conduction to the aluminum airframe.  The damage to the heat shield during liftoff of Space Shuttle Columbia contributed to its destruction upon reentry.


== References ==

Moore, F.G., Approximate Methods for Weapon Aerodynamics, AIAA Progress in Astronautics and Aeronautics, Volume 186
Chapman, A.J., Heat Transfer, Third Edition, Macmillan Publishing Company, 1974
Bell Laboratories R&D, ABM Research and Development At Bell Laboratories, 1974. Stanley R. Mickelsen Safeguard Complex","pandas(index=31, _1=31, text=""aerodynamic heating is the heating of a solid body produced by its high-speed passage through air (or by the passage of air past a static body), whereby its kinetic energy is converted to heat by adiabatic heating, and by skin friction on the surface of the object at a rate that depends on the viscosity and speed of the air. in science and engineering, it is most frequently a concern regarding meteors, atmospheric reentry of spacecraft, and the design of high-speed aircraft.   == physics == when moving through air at high speeds, an object's kinetic energy is converted to heat through compression of and friction with the air. at low speeds, the object also loses heat to the air if the air is cooler. the combined temperature effect of heat from the air and from passage through it is called the stagnation temperature; the actual temperature is called the recovery temperature. these viscous dissipative effects to neighboring sub-layers make the boundary layer slow down via a non-isentropic process.  heat then conducts into the surface material from the higher temperature air. the result is an increase in the temperature of the material and a loss of energy from the flow. the forced convection ensures that other material replenishes the gases that have cooled to continue the process.the stagnation and the recovery temperature of a flow increases with the speed of the flow and are greater at high speeds. the total thermal loading of the object is a function of both the recovery temperature and the mass flow rate of the flow. aerodynamic heating is greatest at high speeds and in the lower atmosphere where the density is greater.  in addition to the convective process described above, there is also thermal radiation from the flow to the body and vice versa, with the net direction governed by their temperatures relative to each other.aerodynamic heating increases with the speed of the vehicle. its effects are minimal at subsonic speeds, but are significant enough at supersonic speeds beyond about mach 2.2 that they affect design and material considerations for the vehicle's structure and internal systems. the heating effects are greatest at leading edges, but the whole vehicle heats up to a stable temperature if its speed remains constant. aerodynamic heating is dealt with by the use of alloys that can withstand high temperatures, insulation of the exterior of the vehicle, or the use of ablative material.   == aircraft == aerodynamic heating is a concern for supersonic and hypersonic aircraft. one of the main concerns caused by aerodynamic heating arises in the design of the wing. for subsonic speeds, two main goals of wing design are minimizing weight and maximizing strength. aerodynamic heating, which occurs at supersonic and hypersonic speeds, adds an additional consideration in wing structure analysis. an idealized wing structure is made up of spars, stringers, and skin segments. in a wing that normally experiences subsonic speeds, there must be a sufficient number of stringers to withstand the axial and bending stresses induced by the lift force acting on the wing. in addition, the distance between the stringers must be small enough that the skin panels do not buckle, and the panels must be thick enough to withstand the shear stress and shear flow present in the panels due to the lifting force on the wing. however, the weight of the wing must be made as small as possible, so the choice of material for the stringers and the skin is an important factor.at supersonic speeds, aerodynamic heating adds another element to this structural analysis. at normal speeds, spars and stringers experience a load called delta p, which is a function of the lift force, first and second moments of inertia, and length of the spar. when there are more spars and stringers, the delta p in each member is reduced, and the area of the stringer can be reduced to meet critical stress requirements. however, the increase in temperature caused by energy flowing from the air (heated by skin friction at these high speeds) adds another load factor, called a thermal load, to the spars. this thermal load increases the net force felt by the stringers, and thus the area of the stringers must be increased in order for the critical stress requirement to be met.another issue that aerodynamic heating causes for aircraft design is the effect of high temperatures on common material properties. common materials used in aircraft wing design, such as aluminum and steel, experience a decrease in strength as temperatures get extremely high. the young's modulus of the material, defined as the ratio between stress and strain experienced by the material, decreases as the temperature increases. young's modulus is critical in the selection of materials for wing, as a higher value lets the material resist the yield and shear stress caused by the lift and thermal loads. this is because young's modulus is an important factor in the equations for calculating the critical buckling load for axial members and the critical buckling shear stress for skin panels. if the young's modulus of the material decreases at high temperatures caused by aerodynamic heating, then the wing design will call for larger spars and thicker skin segments in order to account for this decrease in strength as the aircraft goes supersonic. there are some materials that retain their strength at the high temperatures that aerodynamic heating induces. for example, inconel x-750 was used on parts of the airframe of the x-15, a north american aircraft that flew at hypersonic speeds in 1958. titanium is another high-strength material, even at high temperatures, and is often used for wing frames of supersonic aircraft. the sr-71 used titanium skin panels painted black to reduce the temperature and corrugated to accommodate expansion. another important design concept for early supersonic aircraft wings was using a small thickness-to-chord ratio, so that the speed of the flow over the airfoil does not increase too much from the free stream speed. as the flow is already supersonic, increasing the speed even more would not be beneficial for the wing structure. reducing the thickness of the wing brings the top and bottom stringers closer together, reducing the total moment of inertia of the structure. this increases axial load in the stringers, and thus the area, and weight, of the stringers must be increased. some designs for hypersonic missiles have used liquid cooling of the leading edges (usually the fuel en route to the engine). the sprint missile's heat shield needed several design iterations for mach 10 temperatures.   == reentry vehicles == heating caused by the very high reentry speeds (greater than mach 20) is sufficient to destroy the vehicle unless special techniques are used. the early space capsules such as used on mercury, gemini, and apollo were given blunt shapes to produce a stand-off bow shock, allowing most of the heat to dissipate into the surrounding air. additionally, these vehicles had ablative material that sublimates into a gas at high temperature. the act of sublimation absorbs the thermal energy from the aerodynamic heating and erodes the material rather than heating the capsule. the surface of the heat shield for the mercury spacecraft had a coating of aluminum with glassfiber in many layers. as the temperature rose to 1,100 °c (1,400 k) the layers would evaporate and take the heat with it. the spacecraft would become hot but not harmfully so. the space shuttle used insulating tiles on its lower surface to absorb and radiate heat while preventing conduction to the aluminum airframe.  the damage to the heat shield during liftoff of space shuttle columbia contributed to its destruction upon reentry.   == references ==  moore, f.g., approximate methods for weapon aerodynamics, aiaa progress in astronautics and aeronautics, volume 186 chapman, a.j., heat transfer, third edition, macmillan publishing company, 1974 bell laboratories r&d, abm research and development at bell laboratories, 1974. stanley r. mickelsen safeguard complex"")"
32,"In aeronautics, the rate of climb (RoC) is an aircraft's vertical speed – the positive or negative rate of altitude change with respect to time. In most ICAO member countries, even in otherwise metric countries, this is usually expressed in feet per minute (ft/min); elsewhere, it is commonly expressed in metres per second (m/s). The RoC in an aircraft is indicated with a vertical speed indicator (VSI) or instantaneous vertical speed indicator (IVSI).
The temporal rate of decrease in altitude is referred to as the rate of descent (RoD) or sink rate. 
A negative rate of climb corresponds to a positive rate of descent: RoD = −RoC.


== Speed and rate of climb ==
There are a number of designated airspeeds relating to optimum rates of ascent, the two most important of these are VX and VY.
VX is the indicated forward airspeed for best angle of climb. This is the speed at which an aircraft gains the most altitude in a given horizontal distance, typically used to avoid a collision with an object a short distance away. By contrast, VY is the indicated airspeed for best rate of climb,
a rate which allows the aircraft to climb to a specified altitude in the minimum amount of time regardless of the horizontal distance required. Except at the aircraft's ceiling, where they are equal, VX is always lower than VY.
Climbing at VX allows pilots to maximize altitude gain per horizontal distance. This occurs at the speed for which the difference between thrust and drag is the greatest (maximum excess thrust). In a jet airplane, this is approximately minimum drag speed, occurring at the bottom of the drag vs. speed curve.
Climbing at VY allows pilots to maximize altitude gain per time. This occurs at the speed where the difference between engine power and the power required to overcome the aircraft's drag is greatest (maximum excess power).Vx increases with altitude and VY decreases with altitude until they converge at the airplane's absolute ceiling, the altitude above which the airplane cannot climb in steady flight.
The Cessna 172 is a four-seat aircraft. At maximum weight it has a VY of 75 kn (139 km/h) indicated airspeed providing a rate of climb of 721 ft/min (3.66 m/s).
Rate of climb at maximum power for a small aircraft is typically specified in its normal operating procedures but for large jet airliners it is usually mentioned in emergency operating procedures.


== See also ==
ICAO recommendations on use of the International System of Units
Climb (aeronautics)
V speeds
Variometer


== References ==","pandas(index=32, _1=32, text=""in aeronautics, the rate of climb (roc) is an aircraft's vertical speed – the positive or negative rate of altitude change with respect to time. in most icao member countries, even in otherwise metric countries, this is usually expressed in feet per minute (ft/min); elsewhere, it is commonly expressed in metres per second (m/s). the roc in an aircraft is indicated with a vertical speed indicator (vsi) or instantaneous vertical speed indicator (ivsi). the temporal rate of decrease in altitude is referred to as the rate of descent (rod) or sink rate. a negative rate of climb corresponds to a positive rate of descent: rod = −roc.   == speed and rate of climb == there are a number of designated airspeeds relating to optimum rates of ascent, the two most important of these are vx and vy. vx is the indicated forward airspeed for best angle of climb. this is the speed at which an aircraft gains the most altitude in a given horizontal distance, typically used to avoid a collision with an object a short distance away. by contrast, vy is the indicated airspeed for best rate of climb, a rate which allows the aircraft to climb to a specified altitude in the minimum amount of time regardless of the horizontal distance required. except at the aircraft's ceiling, where they are equal, vx is always lower than vy. climbing at vx allows pilots to maximize altitude gain per horizontal distance. this occurs at the speed for which the difference between thrust and drag is the greatest (maximum excess thrust). in a jet airplane, this is approximately minimum drag speed, occurring at the bottom of the drag vs. speed curve. climbing at vy allows pilots to maximize altitude gain per time. this occurs at the speed where the difference between engine power and the power required to overcome the aircraft's drag is greatest (maximum excess power).vx increases with altitude and vy decreases with altitude until they converge at the airplane's absolute ceiling, the altitude above which the airplane cannot climb in steady flight. the cessna 172 is a four-seat aircraft. at maximum weight it has a vy of 75 kn (139 km/h) indicated airspeed providing a rate of climb of 721 ft/min (3.66 m/s). rate of climb at maximum power for a small aircraft is typically specified in its normal operating procedures but for large jet airliners it is usually mentioned in emergency operating procedures.   == see also == icao recommendations on use of the international system of units climb (aeronautics) v speeds variometer   == references =="")"
33,"Trajectory optimization is the process of designing a trajectory that minimizes (or maximizes) some measure of performance while satisfying a set of constraints. Generally speaking, trajectory optimization is a technique for computing an open-loop solution to an optimal control problem. It is often used for systems where computing the full closed-loop solution is not required, impractical or impossible.  If a trajectory optimization problem can be solved at a rate given by the inverse of the Lipschitz constant, then it can be used iteratively to generate a closed-loop solution in the sense of Caratheodory.  If only the first step of the trajectory is executed for an infinite-horizon problem, then this is known as Model Predictive Control (MPC).
Although the idea of trajectory optimization has been around for hundreds of years (calculus of variations, brachystochrone problem), it only became practical for real-world problems with the advent of the computer. Many of the original applications of trajectory optimization were in the aerospace industry, computing rocket and missile launch trajectories. More recently, trajectory optimization has also been used in a wide variety of industrial process and robotics applications.


== History ==
Trajectory optimization first showed up in 1697, with the introduction of the Brachystochrone problem: find the shape of a wire such that a bead sliding along it will move between two points in the minimum time. The interesting thing about this problem is that it is optimizing over a curve (the shape of the wire), rather than a single number. The most famous of the solutions was computed using calculus of variations.
In the 1950s, the digital computer started to make trajectory optimization practical for solving real-world problems. The first optimal control approaches grew out of the calculus of variations, based on the research of Gilbert Ames Bliss and Bryson in America, and Pontryagin in Russia. Pontryagin's maximum principle is of particular note. These early researchers created the foundation of what we now call indirect methods for trajectory optimization.
Much of the early work in trajectory optimization was focused on computing rocket thrust profiles, both in a vacuum and in the atmosphere. This early research discovered many basic principles that are still used today. 
Another successful application was the climb to altitude trajectories for the early jet aircraft.  Because of the high drag associated with the transonic drag region and the low thrust of early jet aircraft, trajectory optimization was the key to maximizing climb to altitude performance.  Optimal control based trajectories were responsible for some of the world records.  In these situations, the pilot followed a Mach versus altitude schedule based on optimal control solutions.
One of the important early problems in trajectory optimization was that of the singular arc, where Pontryagin's maximum principle fails to yield a complete solution. An example of a problem with singular control is the optimization of the thrust of a missile flying at a constant altitude and which is launched at low speed.  Here the problem is one of a bang-bang control at maximum possible thrust until the singular arc is reached.  Then the solution to the singular control provides a lower variable thrust until burnout.  At that point bang-bang control provides that the control or thrust go to its minimum value of zero.  This solution is the foundation of the boost-sustain rocket motor profile widely used today to maximize missile performance.


== Applications ==
There are a wide variety of applications for trajectory optimization, primarily in robotics: industry, manipulation, walking, path-planning, and aerospace. It can also be used for modeling and estimation.


=== Robotic manipulators ===
Depending on the configuration, open-chain robotic manipulators require a degree of trajectory optimization. For instance, a robotic arm with 7 joints and 7 links (7-DOF) is a redundant system where one cartesian position of an end-effector can correspond to an infinite number of joint angle positions, thus this redundancy can be used to optimize a trajectory to, for example, avoid any obstacles in the workspace or minimize the torque in the joints.


=== Quadrotor helicopters ===
Trajectory optimization is often used to compute trajectories for quadrotor helicopters. These applications typically used highly specialized algorithms.

One interesting application shown by the U.Penn GRASP Lab is computing a trajectory that allows a quadrotor to fly through a hoop as it is thrown. Another, this time by the ETH Zurich Flying Machine Arena,  involves two quadrotors tossing a pole back and forth between them, with it balanced like an inverted pendulum. The problem of computing minimum-energy trajectories for a quadcopter, has also been recently studied.


=== Manufacturing ===
Trajectory optimization is used in manufacturing, particularly for controlling chemical processes (such as in 

) or computing the desired path for robotic manipulators (such as in

).


=== Walking robots ===
There are a variety of different applications for trajectory optimization within the field of walking robotics. For example, one paper used trajectory optimization of bipedal gaits on a simple model to show that walking is energetically favorable for moving at a low speed and running is energetically favorable for moving at a high speed.

Like in many other applications, trajectory optimization can be used to compute a nominal trajectory, around which a stabilizing controller is built.

Trajectory optimization can be applied in detailed motion planning complex humanoid robots, such as Atlas.

Finally, trajectory optimization can be used for path-planning of robots with complicated dynamics constraints, using reduced complexity models.


=== Aerospace ===
For tactical missiles, the flight profiles are determined by the thrust and lift histories.  These histories can be controlled by a number of means including such techniques as using an angle of attack command history or an altitude/downrange schedule that the missile must follow.  Each combination of missile design factors, desired missile performance, and system constraints results in a new set of optimal control parameters.


== Terminology ==
Decision variables
The set of unknowns to be found using optimization.Trajectory optimization problem
A special type of optimization problem where the decision variables are functions, rather than real numbers.Parameter optimization
Any optimization problem where the decision variables are real numbers.Nonlinear program
A class of constrained parameter optimization where either the objective function or constraints are nonlinear.Indirect method
An indirect method for solving a trajectory optimization problem proceeds in three steps: 1) Analytically construct the necessary and sufficient conditions for optimality, 2) Discretize these conditions, constructing a constrained parameter optimization problem, 3) Solve that optimization problem.Direct method
A direct method for solving a trajectory optimization problem consists of two steps: 1) Discretize the trajectory optimization problem directly, converting it into a constrained parameter optimization problem, 2) Solve that optimization problem.Transcription
The process by which a trajectory optimization problem is converted into a parameter optimization problem. This is sometimes referred to as discretization. Transcription methods generally fall into two categories: shooting methods and collocation methods.Shooting method
A transcription method that is based on simulation, typically using explicit Runge--Kutta schemes.Collocation method (Simultaneous Method)
A transcription method that is based on function approximation, typically using implicit Runge--Kutta schemes.Pseudospectral method (Global Collocation)
A transcription method that represents the entire trajectory as a single high-order orthogonal polynomial.Mesh (Grid)
After transcription, the formerly continuous trajectory is now represented by a discrete set of points, known as mesh points or grid points.Mesh refinement
The process by which the discretization mesh is improved by solving a sequence of trajectory optimization problems. Mesh refinement is either performed by sub-dividing a trajectory segment or by increasing the order of the polynomial representing that segment.Multi-phase trajectory optimization problem
Trajectory optimization over a system with hybrid dynamics can be achieved by posing it as a multi-phase trajectory optimization problem. This is done by composing a sequence of standard trajectory optimization problems that are connected using constraints.


== Trajectory optimization techniques ==
The techniques to any optimization problems can be divided into two categories: indirect and direct. An indirect method works by analytically constructing the necessary and sufficient conditions for optimality, which are then solved numerically. A direct method attempts a direct numerical solution by constructing a sequence of continually improving approximations to the optimal solution. Direct and indirect methods can be blended by an application of the covector mapping principle of Ross and Fahroo.The optimal control problem is an infinite-dimensional optimization problem, since the decision variables are functions, rather than real numbers. All solution techniques perform transcription, a process by which the trajectory optimization problem (optimizing over functions) is converted into a constrained parameter optimization problem (optimizing over real numbers). Generally, this constrained parameter optimization problem is a non-linear program, although in special cases it can be reduced to a quadratic program or linear program.


=== Single shooting ===
Single shooting is the simplest type of trajectory optimization technique. The basic idea is similar to how you would aim a cannon: pick a set of parameters for the trajectory, simulate the entire thing, and then check to see if you hit the target. The entire trajectory is represented as a single segment, with a single constraint, known as a defect constraint, requiring that the final state of the simulation matches the desired final state of the system. Single shooting is effective for problems that are either simple or have an extremely good initialization. Both the indirect and direct formulation tend to have difficulties otherwise.


=== Multiple shooting ===
Multiple shooting is a simple extension to single shooting that renders it far more effective. Rather than representing the entire trajectory as a single simulation (segment), the algorithm breaks the trajectory into many shorter segments, and a defect constraint is added between each. The result is large sparse non-linear program, which tends to be easier to solve than the small dense programs produced by single shooting.


=== Direct collocation ===
Direct collocation methods work by approximating the state and control trajectories using polynomial splines. These methods are sometimes referred to as direct transcription. Trapezoidal collocation is a commonly used low-order direct collocation method. The dynamics, path objective, and control are all represented using linear splines, and the dynamics are satisfied using trapezoidal quadrature. Hermite-Simpson Collocation is a common medium-order direct collocation method. The state is represented by a cubic-Hermite spline, and the dynamics are satisfied using Simpson quadrature.


=== Orthogonal collocation ===
Orthogonal collocation is technically a subset of direct collocation, but the implementation details are so different that it can reasonably be considered its own set of methods. Orthogonal collocation differs from direct collocation in that it typically uses high-order splines, and each segment of the trajectory might be represented by a spline of a different order. The name comes from the use of orthogonal polynomials in the state and control splines.


=== Pseudospectral collocation ===
Pseudospectral collocation, also known as global collocation, is a subset of orthogonal collocation in which the entire trajectory is represented by a single high-order orthogonal polynomial. As a side note: some authors use orthogonal collocation and pseudospectral collocation interchangeably. When used to solve a trajectory optimization problem whose solution is smooth, a pseudospectral method will achieve spectral (exponential) convergence.


=== Differential dynamic programming ===
Differential dynamic programming, is a bit different than the other techniques described here. In particular, it does not cleanly separate the transcription and the optimization. Instead, it does a sequence of iterative forward and backward passes along the trajectory. Each forward pass satisfies the system dynamics, and each backward pass satisfies the optimality conditions for control. Eventually, this iteration converges to a trajectory that is both feasible and optimal.


== Comparison of techniques ==
There are many techniques to choose from when solving a trajectory optimization problem. There is no best method, but some methods might do a better job on specific problems. This section provides a rough understanding of the trade-offs between methods.


=== Indirect vs. direct methods ===
When solving a trajectory optimization problem with an indirect method, you must explicitly construct the adjoint equations and their gradients. This is often difficult to do, but it gives an excellent accuracy metric for the solution. Direct methods are much easier to set up and solve, but do not have a built-in accuracy metric. As a result, direct methods are more widely used, especially in non-critical applications. Indirect methods still have a place in specialized applications, particularly aerospace, where accuracy is critical.
One place where indirect methods have particular difficulty is on problems with path inequality constraints. These problems tend to have solutions for which the constraint is partially active. When constructing the adjoint equations for an indirect method, the user must explicitly write down when the constraint is active in the solution, which is difficult to know a priori. One solution is to use a direct method to compute an initial guess, which is then used to construct a multi-phase problem where the constraint is prescribed. The resulting problem can then be solved accurately using an indirect method.


=== Shooting vs. collocation ===
Single shooting methods are best used for problems where the control is very simple (or there is an extremely good initial guess). For example, a satellite mission planning problem where the only control is the magnitude and direction of an initial impulse from the engines.Multiple shooting tends to be good for problems with relatively simple control, but complicated dynamics. Although path constraints can be used, they make the resulting nonlinear program relatively difficult to solve.
Direct collocation methods are good for problems where the accuracy of the control and the state are similar. These methods tend to be less accurate than others (due to their low-order), but are particularly robust for problems with difficult path constraints.
Orthogonal collocation methods are best for obtaining high-accuracy solutions to problems where the accuracy of the control trajectory is important. Some implementations have trouble with path constraints. These methods are particularly good when the solution is smooth.


=== Mesh refinement: h vs. p ===
It is common to solve a trajectory optimization problem iteratively, each time using a discretization with more points. A h-method for mesh refinement works by increasing the number of trajectory segments along the trajectory, while a p-method increases the order of the transcription method within each segment.
Direct collocation methods tend to exclusively use h-method type refinement, since each method is a fixed order. Shooting methods and orthogonal collocation methods can both use h-method and p-method mesh refinement, and some use a combination, known as hp-adaptive meshing. It is best to use h-method when the solution is non-smooth, while a p-method is best for smooth solutions.


== Software ==
Examples of trajectory optimization programs include:

APMonitor: Large-scale optimization software based on orthogonal collocation.
ASTOS: Analysis, Simulation and Trajectory Optimization Software for Space Applications. The ASTOS software is a multi-purpose tool for space applications. Originally designed for trajectory optimization, it provides now modules for a variety of analysis, simulation and design capabilities
Bocop - The optimal control solver: Open source toolbox for optimal control problems (user friendly and advanced GUI for efficient use).
PyKEP, PyGMO (Open Source, from the European Space Agency for interplanetary trajectory optimization)
Copernicus Trajectory Design and Optimization System [1]
DIDO: MATLAB optimal control toolbox used at NASA and academia and distributed by Elissar Global.
QuickShot: A general-purpose, multi-threaded 3-DOF/4-DOF trajectory simulation tool for robust global optimization from SpaceWorks Enterprises, Inc.
DIRCOL: A general-purpose trajectory optimization software based on direct collocation.
Drake: A planning, control, and analysis toolbox for nonlinear dynamical systems.
FALCON.m: The FSD Optimal Control Tool for Matlab, developed at the Institute of Flight System Dynamics of Technical University of Munich.
Gekko (optimization software): A Python optimization package with trajectory optimization applications of HALE Aircraft and aerial towed cable systems.
General Mission Analysis Tool
GPOPS-II (General Purpose OPtimal Control Software) Solves multi-phase trajectory optimization problems. (Matlab)
HamPath: On solving optimal control problems by indirect and path following methods (Matlab and Python interfaces).
JModelica.org (Modelica-based open source platform for dynamic optimization)
LOTOS (Low-Thrust Orbit Transfer Trajectory Optimization Software) from Astos Solutions
MIDACO Optimization software particularly developed for interplanetary space trajectories. (Avail. in Matlab, Octave, Python, C/C++, R and Fortran)
OpenOCL Open Optimal Control Library, optimal control modeling library, automatic differentiation, non-linear optimization, Matlab/Octave.
OTIS (Optimal Trajectories by Implicit Simulation) [2]
Opty Python package utilizing SymPy for symbolic description of ordinary differential equations to form constraints needed to solve optimal control and parameter identification problems using the direct collocation method and non-linear programming.
POST (Program to Optimize Simulated Trajectories) [3], [4]
OptimTraj: An open-source trajectory optimization library for Matlab
ZOOM, Conceptual Design and Analysis of Rocket Configurations and Trajectories) [5]
PSOPT, an open source optimal control software package written in C++ that uses direct collocation methods [6]
OpenGoddard An open source optimal control software package written in Python that uses pseudospectral methods.
Systems Tool Kit Astrogator (STK Astrogator): A specialized analysis module for orbit maneuver and space trajectory design. Astrogator offers orthogonal-collocation-based trajectory optimization using high-fidelity force models.
beluga: An open source Python package for trajectory optimization using indirect methods.A collection of low thrust trajectory optimization tools, including members of the Low Thrust Trajectory Tool (LTTT) set, can be found here: LTTT Suite Optimization Tools.


== References ==","pandas(index=33, _1=33, text=""trajectory optimization is the process of designing a trajectory that minimizes (or maximizes) some measure of performance while satisfying a set of constraints. generally speaking, trajectory optimization is a technique for computing an open-loop solution to an optimal control problem. it is often used for systems where computing the full closed-loop solution is not required, impractical or impossible.  if a trajectory optimization problem can be solved at a rate given by the inverse of the lipschitz constant, then it can be used iteratively to generate a closed-loop solution in the sense of caratheodory.  if only the first step of the trajectory is executed for an infinite-horizon problem, then this is known as model predictive control (mpc). although the idea of trajectory optimization has been around for hundreds of years (calculus of variations, brachystochrone problem), it only became practical for real-world problems with the advent of the computer. many of the original applications of trajectory optimization were in the aerospace industry, computing rocket and missile launch trajectories. more recently, trajectory optimization has also been used in a wide variety of industrial process and robotics applications.   == history == trajectory optimization first showed up in 1697, with the introduction of the brachystochrone problem: find the shape of a wire such that a bead sliding along it will move between two points in the minimum time. the interesting thing about this problem is that it is optimizing over a curve (the shape of the wire), rather than a single number. the most famous of the solutions was computed using calculus of variations. in the 1950s, the digital computer started to make trajectory optimization practical for solving real-world problems. the first optimal control approaches grew out of the calculus of variations, based on the research of gilbert ames bliss and bryson in america, and pontryagin in russia. pontryagin's maximum principle is of particular note. these early researchers created the foundation of what we now call indirect methods for trajectory optimization. much of the early work in trajectory optimization was focused on computing rocket thrust profiles, both in a vacuum and in the atmosphere. this early research discovered many basic principles that are still used today. another successful application was the climb to altitude trajectories for the early jet aircraft.  because of the high drag associated with the transonic drag region and the low thrust of early jet aircraft, trajectory optimization was the key to maximizing climb to altitude performance.  optimal control based trajectories were responsible for some of the world records.  in these situations, the pilot followed a mach versus altitude schedule based on optimal control solutions. one of the important early problems in trajectory optimization was that of the singular arc, where pontryagin's maximum principle fails to yield a complete solution. an example of a problem with singular control is the optimization of the thrust of a missile flying at a constant altitude and which is launched at low speed.  here the problem is one of a bang-bang control at maximum possible thrust until the singular arc is reached.  then the solution to the singular control provides a lower variable thrust until burnout.  at that point bang-bang control provides that the control or thrust go to its minimum value of zero.  this solution is the foundation of the boost-sustain rocket motor profile widely used today to maximize missile performance.   == applications == there are a wide variety of applications for trajectory optimization, primarily in robotics: industry, manipulation, walking, path-planning, and aerospace. it can also be used for modeling and estimation. it is common to solve a trajectory optimization problem iteratively, each time using a discretization with more points. a h-method for mesh refinement works by increasing the number of trajectory segments along the trajectory, while a p-method increases the order of the transcription method within each segment. direct collocation methods tend to exclusively use h-method type refinement, since each method is a fixed order. shooting methods and orthogonal collocation methods can both use h-method and p-method mesh refinement, and some use a combination, known as hp-adaptive meshing. it is best to use h-method when the solution is non-smooth, while a p-method is best for smooth solutions.   == software == examples of trajectory optimization programs include:  apmonitor: large-scale optimization software based on orthogonal collocation. astos: analysis, simulation and trajectory optimization software for space applications. the astos software is a multi-purpose tool for space applications. originally designed for trajectory optimization, it provides now modules for a variety of analysis, simulation and design capabilities bocop - the optimal control solver: open source toolbox for optimal control problems (user friendly and advanced gui for efficient use). pykep, pygmo (open source, from the european space agency for interplanetary trajectory optimization) copernicus trajectory design and optimization system [1] dido: matlab optimal control toolbox used at nasa and academia and distributed by elissar global. quickshot: a general-purpose, multi-threaded 3-dof/4-dof trajectory simulation tool for robust global optimization from spaceworks enterprises, inc. dircol: a general-purpose trajectory optimization software based on direct collocation. drake: a planning, control, and analysis toolbox for nonlinear dynamical systems. falcon.m: the fsd optimal control tool for matlab, developed at the institute of flight system dynamics of technical university of munich. gekko (optimization software): a python optimization package with trajectory optimization applications of hale aircraft and aerial towed cable systems. general mission analysis tool gpops-ii (general purpose optimal control software) solves multi-phase trajectory optimization problems. (matlab) hampath: on solving optimal control problems by indirect and path following methods (matlab and python interfaces). jmodelica.org (modelica-based open source platform for dynamic optimization) lotos (low-thrust orbit transfer trajectory optimization software) from astos solutions midaco optimization software particularly developed for interplanetary space trajectories. (avail. in matlab, octave, python, c/c, r and fortran) openocl open optimal control library, optimal control modeling library, automatic differentiation, non-linear optimization, matlab/octave. otis (optimal trajectories by implicit simulation) [2] opty python package utilizing sympy for symbolic description of ordinary differential equations to form constraints needed to solve optimal control and parameter identification problems using the direct collocation method and non-linear programming. post (program to optimize simulated trajectories) [3], [4] optimtraj: an open-source trajectory optimization library for matlab zoom, conceptual design and analysis of rocket configurations and trajectories) [5] psopt, an open source optimal control software package written in cthat uses direct collocation methods [6] opengoddard an open source optimal control software package written in python that uses pseudospectral methods. systems tool kit astrogator (stk astrogator): a specialized analysis module for orbit maneuver and space trajectory design. astrogator offers orthogonal-collocation-based trajectory optimization using high-fidelity force models. beluga: an open source python package for trajectory optimization using indirect methods.a collection of low thrust trajectory optimization tools, including members of the low thrust trajectory tool (lttt) set, can be found here: lttt suite optimization tools.   == references =="")"
34,"Mach tuck is an aerodynamic effect whereby the nose of an aircraft tends to pitch downward as the airflow around the wing reaches supersonic speeds. This diving tendency is also known as tuck under. The aircraft will first experience this effect at significantly below Mach 1.


== Causes ==
Mach tuck is usually caused by two things, a rearward movement of the centre of pressure of the wing and a decrease in wing downwash velocity at the tailplane both of which cause a nose down pitching moment. For a particular aircraft design only one of these may be significant in causing a tendency to dive, delta-winged aircraft with no foreplane or tailplane in the first case and, for example, the Lockheed P-38 in the second case. Alternatively, a particular design may have no significant tendency, for example the Fokker F28 Fellowship.As an aerofoil generating lift moves through the air, the air flowing over the top surface accelerates to a higher local speed than the air flowing over the bottom surface. When the aircraft speed reaches its critical Mach number the accelerated airflow locally reaches the speed of sound and creates a small shock wave, even though the aircraft is still travelling below the speed of sound. The region in front of the shock wave generates high lift. As the aircraft itself flies faster, the shock wave over the wing gets stronger and moves rearwards, creating high lift further back along the wing. This rearward movement of lift causes the aircraft to tuck or pitch nose-down.
The severity of Mach tuck on any given design is affected by the thickness of the aerofoil, the sweep angle of the wing, and the location of the tailplane relative to the main wing.A tailplane which is positioned further aft can provide a larger stabilizing pitch-up moment.
The camber and thickness of the aerofoil affect the critical Mach number, with a more highly curved upper surface causing a lower critical Mach number.
On a swept wing the shock wave typically forms first at the wing root, especially if it is more cambered than the wing tip. As speed increases, the shock wave and associated lift extend outwards and, because the wing is swept, backwards.
The changing airflow over the wing can reduce the downwash over a conventional tailplane, promoting a stronger nose-down pitching moment.
Another problem with a separate horizontal stabiliser is that it can itself achieve local supersonic flow with its own shock wave. This can affect the operation of a conventional elevator control surface.
Aircraft without enough elevator authority to maintain trim and fly level can enter a steep, sometimes unrecoverable dive.  Until the aircraft is supersonic, the faster top shock wave can reduce the authority of the elevator and horizontal stabilizers.All transonic and supersonic aircraft experience Mach tuck.


== Recovery ==
Recovery is sometimes impossible in subsonic aircraft; however, as an aircraft descends into lower, warmer, denser air, control authority (meaning the ability to control the aircraft) may return because drag tends to slow the aircraft while the speed of sound and control authority both increase.
To prevent Mach stall from progressing, the pilot should keep the airspeed below the type's critical Mach number by reducing throttle, extending speed brakes, and if possible, extending the landing gear.


== Design features ==
A number of design techniques are used to counter the effects of Mach tuck.
On both conventional tailplane and canard foreplane configurations, the horizontal stabiliser may be made large and powerful enough to correct the large trim changes associated with Mach tuck. In place of the conventional elevator control surface, the whole stabiliser may be made moveable or ""all-flying"", sometimes called a stabilator. This both increases the authority of the stabilizer over a wider range of aircraft pitch, but also avoids the controllability issues associated with a separate elevator.Aircraft that fly supersonic for long periods, such as Concorde, may compensate for Mach tuck by moving fuel between tanks in the fuselage to change the position of the centre of mass to match  the changing location of the centre of pressure, thereby minimizing the amount of aerodynamic trim required.
A Mach trimmer is a device which varies the pitch trim automatically as a function of Mach number to oppose Mach tuck and maintain level flight.


== History ==
 The fastest World War II fighters were the first aircraft to experience Mach tuck.   Their wings were not designed to counter Mach tuck because research on supersonic airfoils was just beginning; areas of supersonic flow, together with shock waves and flow separation, were present on the wing. This condition was known at the time as compressibility burble and was known to exist on propeller tips at high aircraft speeds.The P-38 was the first 400 mph fighter, and it suffered more than the usual teething troubles. It had a thick, high-lift wing,  distinctive twin booms and a single, central nacelle containing the cockpit and armament.  It quickly accelerated to terminal velocity in a dive. The short stubby fuselage had a detrimental effect in reducing the critical Mach number of the 15% thick wing center section with high velocities over the canopy adding to those on the upper surface of the wing. Mach tuck occurred at speeds above Mach 0.65; the air flow over the wing center section became transonic, causing a loss of lift. The resultant change in downwash at the tail caused a nose-down pitching moment and the dive to steepen (Mach tuck). The aircraft was very stable in this condition making recovery from the dive very difficult. 
Dive recovery (auxiliary) flaps were added to the underside of the wing (P-38J-LO) to increase the wing lift and downwash at the tail to allow recovery from transonic dives.


== References ==

 This article incorporates public domain material from the United States Government document: ""Airplane Flying Handbook"". This article incorporates public domain material from the United States Government document: ""Pilot's Handbook of Aeronautical Knowledge"".","pandas(index=34, _1=34, text='mach tuck is an aerodynamic effect whereby the nose of an aircraft tends to pitch downward as the airflow around the wing reaches supersonic speeds. this diving tendency is also known as tuck under. the aircraft will first experience this effect at significantly below mach 1.   == causes == mach tuck is usually caused by two things, a rearward movement of the centre of pressure of the wing and a decrease in wing downwash velocity at the tailplane both of which cause a nose down pitching moment. for a particular aircraft design only one of these may be significant in causing a tendency to dive, delta-winged aircraft with no foreplane or tailplane in the first case and, for example, the lockheed p-38 in the second case. alternatively, a particular design may have no significant tendency, for example the fokker f28 fellowship.as an aerofoil generating lift moves through the air, the air flowing over the top surface accelerates to a higher local speed than the air flowing over the bottom surface. when the aircraft speed reaches its critical mach number the accelerated airflow locally reaches the speed of sound and creates a small shock wave, even though the aircraft is still travelling below the speed of sound. the region in front of the shock wave generates high lift. as the aircraft itself flies faster, the shock wave over the wing gets stronger and moves rearwards, creating high lift further back along the wing. this rearward movement of lift causes the aircraft to tuck or pitch nose-down. the severity of mach tuck on any given design is affected by the thickness of the aerofoil, the sweep angle of the wing, and the location of the tailplane relative to the main wing.a tailplane which is positioned further aft can provide a larger stabilizing pitch-up moment. the camber and thickness of the aerofoil affect the critical mach number, with a more highly curved upper surface causing a lower critical mach number. on a swept wing the shock wave typically forms first at the wing root, especially if it is more cambered than the wing tip. as speed increases, the shock wave and associated lift extend outwards and, because the wing is swept, backwards. the changing airflow over the wing can reduce the downwash over a conventional tailplane, promoting a stronger nose-down pitching moment. another problem with a separate horizontal stabiliser is that it can itself achieve local supersonic flow with its own shock wave. this can affect the operation of a conventional elevator control surface. aircraft without enough elevator authority to maintain trim and fly level can enter a steep, sometimes unrecoverable dive.  until the aircraft is supersonic, the faster top shock wave can reduce the authority of the elevator and horizontal stabilizers.all transonic and supersonic aircraft experience mach tuck.   == recovery == recovery is sometimes impossible in subsonic aircraft; however, as an aircraft descends into lower, warmer, denser air, control authority (meaning the ability to control the aircraft) may return because drag tends to slow the aircraft while the speed of sound and control authority both increase. to prevent mach stall from progressing, the pilot should keep the airspeed below the type\'s critical mach number by reducing throttle, extending speed brakes, and if possible, extending the landing gear.   == design features == a number of design techniques are used to counter the effects of mach tuck. on both conventional tailplane and canard foreplane configurations, the horizontal stabiliser may be made large and powerful enough to correct the large trim changes associated with mach tuck. in place of the conventional elevator control surface, the whole stabiliser may be made moveable or ""all-flying"", sometimes called a stabilator. this both increases the authority of the stabilizer over a wider range of aircraft pitch, but also avoids the controllability issues associated with a separate elevator.aircraft that fly supersonic for long periods, such as concorde, may compensate for mach tuck by moving fuel between tanks in the fuselage to change the position of the centre of mass to match  the changing location of the centre of pressure, thereby minimizing the amount of aerodynamic trim required. a mach trimmer is a device which varies the pitch trim automatically as a function of mach number to oppose mach tuck and maintain level flight.   == history == the fastest world war ii fighters were the first aircraft to experience mach tuck.   their wings were not designed to counter mach tuck because research on supersonic airfoils was just beginning; areas of supersonic flow, together with shock waves and flow separation, were present on the wing. this condition was known at the time as compressibility burble and was known to exist on propeller tips at high aircraft speeds.the p-38 was the first 400 mph fighter, and it suffered more than the usual teething troubles. it had a thick, high-lift wing,  distinctive twin booms and a single, central nacelle containing the cockpit and armament.  it quickly accelerated to terminal velocity in a dive. the short stubby fuselage had a detrimental effect in reducing the critical mach number of the 15% thick wing center section with high velocities over the canopy adding to those on the upper surface of the wing. mach tuck occurred at speeds above mach 0.65; the air flow over the wing center section became transonic, causing a loss of lift. the resultant change in downwash at the tail caused a nose-down pitching moment and the dive to steepen (mach tuck). the aircraft was very stable in this condition making recovery from the dive very difficult. dive recovery (auxiliary) flaps were added to the underside of the wing (p-38j-lo) to increase the wing lift and downwash at the tail to allow recovery from transonic dives.   == references ==  this article incorporates public domain material from the united states government document: ""airplane flying handbook"". this article incorporates public domain material from the united states government document: ""pilot\'s handbook of aeronautical knowledge"".')"
35,"A parafoil is a nonrigid (textile) airfoil with an aerodynamic cell structure which is inflated by the wind. Ram-air inflation forces the parafoil into a classic wing cross-section. Parafoils are most commonly constructed out of ripstop nylon.
The device was developed in 1964 by Domina Jalbert (1904–1991). Jalbert had a history of designing kites and was involved in the development of hybrid balloon-kite aerial platforms for carrying scientific instruments. He envisaged the parafoil would be used to suspend an aerial platform or for the recovery of space equipment. A patent was granted in 1966. US patent 3285546 
Deployment shock prevented the parafoil's immediate acceptance as a parachute. It was not until the addition of a drag canopy on the riser lines (known as a ""slider"") which slowed their spread that the parafoil became a suitable parachute. Compared to a simple round canopy, a parafoil parachute has greater steerability, will glide further and allows greater control of the rate of descent; the parachute format is mechanically a glider of the free-flight kite type and such aspects spawned paraglider use.The air flow into the parafoil is coming more from below than the flight path might suggest, so the frontmost ropes tow against the airflow. When gliding, the angle of attack is lowered and the airflow meets the parafoil head on. This makes it difficult to achieve an optimum gliding angle without the parafoil deflating.
In 1984 Jalbert was awarded the Fédération Aéronautique Internationale (FAI) Gold Parachuting Medal for inventing the parafoil.Parafoils see wide use in a variety of windsports such as kite flying, powered parachutes, paragliding, kitesurfing, speed flying, wingsuit flying and skydiving. The world's largest kite is a parafoil-variant.Today, SpaceX uses steerable Parafoils to recover the Fairings of their Falcon 9 Rocket on two ships, GO Ms. Tree and GO Ms. Chief.


== Patents ==
U.S. Patent 3,285,546 Multi-cell wing type aerial device, filed October 1964, issued November 1966


== See also ==
Foil kite


== References ==","pandas(index=35, _1=35, text='a parafoil is a nonrigid (textile) airfoil with an aerodynamic cell structure which is inflated by the wind. ram-air inflation forces the parafoil into a classic wing cross-section. parafoils are most commonly constructed out of ripstop nylon. the device was developed in 1964 by domina jalbert (1904–1991). jalbert had a history of designing kites and was involved in the development of hybrid balloon-kite aerial platforms for carrying scientific instruments. he envisaged the parafoil would be used to suspend an aerial platform or for the recovery of space equipment. a patent was granted in 1966. us patent 3285546 deployment shock prevented the parafoil\'s immediate acceptance as a parachute. it was not until the addition of a drag canopy on the riser lines (known as a ""slider"") which slowed their spread that the parafoil became a suitable parachute. compared to a simple round canopy, a parafoil parachute has greater steerability, will glide further and allows greater control of the rate of descent; the parachute format is mechanically a glider of the free-flight kite type and such aspects spawned paraglider use.the air flow into the parafoil is coming more from below than the flight path might suggest, so the frontmost ropes tow against the airflow. when gliding, the angle of attack is lowered and the airflow meets the parafoil head on. this makes it difficult to achieve an optimum gliding angle without the parafoil deflating. in 1984 jalbert was awarded the fédération aéronautique internationale (fai) gold parachuting medal for inventing the parafoil.parafoils see wide use in a variety of windsports such as kite flying, powered parachutes, paragliding, kitesurfing, speed flying, wingsuit flying and skydiving. the world\'s largest kite is a parafoil-variant.today, spacex uses steerable parafoils to recover the fairings of their falcon 9 rocket on two ships, go ms. tree and go ms. chief.   == patents == u.s. patent 3,285,546 multi-cell wing type aerial device, filed october 1964, issued november 1966   == see also == foil kite   == references ==')"
36,"Rudder ratio refers to a value that is monitored by the computerized flight control systems in modern aircraft. The ratio relates the  aircraft airspeed to the rudder deflection setting that is in effect at the time. As an aircraft accelerates, the deflection of the rudder needs to be reduced proportionately within the range of the rudder pedal depression by the pilot. This automatic reduction process is needed because if the rudder is fully deflected when the aircraft is in high-speed flight, it will cause the plane to sharply and violently yaw, or swing from side to side, leading to loss of control and rudder, tail and other damages, even causing the aircraft to crash.


== See also ==
American Airlines Flight 587","pandas(index=36, _1=36, text='rudder ratio refers to a value that is monitored by the computerized flight control systems in modern aircraft. the ratio relates the  aircraft airspeed to the rudder deflection setting that is in effect at the time. as an aircraft accelerates, the deflection of the rudder needs to be reduced proportionately within the range of the rudder pedal depression by the pilot. this automatic reduction process is needed because if the rudder is fully deflected when the aircraft is in high-speed flight, it will cause the plane to sharply and violently yaw, or swing from side to side, leading to loss of control and rudder, tail and other damages, even causing the aircraft to crash.   == see also == american airlines flight 587')"
37,"In aerodynamics, the flight envelope, service envelope, or performance envelope of an aircraft or spacecraft refers to the capabilities of a design in terms of airspeed and load factor or atmospheric density, often simplified to altitude for Earth-borne aircraft. The term is somewhat loosely applied, and can also refer to other measurements such as manoeuvrability. When a plane is pushed, for instance by diving it at high speeds, it is said to be flown ""outside the envelope"", something considered rather dangerous.
Flight envelope is one of a number of related terms that are all used in a similar fashion. It is perhaps the most common term because it is the oldest, first being used in the early days of test flying. It is closely related to more modern terms known as extra power and a doghouse plot which are different ways of describing a flight envelope. In addition, the term has been widened in scope outside the field of engineering, to refer to the strict limits in which an event will take place or more generally to the predictable behaviour of a given phenomenon or situation, and hence, its ""flight envelope"".


== Extra power ==
Extra power, or specific excess power, is a very basic method of determining an aircraft's flight envelope. It is easily calculated, but as a downside does not tell very much about the actual performance of the aircraft at different altitudes. 
Choosing any particular set of parameters will generate the needed power for a particular aircraft for those conditions. For instance a Cessna 150 at 2,500-foot (760 m) altitude and 90-mile-per-hour (140 km/h) speed needs about 60 horsepower (45 kW) to fly straight and level. The C150 is normally equipped with a 100-horsepower (75 kW) engine, so in this particular case the plane has 40 horsepower (30 kW) of extra power. In overall terms this is very little extra power, 60% of the engine's output is already used up just keeping the plane in the air. The leftover 40 hp is all that the aircraft has to manoeuvre with, meaning it can climb, turn, or speed up only a small amount. To put this in perspective, the C150 could not maintain a 2g (20 m/s²) turn, which would require a minimum of 120 horsepower (89 kW) under the same conditions.
For the same conditions a fighter aircraft might require considerably more power due to their wings being designed for high speed, high agility, or both. It could require 10,000 horsepower (7.5 MW) to achieve similar performance. However modern jet engines can provide considerable power with the equivalent of 50,000 horsepower (37 MW) not being atypical. With this amount of extra power the aircraft can achieve very high maximum rate of climb, even climb straight up, make powerful continual manoeuvres, or fly at very high speeds.


== Doghouse plot ==

A doghouse plot generally shows the relation between speed at level flight and altitude, although other variables are also possible. It takes more effort to make than an extra power calculation, but in turn provides much more information such as ideal flight altitude. The plot typically looks something like an upside-down U and is commonly referred to as a doghouse plot due to its resemblance to a kennel (sometimes known as a 'doghouse' in American English). The diagram on the right shows a very simplified plot which shall be used to explain the general shape of the plot.
The outer edges of the diagram, the envelope, show the possible conditions that the aircraft can reach in straight and level flight. For instance, the aircraft described by the black altitude envelope on the right can fly at altitudes up to about 52,000 feet (16,000 m), at which point the thinner air means it can no longer climb. The aircraft can also fly at up to Mach 1.1 at sea level, but no faster. This outer surface of the curve represents the zero-extra-power condition. All of the area under the curve represents conditions that the plane can fly at with power to spare, for instance, this aircraft can fly at Mach 0.5 at 30,000 feet (9,100 m) while using less than full power.
In the case of high-performance aircraft, including fighters, this ""1-g"" line showing straight-and-level flight is augmented with additional lines showing the maximum performance at various g loadings. In the diagram at right, the green line represents, 2-g, the blue line 3-g, and so on. The F-16 Fighting Falcon has a very small area just below Mach 1 and close to sea level where it can maintain a 9-g turn.
Flying outside the envelope is possible, since it represents the straight-and-level condition only. For instance diving the aircraft allows higher speeds, using gravity as a source of additional power. Likewise higher altitude can be reached by first speeding up and then going ballistic, a manoeuvre known as a zoom climb.


=== Stalling speed ===
All fixed-wing aircraft have a minimum speed at which they can maintain level flight, the stall speed (left limit line in the diagram). As the aircraft gains altitude the stall speed increases; since the wing is not growing any larger the only way to support the aircraft's weight with less air is to increase speed. While the exact numbers will vary widely from aircraft to aircraft, the nature of this relationship is typically the same; plotted on a graph of speed (x-axis) vs. altitude (y-axis) it forms a diagonal line.


=== Service ceiling ===
Inefficiencies in the wings also make this line ""tilt over"" with increased altitude, until it becomes horizontal and no additional speed will result in increased altitude. This maximum altitude is known as the service ceiling (top limit line in the diagram), and is often quoted for aircraft performance. The area where the altitude for a given speed can no longer be increased at level flight is known as zero rate of climb and is caused by the lift of the aircraft getting smaller at higher altitudes, until it no longer exceeds gravity.


=== Top speed ===
The right side of the graph represents the maximum speed of the aircraft. This is typically sloped in the same manner as the stall line due to air resistance getting lower at higher altitudes, up to the point where an increase in altitude no longer increases the maximum speed due to lack of oxygen to feed the engines.
The power needed varies almost linearly with altitude, but the nature of drag means that it varies with the square of speed—in other words it is typically easier to go higher than faster, up to the altitude where lack of oxygen for the engines starts to play a significant role.


== Velocity vs. load factor chart ==

A chart of velocity versus load factor (or V-n diagram) is another way of showing limits of aircraft performance. It shows how much load factor can be safely achieved at different airspeeds.At higher temperatures, air is less dense and planes must fly faster to generate the same amount of lift. High heat may reduce the amount of cargo a plane can carry, increase the length of runway a plane needs to take off,
and make it more difficult to avoid obstacles such as mountains. In unusual weather conditions this may make it unsafe or uneconomical to fly, occasionally resulting in the cancellation of commercial flights.


== Sidenotes ==
Although it is easy to compare aircraft on simple numbers such as maximum speed or service ceiling, an examination of the flight envelope will reveal far more information. Generally a design with a larger area under the curve will have better all-around performance. This is because when the plane is not flying at the edges of the envelope, its extra power will be greater, and that means more power for things like climbing or manoeuvring. General aviation aircraft have very small flight envelopes, with speeds ranging from perhaps 50 to 200 mph, whereas the extra power available to modern fighter aircraft result in huge flight envelopes with many times the area. As a tradeoff however, military aircraft often have a higher stalling speed. As a result of this the landing speed is also higher.


== ""Pushing the envelope"" ==

This phrase is used to refer to an aircraft being taken to or beyond its designated altitude and speed limits. By extension, this phrase may be used to mean testing other limits, either within aerospace or in other fields e.g. Plus ultra (motto).


== See also ==
Coffin corner (aviation)
Manoeuvring speed
Helicopter height–velocity diagram
Küssner effect


== Notes ==","pandas(index=37, _1=37, text='in aerodynamics, the flight envelope, service envelope, or performance envelope of an aircraft or spacecraft refers to the capabilities of a design in terms of airspeed and load factor or atmospheric density, often simplified to altitude for earth-borne aircraft. the term is somewhat loosely applied, and can also refer to other measurements such as manoeuvrability. when a plane is pushed, for instance by diving it at high speeds, it is said to be flown ""outside the envelope"", something considered rather dangerous. flight envelope is one of a number of related terms that are all used in a similar fashion. it is perhaps the most common term because it is the oldest, first being used in the early days of test flying. it is closely related to more modern terms known as extra power and a doghouse plot which are different ways of describing a flight envelope. in addition, the term has been widened in scope outside the field of engineering, to refer to the strict limits in which an event will take place or more generally to the predictable behaviour of a given phenomenon or situation, and hence, its ""flight envelope"".   == extra power == extra power, or specific excess power, is a very basic method of determining an aircraft\'s flight envelope. it is easily calculated, but as a downside does not tell very much about the actual performance of the aircraft at different altitudes. choosing any particular set of parameters will generate the needed power for a particular aircraft for those conditions. for instance a cessna 150 at 2,500-foot (760 m) altitude and 90-mile-per-hour (140 km/h) speed needs about 60 horsepower (45 kw) to fly straight and level. the c150 is normally equipped with a 100-horsepower (75 kw) engine, so in this particular case the plane has 40 horsepower (30 kw) of extra power. in overall terms this is very little extra power, 60% of the engine\'s output is already used up just keeping the plane in the air. the leftover 40 hp is all that the aircraft has to manoeuvre with, meaning it can climb, turn, or speed up only a small amount. to put this in perspective, the c150 could not maintain a 2g (20 m/s²) turn, which would require a minimum of 120 horsepower (89 kw) under the same conditions. for the same conditions a fighter aircraft might require considerably more power due to their wings being designed for high speed, high agility, or both. it could require 10,000 horsepower (7.5 mw) to achieve similar performance. however modern jet engines can provide considerable power with the equivalent of 50,000 horsepower (37 mw) not being atypical. with this amount of extra power the aircraft can achieve very high maximum rate of climb, even climb straight up, make powerful continual manoeuvres, or fly at very high speeds.   == doghouse plot ==  a doghouse plot generally shows the relation between speed at level flight and altitude, although other variables are also possible. it takes more effort to make than an extra power calculation, but in turn provides much more information such as ideal flight altitude. the plot typically looks something like an upside-down u and is commonly referred to as a doghouse plot due to its resemblance to a kennel (sometimes known as a \'doghouse\' in american english). the diagram on the right shows a very simplified plot which shall be used to explain the general shape of the plot. the outer edges of the diagram, the envelope, show the possible conditions that the aircraft can reach in straight and level flight. for instance, the aircraft described by the black altitude envelope on the right can fly at altitudes up to about 52,000 feet (16,000 m), at which point the thinner air means it can no longer climb. the aircraft can also fly at up to mach 1.1 at sea level, but no faster. this outer surface of the curve represents the zero-extra-power condition. all of the area under the curve represents conditions that the plane can fly at with power to spare, for instance, this aircraft can fly at mach 0.5 at 30,000 feet (9,100 m) while using less than full power. in the case of high-performance aircraft, including fighters, this ""1-g"" line showing straight-and-level flight is augmented with additional lines showing the maximum performance at various g loadings. in the diagram at right, the green line represents, 2-g, the blue line 3-g, and so on. the f-16 fighting falcon has a very small area just below mach 1 and close to sea level where it can maintain a 9-g turn. flying outside the envelope is possible, since it represents the straight-and-level condition only. for instance diving the aircraft allows higher speeds, using gravity as a source of additional power. likewise higher altitude can be reached by first speeding up and then going ballistic, a manoeuvre known as a zoom climb. the right side of the graph represents the maximum speed of the aircraft. this is typically sloped in the same manner as the stall line due to air resistance getting lower at higher altitudes, up to the point where an increase in altitude no longer increases the maximum speed due to lack of oxygen to feed the engines. the power needed varies almost linearly with altitude, but the nature of drag means that it varies with the square of speed—in other words it is typically easier to go higher than faster, up to the altitude where lack of oxygen for the engines starts to play a significant role.   == velocity vs. load factor chart ==  a chart of velocity versus load factor (or v-n diagram) is another way of showing limits of aircraft performance. it shows how much load factor can be safely achieved at different airspeeds.at higher temperatures, air is less dense and planes must fly faster to generate the same amount of lift. high heat may reduce the amount of cargo a plane can carry, increase the length of runway a plane needs to take off, and make it more difficult to avoid obstacles such as mountains. in unusual weather conditions this may make it unsafe or uneconomical to fly, occasionally resulting in the cancellation of commercial flights.   == sidenotes == although it is easy to compare aircraft on simple numbers such as maximum speed or service ceiling, an examination of the flight envelope will reveal far more information. generally a design with a larger area under the curve will have better all-around performance. this is because when the plane is not flying at the edges of the envelope, its extra power will be greater, and that means more power for things like climbing or manoeuvring. general aviation aircraft have very small flight envelopes, with speeds ranging from perhaps 50 to 200 mph, whereas the extra power available to modern fighter aircraft result in huge flight envelopes with many times the area. as a tradeoff however, military aircraft often have a higher stalling speed. as a result of this the landing speed is also higher.   == ""pushing the envelope"" ==  this phrase is used to refer to an aircraft being taken to or beyond its designated altitude and speed limits. by extension, this phrase may be used to mean testing other limits, either within aerospace or in other fields e.g. plus ultra (motto).   == see also == coffin corner (aviation) manoeuvring speed helicopter height–velocity diagram küssner effect   == notes ==')"
38,"This is an alphabetical list of articles pertaining specifically to aerospace engineering. For a broad overview of engineering, see List of engineering topics. For biographies, see List of engineers.


== A ==
Ablative laser propulsion —
Absolute value —
Acceleration —
Action —
Advanced Space Vision System —
Aeroacoustics —
Aerobrake —
Aerobraking —
Aerocapture —
Aerodynamics —
Aeroelasticity —
Aeronautical abbreviations —
Aeronautics —
Aerospace engineering —
Aerospike engine —
Aerostat —
Aft-crossing trajectory —
Aileron —
Air-augmented rocket —
Aircraft —
Aircraft flight control systems —
Aircraft flight mechanics —
Airfoil —
Airlock —
Airship —
Alcubierre drive —
Angle of attack —
Angular momentum —
Angular velocity —
Antimatter rocket —
Apsis —
Arcjet rocket —
Areal velocity —
ARP4761 —
Aspect ratio (wing) —
Astrodynamics —
Atmospheric reentry —
Attitude control —
Avionics —
[1] —


== B ==
Balloon —
Ballute —
Beam-powered propulsion —
Bernoulli's equation —
Bi-elliptic transfer —
Big dumb booster —
Bipropellant rocket —
Bleed air —
Booster rocket —
Breakthrough Propulsion Physics Program —
Buoyancy —
Bussard ramjet —–


== C ==
Canard —
Centennial challenges —
Center of gravity —
Center of mass —
Center of pressure —
Chord —
Collimated light —
Compressibility —
Computational fluid dynamics —
Computing —
Control engineering —
Conservation of momentum —
Crew Exploration Vehicle —
Critical mach —
Centrifugal compressor —
Chevron nozzle —


== D ==
De Laval nozzle —
Deflection —
Delta-v —
Delta-v budget —
Density —
Derivative —
Digital Datcom —
Displacement (vector) —
DO-178B —
DO-254 —
Drag (physics) —
Drag coefficient —
Drag equation —
Dual mode propulsion rocket —
Delta wing—


== E ==
Earth's atmosphere —
Electrostatic ion thruster —
Elliptic partial differential equation —
Energy —
Engineering —
Engineering economics —
Enstrophy —
Equation of motion —
Euler angles —
European Space Agency —
Expander cycle (rocket) —


== F ==
Field Emission Electric Propulsion —
Fixed-wing aircraft —
Flight control surfaces —
Flight control system (aircraft) —
Flight control system (helicopter) —
Flight dynamics —
Floatstick —
Fluid —
Fluid dynamics —
Fluid mechanics —
Fluid statics —
Force —
Freefall —
Fuselage —
Future Air Navigation System —
Flying wing —


== G ==
Gas-generator cycle (rocket) —
Geostationary orbit —
Geosynchronous orbit—
Glide ratio —
GPS —
Gravitational constant —
Gravitational slingshot —
Gravity —
Gravity turn —
Guidance, navigation and control —
Guidance system —


== H ==
Hall effect thruster —
Heat shield —
Helicopter —
Hohmann transfer orbit —
Hybrid rocket —
Hydrodynamics —
Hydrostatics —
Hyperbolic partial differential equation —
Hypersonic —
HyShot —


== I ==
Impulse —
Inertial navigation system —
Instrument landing system —
Integral —
Internal combustion —
Interplanetary Transport Network —
Interplanetary travel —
Interstellar travel —
Ion thruster —
ISRO


== J ==
Jet engine —


== K ==
Kepler's laws of planetary motion —
Kessler syndrome —
Kestrel rocket engine —
Kinetic energy —
Kite —
Kutta condition —
Kutta–Joukowski theorem —


== L ==
Landing —
Landing gear —
Lagrangian —
Lagrangian point —
Laser broom —
Laser Camera System —
Latus rectum —
Launch window —
Law of universal gravitation —
Leading edge —
Lift —
Lift coefficient —
Lightcraft —
Lighter than air —
Liquid air cycle engine —
Liquid fuels —
Liquid rocket propellants —
Lithobraking —
Loiter —
Low Earth orbit —
Lunar space elevator —


== M ==
Mach number —
Magnetic sail —
Magnetoplasmadynamic thruster —
Mass —
Mass driver —
Mechanics of fluids —
Membrane mirror —
Metre per second —
Microwave landing system —
Mini-magnetospheric plasma propulsion —
Missile guidance —
Moment of inertia —
Momentum —
Momentum wheel —
Monopropellant rocket —
Motion —
Multistage rocket —


== N ==
Nanotechnology —
NASA —
Navier-Stokes equations —
Newton (unit) —
Newton's laws of motion —
Nose cone design —
Nozzle —


== O ==
Orbit —
Orbit phasing —
Orbiter Boom Sensor System —
Orbital elements —
Orbital inclination change —
Orbital maneuver —
Orbital node —
Orbital period —
Orbital stationkeeping —
Osculating orbit —


== P ==
Parallel axes rule —
Parasitic drag —
Parawing —
Perpendicular axes rule —
Physics —
Planetary orbit —
Plasma (physics) —
Plug nozzle —
Pogo oscillation —
Prandtl-Glauert singularity —
Precession —
Pressure —
Pressure altitude —
Pressure-fed engine —
Propeller —
Proper orbital elements —
Pulsed inductive thruster —
Pulsed plasma thruster —
Propulsion —


== Q ==


== R ==
Radar —
Railgun —
Ram accelerator —
Ramjet —
Reaction control system —
Reentry —
Reflection —
Relativistic rocket —
Remote Manipulator System —
Resistojet rocket —
Reusable launch system —
Reynolds number —
RL-10 (rocket engine) —
Rocket —
Rocket engine nozzle —
Rocket fuel —
Rocket launch —
Rudder —


== S ==
SABRE —
Satellite —
Saturn (rocket family) —
Scalar (physics) —
Schlieren —
Schlieren photography —
Scramjet —
Second moment of area —
Shock wave —
SI —
Single-stage to orbit —
Skyhook (structure) —
Stream function —
Streamline —
Solar panel —
Solar sail —
Solar thermal rocket —
Solid of revolution —
Solid rocket —
Sound barrier —
Space activity suit —
Space elevator —
Space fountain —
Space plane —
Space Shuttle —
Space Shuttle external tank —
Space Shuttle Main Engine —
Space station —
Space suit —
Space technology —
Space transport —
Spacecraft —
Spacecraft design —
Spacecraft propulsion —
Special relativity —
Specific impulse —
Speed of sound —
Staged combustion cycle (rocket) —
Subsonic —
Supersonic —
Surface of revolution —
Sweep theory —


== T ==
Tait–Bryan rotations —
Temperature —
Terminal velocity —
Test target —
Tether propulsion —
Thermal protection system —
Thermodynamics —
Thrust —
Thrust vector control —
Thruster —
Torricelli's equation —
Trajectory —
Trailing edge —
Trans Lunar Injection —
Transonic —
Transverse wave —
Tripropellant rocket —
Tsiolkovsky rocket equation —
Turbomachinery —
Two stage to orbit —


== U ==
UFO
UAV


== V ==
V-2 rocket —
Variable specific impulse magnetoplasma rocket —
Velocity —
Viscometer —
Viscosity —
Vortex generator —


== W ==
Wave drag —
Weight —
Weight function —
Wind tunnel —
Wing —
Woodward effect —
Wright Flyer —
Wright Glider of 1902 —


== X ==


== Y ==


== Z ==","pandas(index=38, _1=38, text=""this is an alphabetical list of articles pertaining specifically to aerospace engineering. for a broad overview of engineering, see list of engineering topics. for biographies, see list of engineers.   == a == ablative laser propulsion — absolute value — acceleration — action — advanced space vision system — aeroacoustics — aerobrake — aerobraking — aerocapture — aerodynamics — aeroelasticity — aeronautical abbreviations — aeronautics — aerospace engineering — aerospike engine — aerostat — aft-crossing trajectory — aileron — air-augmented rocket — aircraft — aircraft flight control systems — aircraft flight mechanics — airfoil — airlock — airship — alcubierre drive — angle of attack — angular momentum — angular velocity — antimatter rocket — apsis — arcjet rocket — areal velocity — arp4761 — aspect ratio (wing) — astrodynamics — atmospheric reentry — attitude control — avionics — [1] —   == b == balloon — ballute — beam-powered propulsion — bernoulli's equation — bi-elliptic transfer — big dumb booster — bipropellant rocket — bleed air — booster rocket — breakthrough propulsion physics program — buoyancy — bussard ramjet —–   == c == canard — centennial challenges — center of gravity — center of mass — center of pressure — chord — collimated light — compressibility — computational fluid dynamics — computing — control engineering — conservation of momentum — crew exploration vehicle — critical mach — centrifugal compressor — chevron nozzle —   == d == de laval nozzle — deflection — delta-v — delta-v budget — density — derivative — digital datcom — displacement (vector) — do-178b — do-254 — drag (physics) — drag coefficient — drag equation — dual mode propulsion rocket — delta wing—   == e == earth's atmosphere — electrostatic ion thruster — elliptic partial differential equation — energy — engineering — engineering economics — enstrophy — equation of motion — euler angles — european space agency — expander cycle (rocket) —   == f == field emission electric propulsion — fixed-wing aircraft — flight control surfaces — flight control system (aircraft) — flight control system (helicopter) — flight dynamics — floatstick — fluid — fluid dynamics — fluid mechanics — fluid statics — force — freefall — fuselage — future air navigation system — flying wing —   == g == gas-generator cycle (rocket) — geostationary orbit — geosynchronous orbit— glide ratio — gps — gravitational constant — gravitational slingshot — gravity — gravity turn — guidance, navigation and control — guidance system —   == h == hall effect thruster — heat shield — helicopter — hohmann transfer orbit — hybrid rocket — hydrodynamics — hydrostatics — hyperbolic partial differential equation — hypersonic — hyshot —   == i == impulse — inertial navigation system — instrument landing system — integral — internal combustion — interplanetary transport network — interplanetary travel — interstellar travel — ion thruster — isro   == j == jet engine —   == k == kepler's laws of planetary motion — kessler syndrome — kestrel rocket engine — kinetic energy — kite — kutta condition — kutta–joukowski theorem —   == l == landing — landing gear — lagrangian — lagrangian point — laser broom — laser camera system — latus rectum — launch window — law of universal gravitation — leading edge — lift — lift coefficient — lightcraft — lighter than air — liquid air cycle engine — liquid fuels — liquid rocket propellants — lithobraking — loiter — low earth orbit — lunar space elevator —   == m == mach number — magnetic sail — magnetoplasmadynamic thruster — mass — mass driver — mechanics of fluids — membrane mirror — metre per second — microwave landing system — mini-magnetospheric plasma propulsion — missile guidance — moment of inertia — momentum — momentum wheel — monopropellant rocket — motion — multistage rocket —   == n == nanotechnology — nasa — navier-stokes equations — newton (unit) — newton's laws of motion — nose cone design — nozzle —   == o == orbit — orbit phasing — orbiter boom sensor system — orbital elements — orbital inclination change — orbital maneuver — orbital node — orbital period — orbital stationkeeping — osculating orbit —   == p == parallel axes rule — parasitic drag — parawing — perpendicular axes rule — physics — planetary orbit — plasma (physics) — plug nozzle — pogo oscillation — prandtl-glauert singularity — precession — pressure — pressure altitude — pressure-fed engine — propeller — proper orbital elements — pulsed inductive thruster — pulsed plasma thruster — propulsion —   == q ==   == r == radar — railgun — ram accelerator — ramjet — reaction control system — reentry — reflection — relativistic rocket — remote manipulator system — resistojet rocket — reusable launch system — reynolds number — rl-10 (rocket engine) — rocket — rocket engine nozzle — rocket fuel — rocket launch — rudder —   == s == sabre — satellite — saturn (rocket family) — scalar (physics) — schlieren — schlieren photography — scramjet — second moment of area — shock wave — si — single-stage to orbit — skyhook (structure) — stream function — streamline — solar panel — solar sail — solar thermal rocket — solid of revolution — solid rocket — sound barrier — space activity suit — space elevator — space fountain — space plane — space shuttle — space shuttle external tank — space shuttle main engine — space station — space suit — space technology — space transport — spacecraft — spacecraft design — spacecraft propulsion — special relativity — specific impulse — speed of sound — staged combustion cycle (rocket) — subsonic — supersonic — surface of revolution — sweep theory —   == t == tait–bryan rotations — temperature — terminal velocity — test target — tether propulsion — thermal protection system — thermodynamics — thrust — thrust vector control — thruster — torricelli's equation — trajectory — trailing edge — trans lunar injection — transonic — transverse wave — tripropellant rocket — tsiolkovsky rocket equation — turbomachinery — two stage to orbit —   == u == ufo uav   == v == v-2 rocket — variable specific impulse magnetoplasma rocket — velocity — viscometer — viscosity — vortex generator —   == w == wave drag — weight — weight function — wind tunnel — wing — woodward effect — wright flyer — wright glider of 1902 —   == x ==   == y ==   == z =="")"
39,"Corrected Flow is the mass flow that would pass through a device (e.g. compressor, bypass duct, etc.) if the inlet pressure and temperature corresponded to ambient conditions at Sea Level, on a Standard Day (e.g. 101.325 kPa, 288.15 K).
Corrected Flow, 
  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }}
  , can be calculated as follows, assuming Imperial Units:

  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        =
        w
        
          
            T
            
              /
            
            518.67
          
        
        
          /
        
        (
        P
        
          /
        
        14.696
        )
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }=w{\sqrt {T/518.67}}/(P/14.696)}
  
Corrected Flow is often given the symbol 
  
    
      
        w
        c
      
    
    {\displaystyle wc}
   or 
  
    
      
        w
        r
      
    
    {\displaystyle wr}
   (for referred flow).
So-called Non-Dimensional Flow, 
  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}}
  , is proportional to Corrected Flow:

  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
        =
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        ∗
        
          
            518.67
          
        
        
          /
        
        
          14.696
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}=w{\sqrt {\theta }}/{\delta }*{\sqrt {518.67}}/{14.696}}
  
The equivalent equations for Preferred SI Units are: (101.325kPa, 288.15K)

  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        =
        w
        
          
            T
            
              /
            
            288.15
          
        
        
          /
        
        (
        P
        
          /
        
        101.325
        )
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }=w{\sqrt {T/288.15}}/(P/101.325)}
  

  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
        =
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        ∗
        
          
            288.15
          
        
        
          /
        
        
          101.325
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}=w{\sqrt {\theta }}/{\delta }*{\sqrt {288.15}}/{101.325}}
  
Nomenclature:

  
    
      
        P
      
    
    {\displaystyle P}
   Stagnation (or Total) Pressure (in kPa)

  
    
      
        T
      
    
    {\displaystyle T}
   Stagnation (or Total) Temperature (in K)

  
    
      
        w
      
    
    {\displaystyle w}
   Real Mass Flow

  
    
      
        
          δ
        
      
    
    {\displaystyle {\delta }}
   Referred Pressure

  
    
      
        
          θ
        
      
    
    {\displaystyle {\theta }}
   Referred TemperatureIn relative form, Corrected Flow, Referred Flow and Non-Dimensional Flow are all measures of axial Mach number.
Side note:
If the mass flow can be considered an energy source such as fuel flow, the corrected flow is calculated as follows:

  
    
      
        w
        
          /
        
        (
        δ
        ∗
        
          
            θ
          
        
        )
      
    
    {\displaystyle w/(\delta *{\sqrt {\theta }})}
  
Note: 
The source of coefficients is coming out from the fact that rotating components will in fact change the fluid properties, because of vibrations (compressibility of the flow change). 
So if one wants to correct the fuel flow then a theta correction exponent should be found through iterations, but for corrections for flow at engine entry (W2) then this aspect on rotating components vibration is not accounted because much smaller than one inside an engine. As a consequence the inlet flow is corrected just by square root of Theta and divided by delta.  


== See also ==
Compressor map
Turbine map
Corrected speed","pandas(index=39, _1=39, text='corrected flow is the mass flow that would pass through a device (e.g. compressor, bypass duct, etc.) if the inlet pressure and temperature corresponded to ambient conditions at sea level, on a standard day (e.g. 101.325 kpa, 288.15 k). corrected flow,    w   θ    /   δ      note: the source of coefficients is coming out from the fact that rotating components will in fact change the fluid properties, because of vibrations (compressibility of the flow change). so if one wants to correct the fuel flow then a theta correction exponent should be found through iterations, but for corrections for flow at engine entry (w2) then this aspect on rotating components vibration is not accounted because much smaller than one inside an engine. as a consequence the inlet flow is corrected just by square root of theta and divided by delta.   == see also == compressor map turbine map corrected speed')"
40,"An arming plug is a small plug that is fitted into flight hardware to enable functions that, for instrument or personnel safety, should not be activated before flight.  In the case of a missile or bomb, the (lack of the) arming plug prevents explosion before flight; in the case of a spacecraft or scientific sounding rocket, it might prevent premature firing of a hydrazine thruster system (hydrazine is extremely toxic) or block cryogenic or photographic film systems from operating before launch.","pandas(index=40, _1=40, text='an arming plug is a small plug that is fitted into flight hardware to enable functions that, for instrument or personnel safety, should not be activated before flight.  in the case of a missile or bomb, the (lack of the) arming plug prevents explosion before flight; in the case of a spacecraft or scientific sounding rocket, it might prevent premature firing of a hydrazine thruster system (hydrazine is extremely toxic) or block cryogenic or photographic film systems from operating before launch.')"
41,"Corrected speed is the speed a component would rotate at if the inlet temperature corresponded to ambient conditions at sea level, on a standard day (i.e. 288.15 K).
Corrected speed 
  
    
      
        N
        
          /
        
        
          
            θ
          
        
      
    
    {\displaystyle N/{\sqrt {\theta }}}
   can be calculated as follows:

  
    
      
        N
        
          /
        
        
          
            θ
          
        
        =
        N
        
          /
        
        
          
            T
            
              /
            
            288.15
          
        
        .
      
    
    {\displaystyle N/{\sqrt {\theta }}=N/{\sqrt {T/288.15}}.}
  Corrected speed is often abbreviated to 
  
    
      
        
          N
          
            c
          
        
      
    
    {\displaystyle N_{c}}
   or 
  
    
      
        
          N
          
            r
          
        
      
    
    {\displaystyle N_{r}}
   (for referred speed).
So-called non-dimensional speed 
  
    
      
        N
        
          /
        
        
          
            T
          
        
      
    
    {\displaystyle N/{\sqrt {T}}}
   is proportional to corrected speed:

  
    
      
        N
        
          /
        
        
          
            T
          
        
        =
        (
        N
        
          /
        
        
          
            θ
          
        
        )
        
          /
        
        
          
            288.15
          
        
      
    
    {\displaystyle N/{\sqrt {T}}=(N/{\sqrt {\theta }})/{\sqrt {288.15}}}
  Nomenclature:

  
    
      
        T
      
    
    {\displaystyle T}
   – stagnation (or total) temperature (in kelvins),

  
    
      
        N
      
    
    {\displaystyle N}
   – real shaft speed,

  
    
      
        θ
      
    
    {\displaystyle \theta }
   – referred temperature.In relative form, corrected speed, referred speed and non-dimensional speed are all measures of peripheral Mach number.


== See also ==
Compressor map
Turbine map
Corrected flow","pandas(index=41, _1=41, text='corrected speed is the speed a component would rotate at if the inlet temperature corresponded to ambient conditions at sea level, on a standard day (i.e. 288.15 k). corrected speed    n  /    θ      – referred temperature.in relative form, corrected speed, referred speed and non-dimensional speed are all measures of peripheral mach number.   == see also == compressor map turbine map corrected flow')"
42,"A constant speed drive (CSD) is a type of transmission that takes an input shaft rotating at a wide range of speeds, delivering this power to an output shaft that rotates at a constant speed, despite the varying input. They are used to drive mechanisms, typically electrical generators, that require a constant input speed.
The term is most commonly applied to hydraulic transmissions found on the accessory drives of gas turbine engines, such as aircraft jet engines. On modern aircraft, the CSD is often combined with a generator into a single unit known as an integrated drive generator (IDG).


== Mechanism ==
CSDs are mainly used on airliner and military aircraft jet engines to drive the alternating current (AC) electrical generator. In order to produce the proper voltage at a constant AC frequency, usually three-phase 115 VAC at 400 Hz, an alternator needs to spin at a constant specific speed (typically 6,000 RPM for air-cooled generators). Since the jet engine gearbox speed varies from idle to full power, this creates the need for a constant speed drive (CSD).  The CSD takes the variable speed output of the accessory drive gearbox and hydro-mechanically produces a constant output RPM.Different systems have been used to control the alternator speed. Modern designs are mostly hydrokinetic, but early designs often took advantage of the bleed air available from the engines. Some of these were mostly mechanically powered, with an air turbine to provide a vernier speed adjustment. Others were purely turbine-driven.


== Integrated drive generator ==
On aircraft such as the Airbus A310, Airbus A320 family, Airbus A320neo, Airbus A330, Airbus A330neo, Airbus A340, Boeing 737 Next Generation, 747, 757, 767 and 777, an integrated drive generator (IDG) is used. This unit is simply a CSD and an oil cooled generator inside the same case. Troubleshooting is simplified as this unit is the line-replaceable electrical generation unit on the engine.


== Manufacturers ==
Collins Aerospace (formerly UTC Aerospace Systems (formerly Hamilton Sundstrand)) is an American manufacturer of CSD and IDG units.


== Alternatives ==
A variable-speed constant-frequency (VSCF) generator can be used to provide AC power using an electronic tap converter.
Variable Frequency Starter Generator (VFSG) used primarily on the Boeing 787 are both used for electric start and electric generation.


== See also ==
Centrifugal governor
Continuously variable transmission


== References ==","pandas(index=42, _1=42, text='a constant speed drive (csd) is a type of transmission that takes an input shaft rotating at a wide range of speeds, delivering this power to an output shaft that rotates at a constant speed, despite the varying input. they are used to drive mechanisms, typically electrical generators, that require a constant input speed. the term is most commonly applied to hydraulic transmissions found on the accessory drives of gas turbine engines, such as aircraft jet engines. on modern aircraft, the csd is often combined with a generator into a single unit known as an integrated drive generator (idg).   == mechanism == csds are mainly used on airliner and military aircraft jet engines to drive the alternating current (ac) electrical generator. in order to produce the proper voltage at a constant ac frequency, usually three-phase 115 vac at 400 hz, an alternator needs to spin at a constant specific speed (typically 6,000 rpm for air-cooled generators). since the jet engine gearbox speed varies from idle to full power, this creates the need for a constant speed drive (csd).  the csd takes the variable speed output of the accessory drive gearbox and hydro-mechanically produces a constant output rpm.different systems have been used to control the alternator speed. modern designs are mostly hydrokinetic, but early designs often took advantage of the bleed air available from the engines. some of these were mostly mechanically powered, with an air turbine to provide a vernier speed adjustment. others were purely turbine-driven.   == integrated drive generator == on aircraft such as the airbus a310, airbus a320 family, airbus a320neo, airbus a330, airbus a330neo, airbus a340, boeing 737 next generation, 747, 757, 767 and 777, an integrated drive generator (idg) is used. this unit is simply a csd and an oil cooled generator inside the same case. troubleshooting is simplified as this unit is the line-replaceable electrical generation unit on the engine.   == manufacturers == collins aerospace (formerly utc aerospace systems (formerly hamilton sundstrand)) is an american manufacturer of csd and idg units.   == alternatives == a variable-speed constant-frequency (vscf) generator can be used to provide ac power using an electronic tap converter. variable frequency starter generator (vfsg) used primarily on the boeing 787 are both used for electric start and electric generation.   == see also == centrifugal governor continuously variable transmission   == references ==')"
43,"GIOVE ([ˈdʒɔːve]), or Galileo In-Orbit Validation Element, is the name for two satellites built for the European Space Agency (ESA) to test technology in orbit for the Galileo positioning system.Giove is the Italian word for ""Jupiter"". The name was chosen as a tribute to Galileo Galilei, who discovered the first four natural satellites of Jupiter, and later discovered that they could be used as a universal clock to obtain the longitude of a point on the Earth's surface.
The GIOVE satellites are operated by the GIOVE Mission  (GIOVE-M) segment in the frame of the risk mitigation for the In Orbit Validation (IOV) of the Galileo positioning system.


== Purpose ==
These validation satellites were previously known as the Galileo System Testbed (GSTB) version 2 (GSTB-V2). In 2004 the Galileo System Test Bed Version 1 (GSTB-V1) project validated the on-ground algorithms for Orbit Determination and Time Synchronization (OD&TS). This project, led by ESA and European Satellite Navigation Industries, has provided industry with fundamental knowledge to develop the mission segment of the Galileo positioning system.GIOVE satellites transmitted multifrequency ranging signals equivalent to the signals of future Galileo: L1BC, L1A, E6BC, E6A, E5a, E5b. The main purpose of the GIOVE mission was to test and validate the reception and performance of novel code modulations designed for Galileo including new signals based on the use of the BOC (Binary Offset Carrier) technique, in particular the high-performance E5AltBOC signal.


== Satellites ==


=== GIOVE-A ===
Previously known as GSTB-V2/A, this satellite was constructed by Surrey Satellite Technology Ltd (SSTL).
Its mission has the main goal of claiming the frequencies allocated to Galileo by the ITU. It has two independently developed Galileo signal generation chains and also tests the design of two on-board rubidium atomic clocks and the orbital characteristics of the intermediate circular orbit for future satellites.
GIOVE-A is the first spacecraft whose design is based upon SSTL's new Geostationary Minisatellite Platform (GMP) satellite bus, intended for geostationary orbit. GIOVE-A is also SSTL's first satellite outside low Earth orbit, operating in medium Earth orbit), and is SSTL's first satellite to use deployable Sun-tracking solar arrays. Previous SSTL satellites use body-mounted solar arrays, which generate less power per unit area as they do not face the Sun directly.


==== Launched on 28 December 2005 ====
It was launched at 05:19 UTC on December 28, 2005 on a Soyuz-FG/Fregat from the Baikonur Cosmodrome in Kazakhstan.


==== First Galileo transmissions ====
It began communicating as planned at 09:01 UTC while circling the Earth at a height of 23,222 km. The satellite successfully transmitted its first navigation signals at 17:25 GMT on 12 January 2006. These signals were received at Chilbolton Observatory in Hampshire, UK and the ESA Station at Redu in Belgium. Teams from SSTL and ESA have measured the signal generated by GIOVE-A to ensure it meets the frequency-filing allocation and reservation requirements for the International Telecommunication Union (ITU), a process that was required to be complete by June 2006.


==== Technical details ====
The GIOVE-A signal in space is fully representative of the Galileo signal from the point of view of frequencies and modulations, chip rates, and data rates. However, GIOVE-A can only transmit at two frequency bands at a time (i.e., L1+E5 or L1+E6).
GIOVE-A codes are different from Galileo codes. The GIOVE-A navigation message is not representative from the structure and contents viewpoint (demonstration only purpose). The generation of pseudorange measurements and detailed analysis of the tracking noise and multipath performance of GIOVE-A ranging signals have been performed with the use of the GETR (Galileo Experimental Test Receiver) designed by Septentrio.There has been some public controversy about the open source nature of some of the Pseudo-Random Noise (PRN) codes.  In the early part of 2006, researchers at Cornell monitored the GIOVE-A signal and extracted the PRN codes. The methods used and the codes which were found were published in the June 2006 issue of GPS World. ESA has now made the codes public.


==== Retirement ====
GIOVE A was retired (but not decommissioned) in 30 June 2012, after being raised in altitude to make way for an operational satellite. It remains under command by SSTL.


=== GIOVE-B ===
GIOVE-B (previously called GSTB-V2/B), has a similar mission, but has greatly improved signal generation hardware.
It was originally built by satellite consortium European Satellite Navigation Industries, but following re-organization of the project in 2007, the satellite prime contractor responsibility was passed to Astrium.
GIOVE-B also has MEO environment characterization objectives, as well as signal-in-space and receiver experimentation objectives. GIOVE-B carries three atomic clocks: two rubidium standards and the first space-qualified passive hydrogen maser.


==== Launched on 27 April 2008 ====

The launch was delayed due to various technical problems, and took place on 27 April 2008 at 04:16 Baikonur time (22:16 UTC Saturday) aboard a Soyuz-FG/Fregat rocket provided by Starsem. The Fregat stage was ignited three times to place the satellite into orbit. Giove-B reached its projected orbit after 02:00 UTC and successfully deployed its solar panels.


==== First Galileo navigation transmissions ====
GIOVE-B started transmitting navigation signals on May 7, 2008. The reception of the signals by GETR receivers and other means has been confirmed at a few ESA facilities.


==== Technical details ====
According to ESA, this is ""a truly historic step for satellite navigation since GIOVE-B is now, for the first time, transmitting the GPS-Galileo common signal using a specific optimised waveform, MBOC (multiplexed binary offset carrier), in accordance with the agreement drawn up in July 2007 by the EU and the US for their respective systems, Galileo and the future GPS III"".
“Now with GIOVE-B broadcasting its highly accurate signal in space we have a true representation of what Galileo will offer to provide the most advanced satellite positioning services, while ensuring compatibility and interoperability with GPS”, said ESA Galileo Project Manager, Javier Benedicto.
After launch, early orbit operations and platform commissioning, GIOVE-B's navigation payload was switched on and signal transmission commenced on May 7 and the quality of these signals is now being checked. Several facilities are involved in this process, including the GIOVE-B Control Centre at Telespazio's facilities in Fucino, Italy, the Galileo Processing Centre at ESA's European Space Research and Technology Centre (ESTEC), in the Netherlands, the ESA ground station at Redu, Belgium, and the Rutherford Appleton Laboratory (RAL) Chilbolton Observatory in the United Kingdom.
Chilbolton's 25-metre antenna makes it possible to analyse the characteristics of GIOVE-B signals with great accuracy and verify that they conform to the Galileo system's design specification. Each time the satellite is visible from Redu and Chilbolton, the large antennas are activated and track the satellite. GIOVE-B is orbiting at an altitude of 23 173 kilometres, making a complete journey around the Earth in 14 hours and 3 minutes.
The quality of the signals transmitted by GIOVE-B will have an important influence on the accuracy of the positioning information that will be provided by the user receivers on the ground. On board, GIOVE-B carries a passive hydrogen maser atomic clock, which is expected to deliver unprecedented stability performance.
The signal quality can be affected by the environment of the satellite in its orbit and by the propagation path of the signals travelling from space to ground. Additionally, the satellite signals must not create interference with services operating in adjacent frequency bands, and this is also being checked.
Galileo teams within ESA and industry have the means to observe and record the spectrum of the signals transmitted by GIOVE-B in real time. Several measurements are performed relating to transmitted signal power, centre frequency and bandwidth, as well as the format of the navigation signals generated on board. This allows the analysis of the satellite transmissions in the three frequency bands reserved for it.
The GIOVE-B mission also represents an opportunity for validating in-orbit critical satellite technologies, characterising the Medium Earth Orbit (MEO) radiation environment, and to test a key element of the future Galileo system - the user receivers.


==== Retirement ====
GIOVE B was retired (but not decommissioned) in 23 July 2012.


=== GIOVE-A2 ===
With the delays of GIOVE-B, the European Space Agency again contracted with SSTL for a second satellite, to ensure that the Galileo programme continues without any interruptions that could lead to loss of frequency allocations. Construction of GIOVE-A2 was terminated due to the successful launch and in-orbit operation of GIOVE-B.


== Mission segment ==
The GIOVE Mission segment, or GIOVE-M, is the name of a project dedicated to the exploitation and experimentation of the GIOVE satellites. The GIOVE Mission was intended to ensure risk mitigation of the In Orbit Validation (IOV) phase of the Galileo positioning system.


== GIOVE Mission history ==
The GIOVE Mission Segment began in October 2005 with the purpose of providing experimental results based on real data to be used for risk mitigation throughout the overall Galileo In Orbit Validation (IOV) phase of the Galileo positioning system. 
The GIOVE Mission segment infrastructure was based on evolution of the Galileo System Test Bed Version 1 (GSTB-V1) infrastructure conceived to process data from the GIOVE-A and GIOVE-B satellites. The GIOVE Mission segment was composed of a central processing facility called the Giove Processing Center (GPC) and a network of thirteen experimental Giove Sensor Stations (GESS).
The main objectives of the GIOVE Mission Segment experimentation were in the areas of:

On-board clock characterisation
Navigation message generation
Orbit modelling


== References ==


== External links ==
ESA GIOVE-B launch pages
GIOVE Mission Processing Centre website
eoPortal description of GIOVE
blog of GIOVE-A launch and press releases from Ballard Communications Management, used by SSTL.
Technical papers on GIOVE-A and B missions
GIOVE Mission Processing Centre - Website
eoPortal description of GIOVE","pandas(index=43, _1=43, text='giove ([ˈdʒɔːve]), or galileo in-orbit validation element, is the name for two satellites built for the european space agency (esa) to test technology in orbit for the galileo positioning system.giove is the italian word for ""jupiter"". the name was chosen as a tribute to galileo galilei, who discovered the first four natural satellites of jupiter, and later discovered that they could be used as a universal clock to obtain the longitude of a point on the earth\'s surface. the giove satellites are operated by the giove mission  (giove-m) segment in the frame of the risk mitigation for the in orbit validation (iov) of the galileo positioning system.   == purpose == these validation satellites were previously known as the galileo system testbed (gstb) version 2 (gstb-v2). in 2004 the galileo system test bed version 1 (gstb-v1) project validated the on-ground algorithms for orbit determination and time synchronization (od&ts). this project, led by esa and european satellite navigation industries, has provided industry with fundamental knowledge to develop the mission segment of the galileo positioning system.giove satellites transmitted multifrequency ranging signals equivalent to the signals of future galileo: l1bc, l1a, e6bc, e6a, e5a, e5b. the main purpose of the giove mission was to test and validate the reception and performance of novel code modulations designed for galileo including new signals based on the use of the boc (binary offset carrier) technique, in particular the high-performance e5altboc signal.   == satellites == with the delays of giove-b, the european space agency again contracted with sstl for a second satellite, to ensure that the galileo programme continues without any interruptions that could lead to loss of frequency allocations. construction of giove-a2 was terminated due to the successful launch and in-orbit operation of giove-b.   == mission segment == the giove mission segment, or giove-m, is the name of a project dedicated to the exploitation and experimentation of the giove satellites. the giove mission was intended to ensure risk mitigation of the in orbit validation (iov) phase of the galileo positioning system.   == giove mission history == the giove mission segment began in october 2005 with the purpose of providing experimental results based on real data to be used for risk mitigation throughout the overall galileo in orbit validation (iov) phase of the galileo positioning system. the giove mission segment infrastructure was based on evolution of the galileo system test bed version 1 (gstb-v1) infrastructure conceived to process data from the giove-a and giove-b satellites. the giove mission segment was composed of a central processing facility called the giove processing center (gpc) and a network of thirteen experimental giove sensor stations (gess). the main objectives of the giove mission segment experimentation were in the areas of:  on-board clock characterisation navigation message generation orbit modelling   == references ==   == external links == esa giove-b launch pages giove mission processing centre website eoportal description of giove blog of giove-a launch and press releases from ballard communications management, used by sstl. technical papers on giove-a and b missions giove mission processing centre - website eoportal description of giove')"
44,"A pilot chute is a small auxiliary parachute used to deploy the main or reserve parachute. The pilot chute is connected to the deployment bag containing the parachute by a bridle. Pilot chutes are a critical component of all modern skydiving and BASE jumping gear. Pilot chutes are also used as a component of spacecraft such as NASA's Orion.


== Deployment methods ==


=== Spring-loaded ===
The spring-loaded pilot chute is used in conjunction with a ripcord. When the user pulls the ripcord, the container opens, allowing the pilot chute compressed inside and loaded with a large spring inside it to jump out. Spring-loaded pilot chutes are mainly used to deploy reserve parachutes.  They are often also used to deploy the main parachute on skydiving students' parachute equipment. They are also commonly used in drogue parachute in cars or in planes such as the B52 Bomber.


=== Pull-out ===
The pull-out and throw-out pilot chutes are identical in construction; the difference is in their connection to the handle and the bridle, and in the way they are packed.
With the pull-out system, the pilot chute is packed inside the container. The activation handle is attached to a lanyard, which in turn is attached to the closing pin. The lanyard is also attached to base of the pilot chute, at the point of connection to the bridle. When the user pulls the handle, the closing pin is pulled, opening the container. Continuing the pull, the user pulls the pilot chute out of the container and into the airstream, at which point the pilot chute inflates and pulls the main parachute out of the container.


=== Throw-out ===
The throw-out pilot chute is the most popular type in use today. The pilot chute is packed in a pouch at the bottom of the container (often called BOC for short). The handle is attached to the apex of the pilot chute. When the user grabs the handle and throws the pilot chute into the airstream, the bridle extends, pulling the closing pin and opening the container, as the pilot chute continues in the airstream it extracts the deployment bag containing the main parachute from the container.  The pull-out pilot chute and the throw-out pilot chute were both invented by Bill Booth.


=== Drogues ===
Drogues used on tandem-systems are basically large throw-out pilot chutes, but the bridle is anchored on the container with a release system. When the user throws the drogue, the drogue inflates and the bridle extends. The deployed drogue slows down the free-fall speed of the tandem pair. When the user wants to open the parachute, he pulls a ripcord, releasing the bridle and allowing the drogue to open the main container.


== Types ==


=== Collapsible ===
With the advent of smaller higher performance canopies, the drag induced by trailing a pilot chute behind a canopy has become a significant concern. To reduce this drag some pilot chute designs of the Pull-out and Throw-out variety are collapsible. Once deployment of the parachute has occurred a kill line running up the center of the pilot chute bridle becomes loaded. This kill line pulls down on the apex of the pilot chute collapsing it and greatly reducing its drag on the canopy.Some designs replace the kill line with a fixed length of shock cord, which stretches when the pilot chute is moving quickly, allowing it to inflate. When the pilot slows down (after opening a canopy, for example) the shock cord retracts, killing the pilot chute. While this avoids the possibility of pilot-in-tow malfunction due to an un-cocked pilot, it has the disadvantage of requiring significant airspeed to operate. This could cause a delayed deployment if used for a BASE or balloon jump, or any other jump with a low speed deployment. This type may also begin to re-inflate behind a highly loaded, fast moving canopy, negating the usefulness of a collapsible pilot chute.


=== Vented ===
Pilot chutes for BASE jumping gear are typically larger than skydiving pilot chutes, and often include air vents on the surface. Research on the development of early round parachutes showed that vents can increase stability and reduce oscillation of the parachute. BASE jumpers often use pilot chutes with either apex vents, or ring vents.


== References ==","pandas(index=44, _1=44, text=""a pilot chute is a small auxiliary parachute used to deploy the main or reserve parachute. the pilot chute is connected to the deployment bag containing the parachute by a bridle. pilot chutes are a critical component of all modern skydiving and base jumping gear. pilot chutes are also used as a component of spacecraft such as nasa's orion.   == deployment methods == pilot chutes for base jumping gear are typically larger than skydiving pilot chutes, and often include air vents on the surface. research on the development of early round parachutes showed that vents can increase stability and reduce oscillation of the parachute. base jumpers often use pilot chutes with either apex vents, or ring vents.   == references =="")"
45,"Aerospace architecture is broadly defined to encompass architectural design of non-habitable and habitable structures and living and working environments in aerospace-related facilities, habitats, and vehicles. These environments include, but are not limited to: science platform aircraft and aircraft-deployable systems; space vehicles, space stations, habitats and lunar and planetary surface construction bases; and Earth-based control, experiment, launch, logistics, payload, simulation and test facilities. Earth analogs to space applications may include Antarctic, desert, high altitude, underground, undersea environments and closed ecological systems.
The American Institute of Aeronautics and Astronautics (AIAA) Design Engineering Technical Committee (DETC) meets several times a year to discuss policy, education, standards, and practice issues pertaining to aerospace architecture. See http://www.spacearchitect.org/


== The role of Appearance in Aerospace architecture ==
""The role of design creates and develops concepts and specifications that seek to simultaneously and synergistically optimize function, production, value and appearance."" In connection with, and with respect to, human presence and interactions, appearance is a component of human factors and includes considerations of human characteristics, needs and interests.
Appearance in this context refers to all visual aspects – the statics and dynamics of form(s), color(s), patterns, and textures in respect to all products, systems, services, and experiences. Appearance/esthetics affects humans both psychologically and physiologically and can effect/improving both human efficiency, attitude, and well-being. 
In reference to non-habitable design the influence of appearance is minimal if not non-existent. However, as the industry of aerospace continues to rapidly grow, and missions to put humans on Mars and back to the Moon are being announced. The role that appearance/esthetics to maintain crew well-being and health of multi-month or year missions becomes a monumental factor in mission success. 


== Habitable Structures within Earth's Atmosphere ==


=== Appearance/esthetics ===
Appearance/esthetics in aerospace design must at least co-exist, if not be synergistic, with the overall/societal fundamentals/metrics of aerospace engineering design. These metrics, for atmospheric flight consist of overall/societal factors directed toward productivity, safety, environmental issues such as noise/emissions and accessibly/ affordability. Furthermore, technological parameters such as space, weight and drag minimization and propulsion efficiency highly dictate and restrain the boundaries of appearance/esthetic design. Major factors that need to be considered in atmospheric flight design include producability, maintainability, reliability, flyability, inspectability, flexibility, repairability, operability, durability, and airport compatibility. 


== Habitable Structures outside of Low-Earth Orbit (LEO) ==
What is different concerning space in reference to human-centered design thinking is the nearly complete lack of human presence. Human-centered design influence wholly operates within the context of human interactions; how operations/ missions are ran (operability) or how products, systems, services, or experiences (PSSE’s) affect end users (usability). Currently the human presence involves the space station and the relatively few international rocket systems.


=== Human-Centered Design ===
Due to the large space boom and technological advancements, over the past decade numerous countries and companies have released statements that human expeditions to our solar system are far from done. With long duration confinement in limited interior space in micro-g with little-to-no real variability in environment, attention towards user [crew] subjects well-being, and mental alertness will pose complex human-centered design issues. Mars transit vehicles and surface habitats will constitute highly confined, technical settings characterized by social, emotional and physical deprivation while affording little opportunity to experience privacy and environmental variation. And esthetic/appearance measures for human exploration will emphasize upon “naturalistic countermeasures” to the innate/multitudinous stresses of such expeditions.
Although human wants, needs, and limitations both physically and mentally need to be evaluated and address when designing for space. Design decisions must at least co-exist, if not be synergistic, with the overall metrics of aerospace engineering design. Ex. The International Space Station Toilet. 


=== Past Examples ===
A study conducted in 1989 (reference 2) found that when given multiple photographs and paintings as potential decoration of the international space station. Test (crew) subjects all individually preferred those with naturalistic, irrespective themes, and a large depth of field. Other examples of human-centered design is using pastel paints on the International Space Station (ISS) to contrast and provide “up/down” cues in micro-g environments or  the concept of dynamically and spatially adjusting lighting color and intensities to conform to daily and even seasonal biorhythms similar to earth to mitigate the societal separation effects experienced in space.


== See also ==
Airborne observatory
Atmosphere of Venus
High Altitude Venus Operational Concept (HAVOC)
Colonization of Venus
Floating city (science fiction)


== External links ==
American Institute of Aeronautics and Astronautics
[1]
Design Engineering Technical Committee of the AIAA
Spacearchitect.org
Sasakawa International Center for Space Architecture (SICSA)
MOTHER Aerospace Architecture consultancy
Architecture and Vision, Design Studio specializing on Aerospace Architecture and Technology Transfer
LIQUIFER Systems Group, interdisciplinary design team developing architecture, design and systems for Earth and Space
Synthesis, a fundamental design collaborative with experts from Space Architecture, Engineering and Industrial Design
Earth2Orbit, Satellite & Launch Services, Human Space Systems, Robotic Systems, Infrastructure and High-Tech Facilities, Consulting
The Galactic Suite Space Hotel
Galactic Suite Design Aerospace Architecture and Experiences","pandas(index=45, _1=45, text='aerospace architecture is broadly defined to encompass architectural design of non-habitable and habitable structures and living and working environments in aerospace-related facilities, habitats, and vehicles. these environments include, but are not limited to: science platform aircraft and aircraft-deployable systems; space vehicles, space stations, habitats and lunar and planetary surface construction bases; and earth-based control, experiment, launch, logistics, payload, simulation and test facilities. earth analogs to space applications may include antarctic, desert, high altitude, underground, undersea environments and closed ecological systems. the american institute of aeronautics and astronautics (aiaa) design engineering technical committee (detc) meets several times a year to discuss policy, education, standards, and practice issues pertaining to aerospace architecture. see http://www.spacearchitect.org/   == the role of appearance in aerospace architecture == ""the role of design creates and develops concepts and specifications that seek to simultaneously and synergistically optimize function, production, value and appearance."" in connection with, and with respect to, human presence and interactions, appearance is a component of human factors and includes considerations of human characteristics, needs and interests. appearance in this context refers to all visual aspects – the statics and dynamics of form(s), color(s), patterns, and textures in respect to all products, systems, services, and experiences. appearance/esthetics affects humans both psychologically and physiologically and can effect/improving both human efficiency, attitude, and well-being. in reference to non-habitable design the influence of appearance is minimal if not non-existent. however, as the industry of aerospace continues to rapidly grow, and missions to put humans on mars and back to the moon are being announced. the role that appearance/esthetics to maintain crew well-being and health of multi-month or year missions becomes a monumental factor in mission success.   == habitable structures within earth\'s atmosphere == a study conducted in 1989 (reference 2) found that when given multiple photographs and paintings as potential decoration of the international space station. test (crew) subjects all individually preferred those with naturalistic, irrespective themes, and a large depth of field. other examples of human-centered design is using pastel paints on the international space station (iss) to contrast and provide “up/down” cues in micro-g environments or  the concept of dynamically and spatially adjusting lighting color and intensities to conform to daily and even seasonal biorhythms similar to earth to mitigate the societal separation effects experienced in space.   == see also == airborne observatory atmosphere of venus high altitude venus operational concept (havoc) colonization of venus floating city (science fiction)   == external links == american institute of aeronautics and astronautics [1] design engineering technical committee of the aiaa spacearchitect.org sasakawa international center for space architecture (sicsa) mother aerospace architecture consultancy architecture and vision, design studio specializing on aerospace architecture and technology transfer liquifer systems group, interdisciplinary design team developing architecture, design and systems for earth and space synthesis, a fundamental design collaborative with experts from space architecture, engineering and industrial design earth2orbit, satellite & launch services, human space systems, robotic systems, infrastructure and high-tech facilities, consulting the galactic suite space hotel galactic suite design aerospace architecture and experiences')"
46,"A trijet is a jet aircraft powered by three jet engines. In general, passenger airline trijets are considered to be second-generation jet airliners, due to their innovative engine locations, in addition to the advancement of turbofan technology.
Other variations of three-engine designs are trimotors, which are aircraft with three propellers (driven by piston engines or turboprops).


== Design ==

One issue with trijets is positioning the central engine. This is mostly accomplished by placing the engine along the centerline, but this still poses difficulties.
The most common configuration is having the central engine located in the rear fuselage and supplied with air by an S-shaped duct; this is used on the Hawker Siddeley Trident, Boeing 727, Tupolev Tu-154, Lockheed L-1011 TriStar, and, more recently, the Dassault Falcon 7X. The S-duct has low drag, and since the third engine is mounted closer to the centerline, the aircraft will normally be easy to handle in the event of an engine failure. However, S-duct designs are extremely complex and costly. Furthermore, the central engine bay would require structural changes in the event of a major re-engining. For example, the 727's central bay was only wide enough to fit a low-bypass turbofan and not the newer high-bypass turbofans which were quieter and more powerful. Boeing decided that a redesign was too expensive and ended its production instead of pursuing further development. The Lockheed Tristar's tail section was too short to fit an existing two-spool engine as it was designed only to accommodate the new three-spool Rolls-Royce RB211 engine, and delays in the RB211's development, in turn, pushed back the TriStar's entry into service which affected sales.The McDonnell Douglas DC-10 and related MD-11 use an alternative ""straight-through"" layout, which allows for easier engine installation, modification, and access. It also has the additional benefit of being much easier to re-engine. However, this sacrifices aerodynamics compared to the S-duct. Also, as the engine is located much higher up than the wing-mounted engines, engine failure will produce a greater pitching moment, making it more difficult to control.
The placement of the remaining two engines varies. Most smaller aircraft, like the Hawker Siddeley Trident, the Boeing 727 and the Tupolev Tu-154 have two side-mount engine pylons in a T-tail configuration. The larger widebody Lockheed TriStar and DC-10/MD-11 mount an engine underneath each wing.  Preliminary studies were done on the TriStar to reuse the fuselage and wing for a twinjet design though these never materialized due to Lockheed's lack of funds. Additionally in the late-1990s Boeing, which had taken over McDonnell Douglas, considered removing the tail engine from the MD-11 to make it a twinjet but ending up instead ending its production altogether, as the 767 and 777 would have cannibalized it.


== Advantages and drawbacks ==
One major advantage of the trijet design is that the wings can be located further aft on the fuselage, compared to twinjets and quadjets with all wing-mounted engines, allowing main cabin exit and entry doors to be more centrally located for quicker boarding and deplaning, ensuring shorter turnaround times. The rear-mounted engine and wings shift the aircraft's center of gravity rearwards, improving fuel efficiency, although this will also make the plane slightly less stable and more difficult to handle during takeoff and landing. (The McDonnell Douglas DC-9 twinjet and its derivatives, whose engines are mounted on pylons near the rear empennage, have similar advantages/disadvantages of the trijet design, such as the wings located further aft and a more rearward center of gravity.)
Trijets are more efficient and cheaper than four-engine aircraft, as the engines are the most expensive part of the plane and having more engines consumes more fuel, particularly if quadjets and trijets share engines of similar power, making the trijet configuration more suited to a mid-size airliner compared to larger quadjets. However, higher purchase prices, primarily due to the difficulty and complexity of mounting the third engine through the tail, will somewhat negate this advantage.
Due to their added thrust, trijets will have slightly improved takeoff performance compared to twinjets if an engine fails. Because takeoff performance for aircraft is usually calculated to include an extra margin to account for a possible engine failure, trijets are better able to take off from hot and high airports or those where terrain clearance near the runway is an issue.
Unlike twinjets, trijets are not required to land immediately at the nearest suitable airport if one engine fails (this advantage is also shared with quadjets). This is advantageous if the aircraft is not near one of the operator's maintenance bases, as the pilots may then continue the flight and land at an airport where it is more suitable to perform repairs. Additionally, for trijets on the ground with one engine inoperative, approval can be granted to perform two-engine ferry flights. Prior to the introduction of ETOPS, only trijets and quadjets were able to perform long international flights over areas without any diversion airports. However, this advantage has largely disappeared in recent years as ETOPS-certified twin-engined aircraft are able to do so as well.
The biggest obstacle trijets face today is operating costs, primarily fuel efficiency, as a three-engine design almost certainly consumes more fuel than a comparable two-engine design. This also greatly increases the difficulty of marketing a new trijet aircraft today, especially for passenger service. However this was worth the trade-off between 1970 and the 1990s when trijets and twinjets shared engines of similar output, such as when the DC-10, MD-11, Boeing's 767, and Airbus's A300, A310, and A330 were all powered by the General Electric CF6, and the additional power from the third engine gave the DC-10/MD-11 advantages in longer range and/or heavier payload over the A300/A330 twinjet. Since the 1990s, with further advancements in high-bypass turbofan technology, large twinjets have been equipped with purpose-designed engines like the Boeing 777's General Electric GE90, allowing twinjets to perform the same tasks as most trijets and even many quadjets but more efficiently.


== History ==

The first trijet design to fly was the Tupolev Tu-73 bomber prototype, first flown in 1947. The first commercial trijets were the Hawker Siddeley Trident (1962) and the Boeing 727 (1963). Both were compromises to meet airline requirements; in the case of the Trident, it was to meet BEA's changing needs, while the 727 had to be acceptable for three different airlines. Although collaboration between the manufacturers was considered, it did not come about.Early American twinjet designs were limited by the FAA's 60-minute rule, whereby the flight path of twin-engine jetliners was restricted to within 60 minutes' flying time from a suitable airport, in case of engine failure. In 1964, this rule was lifted for trijet designs, as they had a greater safety margin.
For second-generation jet airliners, with the innovations of the high-bypass turbofan for greater efficiency and reduced noise, and the wide-body (twin-aisle) for greater passenger/cargo capacity, the trijet design was seen as the optimal configuration for the medium wide-body jet airliner, sitting in terms of size, range, and cost between quadjets (four-engine aircraft) and twinjets, and this led to a flurry of trijet designs. The four-engine Boeing 747 was popular for transoceanic flights due to its long-range and large size, but it was expensive and not all routes were able to fill its seating capacity, while the original models of the Airbus A300 twinjet were limited to short- to medium-range distances. During this period, different jet airliners shared engines of similar output, such as when the McDonnell Douglas DC-10, Airbus A300, and Boeing 767 were powered by the General Electric CF6, the additional power from the third engine gave the DC-10 advantages in longer range and/or heavier payload over the A300 and 767 twinjets. Thus trijet designs such as the DC-10 and L-1011 TriStar represented the best compromise with medium- to long-range and medium size that US airlines sought for their domestic and transatlantic routes. As a result of these trijet wide-bodies, as well as the popularity of the Boeing 727, in their heyday of the 1980s trijets made up a majority of all such US jet airliners.
From 1985 to 2003 the number of such planes in service had sunk from 1488 to 602. The number of twinjets, on the other hand, had more than quadrupled in the same period. Both Lockheed and McDonnell Douglas were financially weakened competing in the widebody market, which led to Lockheed ending production of the L-1011 in 1984 after producing only half the units needed to break even, while a number of fatal DC-10 crashes also slowed its sales. In 1984 Boeing ended production of the 727, as its central engine bay would require an extremely expensive redesign to accommodate quieter high-bypass turbofans, and it was soon supplanted by Airbus with their A320 and Boeing with their 737 and 757. Further advancements in high-bypass turbofan technology and subsequent relaxation in airline safety rules made the trijet and even the quadjet nearly obsolete for passenger services, as their range and payload could be covered more efficiently with large twinjets powered with purpose-designed engines such as the 777's General Electric GE90.
During the 1980s, McDonnell Douglas was the only Western manufacturer to continue development of the trijet design with an update to the DC-10, the MD-11, which initially held a range and payload advantage over its closest medium wide-body competitors which were twinjets, the in-production Boeing 767 and upcoming Airbus A330. McDonnell Douglas had planned a new trijet called the MD-XX, which were lengthened versions of the MD-11. The MD-XX Long Range aircraft would have been capable of traveling distances up to 8,320 nautical miles (15,410 km) and had a wingspan of 65 metres (213 ft). The project was canceled in 1996, one year before McDonnell Douglas was acquired by Boeing. Boeing ended production of the MD-11 after filling remaining customer orders since the MD-11 would have competed with the 767 and 777. A study to remove the MD-11's tail-mounted engine (which would have made it a twinjet) never came to fruition as it would have been very expensive, and the MD-11 had very little in common in terms of design or type rating with other Boeing airliners. In contrast to McDonnell Douglas sticking with their existing trijet configuration, Airbus (which never produced a trijet aircraft) and Boeing worked on new widebody twinjet designs that would become the A330 and 777, respectively. The MD-11's long-range advantage was brief as it soon was threatened by the A330's four-engine derivative, the A340, and the 777. The only other notable trijet development during the 1980s was in the Soviet Union, where the Tupolev Tu-154 was re-engined with the Soloviev D-30 engine as well as a new wing design and entered serial production from 1984 as the Tu-154M.
With the exception of the Dassault Falcon 7X, Falcon 8x, and Falcon 900, no manufacturer now produces three-engine airliners.


=== Current status ===
Modern engines have extremely low failure rates and can generate much higher shaft power and thrust than early types. This makes twinjets more suitable than they were before for long-haul trans-oceanic operations, resulting in eased ETOPS restrictions; modern wide-body twin-engine jets usually have an ETOPS 180 or even (in the case of the Boeing 777 and 787) ETOPS 330 rating. As such, having more than two engines is no longer considered necessary, except for very large or heavy aircraft such as the Boeing 747, Airbus A380 (over 400 seats in a mixed-class configuration), Antonov An-124, and An-225, or for flights through the Southern Hemisphere, primarily to and from Australia (which has not yet adopted the ETOPS 330 standard), where the most direct route for some flights is over Antarctica.Today, both narrow-body and wide-body trijet production has ceased for almost all commercial aircraft, being replaced by twinjets. As of 2016, the Falcon 7X, 8X, and 900 business jets, all of which use S-ducts, are the only trijets in production. Some old models, such as the 727, Tu-154, DC-10, and MD-11, have found second careers as cargo aircraft, as well as limited charter, governmental, and military service. The most widely used trijets are the DC-10 and the MD-11, mostly operated by UPS Airlines and FedEx Express in cargo service.
For private and corporate operators, where fuel efficiency is often less important than for airlines, trijets may still be of interest due to their immunity from ETOPS and the ability to take off from shorter runways. As a result, a sizeable number of trijets, such as 727s and newly built Dassault Falcons, are in use by private operators and corporate flight departments.


=== Future of trijets ===
Airbus filed a patent in 2008 for a new, twin-tail trijet design, whose tail engine appears to use a ""straight"" layout similar to the MD-11, but it is unknown if and when this will be developed or produced. However, the proposed Boeing X-48 blended wing body design, Lockheed's N+2 design study, and Aerion AS2 supersonic business jet also have three engines. The AS2 is currently taking orders and a wooden mockup has been constructed.Boom Technology's planned Overture supersonic transport (SST) airliner is planned to use three engines, with the third engine installed in the tail with a Y-shaped duct and air intakes on both sides of the rear.


== Examples ==
Boeing 727
Boeing X-48
Dassault Falcon 50
Dassault Falcon 900
Dassault Falcon 7X
Dassault Falcon 8X
Hawker Siddeley Trident
Lockheed L-1011 TriStar
Martin XB-51
McDonnell Douglas DC-10
McDonnell Douglas MD-11
Tupolev Tu-154
Yakovlev Yak-40
Yakovlev Yak-42


== Proposed or suspended trijet developments ==
Boeing 747-300 Trijet – downsized 747 to compete with the DC-10 and L-1011, changed to four engines
Blended Wing Body Trijet – proposed design based on the Boeing X-48
McDonnell Douglas MD-XX – stretched derivative of the DC-10, project shelved
North American NR-349 – proposed interceptor derivative of the A-5 Vigilante, cancelled
Airbus twin-tail trijet, – status unknown
Dassault Supersonic Business Jet – suspended
Aerion AS2
Sukhoi-Gulfstream S-21
Boom Technology Overture
Boeing 777 – Originally envisioned as a trijet 767 in the 1970s to compete with the DC-10 and the L-1011; later became a new twin-engine design.


== See also ==
Quadjet
S-duct
Trimotor
Twinjet
Wide-body aircraft


== References ==

Modern Commercial Aircraft Willian Green, Gordon Swanborough and John Mowinski, 1987


== External links ==
Stanford University Aircraft Aerodynamics and Design Group Engine Placement Accessed 2007-03-13
Undeveloped MD-11/MD-12 models page
Patent for a triple engine fighter
Patent for a triple engine fighter
NR-349 interceptor proposal","pandas(index=46, _1=46, text='a trijet is a jet aircraft powered by three jet engines. in general, passenger airline trijets are considered to be second-generation jet airliners, due to their innovative engine locations, in addition to the advancement of turbofan technology. other variations of three-engine designs are trimotors, which are aircraft with three propellers (driven by piston engines or turboprops).   == design ==  one issue with trijets is positioning the central engine. this is mostly accomplished by placing the engine along the centerline, but this still poses difficulties. the most common configuration is having the central engine located in the rear fuselage and supplied with air by an s-shaped duct; this is used on the hawker siddeley trident, boeing 727, tupolev tu-154, lockheed l-1011 tristar, and, more recently, the dassault falcon 7x. the s-duct has low drag, and since the third engine is mounted closer to the centerline, the aircraft will normally be easy to handle in the event of an engine failure. however, s-duct designs are extremely complex and costly. furthermore, the central engine bay would require structural changes in the event of a major re-engining. for example, the 727\'s central bay was only wide enough to fit a low-bypass turbofan and not the newer high-bypass turbofans which were quieter and more powerful. boeing decided that a redesign was too expensive and ended its production instead of pursuing further development. the lockheed tristar\'s tail section was too short to fit an existing two-spool engine as it was designed only to accommodate the new three-spool rolls-royce rb211 engine, and delays in the rb211\'s development, in turn, pushed back the tristar\'s entry into service which affected sales.the mcdonnell douglas dc-10 and related md-11 use an alternative ""straight-through"" layout, which allows for easier engine installation, modification, and access. it also has the additional benefit of being much easier to re-engine. however, this sacrifices aerodynamics compared to the s-duct. also, as the engine is located much higher up than the wing-mounted engines, engine failure will produce a greater pitching moment, making it more difficult to control. the placement of the remaining two engines varies. most smaller aircraft, like the hawker siddeley trident, the boeing 727 and the tupolev tu-154 have two side-mount engine pylons in a t-tail configuration. the larger widebody lockheed tristar and dc-10/md-11 mount an engine underneath each wing.  preliminary studies were done on the tristar to reuse the fuselage and wing for a twinjet design though these never materialized due to lockheed\'s lack of funds. additionally in the late-1990s boeing, which had taken over mcdonnell douglas, considered removing the tail engine from the md-11 to make it a twinjet but ending up instead ending its production altogether, as the 767 and 777 would have cannibalized it.   == advantages and drawbacks == one major advantage of the trijet design is that the wings can be located further aft on the fuselage, compared to twinjets and quadjets with all wing-mounted engines, allowing main cabin exit and entry doors to be more centrally located for quicker boarding and deplaning, ensuring shorter turnaround times. the rear-mounted engine and wings shift the aircraft\'s center of gravity rearwards, improving fuel efficiency, although this will also make the plane slightly less stable and more difficult to handle during takeoff and landing. (the mcdonnell douglas dc-9 twinjet and its derivatives, whose engines are mounted on pylons near the rear empennage, have similar advantages/disadvantages of the trijet design, such as the wings located further aft and a more rearward center of gravity.) trijets are more efficient and cheaper than four-engine aircraft, as the engines are the most expensive part of the plane and having more engines consumes more fuel, particularly if quadjets and trijets share engines of similar power, making the trijet configuration more suited to a mid-size airliner compared to larger quadjets. however, higher purchase prices, primarily due to the difficulty and complexity of mounting the third engine through the tail, will somewhat negate this advantage. due to their added thrust, trijets will have slightly improved takeoff performance compared to twinjets if an engine fails. because takeoff performance for aircraft is usually calculated to include an extra margin to account for a possible engine failure, trijets are better able to take off from hot and high airports or those where terrain clearance near the runway is an issue. unlike twinjets, trijets are not required to land immediately at the nearest suitable airport if one engine fails (this advantage is also shared with quadjets). this is advantageous if the aircraft is not near one of the operator\'s maintenance bases, as the pilots may then continue the flight and land at an airport where it is more suitable to perform repairs. additionally, for trijets on the ground with one engine inoperative, approval can be granted to perform two-engine ferry flights. prior to the introduction of etops, only trijets and quadjets were able to perform long international flights over areas without any diversion airports. however, this advantage has largely disappeared in recent years as etops-certified twin-engined aircraft are able to do so as well. the biggest obstacle trijets face today is operating costs, primarily fuel efficiency, as a three-engine design almost certainly consumes more fuel than a comparable two-engine design. this also greatly increases the difficulty of marketing a new trijet aircraft today, especially for passenger service. however this was worth the trade-off between 1970 and the 1990s when trijets and twinjets shared engines of similar output, such as when the dc-10, md-11, boeing\'s 767, and airbus\'s a300, a310, and a330 were all powered by the general electric cf6, and the additional power from the third engine gave the dc-10/md-11 advantages in longer range and/or heavier payload over the a300/a330 twinjet. since the 1990s, with further advancements in high-bypass turbofan technology, large twinjets have been equipped with purpose-designed engines like the boeing 777\'s general electric ge90, allowing twinjets to perform the same tasks as most trijets and even many quadjets but more efficiently.   == history ==  the first trijet design to fly was the tupolev tu-73 bomber prototype, first flown in 1947. the first commercial trijets were the hawker siddeley trident (1962) and the boeing 727 (1963). both were compromises to meet airline requirements; in the case of the trident, it was to meet bea\'s changing needs, while the 727 had to be acceptable for three different airlines. although collaboration between the manufacturers was considered, it did not come about.early american twinjet designs were limited by the faa\'s 60-minute rule, whereby the flight path of twin-engine jetliners was restricted to within 60 minutes\' flying time from a suitable airport, in case of engine failure. in 1964, this rule was lifted for trijet designs, as they had a greater safety margin. for second-generation jet airliners, with the innovations of the high-bypass turbofan for greater efficiency and reduced noise, and the wide-body (twin-aisle) for greater passenger/cargo capacity, the trijet design was seen as the optimal configuration for the medium wide-body jet airliner, sitting in terms of size, range, and cost between quadjets (four-engine aircraft) and twinjets, and this led to a flurry of trijet designs. the four-engine boeing 747 was popular for transoceanic flights due to its long-range and large size, but it was expensive and not all routes were able to fill its seating capacity, while the original models of the airbus a300 twinjet were limited to short- to medium-range distances. during this period, different jet airliners shared engines of similar output, such as when the mcdonnell douglas dc-10, airbus a300, and boeing 767 were powered by the general electric cf6, the additional power from the third engine gave the dc-10 advantages in longer range and/or heavier payload over the a300 and 767 twinjets. thus trijet designs such as the dc-10 and l-1011 tristar represented the best compromise with medium- to long-range and medium size that us airlines sought for their domestic and transatlantic routes. as a result of these trijet wide-bodies, as well as the popularity of the boeing 727, in their heyday of the 1980s trijets made up a majority of all such us jet airliners. from 1985 to 2003 the number of such planes in service had sunk from 1488 to 602. the number of twinjets, on the other hand, had more than quadrupled in the same period. both lockheed and mcdonnell douglas were financially weakened competing in the widebody market, which led to lockheed ending production of the l-1011 in 1984 after producing only half the units needed to break even, while a number of fatal dc-10 crashes also slowed its sales. in 1984 boeing ended production of the 727, as its central engine bay would require an extremely expensive redesign to accommodate quieter high-bypass turbofans, and it was soon supplanted by airbus with their a320 and boeing with their 737 and 757. further advancements in high-bypass turbofan technology and subsequent relaxation in airline safety rules made the trijet and even the quadjet nearly obsolete for passenger services, as their range and payload could be covered more efficiently with large twinjets powered with purpose-designed engines such as the 777\'s general electric ge90. during the 1980s, mcdonnell douglas was the only western manufacturer to continue development of the trijet design with an update to the dc-10, the md-11, which initially held a range and payload advantage over its closest medium wide-body competitors which were twinjets, the in-production boeing 767 and upcoming airbus a330. mcdonnell douglas had planned a new trijet called the md-xx, which were lengthened versions of the md-11. the md-xx long range aircraft would have been capable of traveling distances up to 8,320 nautical miles (15,410 km) and had a wingspan of 65 metres (213 ft). the project was canceled in 1996, one year before mcdonnell douglas was acquired by boeing. boeing ended production of the md-11 after filling remaining customer orders since the md-11 would have competed with the 767 and 777. a study to remove the md-11\'s tail-mounted engine (which would have made it a twinjet) never came to fruition as it would have been very expensive, and the md-11 had very little in common in terms of design or type rating with other boeing airliners. in contrast to mcdonnell douglas sticking with their existing trijet configuration, airbus (which never produced a trijet aircraft) and boeing worked on new widebody twinjet designs that would become the a330 and 777, respectively. the md-11\'s long-range advantage was brief as it soon was threatened by the a330\'s four-engine derivative, the a340, and the 777. the only other notable trijet development during the 1980s was in the soviet union, where the tupolev tu-154 was re-engined with the soloviev d-30 engine as well as a new wing design and entered serial production from 1984 as the tu-154m. with the exception of the dassault falcon 7x, falcon 8x, and falcon 900, no manufacturer now produces three-engine airliners. airbus filed a patent in 2008 for a new, twin-tail trijet design, whose tail engine appears to use a ""straight"" layout similar to the md-11, but it is unknown if and when this will be developed or produced. however, the proposed boeing x-48 blended wing body design, lockheed\'s n2 design study, and aerion as2 supersonic business jet also have three engines. the as2 is currently taking orders and a wooden mockup has been constructed.boom technology\'s planned overture supersonic transport (sst) airliner is planned to use three engines, with the third engine installed in the tail with a y-shaped duct and air intakes on both sides of the rear.   == examples == boeing 727 boeing x-48 dassault falcon 50 dassault falcon 900 dassault falcon 7x dassault falcon 8x hawker siddeley trident lockheed l-1011 tristar martin xb-51 mcdonnell douglas dc-10 mcdonnell douglas md-11 tupolev tu-154 yakovlev yak-40 yakovlev yak-42   == proposed or suspended trijet developments == boeing 747-300 trijet – downsized 747 to compete with the dc-10 and l-1011, changed to four engines blended wing body trijet – proposed design based on the boeing x-48 mcdonnell douglas md-xx – stretched derivative of the dc-10, project shelved north american nr-349 – proposed interceptor derivative of the a-5 vigilante, cancelled airbus twin-tail trijet, – status unknown dassault supersonic business jet – suspended aerion as2 sukhoi-gulfstream s-21 boom technology overture boeing 777 – originally envisioned as a trijet 767 in the 1970s to compete with the dc-10 and the l-1011; later became a new twin-engine design.   == see also == quadjet s-duct trimotor twinjet wide-body aircraft   == references ==  modern commercial aircraft willian green, gordon swanborough and john mowinski, 1987   == external links == stanford university aircraft aerodynamics and design group engine placement accessed 2007-03-13 undeveloped md-11/md-12 models page patent for a triple engine fighter patent for a triple engine fighter nr-349 interceptor proposal')"
47,"Nadcap (formerly NADCAP, the National Aerospace and Defense Contractors Accreditation Program) is a global cooperative accreditation program for aerospace engineering, defense and related industries.


== History of Nadcap ==
The Nadcap program is administered by the Performance Review Institute (PRI). Nadcap was established in 1990 by SAE International. Nadcap's membership consists of ""prime contractors"" who coordinate with aerospace accredited suppliers to develop industry-wide audit criteria for special processes and products. Through PRI, Nadcap provides independent certification of manufacturing processes for the industry. PRI has its headquarters in Warrendale, Pennsylvania with branch offices for Nadcap located in London, Beijing, and Nagoya.


== Fields of Nadcap activities ==
The Nadcap program provides accreditation for special processes in the aerospace and defense industry.
These include:

Aerospace Quality Systems (AQS)
Aero Structure Assembly (ASA)
Chemical Processing (CP)
Coatings (CT)
Composites (COMP)
Conventional Machining as a Special Process (CMSP)
Elastomer Seals (SEAL)
Electronics (ETG)
Fluids Distribution (FLU)
Heat Treating (HT)
Materials Testing Laboratories (MTL)
Measurement & Inspection (M&I)
Metallic Materials Manufacturing (MMM)
Nonconventional Machining and Surface Enhancement (NMSE)
Nondestructive Testing (NDT)
Non Metallic Materials Manufacturing (NMMM)
Non Metallic Materials Testing (NMMT)
Sealants (SLT)
Welding (WLD)


== The Nadcap program and industry ==
PRI schedules an audit and assigns an industry approved auditor who will conduct the audit using an industry agreed checklist. At the end of the audit, any non-conformity issues will be raised through a non-conformance report. PRI will administer and close out the non-conformance reports with the Supplier. Upon completion PRI will present the audit pack to a 'special process Task Group’ made up of members from industry who will review it and vote on its acceptability for approval.
The Nadcap subscribers include:

309th Maintenance Wing-Hill AFB
Aerojet Rocketdyne
Airbus Group - Airbus
Airbus Group - Airbus Defence and Space
Airbus Group - Airbus Helicopters
Airbus Group - Premium AEROTEC GmbH
Airbus Group - Stelia Aerospace
Air Force
BAE Systems Military Air Information (MAI)
BAE Systems
The Boeing Company
Bombardier Inc.
COMAC
Defense Contract Management Agency (DCMA)
Eaton, Aerospace Group
Embraer S.A.
GE Aviation
GE Aviation - GE Avio S.r.l.
General Dynamics - Gulfstream
GKN Aerospace
GKN Aerospace Sweden AB
Harris Corporation
Heroux-Devtek Landing Gear Division Inc.
Honeywell Aerospace
Israel Aerospace Industries
Latécoère
Leonardo S.p.A. Divisione Velivoli
Leonardo S.p.A. – Helicopter Division
Liebherr-Aerospace SAS
Lockheed Martin Corporation
Lockheed Martin - Sikorsky Aircraft
Mitsubishi Aircraft Corporation
Mitsubishi Heavy Industries LTD
MTU Aero Engines AG
NASA
Northrop Grumman Corporation
Parker Aerospace Group
Raytheon Company
Rolls-Royce
SAFRAN Group
Singapore Technologies Aerospace
Sonaca
Spirit AeroSystems
Textron Inc. - Textron Aviation
Textron Inc. - Bell Helicopter
Thales Group
Triumph Group Inc.
Raytheon Technologies  - Goodrich
Raytheon Technologies  - Collins Aerospace (Hamilton Sundstrand)
Raytheon Technologies  - Pratt & Whitney
Raytheon Technologies - Pratt & Whitney Canada
Raytheon Technologies - Collins Aerospace (Rockwell Collins)
Zodiac Aerospace


== Nadcap Meetings ==
Nadcap meetings are held several times a year in different locations worldwide. For example, the 2017 meetings were held in New Orleans, LA, USA in February, Berlin (Germany) in June; and Pittsburgh (Pennsylvania). During these meetings there are open Task Group meetings and other workshops (with participation of Primes, Suppliers, and PRI staff). These meetings are used to discuss the program development and changes to audit criteria among other topics. Agendas and minutes are posted on the PRI website.


== Nadcap Training ==
During the Nadcap meetings, training classes are provided on different topics such as:

Root Cause Corrective Action - RCCA
Special processes, such as, NDT, chemical processing, etc.
Internal auditing
AS/EN/JISQ 9100
Problem Solving Tools
Nadcap Audit Preparation – Chemical Processing
Nadcap Audit Preparation – Heat Treating
Nadcap Audit Preparation – Metallic Material Testing Laboratories
Nadcap Audit Preparation – Non-Destructive Testing
Nadcap Audit Preparation – Welding


== References ==


== External links ==
Official website
Boeing official site
ADS Group official site
Aerospace Manufacturing
Quality Manufacturing Today","pandas(index=47, _1=47, text='nadcap (formerly nadcap, the national aerospace and defense contractors accreditation program) is a global cooperative accreditation program for aerospace engineering, defense and related industries.   == history of nadcap == the nadcap program is administered by the performance review institute (pri). nadcap was established in 1990 by sae international. nadcap\'s membership consists of ""prime contractors"" who coordinate with aerospace accredited suppliers to develop industry-wide audit criteria for special processes and products. through pri, nadcap provides independent certification of manufacturing processes for the industry. pri has its headquarters in warrendale, pennsylvania with branch offices for nadcap located in london, beijing, and nagoya.   == fields of nadcap activities == the nadcap program provides accreditation for special processes in the aerospace and defense industry. these include:  aerospace quality systems (aqs) aero structure assembly (asa) chemical processing (cp) coatings (ct) composites (comp) conventional machining as a special process (cmsp) elastomer seals (seal) electronics (etg) fluids distribution (flu) heat treating (ht) materials testing laboratories (mtl) measurement & inspection (m&i) metallic materials manufacturing (mmm) nonconventional machining and surface enhancement (nmse) nondestructive testing (ndt) non metallic materials manufacturing (nmmm) non metallic materials testing (nmmt) sealants (slt) welding (wld)   == the nadcap program and industry == pri schedules an audit and assigns an industry approved auditor who will conduct the audit using an industry agreed checklist. at the end of the audit, any non-conformity issues will be raised through a non-conformance report. pri will administer and close out the non-conformance reports with the supplier. upon completion pri will present the audit pack to a \'special process task group’ made up of members from industry who will review it and vote on its acceptability for approval. the nadcap subscribers include:  309th maintenance wing-hill afb aerojet rocketdyne airbus group - airbus airbus group - airbus defence and space airbus group - airbus helicopters airbus group - premium aerotec gmbh airbus group - stelia aerospace air force bae systems military air information (mai) bae systems the boeing company bombardier inc. comac defense contract management agency (dcma) eaton, aerospace group embraer s.a. ge aviation ge aviation - ge avio s.r.l. general dynamics - gulfstream gkn aerospace gkn aerospace sweden ab harris corporation heroux-devtek landing gear division inc. honeywell aerospace israel aerospace industries latécoère leonardo s.p.a. divisione velivoli leonardo s.p.a. – helicopter division liebherr-aerospace sas lockheed martin corporation lockheed martin - sikorsky aircraft mitsubishi aircraft corporation mitsubishi heavy industries ltd mtu aero engines ag nasa northrop grumman corporation parker aerospace group raytheon company rolls-royce safran group singapore technologies aerospace sonaca spirit aerosystems textron inc. - textron aviation textron inc. - bell helicopter thales group triumph group inc. raytheon technologies  - goodrich raytheon technologies  - collins aerospace (hamilton sundstrand) raytheon technologies  - pratt & whitney raytheon technologies - pratt & whitney canada raytheon technologies - collins aerospace (rockwell collins) zodiac aerospace   == nadcap meetings == nadcap meetings are held several times a year in different locations worldwide. for example, the 2017 meetings were held in new orleans, la, usa in february, berlin (germany) in june; and pittsburgh (pennsylvania). during these meetings there are open task group meetings and other workshops (with participation of primes, suppliers, and pri staff). these meetings are used to discuss the program development and changes to audit criteria among other topics. agendas and minutes are posted on the pri website.   == nadcap training == during the nadcap meetings, training classes are provided on different topics such as:  root cause corrective action - rcca special processes, such as, ndt, chemical processing, etc. internal auditing as/en/jisq 9100 problem solving tools nadcap audit preparation – chemical processing nadcap audit preparation – heat treating nadcap audit preparation – metallic material testing laboratories nadcap audit preparation – non-destructive testing nadcap audit preparation – welding   == references ==   == external links == official website boeing official site ads group official site aerospace manufacturing quality manufacturing today')"
48,"The max q condition is the point when an aerospace vehicle's atmospheric flight reaches maximum dynamic pressure. This is a significant factor in the design of such vehicles because the aerodynamic structural load on them is proportional to dynamic pressure. This may impose limits on the vehicle's flight envelope.
Dynamic pressure, q, is defined mathematically as

  
    
      
        q
        =
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        ,
      
    
    {\displaystyle q={\tfrac {1}{2}}\,\rho \,v^{2},}
  where ρ is the local air density, and v is the vehicle's velocity; the dynamic pressure can be thought of as the kinetic energy density of the air with respect to the vehicle.  For a launch of a rocket from the ground into space, dynamic pressure is

zero at lift-off, when the air density ρ is high but the vehicle's speed v = 0
zero outside the atmosphere, where the speed v is high, but the air density ρ = 0
always non-negative, given the quantities involvedTherefore, (by Rolle's theorem) there will always be a point where the dynamic pressure is maximum.
In other words, before reaching max q, the dynamic pressure change due to increasing velocity is greater than that due to decreasing air density so that the dynamic pressure (opposing kinetic energy) acting on the craft continues to increase. After passing max q, the opposite is true. The dynamic pressure acting against the craft decreases as the air density decreases, ultimately reaching 0 when the air density becomes zero.


== Rocket launch examples ==
During a normal Space Shuttle launch, for example, max q value of 0.32 atmospheres occurred at an altitude of approximately 11 km (36,000 ft). The three Space Shuttle Main Engines were throttled back to about 60–70% of their rated thrust (depending on payload) as the dynamic pressure approached max q; combined with the propellant grain design of the solid rocket boosters, which reduced the thrust at max q by one third after 50 seconds of burn, the total stresses on the vehicle were kept to a safe level.
During a typical Apollo mission, the max q (also just over 0.3 atmospheres) occurred between 13 and 14 kilometres (43,000–46,000 ft) of altitude; approximately the same values occur for the SpaceX Falcon 9.The point of max q is a key milestone during a rocket launch, as it is the point at which the airframe undergoes maximum mechanical stress.


== See also ==
Prandtl–Glauert singularity
Ideal gas law
Gravity turn
Gravity drag


== References ==","pandas(index=48, _1=48, text=""the max q condition is the point when an aerospace vehicle's atmospheric flight reaches maximum dynamic pressure. this is a significant factor in the design of such vehicles because the aerodynamic structural load on them is proportional to dynamic pressure. this may impose limits on the vehicle's flight envelope. dynamic pressure, q, is defined mathematically as     q =    1 2     ρ   v  2   ,    where ρ is the local air density, and v is the vehicle's velocity; the dynamic pressure can be thought of as the kinetic energy density of the air with respect to the vehicle.  for a launch of a rocket from the ground into space, dynamic pressure is  zero at lift-off, when the air density ρ is high but the vehicle's speed v = 0 zero outside the atmosphere, where the speed v is high, but the air density ρ = 0 always non-negative, given the quantities involvedtherefore, (by rolle's theorem) there will always be a point where the dynamic pressure is maximum. in other words, before reaching max q, the dynamic pressure change due to increasing velocity is greater than that due to decreasing air density so that the dynamic pressure (opposing kinetic energy) acting on the craft continues to increase. after passing max q, the opposite is true. the dynamic pressure acting against the craft decreases as the air density decreases, ultimately reaching 0 when the air density becomes zero.   == rocket launch examples == during a normal space shuttle launch, for example, max q value of 0.32 atmospheres occurred at an altitude of approximately 11 km (36,000 ft). the three space shuttle main engines were throttled back to about 60–70% of their rated thrust (depending on payload) as the dynamic pressure approached max q; combined with the propellant grain design of the solid rocket boosters, which reduced the thrust at max q by one third after 50 seconds of burn, the total stresses on the vehicle were kept to a safe level. during a typical apollo mission, the max q (also just over 0.3 atmospheres) occurred between 13 and 14 kilometres (43,000–46,000 ft) of altitude; approximately the same values occur for the spacex falcon 9.the point of max q is a key milestone during a rocket launch, as it is the point at which the airframe undergoes maximum mechanical stress.   == see also == prandtl–glauert singularity ideal gas law gravity turn gravity drag   == references =="")"
49,"Terrain Contour Matching, or TERCOM, is a navigation system used primarily by cruise missiles. It uses a pre-recorded contour map of the terrain that is compared with measurements made during flight by an on-board radar altimeter. A TERCOM system considerably increases the accuracy of a missile compared with inertial navigation systems (INS). The increased accuracy allows a TERCOM-equipped missile to fly closer to obstacles and generally lower altitudes, making it harder to detect by ground radar.


== Description ==


=== Optical contour matching ===
The Goodyear Aircraft Corporation ATRAN (Automatic Terrain Recognition And Navigation) system for the MGM-13 Mace was the earliest known TERCOM system. In August 1952, Air Materiel Command initiated the mating of the Goodyear ATRAN with the MGM-1 Matador.  This mating resulted in a production contract in June 1954. ATRAN was difficult to jam and was not range-limited by line-of sight, but its range was restricted by the availability of radar maps. In time, it became possible to construct radar maps from topographic maps.
Preparation of the maps required the route to be flown by an aircraft. A radar on the aircraft was set to a fixed angle and made horizontal scans of the land in front. The timing of the return signal indicated the range to the landform and produced an amplitude modulated (AM) signal. This was sent to a light source and recorded on 35 mm film, advancing the film and taking a picture at indicated times. The film could then be processed and copied for use in multiple missiles.
In the missile, a similar radar produced the same signal. A second system scanned the frames of film against a photocell and produced a similar AM signal. By comparing the points along the scan where the brightness changed rapidly, which could be picked out easily by simple electronics, the system could compare the left-right path of the missile compared with that of the pathfinding aircraft. Errors between the two signals drove corrections in the autopilot needed to bring the missile back onto its programmed flight path.


=== Altitude matching ===
Modern TERCOM systems use a different concept, based on the altitude of the ground the missile flies over and comparing that to measurements made by a radar altimeter. TERCOM ""maps"" consist of a series of squares of a selected size. Using a smaller number of larger squares saves memory, at the cost of decreasing accuracy. A series of such maps are produced, typically from data from radar mapping satellites. When flying over water, contour maps are replaced by magnetic field maps.
As a radar altimeter measures the distance between the missile and the terrain, not the absolute altitude compared to sea level, the important measure in the data is the change in altitude from square to square. The missile's radar altimeter feeds measurements into a small buffer that periodically ""gates"" the measurements over a period of time and averages them out to produce a single measurement. The series of such numbers held in the buffer produce a strip of measurements similar to those held in the maps. The series of changes in the buffer is then compared with the values in the map, looking for areas where the changes in altitude are identical. This produces a location and direction. The guidance system can then use this information to correct the flight path of the missile.
During the cruise portion of the flight to the target, the accuracy of the system has to be enough only to avoid terrain features. This allows the maps to be a relatively low resolution in these areas. Only the portion of the map for the terminal approach has to be higher resolution, and would normally be encoded at the highest resolutions available to the satellite mapping system.


=== TAINS ===
Due to the limited amount of memory available in mass storage devices of the 1960s and 70s, and their slow access times, the amount of terrain data that could be stored in a missile-sized package was far too small to encompass the entire flight. Instead, small patches of terrain information were stored and periodically used to update a conventional inertial platform. These systems, combining TERCOM and inertial navigation, are sometimes known as TAINS, for TERCOM-Aided Inertial Navigation System.


=== Advantages ===
TERCOM systems have the advantage of offering accuracy that is not based on the length of the flight; an inertial system slowly drifts after a ""fix"", and its accuracy is lower for longer distances. TERCOM systems receive constant fixes during the flight, and thus do not have any drift. Their absolute accuracy, however, is based on the accuracy of the radar mapping information, which is typically in the range of meters, and the ability of the processor to compare the altimeter data to the map quickly enough as the resolution increases. This generally limits first generation TERCOM systems to targets on the order of hundreds of meters, limiting them to the use of nuclear warheads. Use of conventional warheads requires further accuracy, which in turn demands additional terminal guidance systems.


=== Disadvantages ===
The limited data storage and computing systems of the time meant that the entire route had to be pre-planned, including its launch point. If the missile was launched from an unexpected location or flew too far off-course, it would never fly over the features included in the maps, and would become lost. The INS system can help, allowing it to fly to the general area of the first patch, but gross errors simply cannot be corrected. This made early TERCOM-based systems much less flexible than more modern systems like GPS, which can be set to attack any location from any location, and do not require pre-recorded information which means they can be given their targets immediately before launch.
Improvements in computing and memory, combined with the availability of global digital elevation maps, have reduced this problem, as TERCOM data is no longer limited to small patches, and the availability of side-looking radar allows much larger areas of landscape contour data to be acquired for comparison with the stored contour data.


== Comparison with other guidance systems ==


=== DSMAC ===

DSMAC was an early form of AI which could guide missiles in real time by using camera inputs to determine location. DSMAC was used in Tomahawk Block II onward, and proved itself successfully during the first Gulf War. The system worked by comparing camera inputs during flight to maps computed from spy satellite images. The DSMAC AI system computed contrast maps of images, which it then combined in a buffer and then averaged. It then compared the averages to stored maps computed beforehand by a large mainframe computer, which converted spy satellite pictures to simulate what routes and targets would look like from low level. Since the data were not identical and would change by season and from other unexpected changes and visual effects, the DSMAC system within the missiles had to be able to compare and determine if maps were the same, regardless of changes. It could successfully filter out differences in maps and use the remaining map data to determine its location. Due to its ability to visually identify targets instead of simply attacking estimated coordinates, its accuracy exceeded GPS guided weapons during the first Gulf War.The massive improvements in memory and processing power from the 1950s, when these scene comparison systems were first invented, to the 1980s, when TERCOM was widely deployed, changed the nature of the problem considerably. Modern systems can store numerous images of a target as seen from different directions, and often the imagery can be calculated using image synthesis techniques. Likewise, the complexity of the live imaging systems has been greatly reduced through the introduction of solid-state technologies like CCDs. The combination of these technologies produced the Digitized Scene-Mapping Area Correlator (DSMAC). DSMAC systems are often combined with TERCOM as a terminal guidance system, allowing point attack with conventional warheads.

MGM-31 Pershing II, SS-12 Scaleboard Temp-SM and OTR-23 Oka used an active radar homing version of DSMAC (digitized correlator unit DCU), which compared radar topographic maps taken by satellites or aircraft with information received from the onboard active radar regarding target topography, for terminal guidance.


=== Satellite navigation ===
Yet another way to navigate a cruise missile is by using a satellite positioning system as they are precise and cheap. Unfortunately, they rely on satellites. If the satellites are interfered with (e.g. destroyed) or if the satellite signal is interfered with (e.g. jammed), the satellite navigation system becomes inoperable. Therefore, the GPS-based (or GLONASS-based) navigation is useful in a conflict with a technologically unsophisticated adversary. On the other hand, to be ready for a conflict with a technologically advanced adversary, one needs missiles equipped with TAINS and DSMAC.


== Missiles that employ TERCOM navigation ==
The cruise missiles that employ a TERCOM system include:

Supersonic Low Altitude Missile (early version of TERCOM was slated to be used in this never-built missile)
AGM-86B (United States)
AGM-129 ACM (United States)
BGM-109 Tomahawk (some versions, United States)
C-602 Anti-ship & Land attack cruise missile (China)
Kh-55 Granat NATO reporting name AS-15 Kent (Soviet Union)
Newer Russian cruise missiles, such as Kh-101 and Kh-555 are likely to have TERCOM navigation, but little information is available about these missiles
C-802 or YJ-82 NATO reporting name CSS-N-8 Saccade (China) – it is unclear if this missile employs TERCOM navigation
Hyunmoo III (South Korea)
DH-10 (China)
Babur (Pakistan) Land Attack Cruise Missile
Ra'ad (Pakistan) Air Launched Cruise Missile
Naval Strike Missile (Anti ship and land attack missile, Norway)
SOM (missile) (air launched cruise missile, Turkey)
HongNiao 1/2/3 cruise missiles
9K720 Iskander (Short-range ballistic missile and cruise missile variants, Russia)


== See also ==
Missile guidance
TERPROM


== References ==


== External links ==
""Terrestrial Guidance Methods"", Section 16.5.3 of Fundamentals of Naval Weapons Systems
More info at fas.org
Info at aeronautics.ru","pandas(index=49, _1=49, text='terrain contour matching, or tercom, is a navigation system used primarily by cruise missiles. it uses a pre-recorded contour map of the terrain that is compared with measurements made during flight by an on-board radar altimeter. a tercom system considerably increases the accuracy of a missile compared with inertial navigation systems (ins). the increased accuracy allows a tercom-equipped missile to fly closer to obstacles and generally lower altitudes, making it harder to detect by ground radar.   == description == yet another way to navigate a cruise missile is by using a satellite positioning system as they are precise and cheap. unfortunately, they rely on satellites. if the satellites are interfered with (e.g. destroyed) or if the satellite signal is interfered with (e.g. jammed), the satellite navigation system becomes inoperable. therefore, the gps-based (or glonass-based) navigation is useful in a conflict with a technologically unsophisticated adversary. on the other hand, to be ready for a conflict with a technologically advanced adversary, one needs missiles equipped with tains and dsmac.   == missiles that employ tercom navigation == the cruise missiles that employ a tercom system include:  supersonic low altitude missile (early version of tercom was slated to be used in this never-built missile) agm-86b (united states) agm-129 acm (united states) bgm-109 tomahawk (some versions, united states) c-602 anti-ship & land attack cruise missile (china) kh-55 granat nato reporting name as-15 kent (soviet union) newer russian cruise missiles, such as kh-101 and kh-555 are likely to have tercom navigation, but little information is available about these missiles c-802 or yj-82 nato reporting name css-n-8 saccade (china) – it is unclear if this missile employs tercom navigation hyunmoo iii (south korea) dh-10 (china) babur (pakistan) land attack cruise missile ra\'ad (pakistan) air launched cruise missile naval strike missile (anti ship and land attack missile, norway) som (missile) (air launched cruise missile, turkey) hongniao 1/2/3 cruise missiles 9k720 iskander (short-range ballistic missile and cruise missile variants, russia)   == see also == missile guidance terprom   == references ==   == external links == ""terrestrial guidance methods"", section 16.5.3 of fundamentals of naval weapons systems more info at fas.org info at aeronautics.ru')"
50,"Weight distribution is the apportioning of weight within a vehicle, especially cars, airplanes, and trains. Typically, it is written in the form x/y, where x is the percentage of weight in the front, and y is the percentage in the back.
In a vehicle which relies on gravity in some way, weight distribution directly affects a variety of vehicle characteristics, including handling, acceleration, traction, and component life.  For this reason weight distribution varies with the vehicle's intended usage. For example, a drag car maximizes traction at the rear axle while countering the reactionary pitch-up torque.  It generates this counter-torque by placing a small amount of counterweight at a great distance forward of the rear axle.
In the airline industry, load balancing is used to evenly distribute the weight of passengers, cargo, and fuel throughout an aircraft, so as to keep the aircraft's center of gravity close to its center of pressure to avoid losing pitch control.  In military transport aircraft, it is common to have a loadmaster as a part of the crew; their responsibilities include calculating accurate load information for center of gravity calculations, and ensuring cargo is properly secured to prevent its shifting.
In large aircraft and ships, multiple fuel tanks and pumps are often used, so that as fuel is consumed, the remaining fuel can be positioned to keep the vehicle balanced, and to reduce stability problems associated with the free surface effect.
In the trucking industry, individual axle weight limits require balancing the cargo when the gross vehicle weight nears the legal limit.


== See also ==
Center of mass
Center of percussion
Load transfer
Mass distribution
Roll center
Tilt test
Weight transfer


== References ==


== External links ==
Weight Distribution Calculator","pandas(index=50, _1=50, text=""weight distribution is the apportioning of weight within a vehicle, especially cars, airplanes, and trains. typically, it is written in the form x/y, where x is the percentage of weight in the front, and y is the percentage in the back. in a vehicle which relies on gravity in some way, weight distribution directly affects a variety of vehicle characteristics, including handling, acceleration, traction, and component life.  for this reason weight distribution varies with the vehicle's intended usage. for example, a drag car maximizes traction at the rear axle while countering the reactionary pitch-up torque.  it generates this counter-torque by placing a small amount of counterweight at a great distance forward of the rear axle. in the airline industry, load balancing is used to evenly distribute the weight of passengers, cargo, and fuel throughout an aircraft, so as to keep the aircraft's center of gravity close to its center of pressure to avoid losing pitch control.  in military transport aircraft, it is common to have a loadmaster as a part of the crew; their responsibilities include calculating accurate load information for center of gravity calculations, and ensuring cargo is properly secured to prevent its shifting. in large aircraft and ships, multiple fuel tanks and pumps are often used, so that as fuel is consumed, the remaining fuel can be positioned to keep the vehicle balanced, and to reduce stability problems associated with the free surface effect. in the trucking industry, individual axle weight limits require balancing the cargo when the gross vehicle weight nears the legal limit.   == see also == center of mass center of percussion load transfer mass distribution roll center tilt test weight transfer   == references ==   == external links == weight distribution calculator"")"
51,"The torques or moments acting on an airfoil moving through a fluid can be accounted for by the net lift and net drag applied at some point on the airfoil, and a separate net pitching moment about that point whose magnitude varies with the choice of where the lift is chosen to be applied.  The Aerodynamic center is the point at which the pitching moment coefficient for the airfoil does not vary with lift coefficient (i.e. angle of attack), making analysis simpler.

  
    
      
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  L
                
              
            
          
        
        =
        0
      
    
    {\displaystyle {dC_{m} \over dC_{L}}=0}
   where 
  
    
      
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{L}}
   is the aircraft lift coefficient.The lift and drag forces can be applied at a single point, the center of pressure, about which they exert zero torque. However, the location of the center of pressure moves significantly with a change in angle of attack and is thus impractical for aerodynamic analysis. Instead the aerodynamic center is used and as a result the incremental lift and drag due to change in angle of attack acting at this point is sufficient to describe the aerodynamic forces acting on the given body.


== Theory ==
Within the assumptions embodied in thin airfoil theory, the aerodynamic center is located at the quarter-chord (25% chord position) on a symmetric airfoil while it is close but not exactly equal to the quarter-chord point on a cambered airfoil.
From thin airfoil theory:

  
    
      
         
        
          c
          
            l
          
        
        =
        2
        π
        α
      
    
    {\displaystyle \ c_{l}=2\pi \alpha }
  
where 
  
    
      
        
          c
          
            l
          
        
        
      
    
    {\displaystyle c_{l}\!}
   is the section lift coefficient,

  
    
      
        α
        
      
    
    {\displaystyle \alpha \!}
   is the angle of attack in radian, measured relative to the chord line.

  
    
      
         
        
          
            
              d
              
                c
                
                  m
                  ,
                  c
                  
                    /
                  
                  4
                
              
            
            
              d
              α
            
          
        
        =
        
          m
          
            0
          
        
      
    
    {\displaystyle \ {dc_{m,c/4} \over d\alpha }=m_{0}}
  
where 
  
    
      
         
        
          c
          
            m
            ,
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ c_{m,c/4}}
   is the moment taken at quarter-chord point and 
  
    
      
         
        
          m
          
            0
          
        
      
    
    {\displaystyle \ m_{0}}
   is a constant.

  
    
      
         
        
          M
          
            a
            c
          
        
        =
        L
        (
        c
        
          x
          
            a
            c
          
        
        −
        c
        
          /
        
        4
        )
        +
        
          M
          
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ M_{ac}=L(cx_{ac}-c/4)+M_{c/4}}
  

  
    
      
         
        
          c
          
            m
            ,
            a
            c
          
        
        =
        
          c
          
            l
          
        
        (
        
          x
          
            a
            c
          
        
        −
        0.25
        )
        +
        
          c
          
            m
            ,
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ c_{m,ac}=c_{l}(x_{ac}-0.25)+c_{m,c/4}}
  Differentiating with respect to angle of attack

  
    
      
         
        
          x
          
            a
            c
          
        
        =
        
          
            
              −
              
                m
                
                  0
                
              
            
            
              2
              π
            
          
        
        +
        0.25
      
    
    {\displaystyle \ x_{ac}={-m_{0} \over {2\pi }}+0.25}
  For symmetrical airfoils 
  
    
      
         
        
          m
          
            0
          
        
        =
        0
      
    
    {\displaystyle \ m_{0}=0}
  , so the aerodynamic center is at 25% of chord. But for cambered airfoils the aerodynamic center can be slightly less than 25% of the chord from the leading edge, which depends on the slope of the moment coefficient, 
  
    
      
         
        
          m
          
            0
          
        
      
    
    {\displaystyle \ m_{0}}
  . These results obtained are calculated using the thin airfoil theory so the use of the results are warranted only when the assumptions of thin airfoil theory are realistic. In precision experimentation with real airfoils and advanced analysis, the aerodynamic center is observed to change location slightly as angle of attack varies. In most literature however the aerodynamic center is assumed to be fixed at the 25% chord position.


== Role of aerodynamic center in aircraft stability ==
For longitudinal static stability: 
  
    
      
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              α
            
          
        
        <
        0
      
    
    {\displaystyle {dC_{m} \over d\alpha }<0}
       and    
  
    
      
        
          
            
              d
              
                C
                
                  z
                
              
            
            
              d
              α
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{z} \over d\alpha }>0}
  
For directional static stability:   
  
    
      
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              β
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{n} \over d\beta }>0}
       and    
  
    
      
        
          
            
              d
              
                C
                
                  y
                
              
            
            
              d
              β
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{y} \over d\beta }>0}
  
Where:

  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
        cos
        ⁡
        (
        α
        )
        +
        
          C
          
            d
          
        
        sin
        ⁡
        (
        α
        )
      
    
    {\displaystyle C_{z}=C_{L}\cos(\alpha )+C_{d}\sin(\alpha )}
  

  
    
      
        
          C
          
            x
          
        
        =
        
          C
          
            L
          
        
        sin
        ⁡
        (
        α
        )
        −
        
          C
          
            d
          
        
        cos
        ⁡
        (
        α
        )
      
    
    {\displaystyle C_{x}=C_{L}\sin(\alpha )-C_{d}\cos(\alpha )}
  For a force acting away from the aerodynamic center, which is away from the reference point:

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{z}}}
  Which for small angles 
  
    
      
        cos
        ⁡
        (
        α
        )
        =
        1
      
    
    {\displaystyle \cos(\alpha )=1}
   and 
  
    
      
        sin
        ⁡
        (
        α
        )
        =
        α
      
    
    {\displaystyle \sin(\alpha )=\alpha }
  , 
  
    
      
        β
        =
        0
      
    
    {\displaystyle \beta =0}
  , 
  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
        −
        
          C
          
            d
          
        
        ∗
        α
      
    
    {\displaystyle C_{z}=C_{L}-C_{d}*\alpha }
  , 
  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{z}=C_{L}}
   simplifies to:

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  L
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{L}}}
  

  
    
      
        
          Y
          
            A
            C
          
        
        =
        
          Y
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle Y_{AC}=Y_{\mathrm {ref} }}
  

  
    
      
        
          Z
          
            A
            C
          
        
        =
        
          Z
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle Z_{AC}=Z_{\mathrm {ref} }}
  General Case: From the definition of the AC it follows that

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              
                C
                
                  y
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{z}}+c{dC_{n} \over dC_{y}}}
  
.

  
    
      
        
          Y
          
            A
            C
          
        
        =
        
          Y
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  l
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              
                C
                
                  x
                
              
            
          
        
      
    
    {\displaystyle Y_{AC}=Y_{\mathrm {ref} }+c{dC_{l} \over dC_{z}}+c{dC_{n} \over dC_{x}}}
  
.

  
    
      
        
          Z
          
            A
            C
          
        
        =
        
          Z
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  l
                
              
            
            
              d
              
                C
                
                  y
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  x
                
              
            
          
        
      
    
    {\displaystyle Z_{AC}=Z_{\mathrm {ref} }+c{dC_{l} \over dC_{y}}+c{dC_{m} \over dC_{x}}}
  The Static Margin can then be used to quantify the AC:

  
    
      
        S
        M
        =
        
          
            
              
                X
                
                  A
                  C
                
              
              −
              
                X
                
                  C
                  G
                
              
            
            c
          
        
      
    
    {\displaystyle SM={X_{AC}-X_{CG} \over c}}
  where:

  
    
      
        
          C
          
            n
          
        
      
    
    {\displaystyle C_{n}}
   = yawing moment coefficient

  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   = pitching moment coefficient

  
    
      
        
          C
          
            l
          
        
      
    
    {\displaystyle C_{l}}
   = rolling moment coefficient

  
    
      
        
          C
          
            x
          
        
      
    
    {\displaystyle C_{x}}
   = X-force ~= Drag

  
    
      
        
          C
          
            y
          
        
      
    
    {\displaystyle C_{y}}
   = Y-force ~= Side Force

  
    
      
        
          C
          
            z
          
        
      
    
    {\displaystyle C_{z}}
   = Z-force ~= Lift
ref = reference point (about which moments were taken)
c   = reference length
S   = reference area
q   = dynamic pressure

  
    
      
        α
      
    
    {\displaystyle \alpha }
     = angle of attack

  
    
      
        β
      
    
    {\displaystyle \beta }
     = sideslip angleSM   = Static Margin


== See also ==
Aircraft flight mechanics
Flight dynamics
Longitudinal static stability
Thin-airfoil theory
Joukowsky transform


== References ==","pandas(index=51, _1=51, text='the torques or moments acting on an airfoil moving through a fluid can be accounted for by the net lift and net drag applied at some point on the airfoil, and a separate net pitching moment about that point whose magnitude varies with the choice of where the lift is chosen to be applied.  the aerodynamic center is the point at which the pitching moment coefficient for the airfoil does not vary with lift coefficient (i.e. angle of attack), making analysis simpler.        d  c  m     d  c  l      = 0    = sideslip anglesm   = static margin   == see also == aircraft flight mechanics flight dynamics longitudinal static stability thin-airfoil theory joukowsky transform   == references ==')"
52,"In aerodynamics, the pitching moment on an airfoil is the moment (or torque) produced by the aerodynamic force on the airfoil if that aerodynamic force is considered to be applied, not at the center of pressure, but at the aerodynamic center of the airfoil.  The pitching moment on the wing of an airplane is part of the total moment that must be balanced using the lift on the horizontal stabilizer. More generally, a pitching moment is any moment acting on the pitch axis of a moving body.
The lift on an airfoil is a distributed force that can be said to act at a point called the center of pressure.  However, as angle of attack changes on a cambered airfoil, there is movement of the center of pressure forward and aft.  This makes analysis difficult when attempting to use the concept of the center of pressure.  One of the remarkable properties of a cambered airfoil is that, even though the center of pressure moves forward and aft, if the lift is imagined to act at a point called the aerodynamic center. The moment of the lift force changes in proportion to the square of the airspeed.  If the moment is divided by the dynamic pressure, the area and chord of the airfoil, the result is known as the pitching moment coefficient. This coefficient changes only a little over the operating range of angle of attack of the airfoil but the change in moment slope against the AOA shown in figure below seems very steep so this should be of change in pitching moment of wing about CG rather than about AC.  The combination of the two concepts of aerodynamic center and pitching moment coefficient make it relatively simple to analyse some of the flight characteristics of an aircraft.


== Measurement ==
The aerodynamic center of an airfoil is usually close to 25% of the chord behind the leading edge of the airfoil.  When making tests on a model airfoil, such as in a wind-tunnel, if the force sensor is not aligned with the quarter-chord of the airfoil, but offset by a distance x, the pitching moment about the quarter-chord point, 
  
    
      
        
          M
          
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle M_{c/4}}
   is given by

  
    
      
        
          M
          
            c
            
              /
            
            4
          
        
        =
        
          M
          
            indicated
          
        
        +
        
          x
        
        ×
        (
        
          D
          
            indicated
          
        
        ,
        
          L
          
            indicated
          
        
        )
      
    
    {\displaystyle M_{c/4}=M_{\text{indicated}}+\mathbf {x} \times (D_{\text{indicated}},L_{\text{indicated}})}
  where the indicated values of D and L are the drag and lift on the model, as measured by the force sensor.


== Coefficient ==
The pitching moment coefficient is important in the study of the longitudinal static stability of aircraft and missiles.
The pitching moment coefficient 
  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   is defined as follows

  
    
      
        
          C
          
            m
          
        
        =
        
          
            M
            
              q
              S
              c
            
          
        
      
    
    {\displaystyle C_{m}={\frac {M}{qSc}}}
  where M is the pitching moment, q is the dynamic pressure, S is the wing area, and c is the length of the chord of the airfoil.

  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   is a dimensionless coefficient so consistent units must be used for M, q, S and c.
Pitching moment coefficient is fundamental to the definition of aerodynamic center of an airfoil.  The aerodynamic center is defined to be the point on the chord line of the airfoil at which the pitching moment coefficient does not vary with angle of attack, or at least does not vary significantly over the operating range of angle of attack of the airfoil.
In the case of a symmetric airfoil, the lift force acts through one point for all angles of attack, and the center of pressure does not move as it does in a cambered airfoil.  Consequently, the pitching moment coefficient for a symmetric airfoil is zero.
The pitching moment is, by convention, considered to be positive when it acts to pitch the airfoil in the nose-up direction.  Conventional cambered airfoils supported at the aerodynamic center pitch nose-down so the pitching moment coefficient of these airfoils is negative.


== References ==
L. J. Clancy (1975), Aerodynamics, Pitman Publishing Limited, London, ISBN 0-273-01120-0
Piercy, N.A.V (1943) Aerodynamics, pages 384–386, English Universities Press. London
Low-Speed Stability Retrieved on 2008-07-18


=== Notes ===


== See also ==

Aircraft flight mechanics
Flight dynamics
Longitudinal static stability
Neutral point
Lift coefficient
Drag coefficient","pandas(index=52, _1=52, text='in aerodynamics, the pitching moment on an airfoil is the moment (or torque) produced by the aerodynamic force on the airfoil if that aerodynamic force is considered to be applied, not at the center of pressure, but at the aerodynamic center of the airfoil.  the pitching moment on the wing of an airplane is part of the total moment that must be balanced using the lift on the horizontal stabilizer. more generally, a pitching moment is any moment acting on the pitch axis of a moving body. the lift on an airfoil is a distributed force that can be said to act at a point called the center of pressure.  however, as angle of attack changes on a cambered airfoil, there is movement of the center of pressure forward and aft.  this makes analysis difficult when attempting to use the concept of the center of pressure.  one of the remarkable properties of a cambered airfoil is that, even though the center of pressure moves forward and aft, if the lift is imagined to act at a point called the aerodynamic center. the moment of the lift force changes in proportion to the square of the airspeed.  if the moment is divided by the dynamic pressure, the area and chord of the airfoil, the result is known as the pitching moment coefficient. this coefficient changes only a little over the operating range of angle of attack of the airfoil but the change in moment slope against the aoa shown in figure below seems very steep so this should be of change in pitching moment of wing about cg rather than about ac.  the combination of the two concepts of aerodynamic center and pitching moment coefficient make it relatively simple to analyse some of the flight characteristics of an aircraft.   == measurement == the aerodynamic center of an airfoil is usually close to 25% of the chord behind the leading edge of the airfoil.  when making tests on a model airfoil, such as in a wind-tunnel, if the force sensor is not aligned with the quarter-chord of the airfoil, but offset by a distance x, the pitching moment about the quarter-chord point,     m  c  /  4      is a dimensionless coefficient so consistent units must be used for m, q, s and c. pitching moment coefficient is fundamental to the definition of aerodynamic center of an airfoil.  the aerodynamic center is defined to be the point on the chord line of the airfoil at which the pitching moment coefficient does not vary with angle of attack, or at least does not vary significantly over the operating range of angle of attack of the airfoil. in the case of a symmetric airfoil, the lift force acts through one point for all angles of attack, and the center of pressure does not move as it does in a cambered airfoil.  consequently, the pitching moment coefficient for a symmetric airfoil is zero. the pitching moment is, by convention, considered to be positive when it acts to pitch the airfoil in the nose-up direction.  conventional cambered airfoils supported at the aerodynamic center pitch nose-down so the pitching moment coefficient of these airfoils is negative.   == references == l. j. clancy (1975), aerodynamics, pitman publishing limited, london, isbn 0-273-01120-0 piercy, n.a.v (1943) aerodynamics, pages 384–386, english universities press. london low-speed stability retrieved on 2008-07-18 == see also ==  aircraft flight mechanics flight dynamics longitudinal static stability neutral point lift coefficient drag coefficient')"
53,"Tokyo tanks were internally mounted self-sealing fuel tanks used in the Boeing B-17 Flying Fortress and Consolidated B-24 Liberator bombers during World War II. Although nicknamed ""Tokyo"" tanks to dramatically illustrate the significant range they added to the B-17 (approximately 40% greater with combat weights), it was also an exaggeration in that no B-17 ever had the range to bomb Japan from any base in World War II.


== Description ==
These fuel tanks consisted of eighteen removable containers made of a rubberized compound, called cells, installed inside the wings of the airplane, nine to each side. The wings of the B-17 consisted of an ""inboard wing"" structure mounted to the fuselage which held the engines and flaps, and an ""outboard wing"" structure joined to the inboard wing and carrying the ailerons. The Tokyo tanks were installed on either side of the joint (a load-bearing point) where the two wing portions were connected. Five cells, totaling 270 US gallons (1,000 L) capacity, sat side by side in the outboard wing and were joined by a fuel line to the main tank delivering fuel to the outboard engine. The sixth cell was located in the space where the wing sections joined, and the remaining three cells were located side-by-side in the inboard wing; these four cells delivered 270 US gallons (1,000 L) of fuel to the feeder tank for the inboard engine. The same arrangement was repeated on the opposite wing. The Tokyo tanks added 1,080 US gallons (4,100 L) of fuel to the 1,700 US gallons (6,400 L) already carried in the six regular wing tanks and the 820 US gallons (3,100 L) that could be carried in an auxiliary tank that could be mounted in the bomb bay, for a combined total of 3,600 US gallons (14,000 L).
All B-17F aircraft built by Boeing from Block 80, by Douglas from Block 25, and by Vega from Block 30 were equipped with Tokyo tanks, and the entire run of B-17Gs by all three manufacturers had Tokyo tanks. B-17s with factory-mounted Tokyo tanks were first introduced to the Eighth Air Force in England in April 1943 with the arrival of the 94th and 95th Bomb Groups, equipped with new aircraft. By June 1943, aircraft that were so equipped began to appear in greater numbers as replacements, and from the beginning of July 1943, all replacement aircraft that did not have the tanks already installed were equipped before issue.
Although the tanks were removable, this could only be done by first removing the wing panels, and so was not a routine maintenance task. A drawback to the tanks was that there was no means of measuring remaining fuel quantity within the cells. Fuel was moved from the cells to the engine tanks by opening control valves within the bomb bay so that the fuel drained by gravity. Although the tanks were specified as self-sealing, vapor buildup within partially drained tanks made them explosive hazards in combat.


== References ==

Bishop, Cliff T. Fortresses of the Big Triangle First (1986), pp. 50–51. ISBN 1-869987-00-4","pandas(index=53, _1=53, text='tokyo tanks were internally mounted self-sealing fuel tanks used in the boeing b-17 flying fortress and consolidated b-24 liberator bombers during world war ii. although nicknamed ""tokyo"" tanks to dramatically illustrate the significant range they added to the b-17 (approximately 40% greater with combat weights), it was also an exaggeration in that no b-17 ever had the range to bomb japan from any base in world war ii.   == description == these fuel tanks consisted of eighteen removable containers made of a rubberized compound, called cells, installed inside the wings of the airplane, nine to each side. the wings of the b-17 consisted of an ""inboard wing"" structure mounted to the fuselage which held the engines and flaps, and an ""outboard wing"" structure joined to the inboard wing and carrying the ailerons. the tokyo tanks were installed on either side of the joint (a load-bearing point) where the two wing portions were connected. five cells, totaling 270 us gallons (1,000 l) capacity, sat side by side in the outboard wing and were joined by a fuel line to the main tank delivering fuel to the outboard engine. the sixth cell was located in the space where the wing sections joined, and the remaining three cells were located side-by-side in the inboard wing; these four cells delivered 270 us gallons (1,000 l) of fuel to the feeder tank for the inboard engine. the same arrangement was repeated on the opposite wing. the tokyo tanks added 1,080 us gallons (4,100 l) of fuel to the 1,700 us gallons (6,400 l) already carried in the six regular wing tanks and the 820 us gallons (3,100 l) that could be carried in an auxiliary tank that could be mounted in the bomb bay, for a combined total of 3,600 us gallons (14,000 l). all b-17f aircraft built by boeing from block 80, by douglas from block 25, and by vega from block 30 were equipped with tokyo tanks, and the entire run of b-17gs by all three manufacturers had tokyo tanks. b-17s with factory-mounted tokyo tanks were first introduced to the eighth air force in england in april 1943 with the arrival of the 94th and 95th bomb groups, equipped with new aircraft. by june 1943, aircraft that were so equipped began to appear in greater numbers as replacements, and from the beginning of july 1943, all replacement aircraft that did not have the tanks already installed were equipped before issue. although the tanks were removable, this could only be done by first removing the wing panels, and so was not a routine maintenance task. a drawback to the tanks was that there was no means of measuring remaining fuel quantity within the cells. fuel was moved from the cells to the engine tanks by opening control valves within the bomb bay so that the fuel drained by gravity. although the tanks were specified as self-sealing, vapor buildup within partially drained tanks made them explosive hazards in combat.   == references ==  bishop, cliff t. fortresses of the big triangle first (1986), pp. 50–51. isbn 1-869987-00-4')"
54,"A cambered aerofoil generates no lift when it is moving parallel to an axis called the zero-lift axis (or the zero-lift line.)  When the angle of attack on an aerofoil is measured relative to the zero-lift axis it is true to say the lift coefficient is zero when the angle of attack is zero.  For this reason, on a cambered aerofoil the zero-lift line is better than the chord line when describing the angle of attack.When symmetric aerofoils are moving parallel to the chord line of the aerofoil, zero lift is generated.  However, when cambered aerofoils are moving parallel to the chord line, lift is generated.  (See diagram at right.) For symmetric aerofoils, the chord line and the zero lift line are the same.


== See also ==
Angle of attack
Aerobatics
Aerobatic maneuver


== References ==
Anderson, John D. Jr (2005), Introduction to Flight, Section 7.4 (fifth edition), McGraw-Hill ISBN 0-07-282569-3
L. J. Clancy (1975), Aerodynamics, Sections 5.6 and 5.7, Pitman Publishing, London.  ISBN 0-273-01120-0
Kermode, A.C. (1972), Mechanics of Flight, Chapter 3, (p. 76, eighth edition), Pitman Publishing ISBN 0-273-31623-0


== Notes ==","pandas(index=54, _1=54, text='a cambered aerofoil generates no lift when it is moving parallel to an axis called the zero-lift axis (or the zero-lift line.)  when the angle of attack on an aerofoil is measured relative to the zero-lift axis it is true to say the lift coefficient is zero when the angle of attack is zero.  for this reason, on a cambered aerofoil the zero-lift line is better than the chord line when describing the angle of attack.when symmetric aerofoils are moving parallel to the chord line of the aerofoil, zero lift is generated.  however, when cambered aerofoils are moving parallel to the chord line, lift is generated.  (see diagram at right.) for symmetric aerofoils, the chord line and the zero lift line are the same.   == see also == angle of attack aerobatics aerobatic maneuver   == references == anderson, john d. jr (2005), introduction to flight, section 7.4 (fifth edition), mcgraw-hill isbn 0-07-282569-3 l. j. clancy (1975), aerodynamics, sections 5.6 and 5.7, pitman publishing, london.  isbn 0-273-01120-0 kermode, a.c. (1972), mechanics of flight, chapter 3, (p. 76, eighth edition), pitman publishing isbn 0-273-31623-0   == notes ==')"
55,"Clean configuration is the flight configuration of a fixed-wing aircraft when its external equipment is retracted to minimize drag, and thus maximize airspeed for a given power setting.
For most airplanes, clean configuration means simply that the wing flaps and landing gear are retracted, as these are the cause of drag due to the lack of streamlined shape. On more complex airplanes, it also means that other devices on the wings (such as slats, spoilers, and leading edge flaps) are retracted. Clean configuration is used for normal cruising at altitude during which lift, or rise in altitude, is not needed.
In military aviation, a clean configuration is generally without external stores which reduce maximum performance both due to increased weight and even more so due to increased drag.


== References ==","pandas(index=55, _1=55, text='clean configuration is the flight configuration of a fixed-wing aircraft when its external equipment is retracted to minimize drag, and thus maximize airspeed for a given power setting. for most airplanes, clean configuration means simply that the wing flaps and landing gear are retracted, as these are the cause of drag due to the lack of streamlined shape. on more complex airplanes, it also means that other devices on the wings (such as slats, spoilers, and leading edge flaps) are retracted. clean configuration is used for normal cruising at altitude during which lift, or rise in altitude, is not needed. in military aviation, a clean configuration is generally without external stores which reduce maximum performance both due to increased weight and even more so due to increased drag.   == references ==')"
56,"Parts Manufacturer Approval (PMA) is an approval granted by the United States Federal Aviation Administration (FAA) to a manufacturer of aircraft parts.


== Approval ==
It is generally illegal in the United States to install replacement or modification parts on a certificated aircraft without an airworthiness release such as a Supplemental Type Certificate (STC) or Parts Manufacturing Approval (PMA).  There are a number of other methods of compliance, including parts manufactured to government or industry standards, parts manufactured under technical standard order authorization [TSO], owner-/operator-produced parts, experimental aircraft, field approvals, etc.PMA-holding manufacturers are permitted to make replacement parts for aircraft, even though they are not the original manufacturer of the aircraft.  The process is analogous to 'after-market' parts for automobiles, except that the United States aircraft parts production market remains tightly regulated by the FAA.
An applicant for a PMA applies for approval from the FAA. The FAA prioritizes its review of a new application based on its internal process called Project Prioritization.The FAA Order covering the application for PMA is Order 8110.42 revision D.  This document is worded as instructions to the FAA reviewing personnel.  An accompanying Advisory Circular (AC) 21.303-4 is intended to address the applicant.  8110.42C addressed both the applicant and the reviewer.  Per the order, application for a PMA can be made per the following ways:  Identicality in which the applicant attempts to convince the FAA that the PMA part is identical to the OAH (Original Approval Holder) part.  Identicality by Licensure is accomplished by providing evidence to the FAA that the applicant has licensed the part data from the OAH.  This evidence is usually in the form of an Assist Letter provided to the applicant by the OAH.  PMA may also be granted based upon prior approval of an STC .  As an example:  If an STC were granted to alter an existing aircraft design then that approval would also apply to the parts needed to make that modification.  A PMA would be required, however, to manufacture the parts.  The last method to obtain a PMA is Test & Computation.  This approach consist of one or a combination of both of the following methods:  General Analysis and Comparative Analysis.  General analysis compares the proposed part to the functional requirements of that part when installed.  Comparative Analysis compares the function of the proposed part to the OAH part.  As an example:  If a PMA application for flight control cables were to show that the PMA part exceeds the pull strength requirements of the aircraft system it is meant for, that is general analysis.  To show that it exceeds that of the OAH part is comparative analysis.  The modern trend is to use a variety of techniques in combination in order to obtain approval of complicated parts - relying on the techniques that are most accurate and best able to provide the proof of airworthiness desired.  The cognizant regional FAA Aircraft Certification Office (ACO) determines if the applicant has shown compliance with all relevant airworthiness regulations and is thus entitled to design approval.
The second step in the application process is to apply to the FAA Manufacturing Inspection Divisional Office (MIDO) to obtain approval of the manufacturing quality assurance system (known as production approval).  Production approval will be granted when the FAA is satisfied that the system will not permit parts to leave the system until the parts have been verified to meet the requirements of the approved design, and the system otherwise meets the requirements of the FAA quality system regulations.  A Production Approval Holder (PAH) will typically already have satisfied this requirement before PMA application is made.
PMA applications based upon licensure or STC do not require ACO approval (since the data has already been approved) and can go straight to the MIDO.


== History ==
Under the Civil Air Regulations (CARs), the government had the authority to approve aircraft parts in a predecessor to the PMA rules.  This authority was found in each of the sets of airworthiness standards published in the Civil Air Regulations.  CAR 3.31, for example, permitted the Administrator to approve aircraft parts as early as 1947.In 1952, the Civil Aeronautics Board adjusted the location of the parts production authority from the "".31"" regulations to the "".18"" regulations.  For example, the CAR 3 authority for modification and replacement parts could be found in section 3.18 after 1952.
In 1955, the Civil Aeronautics Board separated the parts authority out of the airworthiness standards, and placed it in a more general location so that one standard would apply to replacement and modification parts for all different forms of aircraft.In 1965 CAR 1.55 became Federal Aviation Regulation section 21.303.The 1965 regulatory change also imposed specific obligations on the PMA holder related to the Fabrication Inspection System.Amendment 21-38 of Part 21 was published May 26, 1972.  This was the next rule change to affect PMAs.  This rule eliminated the incorporation by reference of type certification requirements in favor of PMA-specific data submission requirements.  This change established the separate process and separate requirements for data that must be submitted by an applicant for a PMA (prior to this there was no explicit distinction between the application data requirements for type certificated products and the data requirements for PMAed articles).The aircraft parts aftermarket expanded greatly in the 1980s as airlines sought to reduce the costs of spares by finding alternative sources of parts. During this time period, though, many manufacturers failed to obtain PMA approvals from the FAA.
In the 1990s, the FAA engaged in an ""Enhanced Enforcement"" program that educated the industry about the importance of approval and as a consequence a huge number of parts were approved through formal FAA mechanisms.   Under this program, companies that had previously manufactured aircraft parts without PMAs could apply for PMAs in order to bring their manufacturing operations into full compliance with the regulations.  This movement brought an explosion of PMA parts to the marketplace.


== 2009 Rule Change ==
The FAA published a significant revision to the U.S. manufacturing regulations on October 16, 2009.  This new rule eliminates some of the legal distinctions between forms of production approval issued by the FAA, which should have the effect of further demonstrating the FAA's support of the quality systems implemented by PMA manufacturers.  Specifically, instead of having a separate body of regulations for a PMA Fabrication Inspection System (FIS), as was the case in prior regulations, the PMA regulations now include a cross reference to the 14 C.F.R. § 21.137, which is the regulation defining the elements of a quality system for all production approval holders.  In practice, all production approval holders were held to the same production quality standards before the rule change - this will now be more obvious in the FAA's regulations.  Accomplishing this harmonization of standards was an important goal of the Modification and Replacement Parts Association (MARPA).
The new rule became effective April 16, 2011.  The  FAA's FAQ on Part 21 stated that PMA quality systems would be evaluated for compliance by the FAA during certificate management activity after the compliance date of the rule.  Today, all FAA production approvals - whether for complete aircraft or for piece parts - rely on a common set of quality assurance system elements.  E.g. 14 C.F.R. §§ 21.137 (quality system requirements for production certificates), 21.307 (requiring PMA holders to establish a quality system that meets the requirements of § 21.137), 21.607 (requiring TSOA holders to establish a quality system that meets the requirements of § 21.137).


== Relationship to repair ==
The FAA is also working on new policies concerning parts fabricated in the course of repair.  This practice has historically been confused with PMA manufacturing, although the two are actually quite different practices supported by different FAA regulations.  Today, FAA Advisory Circular 43.18 provides guidance for the fabrication of parts to be consumed purely during a maintenance operation, and additional guidance is expected to be released in the near future.  One of the key features of FAC 43.18 is that it recommends implementation of a quality assurance system quite similar to the fabrication inspection systems that PMA manufacturers are required to have.


== Industry association ==
The trade association representing the PMA industry is the Modification and Replacement Parts Association (MARPA).  MARPA works closely with the FAA and other agencies to promote PMA safety.  MARPA maintains a website at http://www.pmaparts.org.


== Developments Outside the United States ==
The United States has Bilateral Aviation Safety Agreements (BASA) with most of its major trading partners, and the standard language of these BASAs requires the trading partner to treat FAA-PMA as an importable aircraft part that is airworthy and eligible for installation on aircraft registered in the importing jurisdiction.  This process has been facilitated by the International Air Transport Association (IATA) which has published a book on accepting PMA parts. Although the PMA industry began in the United States, several countries have begun promoting production of approved aircraft parts within their own borders.  These jurisdictions include:

Australia
China
The European Union (which produces them as ""EPA Parts"")Other jurisdictions have established PMA regulations and are working with trading partners to achieve acceptance of their PMA industries, and thus should be expected to enter the PMA marketplace in the near future.  For example, Japan has PMA regulations and has secured a bilateral agreement with the United States that authorizes the export of these parts to the United States as airworthy aircraft parts.  


== References ==


== External links ==
MARPA
YouTube Video: ""What is PMA?""","pandas(index=56, _1=56, text='parts manufacturer approval (pma) is an approval granted by the united states federal aviation administration (faa) to a manufacturer of aircraft parts.   == approval == it is generally illegal in the united states to install replacement or modification parts on a certificated aircraft without an airworthiness release such as a supplemental type certificate (stc) or parts manufacturing approval (pma).  there are a number of other methods of compliance, including parts manufactured to government or industry standards, parts manufactured under technical standard order authorization [tso], owner-/operator-produced parts, experimental aircraft, field approvals, etc.pma-holding manufacturers are permitted to make replacement parts for aircraft, even though they are not the original manufacturer of the aircraft.  the process is analogous to \'after-market\' parts for automobiles, except that the united states aircraft parts production market remains tightly regulated by the faa. an applicant for a pma applies for approval from the faa. the faa prioritizes its review of a new application based on its internal process called project prioritization.the faa order covering the application for pma is order 8110.42 revision d.  this document is worded as instructions to the faa reviewing personnel.  an accompanying advisory circular (ac) 21.303-4 is intended to address the applicant.  8110.42c addressed both the applicant and the reviewer.  per the order, application for a pma can be made per the following ways:  identicality in which the applicant attempts to convince the faa that the pma part is identical to the oah (original approval holder) part.  identicality by licensure is accomplished by providing evidence to the faa that the applicant has licensed the part data from the oah.  this evidence is usually in the form of an assist letter provided to the applicant by the oah.  pma may also be granted based upon prior approval of an stc .  as an example:  if an stc were granted to alter an existing aircraft design then that approval would also apply to the parts needed to make that modification.  a pma would be required, however, to manufacture the parts.  the last method to obtain a pma is test & computation.  this approach consist of one or a combination of both of the following methods:  general analysis and comparative analysis.  general analysis compares the proposed part to the functional requirements of that part when installed.  comparative analysis compares the function of the proposed part to the oah part.  as an example:  if a pma application for flight control cables were to show that the pma part exceeds the pull strength requirements of the aircraft system it is meant for, that is general analysis.  to show that it exceeds that of the oah part is comparative analysis.  the modern trend is to use a variety of techniques in combination in order to obtain approval of complicated parts - relying on the techniques that are most accurate and best able to provide the proof of airworthiness desired.  the cognizant regional faa aircraft certification office (aco) determines if the applicant has shown compliance with all relevant airworthiness regulations and is thus entitled to design approval. the second step in the application process is to apply to the faa manufacturing inspection divisional office (mido) to obtain approval of the manufacturing quality assurance system (known as production approval).  production approval will be granted when the faa is satisfied that the system will not permit parts to leave the system until the parts have been verified to meet the requirements of the approved design, and the system otherwise meets the requirements of the faa quality system regulations.  a production approval holder (pah) will typically already have satisfied this requirement before pma application is made. pma applications based upon licensure or stc do not require aco approval (since the data has already been approved) and can go straight to the mido.   == history == under the civil air regulations (cars), the government had the authority to approve aircraft parts in a predecessor to the pma rules.  this authority was found in each of the sets of airworthiness standards published in the civil air regulations.  car 3.31, for example, permitted the administrator to approve aircraft parts as early as 1947.in 1952, the civil aeronautics board adjusted the location of the parts production authority from the "".31"" regulations to the "".18"" regulations.  for example, the car 3 authority for modification and replacement parts could be found in section 3.18 after 1952. in 1955, the civil aeronautics board separated the parts authority out of the airworthiness standards, and placed it in a more general location so that one standard would apply to replacement and modification parts for all different forms of aircraft.in 1965 car 1.55 became federal aviation regulation section 21.303.the 1965 regulatory change also imposed specific obligations on the pma holder related to the fabrication inspection system.amendment 21-38 of part 21 was published may 26, 1972.  this was the next rule change to affect pmas.  this rule eliminated the incorporation by reference of type certification requirements in favor of pma-specific data submission requirements.  this change established the separate process and separate requirements for data that must be submitted by an applicant for a pma (prior to this there was no explicit distinction between the application data requirements for type certificated products and the data requirements for pmaed articles).the aircraft parts aftermarket expanded greatly in the 1980s as airlines sought to reduce the costs of spares by finding alternative sources of parts. during this time period, though, many manufacturers failed to obtain pma approvals from the faa. in the 1990s, the faa engaged in an ""enhanced enforcement"" program that educated the industry about the importance of approval and as a consequence a huge number of parts were approved through formal faa mechanisms.   under this program, companies that had previously manufactured aircraft parts without pmas could apply for pmas in order to bring their manufacturing operations into full compliance with the regulations.  this movement brought an explosion of pma parts to the marketplace.   == 2009 rule change == the faa published a significant revision to the u.s. manufacturing regulations on october 16, 2009.  this new rule eliminates some of the legal distinctions between forms of production approval issued by the faa, which should have the effect of further demonstrating the faa\'s support of the quality systems implemented by pma manufacturers.  specifically, instead of having a separate body of regulations for a pma fabrication inspection system (fis), as was the case in prior regulations, the pma regulations now include a cross reference to the 14 c.f.r. § 21.137, which is the regulation defining the elements of a quality system for all production approval holders.  in practice, all production approval holders were held to the same production quality standards before the rule change - this will now be more obvious in the faa\'s regulations.  accomplishing this harmonization of standards was an important goal of the modification and replacement parts association (marpa). the new rule became effective april 16, 2011.  the  faa\'s faq on part 21 stated that pma quality systems would be evaluated for compliance by the faa during certificate management activity after the compliance date of the rule.  today, all faa production approvals - whether for complete aircraft or for piece parts - rely on a common set of quality assurance system elements.  e.g. 14 c.f.r. §§ 21.137 (quality system requirements for production certificates), 21.307 (requiring pma holders to establish a quality system that meets the requirements of § 21.137), 21.607 (requiring tsoa holders to establish a quality system that meets the requirements of § 21.137).   == relationship to repair == the faa is also working on new policies concerning parts fabricated in the course of repair.  this practice has historically been confused with pma manufacturing, although the two are actually quite different practices supported by different faa regulations.  today, faa advisory circular 43.18 provides guidance for the fabrication of parts to be consumed purely during a maintenance operation, and additional guidance is expected to be released in the near future.  one of the key features of fac 43.18 is that it recommends implementation of a quality assurance system quite similar to the fabrication inspection systems that pma manufacturers are required to have.   == industry association == the trade association representing the pma industry is the modification and replacement parts association (marpa).  marpa works closely with the faa and other agencies to promote pma safety.  marpa maintains a website at http://www.pmaparts.org.   == developments outside the united states == the united states has bilateral aviation safety agreements (basa) with most of its major trading partners, and the standard language of these basas requires the trading partner to treat faa-pma as an importable aircraft part that is airworthy and eligible for installation on aircraft registered in the importing jurisdiction.  this process has been facilitated by the international air transport association (iata) which has published a book on accepting pma parts. although the pma industry began in the united states, several countries have begun promoting production of approved aircraft parts within their own borders.  these jurisdictions include:  australia china the european union (which produces them as ""epa parts"")other jurisdictions have established pma regulations and are working with trading partners to achieve acceptance of their pma industries, and thus should be expected to enter the pma marketplace in the near future.  for example, japan has pma regulations and has secured a bilateral agreement with the united states that authorizes the export of these parts to the united states as airworthy aircraft parts.   == references ==   == external links == marpa youtube video: ""what is pma?""')"
57,"Decalage on a fixed-wing aircraft is the angle difference between the upper and lower wings of a biplane, i.e. the acute angle contained between the chords of the wings in question.
Decalage is said to be positive when the upper wing has a higher angle of incidence than the lower wing, and negative when the lower wing's incidence is greater than that of the upper wing. Positive decalage results in greater lift from the upper wing than the lower wing, the difference increasing with the amount of decalage.In a survey of representative biplanes, real-life design decalage is typically zero, with both wings having equal incidence.  A notable exception is the Stearman PT-17, which has 4° of incidence in the lower wing, and 3° in the upper wing.  Considered from an aerodynamic perspective, it is desirable to have the forward-most wing stall first, which will induce a pitch-down moment, aiding in stall recovery.  Biplane designers may use incidence to control stalling behavior, but may also use airfoil selection or other means to accomplish correct behavior.
Decalage angle can also refer to the difference in angle of the chord line of the wing and the chord line of the horizontal stabilizer. This is different from the angle of incidence, which refers to the angle of the wing chord to the longitudinal axis of the fuselage, without reference to the horizontal stabilizer.


== References ==


== External links ==
National Advisory Committee for Aeronautics test reports accessed through the Cranfield University AERADE website.","pandas(index=57, _1=57, text=""decalage on a fixed-wing aircraft is the angle difference between the upper and lower wings of a biplane, i.e. the acute angle contained between the chords of the wings in question. decalage is said to be positive when the upper wing has a higher angle of incidence than the lower wing, and negative when the lower wing's incidence is greater than that of the upper wing. positive decalage results in greater lift from the upper wing than the lower wing, the difference increasing with the amount of decalage.in a survey of representative biplanes, real-life design decalage is typically zero, with both wings having equal incidence.  a notable exception is the stearman pt-17, which has 4° of incidence in the lower wing, and 3° in the upper wing.  considered from an aerodynamic perspective, it is desirable to have the forward-most wing stall first, which will induce a pitch-down moment, aiding in stall recovery.  biplane designers may use incidence to control stalling behavior, but may also use airfoil selection or other means to accomplish correct behavior. decalage angle can also refer to the difference in angle of the chord line of the wing and the chord line of the horizontal stabilizer. this is different from the angle of incidence, which refers to the angle of the wing chord to the longitudinal axis of the fuselage, without reference to the horizontal stabilizer.   == references ==   == external links == national advisory committee for aeronautics test reports accessed through the cranfield university aerade website."")"
58,"Flying qualities is one of the three principal regimes in the science of flight test, which also includes performance and systems. Flying qualities involves the study and evaluation of the stability and control characteristics of an aircraft. They have a critical bearing on the safety of flight and on the ease of controlling an airplane in steady flight and in maneuvers.


== Relation to stability ==
To understand the discipline of flying qualities, the concept of stability should be understood. Stability can be defined only when the vehicle is in trim; that is, there are no unbalanced forces or moments acting on the vehicle to cause it to deviate from steady flight. If this condition exists, and if the vehicle is disturbed, stability refers to the tendency of the vehicle to return to the trimmed condition. If the vehicle initially tends to return to a trimmed condition, it is said to be statically stable. If it continues to approach the trimmed condition without overshooting, the motion is called a subsidence. If the motion causes the vehicle to overshoot the trimmed condition, it may oscillate back and forth. If this oscillation damps out, the motion is called a damped oscillation and the vehicle is said to be dynamically stable. On the other hand, if the motion increases in amplitude, the vehicle is said to be dynamically unstable.
The theory of stability of airplanes was worked out by G. H. Bryan in England in 1904. This theory is essentially equivalent to the theory taught to aeronautical students today and was a remarkable intellectual achievement considering that at the time Bryan developed the theory, he had not even heard of the Wright brothers' first flight. Because of the complication of the theory and the tedious computations required in its use, it was rarely applied by airplane designers. Obviously, to fly successfully, pilotless airplanes had to be dynamically stable. The airplane flown by the Wright brothers, and most airplanes flown thereafter, were not stable, but by trial and error, designers developed a few planes that had satisfactory flying qualities. Many other airplanes, however, had poor flying qualities, which sometimes resulted in crashes.
Handling qualities are those characteristics of a flight vehicle that govern the ease and precision with which a pilot is able to perform a flying task. This includes the human-machine interface.  The way in which particular vehicle factors affect flying qualities has been studied in aircraft for decades, and reference standards for the flying qualities of both fixed-wing aircraft and rotary-wing aircraft have been developed and are now in common use. These standards define a subset of the dynamics and control design space that provides good handling qualities for a given vehicle type and flying task.


== Historical development ==
Bryan showed that the stability characteristics of airplanes could be separated into longitudinal and lateral groups with the corresponding motions called modes of motion. These modes of motion were either aperiodic, which means that the airplane steadily approaches or diverges from a trimmed condition, or oscillatory, which means that the airplane oscillates about the trim condition. The longitudinal modes of a statically stable airplane following a disturbance were shown to consist of a long-period oscillation called the phugoid oscillation, usually with a period in seconds about one-quarter of the airspeed in miles per hour and a short-period oscillation with a period of only a few seconds. The lateral motion had three modes of motion: an aperiodic mode called the spiral mode that could be a divergence or subsidence, a heavily damped aperiodic mode called the roll subsidence, and a short-period oscillation, usually poorly damped, called the Dutch roll mode.
Some early airplane designers attempted to make airplanes that were dynamically stable, but it was found that the requirements for stability conflicted with those for satisfactory flying qualities. Meanwhile, no information was available to guide the designer as to just what characteristics should be incorporated to provide satisfactory flying qualities.
By the 1930s, there was a general feeling that airplanes should be dynamically stable, but some aeronautical engineers were starting to recognize the conflict between the requirements for stability and flying qualities. To resolve this question, Edward Warner, who was working as a consultant to the Douglas Aircraft Company on the design of the DC-4, a large four-engine transport airplane, made the first effort in the United States to write a set of requirements for satisfactory flying qualities. Dr. Warner, a member of the main committee of the NACA, also requested that a flight study be made to determine the flying qualities of an airplane along the lines of the suggested requirements. This study was conducted by Hartley A. Soulé of Langley. Entitled Preliminary Investigation of the Flying Qualities of Airplanes, Soulé's report showed several areas in which the suggested requirements needed revision and showed the need for more research on other types of airplanes. As a result, a program was started by Robert R. Gilruth with Melvin N. Gough as the chief test pilot.


== Evaluation of flying qualities ==
The technique for the study of flying qualities requirements used by Gilruth was first to install instruments to record relevant quantities such as control positions and forces, airplane angular velocities, linear accelerations, airspeed, and altitude. Then a program of specified flight conditions and maneuvers was flown by an experienced test pilot. After the flight, data were transcribed from the records and the results were correlated with pilot opinion. This approach would be considered routine today, but it was a notable original contribution by Gilruth that took advantage of the flight recording instruments already available at Langley and the variety of airplanes available for tests under comparable conditions.
An important quantity in flying qualities measurements in turns or pull-ups is the variation of control force on the control stick or wheel with the value of acceleration normal to the flight direction expressed in g units. This quantity is usually called the force per g.


== Relation to Spacecraft ==
A new generation of spacecraft now under development by NASA to replace the Space Shuttle and return astronauts to the Moon will have a manual control capability for several mission tasks, and the ease and precision with which pilots can execute these tasks will have an important effect on performance, mission risk and training costs. No reference standards currently exist for flying qualities of piloted spacecraft.


== See also ==
Flight test
Cooper-Harper rating scale
Pilot-induced oscillation
Longitudinal static stability
Flight envelope


== References ==


== External links ==
William Hewitt Phillips. ""Flying Qualities"". Journey in Aeornautical Research: A Career at NASA Langley Research Center. Retrieved 2010-07-31.
Airplane Stability and Control by Malcolm L. Abzug
Stengel R F: Flight Dynamics. Princeton University Press 2004, ISBN 0-691-11407-2.","pandas(index=58, _1=58, text='flying qualities is one of the three principal regimes in the science of flight test, which also includes performance and systems. flying qualities involves the study and evaluation of the stability and control characteristics of an aircraft. they have a critical bearing on the safety of flight and on the ease of controlling an airplane in steady flight and in maneuvers.   == relation to stability == to understand the discipline of flying qualities, the concept of stability should be understood. stability can be defined only when the vehicle is in trim; that is, there are no unbalanced forces or moments acting on the vehicle to cause it to deviate from steady flight. if this condition exists, and if the vehicle is disturbed, stability refers to the tendency of the vehicle to return to the trimmed condition. if the vehicle initially tends to return to a trimmed condition, it is said to be statically stable. if it continues to approach the trimmed condition without overshooting, the motion is called a subsidence. if the motion causes the vehicle to overshoot the trimmed condition, it may oscillate back and forth. if this oscillation damps out, the motion is called a damped oscillation and the vehicle is said to be dynamically stable. on the other hand, if the motion increases in amplitude, the vehicle is said to be dynamically unstable. the theory of stability of airplanes was worked out by g. h. bryan in england in 1904. this theory is essentially equivalent to the theory taught to aeronautical students today and was a remarkable intellectual achievement considering that at the time bryan developed the theory, he had not even heard of the wright brothers\' first flight. because of the complication of the theory and the tedious computations required in its use, it was rarely applied by airplane designers. obviously, to fly successfully, pilotless airplanes had to be dynamically stable. the airplane flown by the wright brothers, and most airplanes flown thereafter, were not stable, but by trial and error, designers developed a few planes that had satisfactory flying qualities. many other airplanes, however, had poor flying qualities, which sometimes resulted in crashes. handling qualities are those characteristics of a flight vehicle that govern the ease and precision with which a pilot is able to perform a flying task. this includes the human-machine interface.  the way in which particular vehicle factors affect flying qualities has been studied in aircraft for decades, and reference standards for the flying qualities of both fixed-wing aircraft and rotary-wing aircraft have been developed and are now in common use. these standards define a subset of the dynamics and control design space that provides good handling qualities for a given vehicle type and flying task.   == historical development == bryan showed that the stability characteristics of airplanes could be separated into longitudinal and lateral groups with the corresponding motions called modes of motion. these modes of motion were either aperiodic, which means that the airplane steadily approaches or diverges from a trimmed condition, or oscillatory, which means that the airplane oscillates about the trim condition. the longitudinal modes of a statically stable airplane following a disturbance were shown to consist of a long-period oscillation called the phugoid oscillation, usually with a period in seconds about one-quarter of the airspeed in miles per hour and a short-period oscillation with a period of only a few seconds. the lateral motion had three modes of motion: an aperiodic mode called the spiral mode that could be a divergence or subsidence, a heavily damped aperiodic mode called the roll subsidence, and a short-period oscillation, usually poorly damped, called the dutch roll mode. some early airplane designers attempted to make airplanes that were dynamically stable, but it was found that the requirements for stability conflicted with those for satisfactory flying qualities. meanwhile, no information was available to guide the designer as to just what characteristics should be incorporated to provide satisfactory flying qualities. by the 1930s, there was a general feeling that airplanes should be dynamically stable, but some aeronautical engineers were starting to recognize the conflict between the requirements for stability and flying qualities. to resolve this question, edward warner, who was working as a consultant to the douglas aircraft company on the design of the dc-4, a large four-engine transport airplane, made the first effort in the united states to write a set of requirements for satisfactory flying qualities. dr. warner, a member of the main committee of the naca, also requested that a flight study be made to determine the flying qualities of an airplane along the lines of the suggested requirements. this study was conducted by hartley a. soulé of langley. entitled preliminary investigation of the flying qualities of airplanes, soulé\'s report showed several areas in which the suggested requirements needed revision and showed the need for more research on other types of airplanes. as a result, a program was started by robert r. gilruth with melvin n. gough as the chief test pilot.   == evaluation of flying qualities == the technique for the study of flying qualities requirements used by gilruth was first to install instruments to record relevant quantities such as control positions and forces, airplane angular velocities, linear accelerations, airspeed, and altitude. then a program of specified flight conditions and maneuvers was flown by an experienced test pilot. after the flight, data were transcribed from the records and the results were correlated with pilot opinion. this approach would be considered routine today, but it was a notable original contribution by gilruth that took advantage of the flight recording instruments already available at langley and the variety of airplanes available for tests under comparable conditions. an important quantity in flying qualities measurements in turns or pull-ups is the variation of control force on the control stick or wheel with the value of acceleration normal to the flight direction expressed in g units. this quantity is usually called the force per g.   == relation to spacecraft == a new generation of spacecraft now under development by nasa to replace the space shuttle and return astronauts to the moon will have a manual control capability for several mission tasks, and the ease and precision with which pilots can execute these tasks will have an important effect on performance, mission risk and training costs. no reference standards currently exist for flying qualities of piloted spacecraft.   == see also == flight test cooper-harper rating scale pilot-induced oscillation longitudinal static stability flight envelope   == references ==   == external links == william hewitt phillips. ""flying qualities"". journey in aeornautical research: a career at nasa langley research center. retrieved 2010-07-31. airplane stability and control by malcolm l. abzug stengel r f: flight dynamics. princeton university press 2004, isbn 0-691-11407-2.')"
59,"The Parker variable wing is a wing configuration in biplane or triplane aircraft designed by H.F. Parker in 1920. His design allows a supplement in lift while landing or taking-off.
The system is depicted in the figure. The figure shows the biplane configuration. The lower airfoil is rigid. The upper airfoil is flexible. At high angle of attack the flow over the lower airfoil will cause the airflow to bend up and create an upward force at the lower surface of the upper airfoil. This upward force will cause the flexible section to be
pushed upward. The flexible wing section is held at points A and B. The trailing edge is rigid and can rotate about point B. Due to this effect the camber of the airfoil is increased, and hence the lift it creates is increased.


== See also ==
Aeroelasticity
X-53 Active Aeroelastic Wing
Adaptive compliant wing


== References ==","pandas(index=59, _1=59, text='the parker variable wing is a wing configuration in biplane or triplane aircraft designed by h.f. parker in 1920. his design allows a supplement in lift while landing or taking-off. the system is depicted in the figure. the figure shows the biplane configuration. the lower airfoil is rigid. the upper airfoil is flexible. at high angle of attack the flow over the lower airfoil will cause the airflow to bend up and create an upward force at the lower surface of the upper airfoil. this upward force will cause the flexible section to be pushed upward. the flexible wing section is held at points a and b. the trailing edge is rigid and can rotate about point b. due to this effect the camber of the airfoil is increased, and hence the lift it creates is increased.   == see also == aeroelasticity x-53 active aeroelastic wing adaptive compliant wing   == references ==')"
60,"A swept wing is a wing that angles either backward or occasionally forward from its root rather than in a straight sideways direction.
Swept wings have been flown since the pioneer days of aviation. Wing sweep at high speeds was first investigated in Germany as early as 1935 by Albert Betz and Adolph Busemann, finding application just before the end of the Second World War. It has the effect of delaying the shock waves and accompanying aerodynamic drag rise caused by fluid compressibility near the speed of sound, improving performance. Swept wings are therefore almost always used on jet aircraft designed to fly at these speeds. Swept wings are also sometimes used for other reasons, such as low drag, low observability, structural convenience or pilot visibility.
The term ""swept wing"" is normally used to mean ""swept back"", but variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back. The delta wing is also aerodynamically a form of swept wing.


== Design characteristics ==
For a wing of given span, sweeping it increases the length of the spars running along it from root to tip. This tends to increase weight and reduce stiffness. If the fore-aft chord of the wing also remains the same, the distance between leading and trailing edges reduces, reducing its ability to resist twisting (torsion) forces. A swept wing of given span and chord must therefore be strengthened and will be heavier than the equivalent unswept wing.
A swept wing typically angles backward from its root rather than forwards. Because wings are made as light as possible, they tend to flex under load. This aeroelasticity under aerodynamic load causes the tips to bend upwards in normal flight. Backwards sweep causes the tips to reduce their angle of attack as they bend, reducing their lift and limiting the effect. Forward sweep causes the tips to increase their angle of attack as they bend. This increases their lift causing further bending and hence yet more lift in a cycle which can cause a runaway structural failure. For this reason forward sweep is rare and the wing must be unusually rigid.
The characteristic ""sweep angle"" is normally measured by drawing a line from root to tip, typically 25% of the way back from the leading edge, and comparing that with the perpendicular to the longitudinal axis of the aircraft.  Typical sweep angles vary from 0 for a straight-wing aircraft, to 45 degrees or more for fighters and other high-speed designs.


== Aerodynamics ==


=== Subsonic and transonic flight ===

As an aircraft enters the transonic speeds just below the speed of sound, the pressure waves associated with subsonic flight converge and begin to impinge on the aircraft. As the pressure waves converge the air in front of the aircraft begins to compress. This creates a force known as wave drag. This wave drag increases steeply until the whole aircraft is supersonic and then reduces.
However, shock waves can form on some parts of an aircraft moving at less than the speed of sound. Low-pressure regions around an aircraft cause the flow to accelerate, and at transonic speeds this local acceleration can exceed Mach 1. Localized supersonic flow must return to the freestream conditions around the rest of the aircraft, and as the flow enters an adverse pressure gradient in the aft section of the wing, a discontinuity emerges in the form of a shock wave as the air is forced to rapidly slow and return to ambient pressure.
With objects where there is a sudden reduction in profile/thickness and the local air expands rapidly to fill the space taken by the solid object or where a rapid angular change is imparted to the airflow causing a momentary increase of volume/decrease in density, an oblique shock wave is generated. This is why shock waves are often associated with the part of a fighter aircraft cockpit canopy with the highest local curvature, appearing immediately behind this point.
At the point where the density drops, the local speed of sound correspondingly drops and a shock wave can form. This is why in conventional wings, shock waves form first after the maximum Thickness/Chord and why all airliners designed for cruising in the transonic range (above M0.8) have supercritical wings that are flatter on top resulting in minimized angular change of flow to upper surface air. The angular change to the air that is normally part of lift generation is decreased and this lift reduction is compensated for by deeper curved lower surfaces accompanied by a reflex curve at the trailing edge. This results in a much weaker standing shock wave towards the rear of the upper wing surface and a corresponding increase in critical mach number.
Shock waves require energy to form. This energy is taken out of the aircraft, which has to supply extra thrust to make up for this energy loss. Thus the shocks are seen as a form of drag. Since the shocks form when the local air velocity reaches supersonic speeds, there is a certain ""critical mach"" speed where sonic flow first appears on the wing. There is a following point called the drag divergence mach number where the effect of the drag from the shocks becomes noticeable.  This is normally when the shocks start generating over the wing, which on most aircraft is the largest continually curved surface, and therefore the largest contributor to this effect.
Sweeping the wing has the effect of reducing the curvature of the body as seen from the airflow, by the cosine of the angle of sweep. For instance, a wing with a 45 degree sweep will see a reduction in effective curvature to about 70% of its straight-wing value. This has the effect of increasing the critical Mach by 30%. When applied to large areas of the aircraft, like the wings and empennage, this allows the aircraft to reach speeds closer to Mach 1.
One of the simplest and best explanations of how the swept wing works was offered by Robert T. Jones:
""Suppose a cylindrical wing (constant chord, incidence, etc.) is placed in an airstream at an angle of yaw – i.e., it is swept back. Now, even if the local speed of the air on the upper surface of the wing becomes supersonic, a shock wave cannot form there because it would have to be a sweptback shock – swept at the same angle as the wing – i.e., it would be an oblique shock. Such an oblique shock cannot form until the velocity component normal to it becomes supersonic.""One limiting factor in swept wing design is the so-called ""middle effect"". If a swept wing is continuous – an oblique swept wing, the pressure iso-bars will be swept at a continuous angle from tip to tip. However, if the left and right halves are swept back equally, as is common practice, the pressure iso-bars on the left wing in theory will meet the pressure iso-bars of the right wing on the centerline at a large angle.  As the iso-bars cannot meet in such a fashion, they will tend to curve on each side as they near the centerline, so that the iso-bars cross the centerline at right angles to the centerline. This causes an ""unsweeping"" of the iso-bars in the wing root region.  To combat this unsweeping, German aerodynamicist Dietrich Küchemann proposed and had tested a local indentation of the fuselage above and below the wing root. This proved to not be very effective. During the development of the Douglas DC-8 airliner, uncambered airfoils were used in the wing root area to combat the unsweeping. Similarly, a decambered wing root glove was added to the Boeing 707 wing to create the Boeing 720.


=== Supersonic flight ===

Airflow at supersonic speeds generates lift through the formation of shock waves, as opposed to the patterns of airflow over and under the wing. These shock waves, as in the transonic case, generate large amounts of drag. One of these shock waves is created by the leading edge of the wing, but contributes little to the lift. In order to minimize the strength of this shock it needs to remain ""attached"" to the front of the wing, which demands a very sharp leading edge. To better shape the shocks that will contribute to lift, the rest of an ideal supersonic airfoil is roughly diamond-shaped in cross-section. For low-speed lift these same airfoils are very inefficient, leading to poor handling and very high landing speeds.One way to avoid the need for a dedicated supersonic wing is to use a highly swept subsonic design. Airflow behind the shock waves of a moving body are reduced to subsonic speeds. This effect is used within the intakes of engines meant to operate in the supersonic, as jet engines are generally incapable of ingesting supersonic air directly. This can also be used to reduce the speed of the air as seen by the wing, using the shocks generated by the nose of the aircraft. As long as the wing lies behind the cone-shaped shock wave, it will ""see"" subsonic airflow and work as normal. The angle needed to lie behind the cone increases with increasing speed, at Mach 1.3 the angle is about 45 degrees, at Mach 2.0 it is 60 degrees. For instance, at Mach 1.3 the angle of the Mach cone formed off the body of the aircraft will be at about sinμ = 1/M (μ is the sweep angle of the Mach cone)Generally it is not possible to arrange the wing so it will lie entirely outside the supersonic airflow and still have good subsonic performance. Some aircraft, like the English Electric Lightning are tuned almost entirely for high-speed flight and feature highly swept wings that make little to no compromise for the low-speed problems that such a profile creates. In other cases the use of variable geometry wings, as on the Grumman F-14 Tomcat and Panavia Tornado, allows an aircraft to move the wing to keep it at the most efficient angle regardless of speed, although the drawbacks incurred of increased complexity and weight have led to this being a rare feature.Most high-speed aircraft have a wing that spends at least some of its time in the supersonic airflow. But since the shock cone moves towards the fuselage with increased speed (that is, the cone becomes narrower), the portion of the wing in the supersonic flow also changes with speed. Since these wings are swept, as the shock cone moves inward, the lift vector moves forward as the outer, rearward portions of the wing are generating less lift. This results in powerful pitching moments and their associated required trim changes.


=== Disadvantages ===

When a swept wing travels at high speed, the airflow has little time to react and simply flows over the wing almost straight from front to back. At lower speeds the air does have time to react, and is pushed spanwise by the angled leading edge, towards the wing tip. At the wing root, by the fuselage, this has little noticeable effect, but as one moves towards the wingtip the airflow is pushed spanwise not only by the leading edge, but the spanwise moving air beside it. At the tip the airflow is moving along the wing instead of over it, a problem known as spanwise flow.
The lift from a wing is generated by the airflow over it from front to rear. With increasing span-wise flow the boundary layers on the surface of the wing have longer to travel, and so are thicker and more susceptible to transition to turbulence or flow separation, also the effective aspect ratio of the wing is less and so air ""leaks"" around the wing tips reducing their effectiveness. The spanwise flow on swept wings produces airflow that moves the stagnation point on the leading edge of any individual wing segment further beneath the leading edge, increasing effective angle of attack of wing segments relative to its neighbouring forward segment. The result is that wing segments farther towards the rear operate at increasingly higher angles of attack promoting early stall of those segments. This promotes tip stall on back swept wings, as the tips are most rearward, while delaying tip stall for forward swept wings, where the tips are forward. With both forward and back swept wings, the rear of the wing will stall first. This creates a nose-up pressure on the aircraft. If this is not corrected by the pilot it causes the plane to pitch up, leading to more of the wing stalling, leading to more pitch up, and so on. This problem came to be known as the Sabre dance in reference to the number of North American F-100 Super Sabres that crashed on landing as a result.The solution to this problem took on many forms. One was the addition of a fin known as a wing fence on the upper surface of the wing to redirect the flow to the rear ; the MiG-15 was one example of an aircraft fitted with wing fences. Another closely related design was addition of a dogtooth notch to the leading edge, as present on the Avro Arrow interceptor. Other designs took a more radical approach, including the Republic XF-91 Thunderceptor's wing that grew wider towards the tip to provide more lift at the tip. The Handley Page Victor was equipped with a crescent wing, featuring substantial sweep-back near the wing root where the wing was thickest, and progressively reducing sweep along the span as the wing thickness reduced towards the tip.Modern solutions to the problem no longer require ""custom"" designs such as these. The addition of leading-edge slats and large compound flaps to the wings has largely resolved the issue. On fighter designs, the addition of leading-edge extensions, which are typically included to achieve a high level of maneuverability, also serve to add lift during landing and reduce the problem.The swept wing also has several more problems. One is that for any given length of wing, the actual span from tip-to-tip is shorter than the same wing that is not swept. Low speed drag is strongly correlated with the aspect ratio, the span compared to chord, so a swept wing always has more drag at lower speeds. Another concern is the torque applied by the wing to the fuselage, as much of the wing's lift lies behind the point where the wing root connects to the plane. Finally, while it is fairly easy to run the main spars of the wing right through the fuselage in a straight wing design to use a single continuous piece of metal, this is not possible on the swept wing because the spars will meet at an angle.


=== Sweep theory ===
Sweep theory is an aeronautical engineering description of the behavior of airflow over a wing when the wing's leading edge encounters the airflow at an oblique angle. The development of sweep theory resulted in the swept wing design used by most modern jet aircraft, as this design performs more effectively at transonic and supersonic speeds. In its advanced form, sweep theory led to the experimental oblique wing concept.
Adolf Busemann introduced the concept of the swept wing and presented this in 1935 at the 5. Volta-Congress in Rome.  Sweep theory in general was a subject of development and investigation throughout the 1930s and 1940s, but the breakthrough mathematical definition of sweep theory is generally credited to NACA's Robert T. Jones in 1945. Sweep theory builds on other wing lift theories. Lifting line theory describes lift generated by a straight wing (a wing in which the leading edge is perpendicular to the airflow). Weissinger theory describes the distribution of lift for a swept wing, but does not have the capability to include chordwise pressure distribution. There are other methods that do describe chordwise distributions, but they have other limitations. Jones' sweep theory provides a simple, comprehensive analysis of swept wing performance.
To visualize the basic concept of simple sweep theory, consider a straight, non-swept wing of infinite length, which meets the airflow at a perpendicular angle. The resulting air pressure distribution is equivalent to the length of the wing's chord (the distance from the leading edge to the trailing edge). If we were to begin to slide the wing sideways (spanwise), the sideways motion of the wing relative to the air would be added to the previously perpendicular airflow, resulting in an airflow over the wing at an angle to the leading edge. This angle results in airflow traveling a greater distance from leading edge to trailing edge, and thus the air pressure is distributed over a greater distance (and consequently lessened at any particular point on the surface).
This scenario is identical to the airflow experienced by a swept wing as it travels through the air. The airflow over a swept wing encounters the wing at an angle. That angle can be broken down into two vectors, one perpendicular to the wing, and one parallel to the wing. The flow parallel to the wing has no effect on it, and since the perpendicular vector is shorter (meaning slower) than the actual airflow, it consequently exerts less pressure on the wing. In other words, the wing experiences airflow that is slower - and at lower pressures - than the actual speed of the aircraft.
One of the factors that must be taken into account when designing a high-speed wing is compressibility, which is the effect that acts upon a wing as it approaches and passes through the speed of sound. The significant negative effects of compressibility made it a prime issue with aeronautical engineers. Sweep theory helps mitigate the effects of compressibility in transonic and supersonic aircraft because of the reduced pressures. This allows the mach number of an aircraft to be higher than that actually experienced by the wing.
There is also a negative aspect to sweep theory. The lift produced by a wing is directly related to the speed of the air over the wing. Since the airflow speed experienced by a swept wing is lower than what the actual aircraft speed is, this becomes a problem during slow-flight phases, such as takeoff and landing. There have been various ways of addressing the problem, including the variable-incidence wing design on the Vought F-8 Crusader, and swing wings on aircraft such as the F-14, F-111, and the Panavia Tornado.


== Variant designs ==
The term ""swept wing"" is normally used to mean ""swept back"", but other swept variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back.  The delta wing also incorporates the same advantages as part of its layout.


=== Forward sweep ===

Sweeping a wing forward has approximately the same effect as rearward in terms of drag reduction, but has other advantages in terms of low-speed handling where tip stall problems simply go away. In this case the low-speed air flows towards the fuselage, which acts as a very large wing fence. Additionally, wings are generally larger at the root anyway, which allows them to have better low-speed lift.
However, this arrangement also has serious stability problems. The rearmost section of the wing will stall first causing a pitch-up moment pushing the aircraft further into stall similar to a swept back wing design. Thus swept-forward wings are unstable in a fashion similar to the low-speed problems of a conventional swept wing. However unlike swept back wings, the tips on a forward swept design will stall last, maintaining roll control.
Forward-swept wings can also experience dangerous flexing effects compared to aft-swept wings that can negate the tip stall advantage if the wing is not sufficiently stiff. In aft-swept designs, when the airplane maneuvers at high load factor the wing loading and geometry twists the wing in such a way as to create washout (tip twists leading edge down). This reduces the angle of attack at the tip, thus reducing the bending moment on the wing, as well as somewhat reducing the chance of tip stall.  However, the same effect on forward-swept wings produces a wash-in effect that increases the angle of attack promoting tip stall.
Small amounts of sweep do not cause serious problems, and had been used on a variety of aircraft to move the spar into a convenient location, as on the Junkers Ju 287 or HFB 320 Hansa Jet. However, larger sweep suitable for high-speed aircraft, like fighters, was generally impossible until the introduction of fly by wire systems that could react quickly enough to damp out these instabilities. The Grumman X-29 was an experimental technology demonstration project designed to test the forward swept wing for enhanced maneuverability during the 1980s. The Sukhoi Su-47 Berkut is another notable demonstrator aircraft implementing this technology to achieve high levels of agility. To date, no highly swept-forward design has entered production.


== History ==


=== Early history ===
The first successful aeroplanes adhered to the basic design of rectangular wings at right angles to the body of the machine, but there were experimentalists who explored other geometries to achieve better aerodynamic results. The swept wing geometry appeared before World War I, and was conceived as a means of permitting the design of safe and stable aeroplanes. The best of these designs imposed ""self-damping"" inherent stability upon a tailless swept wing. These inspired several flying wing gliders and some powered aircraft during the interwar years.

The first to achieve stability was British designer J. W. Dunne who was obsessed with achieving inherent stability in flight. He successfully employed swept wings in his tailless aircraft (which, crucially, used washout) as a means of creating positive longitudinal static stability. For a low-speed aircraft, swept wings may be used to resolve problems with the center of gravity, to move the wing spar into a more convenient location, or to improve the sideways view from the pilot's position. By 1905, Dunne had already built a model glider with swept wings and by 1913 he had constructed successful powered variants that were able to cross the English Channel. The Dunne D.5 was exceptionally aerodynamically stable for the time, and the D.8 was sold to the Royal Flying Corps; it was also manufactured under licence by Starling Burgess to the United States Navy amongst other customers.Dunne's work ceased with the onset of war in 1914, but afterwards the idea was taken up by G. T. R. Hill in England who designed a series of gliders and aircraft to Dunne's guidelines, notably the Westland-Hill Pterodactyl series. However, Dunne's theories met with little acceptance amongst the leading aircraft designers and aviation companies at the time.


=== German developments ===

The idea of using swept wings to reduce high-speed drag was developed in Germany in the 1930s. At a Volta Conference meeting in 1935 in Italy, Dr. Adolf Busemann suggested the use of swept wings for supersonic flight. He noted that the airspeed over the wing was dominated by the normal component of the airflow, not the freestream velocity, so by setting the wing at an angle the forward velocity at which the shock waves would form would be higher (the same had been noted by Max Munk in 1924, although not in the context of high-speed flight). Albert Betz immediately suggested the same effect would be equally useful in the transonic. After the presentation the host of the meeting, Arturo Crocco, jokingly sketched ""Busemann's airplane of the future"" on the back of a menu while they all dined. Crocco's sketch showed a classic 1950's fighter design, with swept wings and tail surfaces, although he also sketched a swept propeller powering it.At the time, however, there was no way to power an aircraft to these sorts of speeds, and even the fastest aircraft of the era were only approaching 400 km/h (249 mph).The presentation was largely of academic interest, and soon forgotten. Even notable attendees including Theodore von Kármán and Eastman Jacobs did not recall the presentation 10 years later when it was re-introduced to them.Hubert Ludwieg of the High-Speed Aerodynamics Branch at the AVA Göttingen in 1939 conducted the first wind tunnel tests to investigate Busemann's theory. Two wings, one with no sweep, and one with 45 degrees of sweep were tested at Mach numbers of 0.7 and 0.9 in the 11 x 13 cm wind tunnel.  The results of these tests confirmed the drag reduction offered by swept wings at transonic speeds.  The results of the tests were communicated to Albert Betz who then passed them on to Willy Messerschmitt in December 1939.  The tests were expanded in 1940 to include wings with 15, 30 and -45 degrees of sweep and Mach numbers as high as 1.21.With the introduction of jets in the later half of the Second World War, the swept wing became increasingly applicable to optimally satisfying aerodynamic needs. The German jet-powered Messerschmitt Me 262 and rocket-powered Messerschmitt Me 163 suffered from compressibility effects that made both aircraft very difficult to control at high speeds. In addition, the speeds put them into the wave drag regime, and anything that could reduce this drag would increase the performance of their aircraft, notably the notoriously short flight times measured in minutes. This resulted in a crash program to introduce new swept wing designs, both for fighters as well as bombers. The Blohm & Voss P 215 was designed to take full advantage of the swept wing's aerodynamic properties; however, an order for three prototypes was received only weeks before the war ended and no examples were ever built. The Focke-Wulf Ta 183 was another swept wing fighter design, but was also not produced before the war's end. In the post-war era, Kurt Tank developed the Ta 183 into the IAe Pulqui II, but this proved unsuccessful.A prototype test aircraft, the Messerschmitt Me P.1101, was built to research the tradeoffs of the design and develop general rules about what angle of sweep to use. When it was 80% complete, the P.1101 was captured by US forces and returned to the United States, where two additional copies with US-built engines carried on the research as the Bell X-5. Germany's wartime experience with the swept wings and its high value for supersonic flight stood in strong contract to the prevailing views of Allied experts of the era, who commonly espoused their belief in the impossibility of manned vehicles travelling at such speeds.


=== Postwar advancements ===

During the immediate post-war era, several nations were conducting research into high speed aircraft. In the United Kingdom, work commenced during 1943 on the Miles M-52, a high-speed experimental aircraft equipped with a straight wing that was developed in conjunction with Frank Whittle's Power Jets company, the Royal Aircraft Establishment (RAE) in Farnborough, and the National Physical Laboratory. Despite being envisioned to be capable of achieving 1,000 miles per hour (1,600 km/h) in level flight, thus enabling the aircraft to potentially be the first to exceed the speed of sound in the world, in February 1946, the programme was abrupted discontinued for unclear reasons. It has since been widely recognised that the cancellation of the M.52 was a major setback in British progress in the field of supersonic design.Another, more successful, programme was the US's Bell X-1, which also was equipped with a straight wing. According to Miles Chief Aerodynamicist Dennis Bancroft, the Bell Aircraft company was given access to the drawings and research on the M.52. On 14 October 1947, the Bell X-1 performed the first manned supersonic flight, piloted by Captain Charles ""Chuck"" Yeager, having been drop launched from the bomb bay of a Boeing B-29 Superfortress and attained a record-breaking speed of Mach 1.06 (700 miles per hour (1,100 km/h; 610 kn)). The news of a successful straight-wing supersonic aircraft surprised many aeronautical experts on both sides of the Atlantic, as it was increasingly believed that a swept-wing design not only highly beneficial but also necessary to break the sound barrier.

During the final years of the Second World War, aircraft designer Sir Geoffrey de Havilland commenced development on the de Havilland Comet, which would become the world's first jet airliner. An early design consideration was whether to apply the new swept-wing configuration. Thus, an experimental aircraft to explore the technology, the de Havilland DH 108, was developed by the firm in 1944, headed by project engineer John Carver Meadows Frost with a team of 8–10 draughtsmen and engineers. The DH 108 primarily consisted of the pairing of the front fuselage of the de Havilland Vampire to a swept wing and compact stubby vertical tail; it was the first British swept wing jet, unofficially known as the ""Swallow"". It first flew on 15 May 1946, a mere eight months after the project's go-ahead. Company test pilot and son of the builder, Geoffrey de Havilland Jr., flew the first of three aircraft and found it extremely fast – fast enough to try for a world speed record. On 12 April 1948, a D.H.108 did set a world's speed record at 973.65 km/h (605 mph), it subsequently became the first jet aircraft to exceed the speed of sound.Around this same timeframe, the Air Ministry introduced a program of experimental aircraft to examine the effects of swept wings, as well as the delta wing configuration. Furthermore, the Royal Air Force (RAF) identified a pair of proposed fighter aircraft equipped with swept wings from Hawker Aircraft and Supermarine, the Hawker Hunter and Supermarine Swift respectively, and successfully pressed for orders to be placed 'off the drawing board' in 1950. On 7 September 1953, the sole Hunter Mk 3 (the modified first prototype, WB 188) flown by Neville Duke broke the world air speed record for jet-powered aircraft, attaining a speed of 727.63 mph (1,171.01 km/h) over Littlehampton, West Sussex. This world record stood for less than three weeks before being broken on 25 September 1953 by the Hunter's early rival, the Supermarine Swift, being flown by Michael Lithgow.In February 1945, NACA engineer Robert T. Jones started looking at highly swept delta wings and V shapes, and discovered the same effects as Busemann. He finished a detailed report on the concept in April, but found his work was heavily criticised by other members of NACA Langley, notably Theodore Theodorsen, who referred to it as ""hocus-pocus"" and demanded some ""real mathematics"". However, Jones had already secured some time for free-flight models under the direction of Robert Gilruth, whose reports were presented at the end of May and showed a fourfold decrease in drag at high speeds. All of this was compiled into a report published on June 21, 1945, which was sent out to the industry three weeks later. Ironically, by this point Busemann's work had already been passed around.

On May 1945, the American Operation Paperclip reached Braunschweig, where US personnel discovered a number of swept wing models and a mass of technical data from the wind tunnels. One member of the US team was George S. Schairer, who was at that time working at the Boeing company. He immediately forwarded a letter to Ben Cohn at Boeing, communicating the value of the swept wing concept. He also told Cohn to distribute the letter to other companies as well, although only Boeing and North American made immediate use of it.Boeing was in the midst of designing the B-47 Stratojet, and the initial Model 424 was a straight-wing design similar to the B-45, B-46 and B-48 it competed with. Analysis by Boeing engineer Vic Ganzer suggested an optimum sweepback angle of about 35 degrees. By September 1945, the Braunschweig data had been worked into the design, which re-emerged as the Model 448, a larger six-engine design with more robust wings swept at 35 degrees. Another re-work moved the engines into strut-mounted pods under the wings due to concerns of the uncontained failure of an internal engine could potentially destroy the aircraft via either fire or vibration. The resulting B-47 was hailed as the fastest of its class in the world during the late 1940s, and trounced the straight-winged competition. Boeing's jet-transport formula of swept wings and pylon-mounted engines has since been universally adopted.In fighters, North American Aviation was in the midst of working on a straight-wing jet-powered naval fighter, then known as the FJ-1; it was later submitted to the United States Air Force as the XP-86. Larry Green, who could read German, studied the Busemann reports and convinced management to allow a redesign starting in August 1945. The performance of the F-86A allowed it set the first of several official world speed records, attaining 671 miles per hour (1,080 km/h) on 15 September 1948, flown by Major Richard L. Johnson. With the appearance of the MiG-15, the F-86 was rushed into combat, while straight-wing jets like the Lockheed P-80 Shooting Star and Republic F-84 Thunderjet were quickly relegated to ground attack missions. Some, such as the F-84 and Grumman F-9 Cougar, were later redesigned with swept wings from straight-winged aircraft. Later planes, such as the North American F-100 Super Sabre, would be designed with swept wings from the start, though additional innovations such as the afterburner, area-rule and new control surfaces would be necessary to master supersonic flight.

The Soviet Union was also intrigued about the idea of swept wings on aircraft, when their ""captured aviation technology"" counterparts to the western Allies spread out across the defeated Third Reich. Artem Mikoyan was asked by the Soviet government's TsAGI aviation research department to develop a test-bed aircraft to research the swept wing idea — the result was the late 1945-flown, unusual MiG-8 Utka pusher canard layout aircraft, with its rearwards-located wings being swept back for this type of research. The swept wing was applied to the MiG-15, an early jet-powered fighter, its maximum speed of 1,075 km/h (668 mph) outclassed the straight-winged American jets and piston-engined fighters initially deployed during the Korean War. The MiG-15 is believed to have been one of the most produced jet aircraft; in excess of 13,000 would ultimately be manufactured.The MiG-15, which could not safely exceed Mach 0.92, served as the basis for the MiG-17, which was designed to be controllable at higher Mach numbers. Its wing featured a ""sickle sweep"" compound shape, somewhat similar to the F-100 Super Sabre, with a 45° angle near the fuselage and a 42° angle for the outboard part of the wings. A further derivative of the design, designated MiG-19, featured a relatively thin wing suited to supersonic flight that was designed at TsAGI, the Soviet Central Aerohydrodynamic Institute; swept back at an angle of 55 degrees, this wing featured a single wing fence on each side. A specialist high-altitude variant, the Mig-19SV, featured, amongst other changes, flap adjusted to generate greater lift at higher altitudes, helping to increase the aircraft's ceiling from 17,500 m (57,400 ft) to 18,500 m (60,700 ft).Germany's swept wing research also made its way to the Swedish aircraft manufacturer SAAB, allegedly via a group of ex-Messerschmitt engineers that had fled to Switzerland during late 1945. At the time, SAAB was eager to make aeronautic advances, particularly in the new field of jet propulsion. The company incorporated both the jet engine and the swept wing to produce the Saab 29 Tunnan fighter; on 1 September 1948, the first prototype conducted its maiden flight, flown by the English test pilot S/L Robert A. 'Bob' Moore, DFC and bar, Although not well known outside Sweden, the Tunnan was the first Western European fighter to be introduced with such a wing configuration. In parallel, SAAB also developed another swept wing aircraft, the Saab 32 Lansen, primarily to serve as Sweden's standard attack aircraft. Its wing, which had a 10 per cent laminar profile and a 35° sweep, featured triangular fences near the wing roots in order to improve airflow when the aircraft was being flown at a high angle of attack. On 25 October 1953, a SAAB 32 Lansen attained a Mach number of at least 1.12 while in a shallow dive, exceeding the sound barrier.

The dramatic successes of aircraft such as Hawker Hunter, the B-47, and F-86 embodied the widespread acceptance of the swept wing research acquired from Germany. Eventually, almost all advanced design efforts would incorporate a swept wing configuration. The classic Boeing B-52, designed in the 1950s, continues in service as a high-subsonic long-range heavy bomber despite the development of the triple-sonic North American B-70 Valkyrie, supersonic swing-wing Rockwell B-1 Lancer, and flying wing designs. While the Soviets never matched the performance of the Boeing B-52 Stratofortress with a jet aircraft, the intercontinental range Tupolev Tu-95 turboprop bomber with its near-jet class top speed of 920 km/h, combining swept wings with propeller propulsion, also remains in service today, being the fastest propeller-powered production aircraft. In Britain, a range of swept-wing bombers were designed, these being the Vickers Valiant (1951), the Avro Vulcan (1952), and the Handley Page Victor (1952).By the early 1950s, nearly every new fighter was either rebuilt or designed from scratch with a swept wing. By the 1960s, most civilian jets also adopted swept wings. The Douglas A-4 Skyhawk and Douglas F4D Skyray were examples of delta wings that also have swept leading edges with or without a tail. Most early transonic and supersonic designs such as the MiG-19 and F-100 used long, highly swept wings. Swept wings would reach Mach 2 in the arrow-winged BAC Lightning, and stubby winged Republic F-105 Thunderchief, which was found to be wanting in turning ability in Vietnam combat. By the late 1960s, the F-4 Phantom and Mikoyan-Gurevich MiG-21 that both used variants on tailed delta wings came to dominate front line air forces. Variable geometry wings were employed on the American F-111, Grumman F-14 Tomcat and Soviet Mikoyan MiG-27, although the idea would be abandoned for the American SST design. After the 1970s, most newer generation fighters optimized for maneuvering air combat since the USAF F-15 and Soviet Mikoyan MiG-29 have employed relatively short-span fixed wings with relatively large wing area.


== See also ==
Delta wing
Theodore von Kármán, first to recognize the importance of the swept wing
Trapezoidal wing
Wing configuration


== References ==


=== Citations ===


=== Bibliography ===


== Further reading ==
""The High-speed Shape: Pitch-up and palliatives adopted on swept-wing aircraft"", Flight International, 2 January 1964


== External links ==
Swept Wings and Effective Dihedral
The development of swept wings
Simple sweep theory math
Advanced math of swept and oblique wings
The L-39 and swept wing research
Sweep theory in a 3D environment
CFD results showing the three-dimensional supersonic bubble over the wing of an A 320. Another CFD result showing the MDXX and how the shock vanishes close to the fuselage where the aerofoil is more slender","pandas(index=60, _1=60, text='a swept wing is a wing that angles either backward or occasionally forward from its root rather than in a straight sideways direction. swept wings have been flown since the pioneer days of aviation. wing sweep at high speeds was first investigated in germany as early as 1935 by albert betz and adolph busemann, finding application just before the end of the second world war. it has the effect of delaying the shock waves and accompanying aerodynamic drag rise caused by fluid compressibility near the speed of sound, improving performance. swept wings are therefore almost always used on jet aircraft designed to fly at these speeds. swept wings are also sometimes used for other reasons, such as low drag, low observability, structural convenience or pilot visibility. the term ""swept wing"" is normally used to mean ""swept back"", but variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back. the delta wing is also aerodynamically a form of swept wing.   == design characteristics == for a wing of given span, sweeping it increases the length of the spars running along it from root to tip. this tends to increase weight and reduce stiffness. if the fore-aft chord of the wing also remains the same, the distance between leading and trailing edges reduces, reducing its ability to resist twisting (torsion) forces. a swept wing of given span and chord must therefore be strengthened and will be heavier than the equivalent unswept wing. a swept wing typically angles backward from its root rather than forwards. because wings are made as light as possible, they tend to flex under load. this aeroelasticity under aerodynamic load causes the tips to bend upwards in normal flight. backwards sweep causes the tips to reduce their angle of attack as they bend, reducing their lift and limiting the effect. forward sweep causes the tips to increase their angle of attack as they bend. this increases their lift causing further bending and hence yet more lift in a cycle which can cause a runaway structural failure. for this reason forward sweep is rare and the wing must be unusually rigid. the characteristic ""sweep angle"" is normally measured by drawing a line from root to tip, typically 25% of the way back from the leading edge, and comparing that with the perpendicular to the longitudinal axis of the aircraft.  typical sweep angles vary from 0 for a straight-wing aircraft, to 45 degrees or more for fighters and other high-speed designs.   == aerodynamics == == further reading == ""the high-speed shape: pitch-up and palliatives adopted on swept-wing aircraft"", flight international, 2 january 1964   == external links == swept wings and effective dihedral the development of swept wings simple sweep theory math advanced math of swept and oblique wings the l-39 and swept wing research sweep theory in a 3d environment cfd results showing the three-dimensional supersonic bubble over the wing of an a 320. another cfd result showing the mdxx and how the shock vanishes close to the fuselage where the aerofoil is more slender')"
61,"Space Power Facility (SPF) is a NASA facility used to test spaceflight hardware under simulated launch and spaceflight conditions. The SPF is part of NASA's Plum Brook Station, which in turn is part of the Glenn Research Center. The Plum Brook Station and the SPF are located near Sandusky, Ohio (Oxford Township, Erie County, Ohio).
The SPF is able to simulate a spacecraft's launch environment, as well as in-space environments. NASA has developed these capabilities under one roof to optimize testing of spaceflight hardware while minimizing transportation issues. Space Power Facility has become a ""One Stop Shop"" to qualify flight hardware for manned space flight. This facility provides the capability to perform the following environmental testing:

Thermal-vacuum testing
Reverberation acoustic testing
Mechanical vibration testing
Modal testing
Electromagnetic interference and compatibility testing


== Thermal-Vacuum Test Chamber ==
The Space Power Facility (SPF) is a vacuum chamber built by NASA in 1969. It stands 122 feet (37 m) high and 100 feet (30 m) in diameter, enclosing a bullet-shaped space. It is the world's largest thermal vacuum chamber. It was originally commissioned for nuclear-electric power studies under vacuum conditions, but was later decommissioned. Recently, it was recommissioned for use in testing spacecraft propulsion systems. Recent uses include testing the airbag landing systems for the Mars Pathfinder and the Mars Exploration Rovers, Spirit and Opportunity, under simulated Mars atmospheric conditions.
The facility was designed and constructed to test both nuclear and non-nuclear space hardware in a simulated Low-Earth-Orbiting environment. Although the facility was designed for testing nuclear hardware, only non-nuclear tests have been performed throughout its history. Some of the test programs that have been performed at the facility include high-energy experiments, rocket-fairing separation tests, Mars Lander system tests, deployable Solar Sail tests and International Space Station hardware tests. The SPF is located at the NASA Glenn Research Center at the Plum Brook site.
The facility can sustain a high vacuum (10−6 torr, 130 μPa); simulate solar radiation via a 4 MW quartz heat lamp array, solar spectrum by a 400 kW arc lamp, and cold environments (−320 °F (−195.6 °C)) with a variable geometry cryogenic cold shroud.
The facility is available on a full-cost reimbursable basis to government, universities, and the private sector.
In Spring 2013 SpaceX conducted a fairing separation test in the vacuum chamber.


=== Aluminum Test Chamber ===
The Aluminum Test Chamber is a vacuum-tight aluminum plate vessel that is 100 feet (30 m) in diameter and 122 feet (37 m) high. Designed for an external pressure of 2.5 psi (17 kPa) and internal pressure of 5 psi (34 kPa), the chamber is constructed of Type 5083 aluminum which is a clad on the interior surface with a 1⁄8 in (3.2 mm) thick type 3003 aluminum for corrosion resistance. This material was selected because of its low neutron absorption cross-section. The floor plate and vertical shell are 1 inch (25 mm) (total) thick, while the dome shell is 1 3⁄8 in (35 mm). Welded circumferentially to the exterior surface is aluminum structural T-section members that are 3 feet (0.9 m) deep and 2 feet (0.6 m) wide. The doors of the test chamber are 50 by 50 feet (15 by 15 m) in size and have double door seals to prevent leakage. The chamber floor was designed for a load of 300 tons.


=== Concrete Chamber Enclosure ===
The concrete chamber enclosure serves not only as a radiological shield but also as a primary vacuum barrier from atmospheric pressure. 130 feet (40 m) in diameter and 150 feet (46 m) in height, the chamber was designed to withstand atmospheric pressure outside of the chamber at the same time vacuum conditions are occurring within. The concrete thickness varies from 6 to 8 feet (1.8 to 2.4 m) and contains a leak-tight steel containment barrier embedded within. The chamber's doors are 50 by 50 feet (15 by 15 m) and have inflatable seals. The space between the concrete enclosure and the aluminum test chamber is pumped down to a pressure of 20 torrs (2.7 kPa) during a test.

		
		
		
		
Brian Cox of the BBC's Human Universe filmed a rock and feather drop episode at the Space Power Facility.  Below is a YouTube clip:
Rock and Feather Drop at NASA's Space Power Facility


=== Electromagnetic Interference/Compatibility (EMI/EMC) functionality ===
Designed specifically as a large-scale thermal-vacuum test chamber for qualification testing of vehicles and equipment in outer-space conditions, it was discovered in the late 2000s that the unique construction of the SPF interior aluminum vacuum chamber also makes it an extremely large and electrically complex RF cavity with excellent reverberant RF characteristics. In 2009 these characteristics were measured by NIST and others after which the facility was understood to be, not only the world's largest Vacuum chamber, but also the world's largest EMI/EMC test facility. In 2011 NASA GRC successfully performed a calibration of the aluminum vacuum chamber using IEC 61000-4-21 methodologies. As a result of these activities, the SPF is capable of performing radiated susceptibility EMI tests for vehicles and equipment per MIL-STD-461 and able to achieve MIL-STD-461F limits above approximately 80 MHz. In the spring of 2017 the low-power characterizations and calibrations from 2009 and 2011 were proven correct in a series of high-power tests performed in the chamber to validate its capabilities. The SPF chamber is currently being prepared for EMI radiated susceptibility testing of the crew module for the Artemis 1 of NASA's Orion spacecraft.

		


== Reverberant Acoustic Test Facility ==
The Reverberant Acoustic Test Facility has 36 nitrogen-driven horns to simulate the high noise levels that will be experienced during a space vehicle launch and supersonic ascent conditions. The RATF is capable of an overall sound pressure level of 163 dB within a 101,500-cubic-foot (2,870 m3) chamber.

		
		
		


== Mechanical Vibration Test Facility ==

The Mechanical Vibration Test Facility (MVF), is a three-axis vibration system. It will apply vibration in each of the three orthogonal axes (not simultaneously) with one direction in parallel to the Earth-launch thrust axis (X) at 5–150 Hz, 0-1.25 g-pk vertical, and 5–150 Hz 0-1.0 g-pk for the horizontal axes. 
Vertical, or the thrust axis, shaking is accomplished by using 16 vertical actuators manufactured by TEAM Corporation, each capable of 30,000 lbf (130 kN). The 16 vertical actuators allow for testing of up to a 75,000 lb (34,000 kg) article at the previously stated frequency and amplitude limits.
Horizontal shaking is accomplished through use of 4 TEAM Corporation Horizontal Actuators. The horizontal actuators are used during Vertical testing to counteract cross axis forces and overturning moments.

		
		
		
		
NASA's Space Power Facility Vibro-Acoustic Construction


== Modal Test Facility ==
In addition to the sine vibe table, a fixed-base Modal floor sufficient for the 20 ft (6.1 m) diameter test article is available. The fixed based Modal Test Facility is a 6 in (150 mm) thick steel floor on top of 19 ft (5.8 m) of concrete, that is tied to the earth using 50 ft (15 m) deep tensioned rock anchors.
There were over 21,000,000 pounds (9,500 t) of rock anchors, and 6,000,000 pounds (2,700 t) of concrete used in the construction of the fixed-base modal test facility and mechanical vibration test facility.

		
		


== Assembly Area ==
The SPF Facility layout is ideal for performing multiple test programs. The facility has two large high bay areas adjacent to either side of the vacuum chamber. The advantage of having both areas available is that it allows for two complex tests to be prepared simultaneously. One test can be prepared in a high bay while another test is being conducted in the vacuum chamber. Large chamber doors provide access to the test chamber from either high bay.

NASA's Space Power Facility Vibro-Acoustic Construction


== References ==


== External links ==
Skylab Shroud in Plum Brook Space Power Facility
NASA image gallery, featuring the SPF
Detailed facility capabilities","pandas(index=61, _1=61, text='space power facility (spf) is a nasa facility used to test spaceflight hardware under simulated launch and spaceflight conditions. the spf is part of nasa\'s plum brook station, which in turn is part of the glenn research center. the plum brook station and the spf are located near sandusky, ohio (oxford township, erie county, ohio). the spf is able to simulate a spacecraft\'s launch environment, as well as in-space environments. nasa has developed these capabilities under one roof to optimize testing of spaceflight hardware while minimizing transportation issues. space power facility has become a ""one stop shop"" to qualify flight hardware for manned space flight. this facility provides the capability to perform the following environmental testing:  thermal-vacuum testing reverberation acoustic testing mechanical vibration testing modal testing electromagnetic interference and compatibility testing   == thermal-vacuum test chamber == the space power facility (spf) is a vacuum chamber built by nasa in 1969. it stands 122 feet (37 m) high and 100 feet (30 m) in diameter, enclosing a bullet-shaped space. it is the world\'s largest thermal vacuum chamber. it was originally commissioned for nuclear-electric power studies under vacuum conditions, but was later decommissioned. recently, it was recommissioned for use in testing spacecraft propulsion systems. recent uses include testing the airbag landing systems for the mars pathfinder and the mars exploration rovers, spirit and opportunity, under simulated mars atmospheric conditions. the facility was designed and constructed to test both nuclear and non-nuclear space hardware in a simulated low-earth-orbiting environment. although the facility was designed for testing nuclear hardware, only non-nuclear tests have been performed throughout its history. some of the test programs that have been performed at the facility include high-energy experiments, rocket-fairing separation tests, mars lander system tests, deployable solar sail tests and international space station hardware tests. the spf is located at the nasa glenn research center at the plum brook site. the facility can sustain a high vacuum (10−6 torr, 130 μpa); simulate solar radiation via a 4 mw quartz heat lamp array, solar spectrum by a 400 kw arc lamp, and cold environments (−320 °f (−195.6 °c)) with a variable geometry cryogenic cold shroud. the facility is available on a full-cost reimbursable basis to government, universities, and the private sector. in spring 2013 spacex conducted a fairing separation test in the vacuum chamber. designed specifically as a large-scale thermal-vacuum test chamber for qualification testing of vehicles and equipment in outer-space conditions, it was discovered in the late 2000s that the unique construction of the spf interior aluminum vacuum chamber also makes it an extremely large and electrically complex rf cavity with excellent reverberant rf characteristics. in 2009 these characteristics were measured by nist and others after which the facility was understood to be, not only the world\'s largest vacuum chamber, but also the world\'s largest emi/emc test facility. in 2011 nasa grc successfully performed a calibration of the aluminum vacuum chamber using iec 61000-4-21 methodologies. as a result of these activities, the spf is capable of performing radiated susceptibility emi tests for vehicles and equipment per mil-std-461 and able to achieve mil-std-461f limits above approximately 80 mhz. in the spring of 2017 the low-power characterizations and calibrations from 2009 and 2011 were proven correct in a series of high-power tests performed in the chamber to validate its capabilities. the spf chamber is currently being prepared for emi radiated susceptibility testing of the crew module for the artemis 1 of nasa\'s orion spacecraft.       == reverberant acoustic test facility == the reverberant acoustic test facility has 36 nitrogen-driven horns to simulate the high noise levels that will be experienced during a space vehicle launch and supersonic ascent conditions. the ratf is capable of an overall sound pressure level of 163 db within a 101,500-cubic-foot (2,870 m3) chamber.             == mechanical vibration test facility ==  the mechanical vibration test facility (mvf), is a three-axis vibration system. it will apply vibration in each of the three orthogonal axes (not simultaneously) with one direction in parallel to the earth-launch thrust axis (x) at 5–150 hz, 0-1.25 g-pk vertical, and 5–150 hz 0-1.0 g-pk for the horizontal axes. vertical, or the thrust axis, shaking is accomplished by using 16 vertical actuators manufactured by team corporation, each capable of 30,000 lbf (130 kn). the 16 vertical actuators allow for testing of up to a 75,000 lb (34,000 kg) article at the previously stated frequency and amplitude limits. horizontal shaking is accomplished through use of 4 team corporation horizontal actuators. the horizontal actuators are used during vertical testing to counteract cross axis forces and overturning moments.              nasa\'s space power facility vibro-acoustic construction   == modal test facility == in addition to the sine vibe table, a fixed-base modal floor sufficient for the 20 ft (6.1 m) diameter test article is available. the fixed based modal test facility is a 6 in (150 mm) thick steel floor on top of 19 ft (5.8 m) of concrete, that is tied to the earth using 50 ft (15 m) deep tensioned rock anchors. there were over 21,000,000 pounds (9,500 t) of rock anchors, and 6,000,000 pounds (2,700 t) of concrete used in the construction of the fixed-base modal test facility and mechanical vibration test facility.          == assembly area == the spf facility layout is ideal for performing multiple test programs. the facility has two large high bay areas adjacent to either side of the vacuum chamber. the advantage of having both areas available is that it allows for two complex tests to be prepared simultaneously. one test can be prepared in a high bay while another test is being conducted in the vacuum chamber. large chamber doors provide access to the test chamber from either high bay.  nasa\'s space power facility vibro-acoustic construction   == references ==   == external links == skylab shroud in plum brook space power facility nasa image gallery, featuring the spf detailed facility capabilities')"
62,"An obturator ring was a type of piston ring used in World War I aero engines for improved sealing in the presence of cylinder distortion.


== Purpose ==
The cylinders of rotary aircraft engines of World War I (engines with the crankshaft fixed to the airframe and rotating cylinders) were notoriously difficult to keep cool leading to  thermal distortion. To keep the weight down they had very thin-wall (1.5 mm) steel cylinders. Obturator rings, made of bronze in the early Gnome engines, could flex to the shape of the cylinder. Wear on the rings was considerable. Engines needed to be overhauled about every 20 hours. The reliability of Gnome engines license-built by The British Gnome and Le Rhone Engine Co. gave an overhaul life of about 80 hours mainly as a result using a special tool to roll the 'L' section obturator rings. The problem of thermal distortion was effectively cured on the Bentley BR1 engine by using aluminium cylinders, for good thermal conductivity, with cast iron liners shrunk in.An 'L' section obturator ring is shown in Patent US 1378109A - ""Obturator ring"".


== See also ==
Piston ring


== References ==","pandas(index=62, _1=62, text='an obturator ring was a type of piston ring used in world war i aero engines for improved sealing in the presence of cylinder distortion.   == purpose == the cylinders of rotary aircraft engines of world war i (engines with the crankshaft fixed to the airframe and rotating cylinders) were notoriously difficult to keep cool leading to  thermal distortion. to keep the weight down they had very thin-wall (1.5 mm) steel cylinders. obturator rings, made of bronze in the early gnome engines, could flex to the shape of the cylinder. wear on the rings was considerable. engines needed to be overhauled about every 20 hours. the reliability of gnome engines license-built by the british gnome and le rhone engine co. gave an overhaul life of about 80 hours mainly as a result using a special tool to roll the \'l\' section obturator rings. the problem of thermal distortion was effectively cured on the bentley br1 engine by using aluminium cylinders, for good thermal conductivity, with cast iron liners shrunk in.an \'l\' section obturator ring is shown in patent us 1378109a - ""obturator ring"".   == see also == piston ring   == references ==')"
63,"An Air Data Inertial Reference Unit (ADIRU) is a key component of the integrated Air Data Inertial Reference System (ADIRS), which supplies air data (airspeed, angle of attack and altitude) and inertial reference (position and attitude) information to the pilots' electronic flight instrument system displays as well as other systems on the aircraft such as the engines, autopilot, aircraft flight control system and landing gear systems. An ADIRU acts as a single, fault tolerant source of navigational data for both pilots of an aircraft. It may be complemented by a secondary attitude air data reference unit (SAARU), as in the Boeing 777 design.This device is used on various military aircraft as well as civilian airliners starting with the Airbus A320 and Boeing 777.


== Description ==
An ADIRS consists of up to three fault tolerant ADIRUs located in the aircraft electronic rack, an associated control and display unit (CDU) in the cockpit and remotely mounted air data modules (ADMs). The No 3 ADIRU is a redundant unit that may be selected to supply data to either the commander's or the co-pilot's displays in the event of a partial or complete failure of either the No 1 or No 2 ADIRU. There is no cross-channel redundancy between the Nos 1 and 2 ADIRUs, as No 3 ADIRU is the only alternate source of air and inertial reference data. An inertial reference (IR) fault in ADIRU No 1 or 2 will cause a loss of attitude and navigation information on their associated primary flight display (PFD) and navigation display (ND) screens. An air data reference (ADR) fault will cause the loss of airspeed and altitude information on the affected display. In either case the information can only be restored by selecting the No 3 ADIRU.Each ADIRU comprises an ADR and an inertial reference (IR) component.


=== Air data reference ===

The air data reference (ADR) component of an ADIRU provides airspeed, Mach number, angle of attack, temperature and barometric altitude data. Ram air pressure and static pressures used in calculating airspeed are measured by small ADMs located as close as possible to the respective pitot and static pressure sensors. ADMs transmit their pressures to the ADIRUs through ARINC 429 data buses.


=== Inertial reference ===
The IR component of an ADIRU gives attitude, flight path vector, ground speed and positional data. The ring laser gyroscope is a core enabling technology in the system, and is used together with accelerometers, GPS and other sensors to provide raw data. The primary benefits of a ring laser over older mechanical gyroscopes are that there are no moving parts, it is rugged and lightweight, frictionless and does not resist a change in precession.


== Complexity in redundancy ==
Analysis of complex systems is itself so difficult as to be subject to errors in the certification process. Complex interactions between flight computers and ADIRU's can lead to counter-intuitive behaviour for the crew in the event of a failure. In the case of Qantas Flight 72, the captain switched the source of IR data from ADIRU1 to ADIRU3 following a failure of ADIRU1; however ADIRU1 continued to supply ADR data to the captain's primary flight display. In addition, the master flight control computer (PRIM1) was switched from PRIM1 to PRIM2, then PRIM2 back to PRIM1, thereby creating a situation of uncertainty for the crew who did not know which redundant systems they were relying upon.Reliance on redundancy of aircraft systems can also lead to delays in executing needed repairs, as airline operators rely on the redundancy to keep the aircraft system working without having to repair faults immediately.


== Failures and directives ==


=== FAA Airworthiness directive 2000-07-27 ===
On May 3, 2000, the FAA issued airworthiness directive 2000-07-27, addressing dual critical failures during flight, attributed to power supply issues affecting early Honeywell HG2030 and HG2050 ADIRU ring laser gyros used on several Boeing 737, 757, Airbus A319, A320, A321, A330, and A340 models.


=== Airworthiness directive 2003-26-03 ===
On 27 January 2004 the FAA issued airworthiness directive 2003-26-03 (later superseded by AD 2008-17-12) which called for modification to the mounting of ADIRU3 in Airbus A320 family aircraft to prevent failure and loss of critical attitude and airspeed data.


=== Alitalia A320 ===
On 25 June 2005, an Alitalia Airbus A320-200 registered as I-BIKE departed Milan with a defective ADIRU as permitted by the Minimum Equipment List. While approaching London Heathrow Airport during deteriorating weather another ADIRU failed, leaving only one operable. In the subsequent confusion the third was inadvertently reset, losing its reference heading and disabling several automatic functions. The crew was able to effect a safe landing after declaring a Pan-pan.


=== Malaysia Airlines Flight 124 ===
On 1 August 2005, a serious incident involving Malaysia Airlines Flight 124 occurred when an ADIRU fault in a Boeing 777-2H6ER (9M-MRG) flying from Perth to Kuala Lumpur International caused the aircraft to act on false indications, resulting in uncommanded manoeuvres. In that incident the incorrect data impacted all planes of movement while the aircraft was climbing through 38,000 feet (11,600 m). The aircraft pitched up and climbed to around 41,000 feet (12,500 m), with the stall warning activated. The pilots recovered the aircraft with the autopilot disengaged and requested a return to Perth. During the return to Perth, both the left and right autopilots were briefly activated by the crew, but in both instances the aircraft pitched down and banked to the right. The aircraft was flown manually for the remainder of the flight and landed safely in Perth. There were no injuries and no damage to the aircraft. The ATSB found that the main probable cause of this incident was a latent software error which allowed the ADIRU to use data from a failed accelerometer.The US Federal Aviation Administration issued Emergency Airworthiness Directive (AD) 2005-18-51 requiring all 777 operators to install upgraded software to resolve the error.


=== Qantas Flight 68 ===
On 12 September 2006, Qantas Flight 68, Airbus A330 registration VH-QPA, from Singapore to Perth exhibited ADIRU problems but without causing any disruption to the flight. At 41,000 feet (12,000 m) and estimated position 530 nautical miles (980 km) north of Learmonth, Western Australia, NAV IR1 FAULT then, 30 minutes later, NAV ADR 1 FAULT notifications were received on the ECAM identifying navigation system faults in Inertial Reference Unit 1, then in ADR 1 respectively. The crew reported to the later Qantas Flight 72 investigation involving the same airframe and ADIRU that they had received numerous warning and caution messages which changed too quickly to be dealt with. While investigating the problem, the crew noticed a weak and intermittent ADR 1 FAULT light and elected to switch off ADR 1, after which they experienced no further problems. There was no impact on the flight controls throughout the event. The ADIRU manufacturer's recommended maintenance procedures were carried out after the flight and system testing found no further fault.


=== Jetstar Flight 7 ===
On 7 February 2008, a similar aircraft (VH-EBC) operated by Qantas subsidiary Jetstar Airways was involved in a similar occurrence while conducting the JQ7 service from Sydney to Ho Chi Minh City, Vietnam. In this event - which occurred 1,760 nautical miles (3,260 km) east of Learmonth - many of the same errors occurred in the ADIRU unit. The crew followed the relevant procedure applicable at the time and the flight continued without problems.


=== Airworthiness directive 2008-17-12 ===
On 6 August 2008, the FAA issued airworthiness directive 2008-17-12 expanding on the requirements of the earlier AD 2003-26-03 which had been determined to be an insufficient remedy. In some cases it called for replacement of ADIRUs with newer models, but allowed 46 months from October 2008 to implement the directive.The ATSB has yet to confirm if this event is related to the other Airbus A330 ADIRU occurrences.


=== Qantas Flight 72 ===
On 7 October 2008, Qantas Flight 72, using the same aircraft involved in the Flight 68 incident, departed Singapore for Perth. Some time into the flight, while cruising at 37,000 ft, a failure in the No.1 ADIRU led to the autopilot automatically disengaging followed by two sudden uncommanded pitch down manoeuvres, according to the Australian Transport Safety Bureau (ATSB). The accident injured up to 74 passengers and crew, ranging from minor to serious injuries. The aircraft was able to make an emergency landing without further injuries. The aircraft was equipped with a Northrop Grumman made ADIRS, which investigators sent to the manufacturer for further testing.


=== Qantas Flight 71 ===
On 27 December 2008, Qantas Flight 71 from Perth to Singapore, a different Qantas A330-300 with registration VH-QPG was involved in an incident at 36,000 feet approximately 260 nautical miles (480 km) north-west of Perth and 350 nautical miles (650 km) south of Learmonth Airport at 1729 WST. The autopilot disconnected and the crew received an alert indicating a problem with ADIRU Number 1.


=== Emergency Airworthiness Directive No 2009-0012-E ===
On 15 January 2009, the European Aviation Safety Agency issued Emergency Airworthiness Directive No 2009-0012-E to address the above A330 and A340 Northrop-Grumman ADIRU problem of incorrectly responding to a defective inertial reference. In the event of a NAV IR fault the directed crew response is now to ""select OFF the relevant IR, select OFF the relevant ADR, and then turn the IR rotary mode selector to the OFF position."" The effect is to ensure that the faulted IR is powered off so that it no longer can send erroneous data to other systems.


=== Air France Flight 447 ===
On 1 June 2009, Air France Flight 447, an Airbus A330 en route from Rio de Janeiro to Paris, crashed in the Atlantic Ocean after transmitting automated messages indicating faults with various equipment, including the ADIRU. While examining possibly related events of weather-related loss of ADIRS, the NTSB decided to investigate two similar cases on cruising A330s. On a 21 May 2009 Miami-Sao Paulo TAM Flight 8091 registered as PT-MVB, and on a 23 June 2009 Hong Kong-Tokyo Northwest Airlines Flight 8 registered as N805NW each saw sudden loss of airspeed data at cruise altitude and consequent loss of ADIRS control.


=== Ryanair Flight 6606 ===
On 9 October 2018, the Boeing 737-800 operating the flight from Porto Airport to Edinburgh Airport suffered a left ADIRU failure that resulted in the aircraft pitching up and climbing 600 feet. The left ADIRU was put in ATT (attitude-only) mode in accordance with the Quick Reference Handbook, but it continued to display erroneous attitude information to the captain. The remainder of the flight was flown manually with an uneventful landing. The UK's AAIB released the final report on 31 October 2019, with the following recommendation:It is recommended that Boeing Commercial Aircraft amend the Boeing 737 Quick Reference Handbook to include a non-normal checklist for situations when pitch and roll comparator annunciations appear on the attitude display.


== See also ==
Acronyms and abbreviations in avionics


== References ==


== Further reading ==
Dave Carbaugh; Doug Forsythe; Melville McIntyre. ""Erroneous flight instrumenent information"". Aero Magazine. Boeing. Archived from the original on 6 September 2008. Retrieved 2008-10-16.
Melville Duncan W. McIntyre, Boeing (2003-11-25). ""US Patent 6654685 - Apparatus and method for navigation of an aircraft"". United States Patent Office. Retrieved 2008-10-16.","pandas(index=63, _1=63, text='an air data inertial reference unit (adiru) is a key component of the integrated air data inertial reference system (adirs), which supplies air data (airspeed, angle of attack and altitude) and inertial reference (position and attitude) information to the pilots\' electronic flight instrument system displays as well as other systems on the aircraft such as the engines, autopilot, aircraft flight control system and landing gear systems. an adiru acts as a single, fault tolerant source of navigational data for both pilots of an aircraft. it may be complemented by a secondary attitude air data reference unit (saaru), as in the boeing 777 design.this device is used on various military aircraft as well as civilian airliners starting with the airbus a320 and boeing 777.   == description == an adirs consists of up to three fault tolerant adirus located in the aircraft electronic rack, an associated control and display unit (cdu) in the cockpit and remotely mounted air data modules (adms). the no 3 adiru is a redundant unit that may be selected to supply data to either the commander\'s or the co-pilot\'s displays in the event of a partial or complete failure of either the no 1 or no 2 adiru. there is no cross-channel redundancy between the nos 1 and 2 adirus, as no 3 adiru is the only alternate source of air and inertial reference data. an inertial reference (ir) fault in adiru no 1 or 2 will cause a loss of attitude and navigation information on their associated primary flight display (pfd) and navigation display (nd) screens. an air data reference (adr) fault will cause the loss of airspeed and altitude information on the affected display. in either case the information can only be restored by selecting the no 3 adiru.each adiru comprises an adr and an inertial reference (ir) component. on 9 october 2018, the boeing 737-800 operating the flight from porto airport to edinburgh airport suffered a left adiru failure that resulted in the aircraft pitching up and climbing 600 feet. the left adiru was put in att (attitude-only) mode in accordance with the quick reference handbook, but it continued to display erroneous attitude information to the captain. the remainder of the flight was flown manually with an uneventful landing. the uk\'s aaib released the final report on 31 october 2019, with the following recommendation:it is recommended that boeing commercial aircraft amend the boeing 737 quick reference handbook to include a non-normal checklist for situations when pitch and roll comparator annunciations appear on the attitude display.   == see also == acronyms and abbreviations in avionics   == references ==   == further reading == dave carbaugh; doug forsythe; melville mcintyre. ""erroneous flight instrumenent information"". aero magazine. boeing. archived from the original on 6 september 2008. retrieved 2008-10-16. melville duncan w. mcintyre, boeing (2003-11-25). ""us patent 6654685 - apparatus and method for navigation of an aircraft"". united states patent office. retrieved 2008-10-16.')"
64,"The planet Mars has been explored remotely by spacecraft. Probes sent from Earth, beginning in the late 20th century, have yielded a large increase in knowledge about the Martian system, focused primarily on understanding its geology and habitability potential. Engineering interplanetary journeys is complicated and the exploration of Mars has experienced a high failure rate, especially the early attempts. Roughly sixty percent of all spacecraft destined for Mars failed before completing their missions and some failed before their observations could begin. Some missions have met with unexpected success, such as the twin Mars Exploration Rovers, Spirit and Opportunity which operated for years beyond their specification.


== Current status ==

As of February 2021, there are two operational rovers on the surface of Mars, the Curiosity and Perseverance rovers, both operated by the United States of America space agency NASA. A third rover, part of the Tianwen-1 mission, is currently attatched to its orbiter, and is planned to land in May 2021. There are eight orbiters surveying the planet: Mars Odyssey, Mars Express, Mars Reconnaissance Orbiter, Mars Orbiter Mission, MAVEN, the Trace Gas Orbiter, the Tianwen-1 orbiter, and the Hope Mars Mission, which have contributed massive amounts of information about Mars. The stationary lander InSight is investigating the deep interior of Mars. No sample return missions have been attempted for Mars and an attempted return mission for Mars' moon Phobos (Fobos-Grunt) failed at launch in 2011. In all, there are 11 probes currently surveying Mars, with a 12th, the Tianwen-1 rover, that is in Martian orbit but has not landed yet. 
The next missions expected to arrive at Mars are:

The joint ExoMars program of Roscosmos and ESA has delayed the launch of the Kazachok landing platform, which will carry the Rosalind Franklin rover, until 2022.
Mars Orbiter Mission 2 by India, planned launch in 2024.


== Martian system ==

Mars has long been the subject of human interest. Early telescopic observations revealed color changes on the surface that were attributed to seasonal vegetation and apparent linear features were ascribed to intelligent design. Further telescopic observations found two moons, Phobos and Deimos, polar ice caps and the feature now known as Olympus Mons, the Solar System's second tallest mountain. The discoveries piqued further interest in the study and exploration of the red planet. Mars is a rocky planet, like Earth, that formed around the same time, yet with only half the diameter of Earth, and a far thinner atmosphere; it has a cold and desert-like surface.One way the surface of Mars has been categorized, is by thirty ""quadrangles"", with each quadrangle named for a prominent physiographic feature within that quadrangle.


== Launch windows ==

The minimum-energy launch windows for a Martian expedition occur at intervals of approximately two years and two months (specifically 780 days, the planet's synodic period with respect to Earth). In addition, the lowest available transfer energy varies on a roughly 16-year cycle. For example, a minimum occurred in the 1969 and 1971 launch windows, rising to a peak in the late 1970s, and hitting another low in 1986 and 1988.


== Past and current missions ==

Starting in 1960, the Soviets launched a series of probes to Mars including the first intended flybys and hard (impact) landing (Mars 1962B). The first successful flyby of Mars was on 14–15 July 1965, by NASA's Mariner 4. On November 14, 1971, Mariner 9 became the first space probe to orbit another planet when it entered into orbit around Mars. The amount of data returned by probes increased dramatically as technology improved.The first to contact the surface were two Soviet probes: Mars 2 lander on November 27 and Mars 3 lander on December 2, 1971—Mars 2 failed during descent and Mars 3 about twenty seconds after the first Martian soft landing. Mars 6 failed during descent but did return some corrupted atmospheric data in 1974. The 1975 NASA launches of the Viking program consisted of two orbiters, each with a lander that successfully soft landed in 1976. Viking 1 remained operational for six years, Viking 2 for three. The Viking landers relayed the first color panoramas of Mars.The Soviet probes Phobos 1 and 2 were sent to Mars in 1988 to study Mars and its two moons, with a focus on Phobos. Phobos 1 lost contact on the way to Mars. Phobos 2, while successfully photographing Mars and Phobos, failed before it was set to release two landers to the surface of Phobos.Mars has a reputation as a difficult space exploration target; just 25 of 55 missions through 2019, or 45.5%, have been fully successful, with a further three partially successful and partially failures. However, of the sixteen missions since 2001, twelve have been successful and eight of these are still operational.
Missions that ended prematurely after Phobos 1 and 2 (1988) include (see Probing difficulties section for more details):

Mars Observer (launched in 1992)
Mars 96 (1996)
Mars Climate Orbiter (1999)
Mars Polar Lander with Deep Space 2 (1999)
Nozomi (2003)
Beagle 2 (2003)
Fobos-Grunt with Yinghuo-1 (2011)
Schiaparelli lander (2016)Following the 1993 failure of the Mars Observer orbiter, the NASA Mars Global Surveyor achieved Mars orbit in 1997. This mission was a complete success, having finished its primary mapping mission in early 2001. Contact was lost with the probe in November 2006 during its third extended program, spending exactly 10 operational years in space. The NASA Mars Pathfinder, carrying a robotic exploration vehicle Sojourner, landed in the Ares Vallis on Mars in the summer of 1997, returning many images.

NASA's Mars Odyssey orbiter entered Mars orbit in 2001. Odyssey's Gamma Ray Spectrometer detected significant amounts of hydrogen in the upper metre or so of regolith on Mars. This hydrogen is thought to be contained in large deposits of water ice.The Mars Express mission of the European Space Agency (ESA) reached Mars in 2003. It carried the Beagle 2 lander, which was not heard from after being released and was declared lost in February 2004.  Beagle 2 was located in January 2015 by HiRise camera on NASA's Mars Reconnaissance Orbiter (MRO) having landed safely but failed to fully deploy its solar panels and antenna. In early 2004, the Mars Express Planetary Fourier Spectrometer team announced the orbiter had detected methane in the Martian atmosphere, a potential biosignature. ESA announced in June 2006 the discovery of aurorae on Mars by the Mars Express.

In January 2004, the NASA twin Mars Exploration Rovers named Spirit (MER-A) and Opportunity (MER-B) landed on the surface of Mars. Both have met and exceeded all their science objectives. Among the most significant scientific returns has been conclusive evidence that liquid water existed at some time in the past at both landing sites. Martian dust devils and windstorms have occasionally cleaned both rovers' solar panels, and thus increased their lifespan. Spirit rover (MER-A) was active until 2010, when it stopped sending data because it got stuck in a sand dune and was unable to reorient itself to recharge its batteries.On 10 March 2006, NASA's Mars Reconnaissance Orbiter (MRO) probe arrived in orbit to conduct a two-year science survey. The orbiter began mapping the Martian terrain and weather to find suitable landing sites for upcoming lander missions. The MRO captured the first image of a series of active avalanches near the planet's north pole in 2008.Rosetta came within 250 km of Mars during its 2007 flyby. Dawn flew by Mars in February 2009 for a gravity assist on its way to investigate Vesta and Ceres.Phoenix landed on the north polar region of Mars on May 25, 2008. Its robotic arm dug into the Martian soil and the presence of water ice was confirmed on June 20, 2008. The mission concluded on November 10, 2008 after contact was lost. In 2008, the price of transporting material from the surface of Earth to the surface of Mars was approximately US$309,000 per kilogram.The Mars Science Laboratory mission was launched on November 26, 2011 and it delivered the Curiosity rover on the surface of Mars on August 6, 2012 UTC. It is larger and more advanced than the Mars Exploration Rovers, with a velocity of up to 90 meters per hour (295 feet per hour). Experiments include a laser chemical sampler that can deduce the composition of rocks at a distance of 7 meters.

MAVEN orbiter was launched on 18 November 2013, and on 22 September 2014, it was injected into an areocentric elliptic orbit 6,200 km (3,900 mi) by 150 km (93 mi) above the planet's surface to study its atmosphere. Mission goals include determining how the planet's atmosphere and water, presumed to have once been substantial, were lost over time.The Indian Space Research Organisation (ISRO) launched their Mars Orbiter Mission (MOM) on November 5, 2013, and it was inserted into Mars orbit on September 24, 2014. India's ISRO is the fourth space agency to reach Mars, after the Soviet space program, NASA and ESA. India successfully placed a spacecraft into Mars orbit, and became the first country to do so in its maiden attempt.The ExoMars Trace Gas Orbiter arrived at Mars in 2016 and deployed the Schiaparelli EDM lander, a test lander. Schiaparelli crashed on surface, but it transmitted key data during its parachute descent, so the test was declared a partial success.


=== Overview of missions ===
The following entails a brief overview of Mars exploration, oriented towards orbiters and flybys; see also Mars landing and Mars rover.


==== Early Soviet missions ====


===== 1960s =====

Between 1960 and 1969, the Soviet Union launched nine probes intended to reach Mars. They all failed: three at launch; three failed to reach near-Earth orbit; one during the burn to put the spacecraft into trans-Mars trajectory; and two during the interplanetary orbit.
The Mars 1M programs (sometimes dubbed Marsnik in Western media) was the first Soviet unmanned spacecraft interplanetary exploration program, which consisted of two flyby probes launched towards Mars in October 1960, Mars 1960A and Mars 1960B (also known as Korabl 4 and Korabl 5 respectively). After launch, the third stage pumps on both launchers were unable to develop enough pressure to commence ignition, so Earth parking orbit was not achieved. The spacecraft reached an altitude of 120 km before reentry.
Mars 1962A was a Mars flyby mission, launched on October 24, 1962 and Mars 1962B an intended first Mars lander mission, launched in late December of the same year (1962). Both failed from either breaking up as they were going into Earth orbit or having the upper stage explode in orbit during the burn to put the spacecraft into trans-Mars trajectory.


====== The first success ======
Mars 1 (1962 Beta Nu 1), an automatic interplanetary spacecraft launched to Mars on November 1, 1962, was the first probe of the Soviet Mars probe program to achieve interplanetary orbit. Mars 1 was intended to fly by the planet at a distance of about 11,000 km and take images of the surface as well as send back data on cosmic radiation, micrometeoroid impacts and Mars' magnetic field, radiation environment, atmospheric structure, and possible organic compounds. Sixty-one radio transmissions were held, initially at 2-day intervals and later at 5-day intervals, from which a large amount of interplanetary data was collected. On 21 March 1963, when the spacecraft was at a distance of 106,760,000 km from Earth, on its way to Mars, communications ceased due to failure of its antenna orientation system.In 1964, both Soviet probe launches, of Zond 1964A on June 4, and Zond 2 on November 30, (part of the Zond program), resulted in failures. Zond 1964A had a failure at launch, while communication was lost with Zond 2 en route to Mars after a mid-course maneuver, in early May 1965.In 1969, and as part of the Mars probe program, the Soviet Union prepared two identical 5-ton orbiters called M-69, dubbed by NASA as Mars 1969A and Mars 1969B.  Both probes were lost in launch-related complications with the newly developed Proton rocket.


===== 1970s =====
The USSR intended to have the first artificial satellite of Mars beating the planned American Mariner 8 and Mariner 9 Mars orbiters. In May 1971, one day after Mariner 8 malfunctioned at launch and failed to reach orbit, Cosmos 419 (Mars 1971C), a heavy probe of the Soviet Mars program M-71, also failed to launch. This spacecraft was designed as an orbiter only, while the next two probes of project M-71, Mars 2 and Mars 3, were multipurpose combinations of an orbiter and a lander with small skis-walking rovers that would be the first planet rovers outside the Moon. They were successfully launched in mid-May 1971 and reached Mars about seven months later. On November 27, 1971 the lander of Mars 2 crash-landed due to an on-board computer malfunction and became the first man-made object to reach the surface of Mars. On 2 December 1971, the Mars 3 lander became the first spacecraft to achieve a soft landing, but its transmission was interrupted after 14.5 seconds.The Mars 2 and 3 orbiters sent back a relatively large volume of data covering the period from December 1971 to March 1972, although transmissions continued through to August. By 22 August 1972, after sending back data and a total of 60 pictures, Mars 2 and 3 concluded their missions. The images and data enabled creation of surface relief maps, and gave information on the Martian gravity and magnetic fields.In 1973, the Soviet Union sent four more probes to Mars: the Mars 4 and Mars 5 orbiters and the Mars 6 and Mars 7 flyby/lander combinations. All missions except Mars 7 sent back data, with Mars 5 being most successful. Mars 5 transmitted just 60 images before a loss of pressurization in the transmitter housing ended the mission. Mars 6 lander transmitted data during descent, but failed upon impact. Mars 4 flew by the planet at a range of 2200 km returning one swath of pictures and radio occultation data, which constituted the first detection of the nightside ionosphere on Mars. Mars 7 probe separated prematurely from the carrying vehicle due to a problem in the operation of one of the onboard systems (attitude control or retro-rockets) and missed the planet by 1,300 kilometres (8.7×10−6 au).


==== Mariner program ====

In 1964, NASA's Jet Propulsion Laboratory made two attempts at reaching Mars. Mariner 3 and Mariner 4 were identical spacecraft designed to carry out the first flybys of Mars. Mariner 3 was launched on November 5, 1964, but the shroud encasing the spacecraft atop its rocket failed to open properly, dooming the mission. Three weeks later, on November 28, 1964, Mariner 4 was launched successfully on a 7½-month voyage to Mars.Mariner 4 flew past Mars on July 14, 1965, providing the first close-up photographs of another planet. The pictures, gradually played back to Earth from a small tape recorder on the probe, showed impact craters.  It provided radically more accurate data about the planet; a surface atmospheric pressure of about 1% of Earth's and daytime temperatures of −100 °C (−148 °F) were estimated. No magnetic field or Martian radiation belts were detected. The new data meant redesigns for then planned Martian landers, and showed life would have a more difficult time surviving there than previously anticipated.

NASA continued the Mariner program with another pair of Mars flyby probes, Mariner 6 and 7. They were sent at the next launch window, and reached the planet in 1969. During the following launch window the Mariner program again suffered the loss of one of a pair of probes. Mariner 9 successfully entered orbit about Mars, the first spacecraft ever to do so, after the launch time failure of its sister ship, Mariner 8.  When Mariner 9 reached Mars in 1971, it and two Soviet orbiters (Mars 2 and Mars 3) found that a planet-wide dust storm was in progress.  The mission controllers used the time spent waiting for the storm to clear to have the probe rendezvous with, and photograph, Phobos.  When the storm cleared sufficiently for Mars' surface to be photographed by Mariner 9, the pictures returned represented a substantial advance over previous missions. These pictures were the first to offer more detailed evidence that liquid water might at one time have flowed on the planetary surface. They also finally discerned the true nature of many Martian albedo features. For example, Nix Olympica was one of only a few features that could be seen during the planetary duststorm, revealing it to be the highest mountain (volcano, to be exact) on any planet in the entire Solar System, and leading to its reclassification as Olympus Mons.


==== Viking program ====

The Viking program launched Viking 1 and Viking 2 spacecraft to Mars in 1975; The program consisted of two orbiters and two landers – these were the second and third spacecraft to successfully land on Mars.

The primary scientific objectives of the lander mission were to search for biosignatures and observe meteorologic, seismic and magnetic properties of Mars. The results of the biological experiments on board the Viking landers remain inconclusive, with a reanalysis of the Viking data published in 2012 suggesting signs of microbial life on Mars.
The Viking orbiters revealed that large floods of water carved deep valleys, eroded grooves into bedrock, and traveled thousands of kilometers. Areas of branched streams, in the southern hemisphere, suggest that rain once fell.


==== Mars Pathfinder ====

Mars Pathfinder was a U.S. spacecraft that landed a base station with a roving probe on Mars on July 4, 1997. It consisted of a lander and a small 10.6 kilograms (23 lb) wheeled robotic rover named Sojourner, which was the first rover to operate on the surface of Mars.  In addition to scientific objectives, the Mars Pathfinder mission was also a ""proof-of-concept"" for various technologies, such as an airbag landing system and automated obstacle avoidance, both later exploited by the Mars Exploration Rovers.


==== Mars Global Surveyor ====

After the 1992 failure of NASA's Mars Observer orbiter, NASA retooled and launched Mars Global Surveyor (MGS).  Mars Global Surveyor launched on November 7, 1996, and entered orbit on September 12, 1997. After a year and a half trimming its orbit from a looping ellipse to a circular track around the planet, the spacecraft began its primary mapping mission in March 1999. It observed the planet from a low-altitude, nearly polar orbit over the course of one complete Martian year, the equivalent of nearly two Earth years. Mars Global Surveyor completed its primary mission on January 31, 2001, and completed several extended mission phases.The mission studied the entire Martian surface, atmosphere, and interior, and returned more data about the red planet than all previous Mars missions combined. The data has been archived and remains available publicly.

Among key scientific findings, Global Surveyor took pictures of gullies and debris flow features that suggest there may be current sources of liquid water, similar to an aquifer, at or near the surface of the planet.  Similar channels on Earth are formed by flowing water, but on Mars the temperature is normally too cold and the atmosphere too thin to sustain liquid water. Nevertheless, many scientists hypothesize that liquid groundwater can sometimes surface on Mars, erode gullies and channels, and pool at the bottom before freezing and evaporating.Magnetometer readings showed that the planet's magnetic field is not globally generated in the planet's core, but is localized in particular areas of the crust. New temperature data and closeup images of the Martian moon Phobos showed that its surface is composed of powdery material at least 1 metre (3 feet) thick, caused by millions of years of meteoroid impacts. Data from the spacecraft's laser altimeter gave scientists their first 3-D views of Mars' north polar ice cap.Faulty software uploaded to the vehicle in June 2006 caused the spacecraft to orient its solar panels incorrectly several months later, resulting in battery overheating and subsequent failure. On November 5, 2006 MGS lost contact with Earth.  NASA ended efforts to restore communication on January 28, 2007.


==== Mars Odyssey and Mars Express ====

In 2001, NASA's Mars Odyssey orbiter arrived at Mars. Its mission is to use spectrometers and imagers to hunt for evidence of past or present water and volcanic activity on Mars. In 2002, it was announced that the probe's gamma-ray spectrometer and neutron spectrometer had detected large amounts of hydrogen, indicating that there are vast deposits of water ice in the upper three meters of Mars' soil within 60° latitude of the south pole.On June 2, 2003, the European Space Agency's Mars Express set off from Baikonur Cosmodrome to Mars. The Mars Express craft consists of the Mars Express Orbiter and the stationary lander Beagle 2. The lander carried a digging device and the smallest mass spectrometer created to date, as well as a range of other devices, on a robotic arm in order to accurately analyze soil beneath the dusty surface to look for biosignatures and biomolecules.The orbiter entered Mars orbit on December 25, 2003, and Beagle 2 entered Mars' atmosphere the same day. However, attempts to contact the lander failed. Communications attempts continued throughout January, but Beagle 2 was declared lost in mid-February, and a joint inquiry was launched by the UK and ESA. The Mars Express Orbiter confirmed the presence of water ice and carbon dioxide ice at the planet's south pole, while NASA had previously confirmed their presence at the north pole of Mars.The lander's fate remained a mystery until it was located intact on the surface of Mars in a series of images from the Mars Reconnaissance Orbiter. The images suggest that two of the spacecraft's four solar panels failed to deploy, blocking the spacecraft's communications antenna. Beagle 2 is the first British and first European probe to achieve a soft landing on Mars.


==== MER and Phoenix ====

NASA's Mars Exploration Rover Mission (MER), started in 2003, was a robotic space mission involving two rovers, Spirit (MER-A) and Opportunity, (MER-B) that explored the Martian surface geology. The mission's scientific objective was to search for and characterize a wide range of rocks and soils that hold clues to past water activity on Mars. The mission was part of NASA's Mars Exploration Program, which includes three previous successful landers: the two Viking program landers in 1976; and Mars Pathfinder probe in 1997.


==== Mars Reconnaissance Orbiter ====

The Mars Reconnaissance Orbiter (MRO) is a multipurpose spacecraft designed to conduct reconnaissance and exploration of Mars from orbit. The US$720 million spacecraft was built by Lockheed Martin under the supervision of the Jet Propulsion Laboratory, launched August 12, 2005, and entered Mars orbit on March 10, 2006.The MRO contains a host of scientific instruments such as the HiRISE camera, CTX camera, CRISM, and SHARAD. The HiRISE camera is used to analyze Martian landforms, whereas CRISM and SHARAD can detect water, ice, and minerals on and below the surface.  Additionally, MRO is paving the way for upcoming generations of spacecraft through daily monitoring of Martian weather and surface conditions, searching for future landing sites, and testing a new telecommunications system that enable it to send and receive information at an unprecedented bitrate, compared to previous Mars spacecraft. Data transfer to and from the spacecraft occurs faster than all previous interplanetary missions combined and allows it to serve as an important relay satellite for other missions.


==== Rosetta and Dawn swingbys ====

The ESA Rosetta space probe mission to the comet 67P/Churyumov-Gerasimenko flew within 250 km of Mars on February 25, 2007, in a gravitational slingshot designed to slow and redirect the spacecraft.The NASA Dawn spacecraft used the gravity of Mars in 2009 to change direction and velocity on its way to Vesta, and tested out Dawn's cameras and other instruments on Mars.


==== Fobos-Grunt ====

On November 8, 2011, Russia's Roscosmos launched an ambitious mission called Fobos-Grunt. It consisted of a lander aimed to retrieve a sample back to Earth from Mars' moon Phobos, and place the Chinese Yinghuo-1 probe in Mars' orbit. The Fobos-Grunt mission suffered a complete control and communications failure shortly after launch and was left stranded in low Earth orbit, later falling back to Earth. The Yinghuo-1 satellite and Fobos-Grunt underwent destructive re-entry on January 15, 2012, finally disintegrating over the Pacific Ocean.


==== Curiosity rover ====

The NASA Mars Science Laboratory mission with its rover named Curiosity, was launched on November 26, 2011, and landed on Mars on August 6, 2012 on Aeolis Palus in Gale Crater. The rover carries instruments designed to look for past or present conditions relevant to the past or present habitability of Mars.


==== MAVEN ====
NASA's MAVEN is an orbiter mission to study the upper atmosphere of Mars. It will also serve as a communications relay satellite for robotic landers and rovers on the surface of Mars. MAVEN was launched 18 November 2013 and reached Mars on 22 September 2014.


==== Mars Orbiter Mission ====
The Mars Orbiter Mission, also called Mangalyaan, was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO). It was successfully inserted into Martian orbit on 24 September 2014.  The mission is a technology demonstrator, and as secondary objective, it will also study the Martian atmosphere. This is India's first mission to Mars, and with it, ISRO became the fourth space agency to successfully reach Mars after the Soviet Union, NASA (USA) and ESA (Europe). It also made ISRO the second space agency to reach Mars orbit on its first attempt (the first national one, after the international ESA), and also the first Asian country to successfully send an orbiter to Mars. It was completed in a record low budget of $71 million, making it the least-expensive Mars mission to date.


==== Trace Gas Orbiter and EDM ====

The ExoMars Trace Gas Orbiter is an atmospheric research orbiter built in collaboration between ESA and Roscosmos. It was injected into Mars orbit on 19 October 2016 to gain a better understanding of methane (CH4) and other trace gases present in the Martian atmosphere that could be evidence for possible biological or geological activity. The Schiaparelli EDM lander was destroyed when trying to land on the surface of Mars.


==== InSight and MarCO ====

In August 2012, NASA selected InSight, a $425 million lander mission with a heat flow probe and seismometer, to determine the deep interior structure of Mars. Two flyby CubeSats called MarCO were launched with InSight on 5 May 2018 to provide real-time telemetry during the entry and landing of InSight. The CubeSats separated from the Atlas V booster 1.5 hours after launch and traveled their own trajectories to Mars. InSight landed successfully on Mars on 26 November 2018.


==== Hope ====
The United Arab Emirates launched the Hope Mars Mission, in July 2020 on the Japanese H-IIA booster. It was successfully placed into orbit on 9 February 2021. It is studying the Martian atmosphere and weather.


==== Tianwen-1 ====
Tianwen-1 is a Chinese mission, launched on 23 July 2020. It includes an orbiter, a lander and a small rover. The orbiter was placed into orbit on 10 February 2021. The lander and rover are currently planned to land in May 2021.


==== Mars 2020 ====

The Mars 2020 mission by NASA was launched on 30 July 2020 on a United Launch Alliance Atlas V rocket from Cape Canaveral. It is based on the Mars Science Laboratory design. The scientific payload is focused on astrobiology. It includes Perseverance rover and Mars Helicopter Ingenuity. Unlike older rovers that relied on solar power, Perseverance is nuclear powered, to survive longer than its predecessors in this harsh, dusty environment. The car-size rover weighs about 1 ton, with a robotic arm that reaches about 7 feet, zoom cameras, a chemical analyzer and a rock drill.After traveling 293 million miles to reach Mars over the course of more than six months, Perseverance successfully landed on February 18, 2021. Its initial mission is set for at least one Martian year, or 687 Earth days. It will search for signs of ancient life and explore the red planet's surface.


== Future missions ==

As part of the ExoMars program, ESA and the Roscosmos plan to send the Rosalind Franklin rover in 2022 to search for evidence of past or present microscopic life on Mars. The lander to deliver the rover is called Kazachok, and it will perform scientific studies for about 2 years.
India's ISRO plans to send a follow-up mission to its Mars Orbiter Mission in 2024; it is called Mars Orbiter Mission 2 (MOM-2) and it will consist of an orbiter, and probably a rover.


=== Proposals ===
The Finnish-Russian Mars MetNet concept would use multiple small meteorological stations on Mars to establish a widespread observation network to investigate the planet's atmospheric structure, physics and meteorology. The MetNet precursor or demonstrator was considered for a piggyback launch on Fobos-Grunt, and on the two proposed to fly on the 2016 and 2020 ExoMars spacecraft.
The Mars-Grunt is a Russian mission concept to bring a sample of Martian soil to Earth.
A ESA-NASA team produced a three-launch architecture concept for a Mars sample return, which uses a rover to cache small samples, a Mars ascent stage to send it into orbit, and an orbiter to rendezvous with it above Mars and take it to Earth. Solar-electric propulsion could allow a one launch sample return instead of three.
The Mars Scout Program's SCIM would involve a probe grazing the upper atmosphere of Mars to collect dust and air for return to Earth.
JAXA is working on a mission concept called MELOS rover that would look for biosignatures of extant life on Mars.Other future mission concepts include polar probes, Martian aircraft and a network of small meteorological stations. Longterm areas of study may include Martian lava tubes, resource utilization, and electronic charge carriers in rocks. Micromissions are another possibility, such as piggybacking a small spacecraft on an Ariane 5 rocket and using a lunar gravity assist to get to Mars.


== Human mission proposals ==

The human exploration of Mars has been an aspiration since the earliest days of modern rocketry; Robert H. Goddard credits the idea of reaching Mars as his own inspiration to study the physics and engineering of space flight.  Proposals for human exploration of Mars have been made throughout the history of space exploration; currently there are multiple active plans and programs to put humans on Mars within the next ten to thirty years, both governmental and private, some of which are listed below.


=== NASA ===

Human exploration by the United States was identified as a long-term goal in the Vision for Space Exploration announced in 2004 by then US President George W. Bush. The planned Orion spacecraft would be used to send a human expedition to Earth's moon by 2020 as a stepping stone to a Mars expedition. On September 28, 2007, NASA administrator Michael D. Griffin stated that NASA aims to put a person on Mars by 2037.On December 2, 2014, NASA's Advanced Human Exploration Systems and Operations Mission Director Jason Crusan and Deputy Associate Administrator for Programs James Reuthner announced tentative support for the Boeing ""Affordable Mars Mission Design"" including radiation shielding, centrifugal artificial gravity, in-transit consumable resupply, and a lander which can return. Reuthner suggested that if adequate funding was forthcoming, the proposed mission would be expected in the early 2030s.On October 8, 2015, NASA published its official plan for human exploration and colonization of Mars. They called it ""Journey to Mars"". The plan operates through three distinct phases leading up to fully sustained colonization.
The first stage, already underway, is the ""Earth Reliant"" phase.  This phase continues utilizing the International Space Station until 2024; validating deep space technologies and studying the effects of long duration space missions on the human body.
The second stage, ""Proving Ground,"" moves away from Earth reliance and ventures into cislunar space for most of its tasks.  This is when NASA plans to capture an asteroid (planned for 2020), test deep space habitation facilities, and validate capabilities required for human exploration of Mars.  Finally, phase three is the transition to independence from Earth resources.
The last stage, the ""Earth Independent"" phase, includes long term missions on the lunar surface which leverage surface habitats that only require routine maintenance, and the harvesting of Martian resources for fuel, water, and building materials.  NASA is still aiming for human missions to Mars in the 2030s, though Earth independence could take decades longer.
On August 28, 2015, NASA funded a year long simulation to study the effects of a year long Mars mission on six scientists. The scientists lived in a bio dome on a Mauna Loa mountain in Hawaii with limited connection to the outside world and were only allowed outside if they were wearing spacesuits.NASAs human Mars exploration plans have evolved through the NASA Mars Design Reference Missions, a series of design studies for human exploration of Mars.
In 2017 the focus of NASA shifted to a return to the Moon by 2024 with the Artemis program, a flight to Mars could follow after this project.


=== SpaceX ===
The long-term goal of the private corporation SpaceX is the establishment of routine flights to Mars to enable colonization. To this end, the company is developing Starship, a spacecraft capable of crew transportation to Mars and other celestial bodies, along with its booster Super Heavy. In 2017 SpaceX announced plans to send two uncrewed Starships to Mars by 2022, followed by two more uncrewed flights and two crewed flights in 2024. Starship is planned to have a payload of at least 100 tonnes. Starship is designed to use a combination of aerobraking and propulsive descent, utilizing fuel produced from a Mars (in situ resource utilization) facility.  As of early 2021, the Starship development program has seen successful testing of several Starship prototypes. Most notably, Starship SN8, which performed its first test flight in December 2020, was a partial success. Although major objectives were achieved, including stable ascent, descent and flip maneuver, it crashed upon landing due to pressurization issues in a fuel tank. A similar approach is intended to be used on Mars.


=== Zubrin ===
Mars Direct, a low-cost human mission proposed by Robert Zubrin, founder of the Mars Society, would use heavy-lift Saturn V class rockets, such as the Ares V, to skip orbital construction, LEO rendezvous, and lunar fuel depots. A modified proposal, called ""Mars to Stay"", involves not returning the first immigrant explorers immediately, if ever (see Colonization of Mars).


== Probing difficulties ==

The challenge, complexity and length of Mars missions have led to many mission failures. The high failure rate of missions attempting to explore Mars is informally called the ""Mars Curse"" or ""Martian Curse"".  The phrase ""Galactic Ghoul"" or ""Great Galactic Ghoul"", referring to a fictitious space monster that subsists on a diet of Mars probes, and is sometimes facetiously used to ""explain"" the recurring difficulties.Two Soviet probes were sent to Mars in 1988 as part of the Phobos program. Phobos 1 operated normally until an expected communications session on 2 September 1988 failed to occur. The problem was traced to a software error, which deactivated Phobos 1's attitude thrusters, causing the spacecraft's solar arrays to no longer point at the Sun, depleting Phobos 1's batteries. Phobos 2 operated normally throughout its cruise and Mars orbital insertion phases on January 29, 1989, gathering data on the Sun, interplanetary medium, Mars, and Phobos. Shortly before the final phase of the mission – during which the spacecraft was to approach within 50 m of Phobos' surface and release two landers, one a mobile 'hopper', the other a stationary platform – contact with Phobos 2 was lost. The mission ended when the spacecraft signal failed to be successfully reacquired on March 27, 1989. The cause of the failure was determined to be a malfunction of the on-board computer.Just a few years later in 1992 Mars Observer, launched by NASA, failed as it approached Mars. Mars 96, an orbiter launched on November 16, 1996 by Russia failed, when the planned second burn of the Block D-2 fourth stage did not occur.Following the success of Global Surveyor and Pathfinder, another spate of failures occurred in 1998 and 1999, with the Japanese Nozomi orbiter and NASA's Mars Climate Orbiter, Mars Polar Lander, and Deep Space 2 penetrators all suffering various fatal errors. The Mars Climate Orbiter was noted for mixing up U.S. customary units with metric units, causing the orbiter to burn up while entering Mars' atmosphere.The European Space Agency has also attempted to land two probes on the Martian surface; Beagle 2, a British-built lander that failed to deploy its solar arrays properly after touchdown in December 2003, and Schiaparelli, which was flown along the ExoMars Trace Gas Orbiter. Contact with the Schiaparelli EDM lander was lost 50 seconds before touchdown. It was later confirmed that the lander struck the surface at a high velocity, possibly exploding.


== See also ==

Mars
General


== References ==


== Bibliography ==
Mars – A Warmer, Wetter Planet by Jeffrey S. Kargel (published July 2004; ISBN 978-1-85233-568-7)
The Compact NASA Atlas of the Solar System by Ronald Greeley and Raymond Batson (published January 2002; ISBN 0-521-80633-X)
Mars: The NASA Mission Reports / edited by Robert Godwin (2000) ISBN 1-896522-62-9


== External links ==
NASA Mars exploration website
Mars Exploration Scientific American Maps and Articles
Next on Mars (Bruce Moomaw, Space Daily, 9 March 2005): An extensive overview of NASA's Mars exploration plans
Catalog of Soviet Mars images Collection of Russian Mars probes' images.
Simplified study of orbits to land on Mars and return to Earth (High School level)
Planetary Society Mars page


== Notes ==
^α  The diagram includes missions that are active on the surface, such as operational rovers and landers, as well as probes in Mars orbit. The diagram does not include missions that are en route to Mars, or probes that performed a fly-by of Mars and moved on.","pandas(index=64, _1=64, text='the planet mars has been explored remotely by spacecraft. probes sent from earth, beginning in the late 20th century, have yielded a large increase in knowledge about the martian system, focused primarily on understanding its geology and habitability potential. engineering interplanetary journeys is complicated and the exploration of mars has experienced a high failure rate, especially the early attempts. roughly sixty percent of all spacecraft destined for mars failed before completing their missions and some failed before their observations could begin. some missions have met with unexpected success, such as the twin mars exploration rovers, spirit and opportunity which operated for years beyond their specification.   == current status ==  as of february 2021, there are two operational rovers on the surface of mars, the curiosity and perseverance rovers, both operated by the united states of america space agency nasa. a third rover, part of the tianwen-1 mission, is currently attatched to its orbiter, and is planned to land in may 2021. there are eight orbiters surveying the planet: mars odyssey, mars express, mars reconnaissance orbiter, mars orbiter mission, maven, the trace gas orbiter, the tianwen-1 orbiter, and the hope mars mission, which have contributed massive amounts of information about mars. the stationary lander insight is investigating the deep interior of mars. no sample return missions have been attempted for mars and an attempted return mission for mars\' moon phobos (fobos-grunt) failed at launch in 2011. in all, there are 11 probes currently surveying mars, with a 12th, the tianwen-1 rover, that is in martian orbit but has not landed yet. the next missions expected to arrive at mars are:  the joint exomars program of roscosmos and esa has delayed the launch of the kazachok landing platform, which will carry the rosalind franklin rover, until 2022. mars orbiter mission 2 by india, planned launch in 2024.   == martian system ==  mars has long been the subject of human interest. early telescopic observations revealed color changes on the surface that were attributed to seasonal vegetation and apparent linear features were ascribed to intelligent design. further telescopic observations found two moons, phobos and deimos, polar ice caps and the feature now known as olympus mons, the solar system\'s second tallest mountain. the discoveries piqued further interest in the study and exploration of the red planet. mars is a rocky planet, like earth, that formed around the same time, yet with only half the diameter of earth, and a far thinner atmosphere; it has a cold and desert-like surface.one way the surface of mars has been categorized, is by thirty ""quadrangles"", with each quadrangle named for a prominent physiographic feature within that quadrangle.   == launch windows ==  the minimum-energy launch windows for a martian expedition occur at intervals of approximately two years and two months (specifically 780 days, the planet\'s synodic period with respect to earth). in addition, the lowest available transfer energy varies on a roughly 16-year cycle. for example, a minimum occurred in the 1969 and 1971 launch windows, rising to a peak in the late 1970s, and hitting another low in 1986 and 1988.   == past and current missions ==  starting in 1960, the soviets launched a series of probes to mars including the first intended flybys and hard (impact) landing (mars 1962b). the first successful flyby of mars was on 14–15 july 1965, by nasa\'s mariner 4. on november 14, 1971, mariner 9 became the first space probe to orbit another planet when it entered into orbit around mars. the amount of data returned by probes increased dramatically as technology improved.the first to contact the surface were two soviet probes: mars 2 lander on november 27 and mars 3 lander on december 2, 1971—mars 2 failed during descent and mars 3 about twenty seconds after the first martian soft landing. mars 6 failed during descent but did return some corrupted atmospheric data in 1974. the 1975 nasa launches of the viking program consisted of two orbiters, each with a lander that successfully soft landed in 1976. viking 1 remained operational for six years, viking 2 for three. the viking landers relayed the first color panoramas of mars.the soviet probes phobos 1 and 2 were sent to mars in 1988 to study mars and its two moons, with a focus on phobos. phobos 1 lost contact on the way to mars. phobos 2, while successfully photographing mars and phobos, failed before it was set to release two landers to the surface of phobos.mars has a reputation as a difficult space exploration target; just 25 of 55 missions through 2019, or 45.5%, have been fully successful, with a further three partially successful and partially failures. however, of the sixteen missions since 2001, twelve have been successful and eight of these are still operational. missions that ended prematurely after phobos 1 and 2 (1988) include (see probing difficulties section for more details):  mars observer (launched in 1992) mars 96 (1996) mars climate orbiter (1999) mars polar lander with deep space 2 (1999) nozomi (2003) beagle 2 (2003) fobos-grunt with yinghuo-1 (2011) schiaparelli lander (2016)following the 1993 failure of the mars observer orbiter, the nasa mars global surveyor achieved mars orbit in 1997. this mission was a complete success, having finished its primary mapping mission in early 2001. contact was lost with the probe in november 2006 during its third extended program, spending exactly 10 operational years in space. the nasa mars pathfinder, carrying a robotic exploration vehicle sojourner, landed in the ares vallis on mars in the summer of 1997, returning many images.  nasa\'s mars odyssey orbiter entered mars orbit in 2001. odyssey\'s gamma ray spectrometer detected significant amounts of hydrogen in the upper metre or so of regolith on mars. this hydrogen is thought to be contained in large deposits of water ice.the mars express mission of the european space agency (esa) reached mars in 2003. it carried the beagle 2 lander, which was not heard from after being released and was declared lost in february 2004.  beagle 2 was located in january 2015 by hirise camera on nasa\'s mars reconnaissance orbiter (mro) having landed safely but failed to fully deploy its solar panels and antenna. in early 2004, the mars express planetary fourier spectrometer team announced the orbiter had detected methane in the martian atmosphere, a potential biosignature. esa announced in june 2006 the discovery of aurorae on mars by the mars express.  in january 2004, the nasa twin mars exploration rovers named spirit (mer-a) and opportunity (mer-b) landed on the surface of mars. both have met and exceeded all their science objectives. among the most significant scientific returns has been conclusive evidence that liquid water existed at some time in the past at both landing sites. martian dust devils and windstorms have occasionally cleaned both rovers\' solar panels, and thus increased their lifespan. spirit rover (mer-a) was active until 2010, when it stopped sending data because it got stuck in a sand dune and was unable to reorient itself to recharge its batteries.on 10 march 2006, nasa\'s mars reconnaissance orbiter (mro) probe arrived in orbit to conduct a two-year science survey. the orbiter began mapping the martian terrain and weather to find suitable landing sites for upcoming lander missions. the mro captured the first image of a series of active avalanches near the planet\'s north pole in 2008.rosetta came within 250 km of mars during its 2007 flyby. dawn flew by mars in february 2009 for a gravity assist on its way to investigate vesta and ceres.phoenix landed on the north polar region of mars on may 25, 2008. its robotic arm dug into the martian soil and the presence of water ice was confirmed on june 20, 2008. the mission concluded on november 10, 2008 after contact was lost. in 2008, the price of transporting material from the surface of earth to the surface of mars was approximately us$309,000 per kilogram.the mars science laboratory mission was launched on november 26, 2011 and it delivered the curiosity rover on the surface of mars on august 6, 2012 utc. it is larger and more advanced than the mars exploration rovers, with a velocity of up to 90 meters per hour (295 feet per hour). experiments include a laser chemical sampler that can deduce the composition of rocks at a distance of 7 meters.  maven orbiter was launched on 18 november 2013, and on 22 september 2014, it was injected into an areocentric elliptic orbit 6,200 km (3,900 mi) by 150 km (93 mi) above the planet\'s surface to study its atmosphere. mission goals include determining how the planet\'s atmosphere and water, presumed to have once been substantial, were lost over time.the indian space research organisation (isro) launched their mars orbiter mission (mom) on november 5, 2013, and it was inserted into mars orbit on september 24, 2014. india\'s isro is the fourth space agency to reach mars, after the soviet space program, nasa and esa. india successfully placed a spacecraft into mars orbit, and became the first country to do so in its maiden attempt.the exomars trace gas orbiter arrived at mars in 2016 and deployed the schiaparelli edm lander, a test lander. schiaparelli crashed on surface, but it transmitted key data during its parachute descent, so the test was declared a partial success. mars direct, a low-cost human mission proposed by robert zubrin, founder of the mars society, would use heavy-lift saturn v class rockets, such as the ares v, to skip orbital construction, leo rendezvous, and lunar fuel depots. a modified proposal, called ""mars to stay"", involves not returning the first immigrant explorers immediately, if ever (see colonization of mars).   == probing difficulties ==  the challenge, complexity and length of mars missions have led to many mission failures. the high failure rate of missions attempting to explore mars is informally called the ""mars curse"" or ""martian curse"".  the phrase ""galactic ghoul"" or ""great galactic ghoul"", referring to a fictitious space monster that subsists on a diet of mars probes, and is sometimes facetiously used to ""explain"" the recurring difficulties.two soviet probes were sent to mars in 1988 as part of the phobos program. phobos 1 operated normally until an expected communications session on 2 september 1988 failed to occur. the problem was traced to a software error, which deactivated phobos 1\'s attitude thrusters, causing the spacecraft\'s solar arrays to no longer point at the sun, depleting phobos 1\'s batteries. phobos 2 operated normally throughout its cruise and mars orbital insertion phases on january 29, 1989, gathering data on the sun, interplanetary medium, mars, and phobos. shortly before the final phase of the mission – during which the spacecraft was to approach within 50 m of phobos\' surface and release two landers, one a mobile \'hopper\', the other a stationary platform – contact with phobos 2 was lost. the mission ended when the spacecraft signal failed to be successfully reacquired on march 27, 1989. the cause of the failure was determined to be a malfunction of the on-board computer.just a few years later in 1992 mars observer, launched by nasa, failed as it approached mars. mars 96, an orbiter launched on november 16, 1996 by russia failed, when the planned second burn of the block d-2 fourth stage did not occur.following the success of global surveyor and pathfinder, another spate of failures occurred in 1998 and 1999, with the japanese nozomi orbiter and nasa\'s mars climate orbiter, mars polar lander, and deep space 2 penetrators all suffering various fatal errors. the mars climate orbiter was noted for mixing up u.s. customary units with metric units, causing the orbiter to burn up while entering mars\' atmosphere.the european space agency has also attempted to land two probes on the martian surface; beagle 2, a british-built lander that failed to deploy its solar arrays properly after touchdown in december 2003, and schiaparelli, which was flown along the exomars trace gas orbiter. contact with the schiaparelli edm lander was lost 50 seconds before touchdown. it was later confirmed that the lander struck the surface at a high velocity, possibly exploding.   == see also ==  mars general   == references ==   == bibliography == mars – a warmer, wetter planet by jeffrey s. kargel (published july 2004; isbn 978-1-85233-568-7) the compact nasa atlas of the solar system by ronald greeley and raymond batson (published january 2002; isbn 0-521-80633-x) mars: the nasa mission reports / edited by robert godwin (2000) isbn 1-896522-62-9   == external links == nasa mars exploration website mars exploration scientific american maps and articles next on mars (bruce moomaw, space daily, 9 march 2005): an extensive overview of nasa\'s mars exploration plans catalog of soviet mars images collection of russian mars probes\' images. simplified study of orbits to land on mars and return to earth (high school level) planetary society mars page   == notes == ^α  the diagram includes missions that are active on the surface, such as operational rovers and landers, as well as probes in mars orbit. the diagram does not include missions that are en route to mars, or probes that performed a fly-by of mars and moved on.')"
65,"An aerostructure is a component of an aircraft's airframe. This may include all or part of the fuselage, wings, or flight control surfaces. Companies that specialize in constructing these components are referred to as ""aerostructures manufacturers"", though many larger aerospace firms with a more diversified product portfolio also build aerostructures.
Mechanical testing of the individual components or complete structure is carried out on a Universal Testing Machine.  Test carried out include tensile, compression, flexure, fatigue, impact, compression after impact. Before testing the component, aerospace engineers build finite element models to simulate the reality.


== Civilian ==
Airplanes designed for civilian use are often cheaper than military aircraft. Smaller passenger airplanes are used for short distance, transcontinental transport. It is more cost efficient for airlines and there is less demand for aircraft transportation at these distances as people can, while inconvenient, drive these distances. While bigger airplanes are manufactured for intercontinental transport, so more passengers can be carried at one time, money can be saved on fuel, and airliners do not have to pay as many pilots. Cargo planes are usually built to be bigger than the average jet. They have a lot of space and large dimensions, so they can carry a lot of weight and a large volume of cargo in one trip. They have large wingspans, a very large cargo hold, and a very tall vertical fin. They are not built to accommodate passengers except for the pilots, so the use of the cargo hold is much more efficient. There does not need to be room for seats and food and bathrooms for everybody, so the companies made a design that optimizes the space in the aircraft.


== Military ==
The YC-14 Prototype was a prototype plane that was being designed by Boeing specifically for the US Air Force. There were a lot of different designs that were considered and different technologies that were used specifically for carrying tanks and paratroopers. There was a computer that was installed and a very powerful vertical wing that could keep the plane flying at a set altitude, so they could drop whatever they needed to in the battlefield without any complications. This allowed for precise troop placement which could be the difference between victory and defeat in a battle. It also talks about different cheaper materials for the prototype which were heavier and used a honeycomb pattern. The cheaper materials were too heavy, and the Air Force was not happy that Boeing did not meet the Air Force's expectations on the prototype even though the Air Force was aware that they would be using different materials in the production of the actual aircraft.The Apache helicopter that Boeing makes is designed so the front of the helicopter is very narrow.  Not only does it create less drag, but it is a smaller target for infantry units to hit the helicopter. They have also designed the F-15 fighter jet, which has two engines instead of one for maximum speed. This particular aircraft can reach speeds of Mach 2.5. It also happens to be the 8th fastest aircraft ever built. The Boeing C-17 Globemaster 3 uses size and a very large design to carry cargo. It has 4 powerful engines and a special T-tail designed by Boeing for precise control of the unusually large aircraft.


== Research ==
There is a new aircraft material that is 20% lighter than other conventional aircraft materials. However FSW aluminum-alloy which is much heavier than this new material, is more advantageous as opposed to using the new CFRP black constructions. The aluminum is more understood and can be crafted to almost exact precision as opposed to the CFRP, which is very hard to shape. The weight of the aircraft is important, but the precision of the measurements of the aircraft is also important. The new methods and testing require a wide variety of material properties, even though weight is very important when choosing a material.Additionally, there is a new method for research, called Thermography, that uses infrared light to look at computer simulated damage to the material and the structure of an aircraft to see how it holds up. They can use this to look at materials and evaluate the integrity of the actual design of an aircraft. It is very accurate, and it will increase the development of materials as the test is much faster than traditional testing methods. It can also be used to predict the behavior of materials under certain stressful conditions that might make it fail while in use.Boeing Australia is creating big new plants that will help them research and develop materials for aircraft faster than anybody else. Their goal is to be the most innovative company and be the most innovative company at the highest speed. As a result, they are making investments in robots to get the job done. They have decided not to use cheap labor, but high cost, quality labor and a high amount of faculty to maintain these robots and ensure that the plant is running well. They will be paying a high amount of very well qualified candidates to research and keep Boeing going. The age of aircraft is moving toward expensive plants to be able to build the aircraft that is so precisely designed to the exact measurements that is needed for optimal performance and reliability. Aircraft are advanced machines that have only been around for a little more than one hundred years.


== Examples ==
Aero Vodochody
Alcoa's Howmet division
Collins Aerospace, currently a subsidiary of Raytheon Technologies
D-J Engineering Inc.
FACC
GKN
Goodrich Aerostructures Group, currently a part of Collins Aerospace
Mitsubishi Heavy Industries Aerospace
Messier-Bugatti-Dowty
Indonesian Aerospace
Premium AEROTEC
Exelis Inc.
Groupe Latécoère
Spirit AeroSystems
Stelia Aerospace
Vought


== References ==","pandas(index=65, _1=65, text='an aerostructure is a component of an aircraft\'s airframe. this may include all or part of the fuselage, wings, or flight control surfaces. companies that specialize in constructing these components are referred to as ""aerostructures manufacturers"", though many larger aerospace firms with a more diversified product portfolio also build aerostructures. mechanical testing of the individual components or complete structure is carried out on a universal testing machine.  test carried out include tensile, compression, flexure, fatigue, impact, compression after impact. before testing the component, aerospace engineers build finite element models to simulate the reality.   == civilian == airplanes designed for civilian use are often cheaper than military aircraft. smaller passenger airplanes are used for short distance, transcontinental transport. it is more cost efficient for airlines and there is less demand for aircraft transportation at these distances as people can, while inconvenient, drive these distances. while bigger airplanes are manufactured for intercontinental transport, so more passengers can be carried at one time, money can be saved on fuel, and airliners do not have to pay as many pilots. cargo planes are usually built to be bigger than the average jet. they have a lot of space and large dimensions, so they can carry a lot of weight and a large volume of cargo in one trip. they have large wingspans, a very large cargo hold, and a very tall vertical fin. they are not built to accommodate passengers except for the pilots, so the use of the cargo hold is much more efficient. there does not need to be room for seats and food and bathrooms for everybody, so the companies made a design that optimizes the space in the aircraft.   == military == the yc-14 prototype was a prototype plane that was being designed by boeing specifically for the us air force. there were a lot of different designs that were considered and different technologies that were used specifically for carrying tanks and paratroopers. there was a computer that was installed and a very powerful vertical wing that could keep the plane flying at a set altitude, so they could drop whatever they needed to in the battlefield without any complications. this allowed for precise troop placement which could be the difference between victory and defeat in a battle. it also talks about different cheaper materials for the prototype which were heavier and used a honeycomb pattern. the cheaper materials were too heavy, and the air force was not happy that boeing did not meet the air force\'s expectations on the prototype even though the air force was aware that they would be using different materials in the production of the actual aircraft.the apache helicopter that boeing makes is designed so the front of the helicopter is very narrow.  not only does it create less drag, but it is a smaller target for infantry units to hit the helicopter. they have also designed the f-15 fighter jet, which has two engines instead of one for maximum speed. this particular aircraft can reach speeds of mach 2.5. it also happens to be the 8th fastest aircraft ever built. the boeing c-17 globemaster 3 uses size and a very large design to carry cargo. it has 4 powerful engines and a special t-tail designed by boeing for precise control of the unusually large aircraft.   == research == there is a new aircraft material that is 20% lighter than other conventional aircraft materials. however fsw aluminum-alloy which is much heavier than this new material, is more advantageous as opposed to using the new cfrp black constructions. the aluminum is more understood and can be crafted to almost exact precision as opposed to the cfrp, which is very hard to shape. the weight of the aircraft is important, but the precision of the measurements of the aircraft is also important. the new methods and testing require a wide variety of material properties, even though weight is very important when choosing a material.additionally, there is a new method for research, called thermography, that uses infrared light to look at computer simulated damage to the material and the structure of an aircraft to see how it holds up. they can use this to look at materials and evaluate the integrity of the actual design of an aircraft. it is very accurate, and it will increase the development of materials as the test is much faster than traditional testing methods. it can also be used to predict the behavior of materials under certain stressful conditions that might make it fail while in use.boeing australia is creating big new plants that will help them research and develop materials for aircraft faster than anybody else. their goal is to be the most innovative company and be the most innovative company at the highest speed. as a result, they are making investments in robots to get the job done. they have decided not to use cheap labor, but high cost, quality labor and a high amount of faculty to maintain these robots and ensure that the plant is running well. they will be paying a high amount of very well qualified candidates to research and keep boeing going. the age of aircraft is moving toward expensive plants to be able to build the aircraft that is so precisely designed to the exact measurements that is needed for optimal performance and reliability. aircraft are advanced machines that have only been around for a little more than one hundred years.   == examples == aero vodochody alcoa\'s howmet division collins aerospace, currently a subsidiary of raytheon technologies d-j engineering inc. facc gkn goodrich aerostructures group, currently a part of collins aerospace mitsubishi heavy industries aerospace messier-bugatti-dowty indonesian aerospace premium aerotec exelis inc. groupe latécoère spirit aerosystems stelia aerospace vought   == references ==')"
66,"The West Wing of the White House houses the offices of the president of the United States.  The West Wing contains the Oval Office, the Cabinet Room, the Situation Room, and the Roosevelt Room.The West Wing's four floors contain offices for the vice president, White House chief of staff, the counselor to the president, the senior advisor to the president, the White House press secretary, and their support staffs. Adjoining the press secretary's office, in the colonnade between the West Wing and the Executive Residence is the James S. Brady Press Briefing Room along with workspace for the White House press corps.


== History ==

Before the construction of the West Wing, presidential staff worked on the western end of the second floor of what is now the Executive Residence. However, when Theodore Roosevelt became president, he found that the existing offices in the mansion were insufficient to accommodate his family of six children as well as his staff.
A year later in 1902, First Lady Edith Roosevelt hired McKim, Mead & White to separate the living quarters from the offices, to enlarge and modernize the public rooms, to re-do the landscaping, and to redecorate the interior.  Congress approved over half a million dollars for the renovation.The West Wing was originally intended as a temporary office structure, built on the site of the extensive greenhouses and stables. The President's Office and the Cabinet Room took up the eastern third of the building closest to the Residence and attached colonnaded terrace. Roosevelt's rectangular office with adjacent Cabinet Room through a set of double doors which was located approximately where the Roosevelt Room is now near the centre.In 1909, William Howard Taft expanded the building southward, covering the tennis court. He placed the first Oval Office at the centre of the addition's south facade, reminiscent of the oval rooms on the three floors of the White House. Later, at the outset of his presidency, Herbert Hoover rebuilt the West Wing, excavating a partial basement, and supporting it with structural steel. The completed building, however, lasted less than seven months. On December 24, 1929, the West Wing was significantly damaged by an electrical fire. Hoover rebuilt it, and added air-conditioning.
The fourth and final major reorganization was undertaken less than three years later by Franklin D. Roosevelt. Dissatisfied with the size and layout of President Hoover's West Wing, he engaged New York architect Eric Gugler to redesign it in 1933. To create additional space without increasing the apparent size of the building, Gugler excavated a full basement, added a set of subterranean offices under the adjacent lawn, and built an unobtrusive ""penthouse"" story. The directive to wring the most office space out of the existing building was responsible for its narrow corridors and cramped staff offices. Gugler's most notable change was the addition to the east side containing a new Cabinet Room, Secretary's Office, and Oval Office. The new office's location gave presidents greater privacy, allowing them to slip back and forth between the White House and the West Wing without being in full view of the staff.As the size of the president's staff grew over the latter half of the 20th century, the West Wing generally came to be seen as too small for its modern governmental functions. Today, most of the staff members of the Executive Office of the President are located in the adjacent Eisenhower Executive Office Building.

		
		
		
		


== First floor ==


=== Oval Office ===


=== Cabinet Room ===


=== Roosevelt Room ===

Richard Nixon also renamed the room, previously called by Franklin Roosevelt the ""Fish Room"" (where he kept aquariums, and where John F. Kennedy displayed trophy fish), in honour of the two presidents Roosevelt: Theodore, who first built the West Wing, and Franklin, who built the current Oval Office. By tradition, a portrait of Franklin Roosevelt hangs over the mantel of the Roosevelt Room during the administration of a president from the Democratic Party and a portrait of Theodore Roosevelt hangs during the administration of a Republican president (although Bill Clinton chose to retain the portrait of Theodore Roosevelt above the mantel). In the past, the portrait not hanging over the mantel hung on the opposite wall. However, during the first term of George W. Bush, an audio-visual cabinet was placed on the opposite wall providing secure audio and visual conference capabilities across the hall from the Oval Office.


=== Press Briefing Room ===

During the 1930s, the March of Dimes constructed a swimming pool so that Franklin Roosevelt could exercise, as therapy for his polio-related disability. Richard Nixon had the swimming pool covered over to create the Press Briefing Room, where the White House Press Secretary gives daily briefings.


=== White House press corps ===

The journalists, correspondents, and others who are part of the White House press corps have offices near the press briefing room.

		
		
		
		


== Ground floor ==


=== Situation Room ===


=== White House Mess ===
The West Wing ground floor is also the site of a small restaurant operated by the Presidential Food Service and staffed by Naval culinary specialists and called the White House Mess. It is located underneath the Oval Office, and was established by President Truman on June 11, 1951.

		
		


== Second floor ==


== Depiction on The West Wing TV series ==

In 1999, The West Wing television series brought greater public attention to the workings of the presidential staff, as well as to the location of those working in the West Wing. The show followed the working lives of a fictional Democratic U.S. president, Josiah Bartlet, and his senior staff. When asked whether the show accurately captured the working environment in 2003, Press Secretary Scott McClellan commented that the show portrayed more foot traffic and larger rooms than in the real West Wing.


== References ==


== External links ==
White House Museum: West Wing, with floorplan and historical images
West Wing Interactive, from National Journal Magazine","pandas(index=66, _1=66, text='the west wing of the white house houses the offices of the president of the united states.  the west wing contains the oval office, the cabinet room, the situation room, and the roosevelt room.the west wing\'s four floors contain offices for the vice president, white house chief of staff, the counselor to the president, the senior advisor to the president, the white house press secretary, and their support staffs. adjoining the press secretary\'s office, in the colonnade between the west wing and the executive residence is the james s. brady press briefing room along with workspace for the white house press corps.   == history ==  before the construction of the west wing, presidential staff worked on the western end of the second floor of what is now the executive residence. however, when theodore roosevelt became president, he found that the existing offices in the mansion were insufficient to accommodate his family of six children as well as his staff. a year later in 1902, first lady edith roosevelt hired mckim, mead & white to separate the living quarters from the offices, to enlarge and modernize the public rooms, to re-do the landscaping, and to redecorate the interior.  congress approved over half a million dollars for the renovation.the west wing was originally intended as a temporary office structure, built on the site of the extensive greenhouses and stables. the president\'s office and the cabinet room took up the eastern third of the building closest to the residence and attached colonnaded terrace. roosevelt\'s rectangular office with adjacent cabinet room through a set of double doors which was located approximately where the roosevelt room is now near the centre.in 1909, william howard taft expanded the building southward, covering the tennis court. he placed the first oval office at the centre of the addition\'s south facade, reminiscent of the oval rooms on the three floors of the white house. later, at the outset of his presidency, herbert hoover rebuilt the west wing, excavating a partial basement, and supporting it with structural steel. the completed building, however, lasted less than seven months. on december 24, 1929, the west wing was significantly damaged by an electrical fire. hoover rebuilt it, and added air-conditioning. the fourth and final major reorganization was undertaken less than three years later by franklin d. roosevelt. dissatisfied with the size and layout of president hoover\'s west wing, he engaged new york architect eric gugler to redesign it in 1933. to create additional space without increasing the apparent size of the building, gugler excavated a full basement, added a set of subterranean offices under the adjacent lawn, and built an unobtrusive ""penthouse"" story. the directive to wring the most office space out of the existing building was responsible for its narrow corridors and cramped staff offices. gugler\'s most notable change was the addition to the east side containing a new cabinet room, secretary\'s office, and oval office. the new office\'s location gave presidents greater privacy, allowing them to slip back and forth between the white house and the west wing without being in full view of the staff.as the size of the president\'s staff grew over the latter half of the 20th century, the west wing generally came to be seen as too small for its modern governmental functions. today, most of the staff members of the executive office of the president are located in the adjacent eisenhower executive office building.                == first floor == the west wing ground floor is also the site of a small restaurant operated by the presidential food service and staffed by naval culinary specialists and called the white house mess. it is located underneath the oval office, and was established by president truman on june 11, 1951.          == second floor ==   == depiction on the west wing tv series ==  in 1999, the west wing television series brought greater public attention to the workings of the presidential staff, as well as to the location of those working in the west wing. the show followed the working lives of a fictional democratic u.s. president, josiah bartlet, and his senior staff. when asked whether the show accurately captured the working environment in 2003, press secretary scott mcclellan commented that the show portrayed more foot traffic and larger rooms than in the real west wing.   == references ==   == external links == white house museum: west wing, with floorplan and historical images west wing interactive, from national journal magazine')"
67,"Aircraft maintenance is the performance of tasks required to ensure the continuing airworthiness of an aircraft or aircraft part, including overhaul, inspection, replacement, defect rectification, and the embodiment of modifications, compliance with airworthiness directives and repair.


== Regulation ==
The maintenance of aircraft is highly regulated, in order to ensure safe and correct functioning during flight. In civil aviation national regulations are coordinated under international standards, established by the International Civil Aviation Organization (ICAO). The ICAO standards have to be implemented by local airworthiness authorities to regulate the maintenance tasks, personnel and inspection system. Maintenance staff must be licensed for the tasks they carry out.


== Aircraft maintenance organization ==


=== Scheduled maintenance checks ===

Aircraft maintenance in civil aviation generally organized using a maintenance checks or blocks which are packages of maintenance tasks that have to be done on an aircraft after a certain amount of time or usage. Packages are constructed by dividing the maintenance tasks into convenient, bite-size chunks to minimize the time the aircraft is out of service, to keep the maintenance workload level, and to maximize the use of maintenance facilities. 


=== Power-by-the-Hour ===
A Power by the Hour program provides budget predictability, avoids installing a loaner during repairs when an aircraft part fails and enrolled aircraft may have a better value and liquidity. This concept of unscheduled maintenance was initially introduced for aircraft engines to mitigate engine failures. The term was coined by Bristol Siddeley in 1962 to support Vipers of the British Aerospace 125 business jets for a fixed sum per flying hour. A complete engine and accessory replacement service was provided, allowing the operator to accurately forecast this cost, and relieving him from purchasing stocks of engines and accessories.In the 1980s, Rolls-Royce plc reinstated the program to provide the operator with a fixed engine maintenance cost over an extended period of time. Operators are assured of an accurate cost projection and avoid the breakdowns costs; the term is trademarked by Rolls-Royce but is the common name in the industry. It is an option for operators of several Rolls-Royce aircraft engines. Other aircraft engine manufacturers such as General Electric and Pratt & Whitney offer similar programs.Jet Support Services provides hourly cost maintenance programs independently of the manufacturers. GEMCO also offers a similar program for piston engines in general aviation aircraft. Bombardier Aerospace offers its Smart Services program, covering parts and maintenance by the hour.


=== Maintenance release ===
At the completion of any maintenance task a person authorized by the national airworthiness authority signs a maintenance release stating that maintenance has been performed in accordance with the applicable airworthiness requirements. In the case of a certified aircraft this may be an Aircraft Maintenance Engineer or Aircraft Maintenance Technician, while for amateur-built aircraft this may be the owner or builder of the aircraft.
A maintenance release can be called a certificate of release to service (CRS).


== Maintenance personnel ==

The ICAO defines the licensed role of aircraft maintenance (technician/engineer/mechanic), noting that ""The terms in brackets are given as acceptable additions to the title of the license. Each Contracting State is expected to use in its own regulations the one it prefers."" Thus, aircraft maintenance technicians, engineers and mechanics all perform essentially the same role. However different countries use these terms in different ways to define their individual levels of qualification and responsibilities.In Americas licenses for aircraft maintenance personnel include:

Aircraft Maintenance Engineer (AME), also called Licensed Aircraft Maintenance Engineer (LAME or L-AME).
Aircraft Maintenance Technician (AMT), or colloquially Airframe and Powerplant (A&P).
Aircraft Maintenance Mechanic (AMM).As there will be 41,030 new airliners by 2036, Boeing expects 648,000 new commercial airline maintenance technicians from 2017 till then: 256,000 in Asia Pacific (39%), 118,000 in North America (19%) and 111,000 in Europe (17%).In Europe aircraft maintenance personnel must comply with Part 66, Certifying Staff, issued by the European Aviation Safety Agency (EASA). This regulation establishes four levels of authorization:

Level 1: General Familiarisation, Unlicensed
Level 2: Ramp and Transit, Category A
can only certify own work performed for tasks which he/she has received documented training
Level 3: Line Certifying Staff and Base Maintenance Supporting Staff, Category B1 (electromechanic) and/or B2(Avionics)
can certify all work performed on an aircraft/engine for which he/she is type rated excluding base maintenance (generally up to and including A-Check)
Level 4: Base Maintenance Certifying Staff, Category C
can certify all work performed on an aircraft/engine for which he/she is type rated, but only if it is base maintenance (additional level-3 staff necessary)
this authorization does not automatically include any level 2 or level 3 license.


== Market ==


=== Aircraft ===
The Maintenance, Repair, Overhaul (MRO) Market was US$135.1 Billion in 2015, three quarters of the $180.3 B aircraft production market. Of this, 60% is for civil aviation : air transport 48%, business and general aviation 9%, rotorcraft 3% ; and military aviation is 40% : fixed wing 27% and rotary 13%. Of the $64.3 Billion air transport MRO market, 40% is for engines, 22% for components, 17% for line, 14% for airframe and 7% for modifications. Its is projected to grow at 4.1% per annum till 2025 to $96B.Airliner MRO should reach $74.3 Billion in 2017 : 51% ($37.9B) single-aisles, 21% ($15.6B) long range twin-aisles, 8% ($5.9B) medium range twin-aisles, 7% ($5.2B) large aircraft, 6% ($4.5B) regional jets as turboprop regional airliners and 1% ($0.7B) short range twin-aisles.
Over the 2017–2026 decade, the worldwide market should reach over $900 billion, led by 23% in North America, 22% in Western Europe, and 19% in Asia Pacific.In 2017, of the $70 billion spent by airlines on maintenance, repair and overhaul (MRO), 31% were for engines, 27% for components, 24% for line maintenance, 10% for modifications and 8% for the airframe; 70% were for mature airliners (Airbus A320 and A330, Boeing 777 and 737NG), 23% were for “sunset” aircraft (MD-80, Boeing 737 Classic, B747 or B757) and 7% was spent on modern models (Boeing 787, Embraer E-Jet, Airbus A350XWB and A380).In 2018, the commercial aviation industry expended $88 billion for MRO, while military aircraft required $79.6 billion, including field maintenance.
Airliner MRO is forecast to reach $115 billion by 2028, a 4% compound annual growth rate from $77.4 billion in 2018.
Major airframe manufacturers Airbus, Boeing and Embraer entered the market, increasing concerns about intellectual property sharing. Shared data-supported predictive maintenance can reduce operational disruptions. Among other factors, prognostics helped Delta Air Lines reduce maintenance cancellations by 98% from 5,600 in 2010 to 78 in 2017.Insourced maintenance can be inefficient for small airlines with a fleet below 50–60 aircraft. They have to either outsource it or sell its MRO services to other carriers for better resource utilization.
For example, the maintenance on South African Comair's 26 Boeing 737s is outsourced to South African Airways' Technical Department.
Another example is Spain's Air Nostrum operates 45 CRJs and ATR72s and its 300-person maintenance department provides line, base maintenance and limited component repair for other airlines 20% of the time.Airframe heavy maintenance is worth $6 billion in 2019: $2.9 billion for C checks and $3.1 billion for D checks, Aviation Week forecasts a growth to $7.5 billion in 2028 – $3.1 billion C and $4.2 billion D – for $70 billion over 10 years, 10% of the overall market compared to 40% for the engines.


=== Engines ===

The commercial aviation engine MRO market is anticipated by Aviation Week to be $25.9 billion in 2018, a 2.5 billion increase from 2017, led by 21% for the Boeing 737NG' CFM56-7B and the A320's CFM56-5B and IAE V2500 (also on the MD-90) tied for second, followed by the mature widebody engines: the GE90 then the Trent 700.Over the 2017–2026 decade, the largest markets for turbofans will be the B737NG's CFM56-7 with 23%, the V2500-A5 with 21%, the GE90-115B with 13%, the A320's CFM56-5B with 13%, the PW1000G with 7%, the Trent 700 with 6%, the CF6-80C2 with 5%, the CFM LEAP with 5% and the CF34-8 with 4%.
Between 2018 and 2022, the largest MRO demand will be for CFM engines with 36%, followed by GE with 24%, Rolls with 13%, IAE with 12% and Pratt with 7%.As an aircraft gets older, a greater percentage of its value is represented by its engines.
Over the course of the engine life it is possible to put value back in by repair and overhaul, to sell it for its remaining useful time, or to disassemble it and sell the used parts, to extract its remaining value.
Its maintenance value includes the value of life-limited parts (LLPs) and the time before overhaul.
The core value is the value of its data plate and non-life-limited-parts.
Engine makers deeply discount their sales, up to 90%, to win the multi-year stream of spares and services, resembling the razor and blades model.Engines installed on a new aircraft are discounted by at least 40% while spare engine values closely follow list prices.
Accounting for 80% of a shop visit cost, LLP prices escalate to recoup the original discount, until engine availability increase with aircraft teardowns.
Between 2001 and 2018 for the Airbus A320 or the Boeing 737-800, their CFM56 value increased from 27–29% to 48–52% of the aircraft value.
The 777-200ER's PW4000 and the A330-300's Trent 700 engines rose from a share of 18–25% in 2001 to 29–40% in 2013.
For the A320neo and 737 MAX, between 52% and 57% of their value lies in their engines: this could rise to 80–90% after ten years, while new A350 or B787 engines are worth 36–40% of the aircraft.
After some time the maintenance reserves exceed the aircraft lease.In 2018, a full set of LLP for a B737-800's CFM56-7B list price is $3.6 million, like for the A320ceo's CFM56-5B for 20–30,000 cycles up from $2.0 million in 2009, while an IAE V2500 is priced at $3.9 million for 20,000 cycles but have a lower overhaul cost.
The LLP parts for and A320neo's PW1127G costs $4 million and its competitor $4.3 million for 20–30,000 cycles.
For an A330ceo, a GE CF6-80 LLP set is priced at $11 million for 15–20,000 cycles and $9 million for a PW4000, and $6 million for a Trent 700 but with a $9–10 million overhaul against $4–5 million for the others.
The LLP set for a B767-300ER's CF6 or PW4000 costs $7 million, and for a B787-8's Trent 1000 $7 million compared to $8.5 million for a GEnx.
An B777-300ER's GE90 LLP set is priced at $9 million while the A380's Trent 900 costs $7 million, both for 15,000 cycles.
Between 2019 and 2038, 5,200 spare airliner engines will be required with at least half leased.An engine overhaul for a B737-800 costs $3.1 million every 20,000 hours, or $3.4 million every 15,000 hours for earlier variants, while for a B757 powerplant it costs $4.5 million every 24,000 hours.
For an A330 turbofan, it costs $7 million every 24,000 hours, $8 million for an A350 or B787 engine, $9 million every 20,000 hours for a B777-200ER powerplant and $10 million every 25,000 hours for a B777-300ER engine.
It costs $4 million every 18,000 hours for each B747-400 turbofan and $7.5 million every 25,000 hours for an A380 engine.


== Future of aircraft maintenance ==


=== Aircraft health monitoring ===
Airbus has indicated that data diagnostics could put an end to aircraft unscheduled grounding for fault repairs around 2025, supported by big data and operational experience. Predictive maintenance, diagnostics and health monitoring could eliminate unscheduled groundings, by making maintenance schedule intervals more frequent to avoid AOGs and the associated operational interruptions, ultimately eliminating them. Data or monitoring can tell that some parts do not need a scheduled check, but a full transition to this model will need much greater experience. With more history, examples and regulatory confidence, the maintenance program and associated manuals could become a dynamic documents for each specific aircraft with maintenance schedule based on operational history of the aircraft.


=== Electric aircraft ===
In October 2018, consultant Roland Berger counted 134 electric propulsion projects: 70% electric engines with batteries recharged on ground and 30% hybrid-electric with a fuel generator, in parallel or in series; 45% are urban air taxis, 43% general aviation and 12% airliners.
All-electric is sometimes selected for sub-19 seats commuters, and more often for smaller 2-4-seat aircraft like urban air taxis or trainers.
Electric motors will probably require less maintenance than a fuel engine, while batteries and cables may need to be exchanged more often than fuel systems.The all-electric Pipistrel Alpha Electro two-seat trainer is already certified as a LSA in Europe, Australia and possibly the US. Redmond, Washington-based MagniX is integrating a 350 hp (260 kW) electric motor on its iron bird testbed before a first flight of a Cessna Caravan in 2019, with a 750 hp (560 kW) Magni500 replacing its PT6 single turboprop. MagniX expects to certify the Magni500 and the 375 hp (280 kW) Magni250 by 2020, and the Caravan conversion by 2022 with a range of 100–200 mi (160–320 km) as it is typically operated over less than 100 mi (160 km). A Britten-Norman Islander retrofitted with electric propulsion should be demonstrated by 2021 by Cranfield Aerospace before commercial service in 2023. Roland Berger expects a 50-seat hybrid-electric airliner in 2032 with a 340 km (210 mi) range.


=== Maintenance automation ===

Automated aircraft inspection systems have the potential to make aircraft maintenance safer and more reliable. Various solutions are currently developed: a collaborative mobile robot named Air-Cobot, and Unmanned aerial vehicles from Donecle or Easyjet.


== See also ==
Airworthiness
Groundcrew
Line-replaceable unit
Maintenance Resource Management
Maintenance (technical)
Professional Aviation Maintenance Association
RAMS
Shop-replaceable unit


== References ==


== External links ==
""Association for Women in Aviation Maintenance"".
Lindsay Bjerregaard, Lee Ann Shay (Oct 11, 2017). ""A Day In American's Line Operations At Chicago O'Hare"". Aviation Week network.
James Pozzi (Oct 17, 2017). ""Inside Iberia's Engine Shop"". Aviation Week network.
James Pozzi (Nov 4, 2017). ""A Look Around Lufthansa Technik Sofia's Expanded Facility"". Aviation Week network.
Lindsay Bjerregaard (Nov 13, 2017). ""On The Ground At JetBlue's JFK Hangar"". Aviation Week network.
GE Aviation. Maintenance Minute. Maintenance Minute videos are produced by GE Aviation's training team to help the aircraft maintainer with everyday engine maintenance tasks.","pandas(index=67, _1=67, text='aircraft maintenance is the performance of tasks required to ensure the continuing airworthiness of an aircraft or aircraft part, including overhaul, inspection, replacement, defect rectification, and the embodiment of modifications, compliance with airworthiness directives and repair.   == regulation == the maintenance of aircraft is highly regulated, in order to ensure safe and correct functioning during flight. in civil aviation national regulations are coordinated under international standards, established by the international civil aviation organization (icao). the icao standards have to be implemented by local airworthiness authorities to regulate the maintenance tasks, personnel and inspection system. maintenance staff must be licensed for the tasks they carry out.   == aircraft maintenance organization == automated aircraft inspection systems have the potential to make aircraft maintenance safer and more reliable. various solutions are currently developed: a collaborative mobile robot named air-cobot, and unmanned aerial vehicles from donecle or easyjet.   == see also == airworthiness groundcrew line-replaceable unit maintenance resource management maintenance (technical) professional aviation maintenance association rams shop-replaceable unit   == references ==   == external links == ""association for women in aviation maintenance"". lindsay bjerregaard, lee ann shay (oct 11, 2017). ""a day in american\'s line operations at chicago o\'hare"". aviation week network. james pozzi (oct 17, 2017). ""inside iberia\'s engine shop"". aviation week network. james pozzi (nov 4, 2017). ""a look around lufthansa technik sofia\'s expanded facility"". aviation week network. lindsay bjerregaard (nov 13, 2017). ""on the ground at jetblue\'s jfk hangar"". aviation week network. ge aviation. maintenance minute. maintenance minute videos are produced by ge aviation\'s training team to help the aircraft maintainer with everyday engine maintenance tasks.')"
68,"Cable lacing is a method for tying wiring harnesses and cable looms, traditionally used in telecommunication, naval, and aerospace applications.  This old cable management technique, taught to generations of lineworkers, is still used in some modern applications since it does not create obstructions along the length of the cable, avoiding the handling problems of cables groomed by plastic or hook-and-loop cable ties.
Cable lacing uses a thin cord, which is traditionally made of waxed linen, to bind together a group of cables using a series of running lockstitches.  Flat lacing tapes made of modern materials such as nylon, polyester, Teflon, fiberglass, and Nomex are also available with a variety of coatings to improve knot holding.


== Styles ==
The lacing begins and ends with a whipping or other knot to secure the free ends.  Wraps are spaced relative to the overall harness diameter to maintain the wiring in a tight, neat bundle, and the ends are then neatly trimmed.  In addition to continuous or running lacing, there are a variety of lacing patterns used in different circumstances.  In some cases stand-alone knots called spot ties are also used.  For lashing large cables and cable bundles to support structures in telecommunications applications, there are two named cable lacing styles: the ""Chicago stitch"" and ""Kansas City stitch"".Some organizations have in-house standards to which cable lacing must conform, for example NASA specifies its cable lacing techniques in chapter 9 of NASA-STD-8739.4.


== Examples ==

		
		
		


== Notes and references ==


== External links ==
NASA Technical Standard NASA-STD-8739.4 on Crimping, Interconnecting Cables, Harnesses, and Wiring
""Workmanship Standards Pictorial Reference for NASA-STD-8739"". NASA. Archived from the original on 2009-07-12.
Online excerpt from Electronic Installation Practices Manual (1951) , ""Chapter 9, Cabling""
Online excerpt from Workmanship and Design Practices for Electronic Equipment (1962)
Cable lacing tutorial using modern lacing tape
History, tools, and techniques
FAA Advisory Circular 43.13B paragraph 11-158","pandas(index=68, _1=68, text='cable lacing is a method for tying wiring harnesses and cable looms, traditionally used in telecommunication, naval, and aerospace applications.  this old cable management technique, taught to generations of lineworkers, is still used in some modern applications since it does not create obstructions along the length of the cable, avoiding the handling problems of cables groomed by plastic or hook-and-loop cable ties. cable lacing uses a thin cord, which is traditionally made of waxed linen, to bind together a group of cables using a series of running lockstitches.  flat lacing tapes made of modern materials such as nylon, polyester, teflon, fiberglass, and nomex are also available with a variety of coatings to improve knot holding.   == styles == the lacing begins and ends with a whipping or other knot to secure the free ends.  wraps are spaced relative to the overall harness diameter to maintain the wiring in a tight, neat bundle, and the ends are then neatly trimmed.  in addition to continuous or running lacing, there are a variety of lacing patterns used in different circumstances.  in some cases stand-alone knots called spot ties are also used.  for lashing large cables and cable bundles to support structures in telecommunications applications, there are two named cable lacing styles: the ""chicago stitch"" and ""kansas city stitch"".some organizations have in-house standards to which cable lacing must conform, for example nasa specifies its cable lacing techniques in chapter 9 of nasa-std-8739.4.   == examples ==             == notes and references ==   == external links == nasa technical standard nasa-std-8739.4 on crimping, interconnecting cables, harnesses, and wiring ""workmanship standards pictorial reference for nasa-std-8739"". nasa. archived from the original on 2009-07-12. online excerpt from electronic installation practices manual (1951) , ""chapter 9, cabling"" online excerpt from workmanship and design practices for electronic equipment (1962) cable lacing tutorial using modern lacing tape history, tools, and techniques faa advisory circular 43.13b paragraph 11-158')"
69,"A reaction engine is an engine or motor that produces thrust by expelling reaction mass, in accordance with Newton's third law of motion. This law of motion is most commonly paraphrased as: ""For every action force there is an equal, but opposite, reaction force.""
Examples include jet engines, rocket engines, pump-jet, and more uncommon variations such as Hall effect thrusters, ion drives, mass drivers, and nuclear pulse propulsion.


== Energy use ==


=== Propulsive efficiency ===

For all reaction engines that carry on-board propellant (such as rocket engines and electric propulsion drives) some energy must go into accelerating the reaction mass. Every engine wastes some energy, but even assuming 100% efficiency, the engine needs energy amounting to

  
    
      
        
          
            
              
                
                  
                    1
                    2
                  
                
              
            
          
        
        M
        
          V
          
            e
          
          
            2
          
        
      
    
    {\displaystyle {\begin{matrix}{\frac {1}{2}}\end{matrix}}MV_{e}^{2}}
  (where M is the mass of propellent expended and 
  
    
      
        
          V
          
            e
          
        
      
    
    {\displaystyle V_{e}}
   is the exhaust velocity), which is simply the energy to accelerate the exhaust.

Comparing the rocket equation (which shows how much energy ends up in the final vehicle) and the above equation (which shows the total energy required) shows that even with 100% engine efficiency, certainly not all energy supplied ends up in the vehicle – some of it, indeed usually most of it, ends up as kinetic energy of the exhaust.
If the specific impulse (
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  ) is fixed, for a mission delta-v, there is a particular 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
   that minimises the overall energy used by the rocket. This comes to an exhaust velocity of about ⅔ of the mission delta-v (see the energy computed from the rocket equation). Drives with a specific impulse that is both high and fixed such as Ion thrusters have exhaust velocities that can be enormously higher than this ideal, and thus end up powersource limited and give very low thrust. Where the vehicle performance is power limited, e.g. if solar power or nuclear power is used, then in the case of a large 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   the maximum acceleration is inversely proportional to it. Hence the time to reach a required delta-v is proportional to 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
  . Thus the latter should not be too large.
On the other hand, if the exhaust velocity can be made to vary so that at each instant it is equal and opposite to the vehicle velocity then the absolute minimum energy usage is achieved. When this is achieved, the exhaust stops in space ^  and has no kinetic energy; and the propulsive efficiency is 100% all the energy ends up in the vehicle (in principle such a drive would be 100% efficient, in practice there would be thermal losses from within the drive system and residual heat in the exhaust). However, in most cases this uses an impractical quantity of propellant, but is a useful theoretical consideration.
Some drives (such as VASIMR or electrodeless plasma thruster) actually can significantly vary their exhaust velocity. This can help reduce propellant usage and improve acceleration at different stages of the flight. However the best energetic performance and acceleration is still obtained when the exhaust velocity is close to the vehicle speed. Proposed ion and plasma drives usually have exhaust velocities enormously higher than that ideal (in the case of VASIMR the lowest quoted speed is around 15 km/s compared to a mission delta-v from high Earth orbit to Mars of about 4 km/s).
For a mission, for example, when launching from or landing on a planet, the effects of gravitational attraction and any atmospheric drag must be overcome by using fuel. It is typical to combine the effects of these and other effects into an effective mission delta-v. For example, a launch mission to low Earth orbit requires about 9.3–10 km/s delta-v. These mission delta-vs are typically numerically integrated on a computer.


=== Cycle efficiency ===
All reaction engines lose some energy, mostly as heat.
Different reaction engines have different efficiencies and losses. For example, rocket engines can be up to 60–70% energy efficient in terms of accelerating the propellant. The rest is lost as heat and thermal radiation, primarily in the exhaust.


=== Oberth effect ===

Reaction engines are more energy efficient when they emit their reaction mass when the vehicle is travelling at high speed.
This is because the useful mechanical energy generated is simply force times distance, and when a thrust force is generated while the vehicle moves, then:

  
    
      
        E
        =
        F
        ×
        d
        
      
    
    {\displaystyle E=F\times d\;}
  where F is the force and d is the distance moved.
Dividing by length of time of motion we get:

  
    
      
        
          
            E
            t
          
        
        =
        P
        =
        
          
            
              F
              ×
              d
            
            t
          
        
        =
        F
        ×
        v
      
    
    {\displaystyle {\frac {E}{t}}=P={\frac {F\times d}{t}}=F\times v}
  Hence:

  
    
      
        P
        =
        F
        ×
        v
        
      
    
    {\displaystyle P=F\times v\;}
  where P is the useful power and v is the speed.
Hence, v should be as high as possible, and a stationary engine does no useful work.


=== Delta-v and propellant ===

Exhausting the entire usable propellant of a spacecraft through the engines in a straight line in free space would produce a net velocity change to the vehicle; this number is termed delta-v (
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  ).
If the exhaust velocity is constant then the total 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of a vehicle can be calculated using the rocket equation, where M is the mass of propellant, P is the mass of the payload (including the rocket structure), and 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   is the velocity of the rocket exhaust. This is known as the Tsiolkovsky rocket equation:

  
    
      
        Δ
        v
        =
        
          v
          
            e
          
        
        ln
        ⁡
        
          (
          
            
              
                M
                +
                P
              
              P
            
          
          )
        
        .
      
    
    {\displaystyle \Delta v=v_{e}\ln \left({\frac {M+P}{P}}\right).}
  For historical reasons, as discussed above, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   is sometimes written as

  
    
      
        
          v
          
            e
          
        
        =
        
          I
          
            sp
          
        
        
          g
          
            0
          
        
      
    
    {\displaystyle v_{e}=I_{\text{sp}}g_{0}}
  where 
  
    
      
        
          I
          
            sp
          
        
      
    
    {\displaystyle I_{\text{sp}}}
   is the specific impulse of the rocket, measured in seconds, and 
  
    
      
        
          g
          
            0
          
        
      
    
    {\displaystyle g_{0}}
   is the gravitational acceleration at sea level.
For a high delta-v mission, the majority of the spacecraft's mass needs to be reaction mass. Because a rocket must carry all of its reaction mass, most of the initially-expended reaction mass goes towards accelerating reaction mass rather than payload. If the rocket has a payload of mass P, the spacecraft needs to change its velocity by 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  , and the rocket engine has exhaust velocity ve, then the reaction mass M which is needed can be calculated using the rocket equation and the formula for 
  
    
      
        
          I
          
            sp
          
        
      
    
    {\displaystyle I_{\text{sp}}}
  :

  
    
      
        M
        =
        P
        
          (
          
            
              e
              
                
                  
                    Δ
                    v
                  
                  
                    v
                    
                      e
                    
                  
                
              
            
            −
            1
          
          )
        
        .
      
    
    {\displaystyle M=P\left(e^{\frac {\Delta v}{v_{e}}}-1\right).}
  For 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   much smaller than ve, this equation is roughly linear, and little reaction mass is needed. If 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   is comparable to ve, then there needs to be about twice as much fuel as combined payload and structure (which includes engines, fuel tanks, and so on).  Beyond this, the growth is exponential; speeds much higher than the exhaust velocity require very high ratios of fuel mass to payload and structural mass.
For a mission, for example, when launching from or landing on a planet, the effects of gravitational attraction and any atmospheric drag must be overcome by using fuel. It is typical to combine the effects of these and other effects into an effective mission delta-v. For example, a launch mission to low Earth orbit requires about 9.3–10 km/s delta-v. These mission delta-vs are typically numerically integrated on a computer.
Some effects such as Oberth effect can only be significantly utilised by high thrust engines such as rockets; i.e., engines that can produce a high g-force (thrust per unit mass, equal to delta-v per unit time).


=== Energy ===

In the ideal case 
  
    
      
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{1}}
   is useful payload and 
  
    
      
        
          m
          
            0
          
        
        −
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{0}-m_{1}}
   is reaction mass (this corresponds to empty tanks having no mass, etc.). The energy required can simply be computed as

  
    
      
        
          
            1
            2
          
        
        (
        
          m
          
            0
          
        
        −
        
          m
          
            1
          
        
        )
        
          v
          
            e
          
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}(m_{0}-m_{1})v_{\text{e}}^{2}}
  This corresponds to the kinetic energy the expelled reaction mass would have at a speed equal to the exhaust speed. If the reaction mass had to be accelerated from zero speed to the exhaust speed, all energy produced would go into the reaction mass and nothing would be left for kinetic energy gain by the rocket and payload. However, if the rocket already moves and accelerates (the reaction mass is expelled in the direction opposite to the direction in which the rocket moves) less kinetic energy is added to the reaction mass. To see this, if, for example, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
  =10 km/s and the speed of the rocket is 3 km/s, then the speed of a small amount of expended reaction mass changes from 3 km/s forwards to 7 km/s rearwards. Thus, although the energy required is 50 MJ per kg reaction mass, only 20 MJ is used for the increase in speed of the reaction mass. The remaining 30 MJ is the increase of the kinetic energy of the rocket and payload.
In general:

  
    
      
        d
        
          (
          
            
              
                1
                2
              
            
            
              v
              
                2
              
            
          
          )
        
        =
        v
        d
        v
        =
        v
        
          v
          
            e
          
        
        
          
            
              d
              m
            
            m
          
        
        =
        
          
            1
            2
          
        
        
          [
          
            
              v
              
                e
              
              
                2
              
            
            −
            
              
                (
                
                  v
                  −
                  
                    v
                    
                      e
                    
                  
                
                )
              
              
                2
              
            
            +
            
              v
              
                2
              
            
          
          ]
        
        
          
            
              d
              m
            
            m
          
        
      
    
    {\displaystyle d\left({\frac {1}{2}}v^{2}\right)=vdv=vv_{\text{e}}{\frac {dm}{m}}={\frac {1}{2}}\left[v_{\text{e}}^{2}-\left(v-v_{\text{e}}\right)^{2}+v^{2}\right]{\frac {dm}{m}}}
  Thus the specific energy gain of the rocket in any small time interval is the energy gain of the rocket including the remaining fuel, divided by its mass, where the energy gain is equal to the energy produced by the fuel minus the energy gain of the reaction mass. The larger the speed of the rocket, the smaller the energy gain of the reaction mass; if the rocket speed is more than half of the exhaust speed the reaction mass even loses energy on being expelled, to the benefit of the energy gain of the rocket; the larger the speed of the rocket, the larger the energy loss of the reaction mass.
We have

  
    
      
        Δ
        ϵ
        =
        ∫
        v
        
        d
        (
        Δ
        v
        )
      
    
    {\displaystyle \Delta \epsilon =\int v\,d(\Delta v)}
  where 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   is the specific energy of the rocket (potential plus kinetic energy) and 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   is a separate variable, not just the change in 
  
    
      
        v
      
    
    {\displaystyle v}
  . In the case of using the rocket for deceleration; i.e., expelling reaction mass in the direction of the velocity, 
  
    
      
        v
      
    
    {\displaystyle v}
   should be taken negative.
The formula is for the ideal case again, with no energy lost on heat, etc. The latter causes a reduction of thrust, so it is a disadvantage even when the objective is to lose energy (deceleration).
If the energy is produced by the mass itself, as in a chemical rocket, the fuel value has to be 
  
    
      
        
          
            
              v
              
                e
              
              
                2
              
            
            
              /
            
            2
          
        
      
    
    {\displaystyle \scriptstyle {v_{\text{e}}^{2}/2}}
  , where for the fuel value also the mass of the oxidizer has to be taken into account. A typical value is 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 4.5 km/s, corresponding to a fuel value of 10.1 MJ/kg. The actual fuel value is higher, but much of the energy is lost as waste heat in the exhaust that the nozzle was unable to extract.
The required energy 
  
    
      
        E
      
    
    {\displaystyle E}
   is

  
    
      
        E
        =
        
          
            1
            2
          
        
        
          m
          
            1
          
        
        
          (
          
            
              e
              
                
                  
                    Δ
                    v
                  
                  
                    v
                    
                      e
                    
                  
                
              
            
            −
            1
          
          )
        
        
          v
          
            e
          
          
            2
          
        
      
    
    {\displaystyle E={\frac {1}{2}}m_{1}\left(e^{\frac {\Delta v}{v_{\text{e}}}}-1\right)v_{\text{e}}^{2}}
  Conclusions:

for 
  
    
      
        Δ
        v
        ≪
        
          v
          
            e
          
        
      
    
    {\displaystyle \Delta v\ll v_{e}}
   we have 
  
    
      
        E
        ≈
        
          
            1
            2
          
        
        
          m
          
            1
          
        
        
          v
          
            e
          
        
        Δ
        v
      
    
    {\displaystyle E\approx {\frac {1}{2}}m_{1}v_{\text{e}}\Delta v}
  
for a given 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  , the minimum energy is needed if 
  
    
      
        
          v
          
            e
          
        
        =
        0.6275
        Δ
        v
      
    
    {\displaystyle v_{\text{e}}=0.6275\Delta v}
  , requiring an energy of
  
    
      
        E
        =
        0.772
        
          m
          
            1
          
        
        (
        Δ
        v
        
          )
          
            2
          
        
      
    
    {\displaystyle E=0.772m_{1}(\Delta v)^{2}}
  .
In the case of acceleration in a fixed direction, and starting from zero speed, and in the absence of other forces, this is 54.4% more than just the final kinetic energy of the payload. In this optimal case the initial mass is 4.92 times the final mass.These results apply for a fixed exhaust speed.
Due to the Oberth effect and starting from a nonzero speed, the required potential energy needed from the propellant may be less than the increase in energy in the vehicle and payload. This can be the case when the reaction mass has a lower speed after being expelled than before – rockets are able to liberate some or all of the initial kinetic energy of the propellant.
Also, for a given objective such as moving from one orbit to another, the required 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   may depend greatly on the rate at which the engine can produce 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   and maneuvers may even be impossible if that rate is too low. For example, a launch to Low Earth orbit (LEO) normally requires a 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of ca. 9.5 km/s (mostly for the speed to be acquired), but if the engine could produce 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   at a rate of only slightly more than g, it would be a slow launch requiring altogether a very large 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   (think of hovering without making any progress in speed or altitude, it would cost a 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of 9.8 m/s each second). If the possible rate is only 
  
    
      
        g
      
    
    {\displaystyle g}
   or less, the maneuver can not be carried out at all with this engine.
The power is given by

  
    
      
        P
        =
        
          
            1
            2
          
        
        m
        a
        
          v
          
            e
          
        
        =
        
          
            1
            2
          
        
        F
        
          v
          
            e
          
        
      
    
    {\displaystyle P={\frac {1}{2}}mav_{\text{e}}={\frac {1}{2}}Fv_{\text{e}}}
  where 
  
    
      
        F
      
    
    {\displaystyle F}
   is the thrust and 
  
    
      
        a
      
    
    {\displaystyle a}
   the acceleration due to it. Thus the theoretically possible thrust per unit power is 2 divided by the specific impulse in m/s. The thrust efficiency is the actual thrust as percentage of this.
If, e.g., solar power is used, this restricts 
  
    
      
        a
      
    
    {\displaystyle a}
  ; in the case of a large 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   the possible acceleration is inversely proportional to it, hence the time to reach a required delta-v is proportional to 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
  ; with 100% efficiency:

for 
  
    
      
        Δ
        v
        ≪
        
          v
          
            e
          
        
      
    
    {\displaystyle \Delta v\ll v_{\text{e}}}
   we have 
  
    
      
        t
        ≈
        
          
            
              m
              
                v
                
                  e
                
              
              Δ
              v
            
            
              2
              P
            
          
        
      
    
    {\displaystyle t\approx {\frac {mv_{\text{e}}\Delta v}{2P}}}
  Examples:

power, 1000 W; mass, 100 kg; 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   = 5 km/s, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 16 km/s, takes 1.5 months.
power, 1000 W; mass, 100 kg; 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   = 5 km/s, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 50 km/s, takes 5 months.Thus 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   should not be too large.


=== Power to thrust ratio ===
The power to thrust ratio is simply:

  
    
      
        
          
            P
            F
          
        
        =
        
          
            
              
                
                  1
                  2
                
              
              
                
                  
                    
                      m
                      ˙
                    
                  
                
                
                  v
                  
                    2
                  
                
              
            
            
              
                
                  
                    m
                    ˙
                  
                
              
              v
            
          
        
        =
        
          
            1
            2
          
        
        v
      
    
    {\displaystyle {\frac {P}{F}}={\frac {{\frac {1}{2}}{{\dot {m}}v^{2}}}{{\dot {m}}v}}={\frac {1}{2}}v}
  Thus for any vehicle power P, the thrust that may be provided is:

  
    
      
        F
        =
        
          
            P
            
              
                
                  1
                  2
                
              
              v
            
          
        
        =
        
          
            
              2
              P
            
            v
          
        
      
    
    {\displaystyle F={\frac {P}{{\frac {1}{2}}v}}={\frac {2P}{v}}}
  


=== Example ===
Suppose a 10,000 kg space probe will be sent to Mars. The required 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   from LEO is approximately 3000 m/s, using a Hohmann transfer orbit. For the sake of argument, assume the following thrusters are options to be used:

Observe that the more fuel-efficient engines can use far less fuel; their mass is almost negligible (relative to the mass of the payload and the engine itself) for some of the engines. However, these require a large total amount of energy. For Earth launch, engines require a thrust to weight ratio of more than one. To do this with the ion or more theoretical electrical drives, the engine would have to be supplied with one to several gigawatts of power, equivalent to a major metropolitan generating station. From the table it can be seen that this is clearly impractical with current power sources.
Alternative approaches include some forms of laser propulsion, where the reaction mass does not provide the energy required to accelerate it, with the energy instead being provided from an external laser or other beam-powered propulsion system. Small models of some of these concepts have flown, although the engineering problems are complex and the ground-based power systems are not a solved problem.
Instead, a much smaller, less powerful generator may be included which will take much longer to generate the total energy needed. This lower power is only sufficient to accelerate a tiny amount of fuel per second, and would be insufficient for launching from Earth. However, over long periods in orbit where there is no friction, the velocity will be finally achieved. For example, it took the SMART-1 more than a year to reach the Moon, whereas with a chemical rocket it takes a few days. Because the ion drive needs much less fuel, the total launched mass is usually lower, which typically results in a lower overall cost, but the journey takes longer.
Mission planning therefore frequently involves adjusting and choosing the propulsion system so as to minimise the total cost of the project, and can involve trading off launch costs and mission duration against payload fraction.


== Types of reaction engines ==
Rocket-like
Rocket engine
Ion thruster
Airbreathing
Turbojet
Turbofan
Pulsejet
Ramjet
Scramjet
Liquid
Pump-jet
Rotary
Aeolipile
Solid exhaust
Mass driver


== See also ==
Internal combustion engine
Jet force
Jet propulsion
List of plasma physics articles
Thruster (disambiguation)


== Notes ==


== References ==


== External links ==
Popular Science May 1945","pandas(index=69, _1=69, text='a reaction engine is an engine or motor that produces thrust by expelling reaction mass, in accordance with newton\'s third law of motion. this law of motion is most commonly paraphrased as: ""for every action force there is an equal, but opposite, reaction force."" examples include jet engines, rocket engines, pump-jet, and more uncommon variations such as hall effect thrusters, ion drives, mass drivers, and nuclear pulse propulsion.   == energy use == for all reaction engines that carry on-board propellant (such as rocket engines and electric propulsion drives) some energy must go into accelerating the reaction mass. every engine wastes some energy, but even assuming 100% efficiency, the engine needs energy amounting to           1 2       m  v  e   2      from leo is approximately 3000 m/s, using a hohmann transfer orbit. for the sake of argument, assume the following thrusters are options to be used:  observe that the more fuel-efficient engines can use far less fuel; their mass is almost negligible (relative to the mass of the payload and the engine itself) for some of the engines. however, these require a large total amount of energy. for earth launch, engines require a thrust to weight ratio of more than one. to do this with the ion or more theoretical electrical drives, the engine would have to be supplied with one to several gigawatts of power, equivalent to a major metropolitan generating station. from the table it can be seen that this is clearly impractical with current power sources. alternative approaches include some forms of laser propulsion, where the reaction mass does not provide the energy required to accelerate it, with the energy instead being provided from an external laser or other beam-powered propulsion system. small models of some of these concepts have flown, although the engineering problems are complex and the ground-based power systems are not a solved problem. instead, a much smaller, less powerful generator may be included which will take much longer to generate the total energy needed. this lower power is only sufficient to accelerate a tiny amount of fuel per second, and would be insufficient for launching from earth. however, over long periods in orbit where there is no friction, the velocity will be finally achieved. for example, it took the smart-1 more than a year to reach the moon, whereas with a chemical rocket it takes a few days. because the ion drive needs much less fuel, the total launched mass is usually lower, which typically results in a lower overall cost, but the journey takes longer. mission planning therefore frequently involves adjusting and choosing the propulsion system so as to minimise the total cost of the project, and can involve trading off launch costs and mission duration against payload fraction.   == types of reaction engines == rocket-like rocket engine ion thruster airbreathing turbojet turbofan pulsejet ramjet scramjet liquid pump-jet rotary aeolipile solid exhaust mass driver   == see also == internal combustion engine jet force jet propulsion list of plasma physics articles thruster (disambiguation)   == notes ==   == references ==   == external links == popular science may 1945')"
70,"Redux  is the generic name of a family of phenol–formaldehyde/polyvinyl–formal adhesives developed by Aero Research Limited (ARL) at Duxford, UK, in the 1940s, subsequently produced by Ciba (ARL). The brand name is now also used for a range of epoxy and bismaleimide adhesives manufactured by Hexcel. The name is a contraction of REsearch at DUXford.


== History ==
Devised at ARL by Dr. Norman de Bruyne and George Newell in 1941 for use in the aircraft industry, the adhesive is used for the bonding of metal-to-metal and metal-to-wood structures. The adhesive system comprises a liquid phenolic resin and a PVF (PolyVinylFormal) thermoplastic powder.
The first formulation available was Redux Liquid E/Formvar, comprising a phenolic liquid (Redux Liquid E) and a PVF powder (Formvar), and after its initial non-aviation related application of bonding clutch plates on Churchill and Cromwell tanks, it was used by de Havilland from 1943 to the early 1960s, on, among other aircraft, the Hornet, the Comet and the derived Nimrod, and the Dove, Heron and Trident. It was also used by Vickers on the Viking and by Chance Vought on the F7U Cutlass.
Typically, Redux would be used to affix stiffening stringers and doublers to wing and fuselage panels, the resulting panel being both stronger and lighter than a riveted structure. In the case of the Hornet it was used to join the aluminium lower-wing skin to the wooden upper wing structure, and in the fabrication of the aluminium/wood main wing spar, both forms of composite construction made possible by the advent of Redux.
After initially supplying de Havilland only, ARL subsequently produced a refined form of Redux Liquid E/Formvar using a new liquid component known as Redux Liquid K6, and a finer-grade (smaller particle-size) PVF powder, and this was later made generally available to the wider aircraft industry as Redux Liquid 775/Powder 775, so-named because it was sold for aircraft use to specification DTD 775*. Available for general non-aerospace use it was called Redux Liquid K6/Powder C.
Redux Liquid 775/Powder 775 was joined in 1954 by the subsequent Redux Film 775 system, used from 1962 by de Havilland (later Hawker Siddeley and subsequently British Aerospace) on the DH.125 and DH.146. Other users included Bristol (on the Britannia), SAAB (on the Lansen & Draken), Fokker (on the F.27), Sud Aviation (on the Alouette II/III), Breguet and Fairchild, the film-form having the advantage of greater gap-filling ability with no loss of strength over Redux Liquid 775/Powder 775, allowing for wider tolerances in component-fit, as well as easier handling and use and controlled ratios of the liquid/powder components.   
Other Redux adhesives available included ""Redux 64"", a solution of the phenolic liquid and PVF powder, used worldwide for bonding linings to brake shoes, pads and clutches.
The Redux range was subsequently expanded to include the current range of adhesives, both in single and two part paste systems and film forms, for both aerospace and industrial uses.
* DTD = Directorate of Technical Development


== Usage ==
To use Redux in its liquid/powder form, a thin film of the phenolic liquid is applied to both mating surfaces and then dusted with or dipped in the PVF powder to give an approximate ratio by weight of 1 part liquid to 2 parts powder. The coated joints are then allowed to stand for not less than 30 minutes and not more than 72 hours before the components are brought together under elevated pressure and temperature. The curing process is by condensation and a typical figure for Redux Liquid 775/Powder 775 is 30 minutes at 145 °C (293 °F) under a pressure of 100 lbf/in2 (690 kPa). This is not critical and variations in curing-time and/or temperature may be used to increase shear and creep strength at temperatures above 60 °C (140 °F). Extending the curing cycle gives benefits in fatigue strength at some cost in the room-temperature peel strength, the practical limit for aluminium alloys being approx 170 °C (338 °F) for one hour, due to the possibility of affecting the alloy's mechanical properties.


== Performance (typical) Redux 775 ==
Lap shear strength at ambient temperature = 34.0 MPa (4,930 lbf/in2)
Young's Modulus (E) = 3.35 GPa (486,000 lbf/in2)
Shear modulus =  1.20 GPa (174,000 lbf/in2)Strength of bonds to materials other than aluminium:
Tensile shear of  0.5 inch (12.7 mm) lap joints at room temperature:

Bright mild steel of thickness 0.0625 in (1.6 mm) - mean failing stress = 4,980 lbf/in2 (33.7 MPa, 3.50 kgf/mm2)
Stainless steel of thickness 0.048 in (1.2 mm) - mean failing stress = 5,600 lbf/in2 (38.6 MPa, 3.94 kgf/mm2)
Magnesium alloy1 of thickness 0.063 in (1.6 mm) - mean failing stress = 3,210 lbf/in2 (22.1 MPa, 2.26 kgf/mm2)
Commercially-pure titanium2 of thickness 0.050 in (1.3 mm) - mean failing stress = 4,070 lbf/in2 (28.1 MPa, 2.86 kgf/mm2)1 = HK31A-H24
2 = ICI Titanium 130


== See also ==
Araldite
Aerolite
Tego film


== References ==

Project 3 – Environmental Durability of Adhesive Bonds – Report No. 9 – Forensic Studies of Adhesive Joints – Part 2 Bonded Aircraft Structure by A. Beevers. September 1995. [1]
Usage on the de Havilland Comet
The de Bruyne Medal
Bonding with Redux.(reprinted from The Aeroplane – Sept 1946)
Hexcel Redux film adhesive – 50th anniversary Press Release
Hexcel Redux 775 Product Data


== External links ==
A 1957 Aero Research advert for Redux
""Joint Economics"" a  short article on Redux by N. A de Bruyne in a 1953 issue of Flight","pandas(index=70, _1=70, text='redux  is the generic name of a family of phenol–formaldehyde/polyvinyl–formal adhesives developed by aero research limited (arl) at duxford, uk, in the 1940s, subsequently produced by ciba (arl). the brand name is now also used for a range of epoxy and bismaleimide adhesives manufactured by hexcel. the name is a contraction of research at duxford.   == history == devised at arl by dr. norman de bruyne and george newell in 1941 for use in the aircraft industry, the adhesive is used for the bonding of metal-to-metal and metal-to-wood structures. the adhesive system comprises a liquid phenolic resin and a pvf (polyvinylformal) thermoplastic powder. the first formulation available was redux liquid e/formvar, comprising a phenolic liquid (redux liquid e) and a pvf powder (formvar), and after its initial non-aviation related application of bonding clutch plates on churchill and cromwell tanks, it was used by de havilland from 1943 to the early 1960s, on, among other aircraft, the hornet, the comet and the derived nimrod, and the dove, heron and trident. it was also used by vickers on the viking and by chance vought on the f7u cutlass. typically, redux would be used to affix stiffening stringers and doublers to wing and fuselage panels, the resulting panel being both stronger and lighter than a riveted structure. in the case of the hornet it was used to join the aluminium lower-wing skin to the wooden upper wing structure, and in the fabrication of the aluminium/wood main wing spar, both forms of composite construction made possible by the advent of redux. after initially supplying de havilland only, arl subsequently produced a refined form of redux liquid e/formvar using a new liquid component known as redux liquid k6, and a finer-grade (smaller particle-size) pvf powder, and this was later made generally available to the wider aircraft industry as redux liquid 775/powder 775, so-named because it was sold for aircraft use to specification dtd 775*. available for general non-aerospace use it was called redux liquid k6/powder c. redux liquid 775/powder 775 was joined in 1954 by the subsequent redux film 775 system, used from 1962 by de havilland (later hawker siddeley and subsequently british aerospace) on the dh.125 and dh.146. other users included bristol (on the britannia), saab (on the lansen & draken), fokker (on the f.27), sud aviation (on the alouette ii/iii), breguet and fairchild, the film-form having the advantage of greater gap-filling ability with no loss of strength over redux liquid 775/powder 775, allowing for wider tolerances in component-fit, as well as easier handling and use and controlled ratios of the liquid/powder components. other redux adhesives available included ""redux 64"", a solution of the phenolic liquid and pvf powder, used worldwide for bonding linings to brake shoes, pads and clutches. the redux range was subsequently expanded to include the current range of adhesives, both in single and two part paste systems and film forms, for both aerospace and industrial uses. * dtd = directorate of technical development   == usage == to use redux in its liquid/powder form, a thin film of the phenolic liquid is applied to both mating surfaces and then dusted with or dipped in the pvf powder to give an approximate ratio by weight of 1 part liquid to 2 parts powder. the coated joints are then allowed to stand for not less than 30 minutes and not more than 72 hours before the components are brought together under elevated pressure and temperature. the curing process is by condensation and a typical figure for redux liquid 775/powder 775 is 30 minutes at 145 °c (293 °f) under a pressure of 100 lbf/in2 (690 kpa). this is not critical and variations in curing-time and/or temperature may be used to increase shear and creep strength at temperatures above 60 °c (140 °f). extending the curing cycle gives benefits in fatigue strength at some cost in the room-temperature peel strength, the practical limit for aluminium alloys being approx 170 °c (338 °f) for one hour, due to the possibility of affecting the alloy\'s mechanical properties.   == performance (typical) redux 775 == lap shear strength at ambient temperature = 34.0 mpa (4,930 lbf/in2) young\'s modulus (e) = 3.35 gpa (486,000 lbf/in2) shear modulus =  1.20 gpa (174,000 lbf/in2)strength of bonds to materials other than aluminium: tensile shear of  0.5 inch (12.7 mm) lap joints at room temperature:  bright mild steel of thickness 0.0625 in (1.6 mm) - mean failing stress = 4,980 lbf/in2 (33.7 mpa, 3.50 kgf/mm2) stainless steel of thickness 0.048 in (1.2 mm) - mean failing stress = 5,600 lbf/in2 (38.6 mpa, 3.94 kgf/mm2) magnesium alloy1 of thickness 0.063 in (1.6 mm) - mean failing stress = 3,210 lbf/in2 (22.1 mpa, 2.26 kgf/mm2) commercially-pure titanium2 of thickness 0.050 in (1.3 mm) - mean failing stress = 4,070 lbf/in2 (28.1 mpa, 2.86 kgf/mm2)1 = hk31a-h24 2 = ici titanium 130   == see also == araldite aerolite tego film   == references ==  project 3 – environmental durability of adhesive bonds – report no. 9 – forensic studies of adhesive joints – part 2 bonded aircraft structure by a. beevers. september 1995. [1] usage on the de havilland comet the de bruyne medal bonding with redux.(reprinted from the aeroplane – sept 1946) hexcel redux film adhesive – 50th anniversary press release hexcel redux 775 product data   == external links == a 1957 aero research advert for redux ""joint economics"" a  short article on redux by n. a de bruyne in a 1953 issue of flight')"
71,"In fluid dynamics, disk loading or disc loading is the average pressure change across an actuator disk, such as an airscrew. Airscrews with a relatively low disk loading are typically called rotors, including helicopter main rotors and tail rotors; propellers typically have a higher disk loading. 
The V-22 Osprey tiltrotor aircraft has a high disk loading relative to a helicopter in the hover mode, but a relatively low disk loading in fixed-wing mode compared to a turboprop aircraft.


== Rotors ==
Disc loading of a hovering helicopter 
is the ratio of its weight to the
total main rotor disc area. It is determined by dividing
the total helicopter weight by the rotor disc area,
which is the area swept by the blades of a rotor. Disc
area can be found by using the span of one rotor blade
as the radius of a circle and then determining the area
the blades encompass during a complete rotation. When a helicopter is being maneuvered, its disc loading changes.
The higher the loading, the more power needed to
maintain rotor speed. A low disc loading is a direct indicator of high lift thrust efficiency.Increasing the weight of a helicopter increases disk loading. For a given weight, a helicopter with shorter rotors will have higher disk loading, and will require more engine power to hover. A low disk loading improves autorotation performance in rotorcraft. Typically, an autogyro (or gyroplane) has a lower rotor disc loading than a helicopter, which provides a slower rate of descent in autorotation.


== Propellers ==
In reciprocating and propeller engines, disk loading can be defined as the ratio between propeller-induced velocity and freestream velocity. Lower disk loading will increase efficiency, so it is generally desirable to have larger propellers from an efficiency standpoint. Maximum efficiency is reduced as disk loading is increased due to the rotating slipstream; using contra-rotating propellers can alleviate this problem allowing high maximum efficiency even at relatively high disc loadings.The Airbus A400M fixed-wing aircraft will have a very high disk loading on its propellers.


== Theory ==
The momentum theory or disk actuator theory describes a mathematical model of an ideal actuator disk, developed by W.J.M. Rankine (1865), Alfred George Greenhill (1888) and R.E. Froude (1889). The helicopter rotor is modeled as an infinitely thin disc with an infinite number of blades that induce a constant pressure jump over the disk area and along the axis of rotation. For a helicopter that is hovering, the aerodynamic force is vertical and exactly balances the helicopter weight, with no lateral force.
The upward action on the helicopter results in a downward reaction on the air flowing through the rotor. The downward reaction produces a downward velocity on the air, increasing its kinetic energy. This energy transfer from the rotor to the air is the induced power loss of the rotary wing, which is analogous to the lift-induced drag of a fixed-wing aircraft.
Conservation of linear momentum relates the induced velocity downstream in the far wake field to the rotor thrust per unit of mass flow. Conservation of energy considers these parameters as well as the induced velocity at the rotor disk. Conservation of mass relates the mass flow to the induced velocity. The momentum theory applied to a helicopter gives the relationship between induced power loss and rotor thrust, which can be used to analyze the performance of the aircraft. Viscosity and compressibility of the air, frictional losses, and rotation of the slipstream in the wake are not considered.


=== Momentum theory ===
For an actuator disk of area 
  
    
      
        A
      
    
    {\displaystyle A}
  , with uniform induced velocity 
  
    
      
        v
      
    
    {\displaystyle v}
   at the rotor disk, and with 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   as the density of air, the mass flow rate 
  
    
      
        
          
          
            
              
                m
                ˙
              
            
          
        
      
    
    {\displaystyle ^{\dot {m}}}
   through the disk area is:

  
    
      
        
          
            
              m
              ˙
            
          
        
        =
        ρ
        
        A
        
        v
        .
      
    
    {\displaystyle {\dot {m}}=\rho \,A\,v.}
  By conservation of mass, the mass flow rate is constant across the slipstream both upstream and downstream of the disk (regardless of velocity). Since the flow far upstream of a helicopter in a level hover is at rest, the starting velocity, momentum, and energy are zero. If the homogeneous slipstream far downstream of the disk has velocity 
  
    
      
        w
      
    
    {\displaystyle w}
  , by conservation of momentum the total thrust 
  
    
      
        T
      
    
    {\displaystyle T}
   developed over the disk is equal to the rate of change of momentum, which assuming zero starting velocity is:

  
    
      
        T
        =
        
          
            
              m
              ˙
            
          
        
        
        w
        .
      
    
    {\displaystyle T={\dot {m}}\,w.}
  By conservation of energy, the work done by the rotor must equal the energy change in the slipstream:

  
    
      
        T
        
        v
        =
        
          
            
              1
              2
            
          
        
        
        
          
            
              m
              ˙
            
          
        
        
        
          
            w
            
              2
            
          
        
        .
      
    
    {\displaystyle T\,v={\tfrac {1}{2}}\,{\dot {m}}\,{w^{2}}.}
  Substituting for 
  
    
      
        T
      
    
    {\displaystyle T}
   and eliminating terms, we get:

  
    
      
        v
        =
        
          
            
              1
              2
            
          
        
        
        w
        .
      
    
    {\displaystyle v={\tfrac {1}{2}}\,w.}
  So the velocity of the slipstream far downstream of the disk is twice the velocity at the disk, which is the same result for an elliptically loaded fixed wing predicted by lifting-line theory.


=== Bernoulli's principle ===
To compute the disk loading using Bernoulli's principle, we assume the pressure in the slipstream far downstream is equal to the starting pressure 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
  , which is equal to the atmospheric pressure. From the starting point to the disk we have:

  
    
      
        
          p
          
            0
          
        
        =
        
        
          p
          
            1
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        .
      
    
    {\displaystyle p_{0}=\,p_{1}+\ {\tfrac {1}{2}}\,\rho \,v^{2}.}
  Between the disk and the distant wake, we have:

  
    
      
        
          p
          
            2
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
        .
      
    
    {\displaystyle p_{2}+\ {\tfrac {1}{2}}\,\rho \,v^{2}=\,p_{0}+\ {\tfrac {1}{2}}\,\rho \,w^{2}.}
  Combining equations, the disk loading 
  
    
      
        T
        
          /
        
        
        A
      
    
    {\displaystyle T/\,A}
   is:

  
    
      
        
          
            T
            A
          
        
        =
        
          p
          
            2
          
        
        −
        
        
          p
          
            1
          
        
        =
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
      
    
    {\displaystyle {\frac {T}{A}}=p_{2}-\,p_{1}={\tfrac {1}{2}}\,\rho \,w^{2}}
  The total pressure in the distant wake is:

  
    
      
        
          p
          
            0
          
        
        +
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}+{\tfrac {1}{2}}\,\rho \,w^{2}=\,p_{0}+{\frac {T}{A}}.}
  So the pressure change across the disk is equal to the disk loading. Above the disk the pressure change is:

  
    
      
        
          p
          
            0
          
        
        −
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        −
        
        
          
            
              1
              4
            
          
        
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}-{\tfrac {1}{2}}\,\rho \,v^{2}=\,p_{0}-\,{\tfrac {1}{4}}{\frac {T}{A}}.}
  Below the disk, the pressure change is:

  
    
      
        
          p
          
            0
          
        
        +
        
          
            
              3
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
        
        
          
            
              3
              4
            
          
        
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}+{\tfrac {3}{2}}\,\rho \,v^{2}=\,p_{0}+\,{\tfrac {3}{4}}{\frac {T}{A}}.}
  The pressure along the slipstream is always falling downstream, except for the positive pressure jump across the disk.


=== Power required ===
From the momentum theory, thrust is:

  
    
      
        T
        =
        
          
            
              m
              ˙
            
          
        
        
        w
        =
        
          
            
              m
              ˙
            
          
        
        
        (
        2
        v
        )
        =
        2
        ρ
        
        A
        
        
          v
          
            2
          
        
        .
      
    
    {\displaystyle T={\dot {m}}\,w={\dot {m}}\,(2v)=2\rho \,A\,v^{2}.}
  The induced velocity is:

  
    
      
        v
        =
        
          
            
              
                T
                A
              
            
            ⋅
            
              
                1
                
                  2
                  ρ
                
              
            
          
        
        .
      
    
    {\displaystyle v={\sqrt {{\frac {T}{A}}\cdot {\frac {1}{2\rho }}}}.}
  Where 
  
    
      
        T
        
          /
        
        A
      
    
    {\displaystyle T/A}
   is the disk loading as before, and the power 
  
    
      
        P
      
    
    {\displaystyle P}
   required in hover (in the ideal case) is:

  
    
      
        P
        =
        T
        v
        =
        T
        
          
            
              
                T
                A
              
            
            ⋅
            
              
                1
                
                  2
                  ρ
                
              
            
          
        
        .
      
    
    {\displaystyle P=Tv=T{\sqrt {{\frac {T}{A}}\cdot {\frac {1}{2\rho }}}}.}
  Therefore, the induced velocity can be expressed as:

  
    
      
        v
        =
        
          
            P
            T
          
        
        =
        
          
            [
            
              
                T
                P
              
            
            ]
          
          
            −
            1
          
        
        .
      
    
    {\displaystyle v={\frac {P}{T}}=\left[{\frac {T}{P}}\right]^{-1}.}
  So, the induced velocity is inversely proportional to the power loading 
  
    
      
        T
        
          /
        
        P
      
    
    {\displaystyle T/P}
  .


== Examples ==


== See also ==
Wing loading


== References ==

 This article incorporates public domain material from the Federal Aviation Administration document: ""Rotorcraft Flying Handbook"" (PDF).","pandas(index=71, _1=71, text='in fluid dynamics, disk loading or disc loading is the average pressure change across an actuator disk, such as an airscrew. airscrews with a relatively low disk loading are typically called rotors, including helicopter main rotors and tail rotors; propellers typically have a higher disk loading. the v-22 osprey tiltrotor aircraft has a high disk loading relative to a helicopter in the hover mode, but a relatively low disk loading in fixed-wing mode compared to a turboprop aircraft.   == rotors == disc loading of a hovering helicopter is the ratio of its weight to the total main rotor disc area. it is determined by dividing the total helicopter weight by the rotor disc area, which is the area swept by the blades of a rotor. disc area can be found by using the span of one rotor blade as the radius of a circle and then determining the area the blades encompass during a complete rotation. when a helicopter is being maneuvered, its disc loading changes. the higher the loading, the more power needed to maintain rotor speed. a low disc loading is a direct indicator of high lift thrust efficiency.increasing the weight of a helicopter increases disk loading. for a given weight, a helicopter with shorter rotors will have higher disk loading, and will require more engine power to hover. a low disk loading improves autorotation performance in rotorcraft. typically, an autogyro (or gyroplane) has a lower rotor disc loading than a helicopter, which provides a slower rate of descent in autorotation.   == propellers == in reciprocating and propeller engines, disk loading can be defined as the ratio between propeller-induced velocity and freestream velocity. lower disk loading will increase efficiency, so it is generally desirable to have larger propellers from an efficiency standpoint. maximum efficiency is reduced as disk loading is increased due to the rotating slipstream; using contra-rotating propellers can alleviate this problem allowing high maximum efficiency even at relatively high disc loadings.the airbus a400m fixed-wing aircraft will have a very high disk loading on its propellers.   == theory == the momentum theory or disk actuator theory describes a mathematical model of an ideal actuator disk, developed by w.j.m. rankine (1865), alfred george greenhill (1888) and r.e. froude (1889). the helicopter rotor is modeled as an infinitely thin disc with an infinite number of blades that induce a constant pressure jump over the disk area and along the axis of rotation. for a helicopter that is hovering, the aerodynamic force is vertical and exactly balances the helicopter weight, with no lateral force. the upward action on the helicopter results in a downward reaction on the air flowing through the rotor. the downward reaction produces a downward velocity on the air, increasing its kinetic energy. this energy transfer from the rotor to the air is the induced power loss of the rotary wing, which is analogous to the lift-induced drag of a fixed-wing aircraft. conservation of linear momentum relates the induced velocity downstream in the far wake field to the rotor thrust per unit of mass flow. conservation of energy considers these parameters as well as the induced velocity at the rotor disk. conservation of mass relates the mass flow to the induced velocity. the momentum theory applied to a helicopter gives the relationship between induced power loss and rotor thrust, which can be used to analyze the performance of the aircraft. viscosity and compressibility of the air, frictional losses, and rotation of the slipstream in the wake are not considered. for an actuator disk of area    a    .   == examples ==   == see also == wing loading   == references ==  this article incorporates public domain material from the federal aviation administration document: ""rotorcraft flying handbook"" (pdf).')"
72,"A Bettsometer is a fabric degradation tester commonly used to measure or test the integrity of fabric coverings (and associated stitching) on aircraft and their wings.The Bettsometer comprises a pen-like instrument (which functions much like a spring balance) and a smooth round needle or pin. The needle is inserted into the fabric and then the instrument is pulled to exert a specific force on the fabric in order to test. A visual inspection is made to check for any rips or tears at the needle insertion point.
The Bettsometer test is often a requirement for the annual 'permit' renewal and is usually carried out by an aircraft inspector who will know the requirements of the test (i.e. the areas of sail and stitching to be tested and the force to be exerted).


== References ==","pandas(index=72, _1=72, text=""a bettsometer is a fabric degradation tester commonly used to measure or test the integrity of fabric coverings (and associated stitching) on aircraft and their wings.the bettsometer comprises a pen-like instrument (which functions much like a spring balance) and a smooth round needle or pin. the needle is inserted into the fabric and then the instrument is pulled to exert a specific force on the fabric in order to test. a visual inspection is made to check for any rips or tears at the needle insertion point. the bettsometer test is often a requirement for the annual 'permit' renewal and is usually carried out by an aircraft inspector who will know the requirements of the test (i.e. the areas of sail and stitching to be tested and the force to be exerted).   == references =="")"
73,"In aerospace engineering, an aircraft's fuel fraction, fuel weight fraction, or a spacecraft's propellant fraction, is the weight of the fuel or propellant divided by the gross take-off weight of the craft (including propellant):

  
    
      
         
        ζ
        =
        
          
            
              Δ
              W
            
            
              W
              
                1
              
            
          
        
      
    
    {\displaystyle \ \zeta ={\frac {\Delta W}{W_{1}}}}
  The fractional result of this mathematical division is often expressed as a percent. For aircraft with external drop tanks, the term internal fuel fraction is used to exclude the weight of external tanks and fuel. 
Fuel fraction is a key parameter in determining an aircraft's range, the distance it can fly without refueling.
Breguet’s aircraft range equation describes the relationship of range with airspeed, lift-to-drag ratio, specific fuel consumption, and the part of the total fuel fraction available for cruise, also known as the cruise fuel fraction, or  cruise fuel weight fraction.


== Fighter aircraft ==
At today’s state of the art for jet fighter aircraft, fuel fractions of 29 percent and below typically yield subcruisers; 33 percent provides a quasi–supercruiser; and 35 percent and above are needed for useful supercruising missions. The U.S. F-22 Raptor’s fuel fraction is 29 percent, Eurofighter is 31 percent, both similar to those of the subcruising F-4 Phantom II, F-15 Eagle and the Russian Mikoyan MiG-29 ""Fulcrum"". The Russian supersonic interceptor, the Mikoyan MiG-31 ""Foxhound"", has a fuel fraction of over 45 percent. The Panavia Tornado had a relatively low internal fuel fraction of 26 percent, and frequently carried drop tanks.


== Airliners ==
Airliners have a fuel fraction of less than half their takeoff weight, between 26% for medium-haul to 45% for long-haul:

The Concorde supersonic transport had a fuel fraction of 51%.


== General aviation ==
The Rutan Voyager took off on its 1986 around-the-world flight at 72 percent, the highest figure ever at the time. Steve Fossett's Virgin Atlantic GlobalFlyer could attain a fuel fraction of nearly 85 percent, meaning that it carried more than five times its empty weight in fuel.


== See also ==
Mass ratio


== References ==","pandas(index=73, _1=73, text='in aerospace engineering, an aircraft\'s fuel fraction, fuel weight fraction, or a spacecraft\'s propellant fraction, is the weight of the fuel or propellant divided by the gross take-off weight of the craft (including propellant):      ζ =    δ w   w  1        the fractional result of this mathematical division is often expressed as a percent. for aircraft with external drop tanks, the term internal fuel fraction is used to exclude the weight of external tanks and fuel. fuel fraction is a key parameter in determining an aircraft\'s range, the distance it can fly without refueling. breguet’s aircraft range equation describes the relationship of range with airspeed, lift-to-drag ratio, specific fuel consumption, and the part of the total fuel fraction available for cruise, also known as the cruise fuel fraction, or  cruise fuel weight fraction.   == fighter aircraft == at today’s state of the art for jet fighter aircraft, fuel fractions of 29 percent and below typically yield subcruisers; 33 percent provides a quasi–supercruiser; and 35 percent and above are needed for useful supercruising missions. the u.s. f-22 raptor’s fuel fraction is 29 percent, eurofighter is 31 percent, both similar to those of the subcruising f-4 phantom ii, f-15 eagle and the russian mikoyan mig-29 ""fulcrum"". the russian supersonic interceptor, the mikoyan mig-31 ""foxhound"", has a fuel fraction of over 45 percent. the panavia tornado had a relatively low internal fuel fraction of 26 percent, and frequently carried drop tanks.   == airliners == airliners have a fuel fraction of less than half their takeoff weight, between 26% for medium-haul to 45% for long-haul:  the concorde supersonic transport had a fuel fraction of 51%.   == general aviation == the rutan voyager took off on its 1986 around-the-world flight at 72 percent, the highest figure ever at the time. steve fossett\'s virgin atlantic globalflyer could attain a fuel fraction of nearly 85 percent, meaning that it carried more than five times its empty weight in fuel.   == see also == mass ratio   == references ==')"
74,"Electromagnetic formation flight (EMFF) investigates the concept of using electromagnets coupled with reaction wheels in place of more traditional propulsion systems to control the positions and attitudes of a number of spacecraft in close proximity.  Unlike traditional propulsion systems, which use exhaustible propellants that often limit lifetime, the EMFF system uses solar power to energize a magnetic field. The Massachusetts Institute of Technology Space Systems Laboratory is exploring this concept by developing dynamics and control models as well as an experimental testbed for their validation.


== How it works ==
The magnetic fields for EMFF are generated by sending current through coils of wire.  The interaction between the magnetic dipoles created is easily understood with a far field approximation where the separation distance between two vehicles is large compared to the physical size of the dipole.  By controlling the dipoles on various vehicles, attraction, repulsion, and shear forces can be created. Combined with reaction wheels, any desired maneuver can be performed as long as the formation’s center of mass is not required to change.


== Applications ==
The EMFF system is most applicable in cases where multiple spacecraft are free-flying relative to one another and there is no need to control the center of mass of the system. NASA’s Terrestrial Planet Finder (TPF) mission and space telescope assembly are just two such types of missions.  EMFF provides the foremost advantage of reduced dependence on consumables.  In addition, it eliminates thruster plumes and enhances the capability of replacing a failed element more economically.


== Testbed ==
The MIT-SSL constructed two EMFF testbed vehicles for demonstrating control of 2-D formations on a large flat floor.  Vehicles are suspended on a frictionless air carriage and are completely self-contained using RF communications, microprocessors, and a metrology system.  Liquid Nitrogen maintains cryogenic temperatures and batteries provide the power to the high-temperature superconductive (HTS) coils.  The testbed has demonstrated control of the relative degrees of freedom (DOF) in open loop and closed loop control using linearized controllers and a nonlinear sliding mode controller.


== Awards ==
Former Space Systems Lab associate director Dr. Raymond Sedwick (now at the University of Maryland, College Park) has been awarded the first Bepi Colombo Prize for a paper on electromagnetic formation flight. According to Aero-Astro Professor Manuel Martinez-Sanchez, who worked with Colombo and was a juror in the competition, ""The jury was unanimous in that Ray's paper best represented 'Bepi' Colombo's spirit of innovation and originality, combined with rigor.""


== Collaborators ==
Research on electromagnetic formation flight or similar projects is also ongoing at:

The Institute of Space and Astronautical Science / JAXA
Space Research Centre, Polish Academy of Sciences
Michigan Technogical University on Colomb Force Spacecraft


== Other journal articles ==
Elias, Laila M., Kwon, Daniel W., Sedwick, Raymond J., and Miller, David W., ""Electromagnetic Formation Flight Dynamics including Reaction Wheel Gyroscopic Stiffening Effects"" Journal of Guidance, Control, and Dynamics, Vol. 30, No. 2, Mar–Apr. 2007, pp. 499–511.


== References ==


== External links ==
SSL's EMFF web page
Other research at the MIT Space Systems Laboratory MSE
SPHERES","pandas(index=74, _1=74, text='electromagnetic formation flight (emff) investigates the concept of using electromagnets coupled with reaction wheels in place of more traditional propulsion systems to control the positions and attitudes of a number of spacecraft in close proximity.  unlike traditional propulsion systems, which use exhaustible propellants that often limit lifetime, the emff system uses solar power to energize a magnetic field. the massachusetts institute of technology space systems laboratory is exploring this concept by developing dynamics and control models as well as an experimental testbed for their validation.   == how it works == the magnetic fields for emff are generated by sending current through coils of wire.  the interaction between the magnetic dipoles created is easily understood with a far field approximation where the separation distance between two vehicles is large compared to the physical size of the dipole.  by controlling the dipoles on various vehicles, attraction, repulsion, and shear forces can be created. combined with reaction wheels, any desired maneuver can be performed as long as the formation’s center of mass is not required to change.   == applications == the emff system is most applicable in cases where multiple spacecraft are free-flying relative to one another and there is no need to control the center of mass of the system. nasa’s terrestrial planet finder (tpf) mission and space telescope assembly are just two such types of missions.  emff provides the foremost advantage of reduced dependence on consumables.  in addition, it eliminates thruster plumes and enhances the capability of replacing a failed element more economically.   == testbed == the mit-ssl constructed two emff testbed vehicles for demonstrating control of 2-d formations on a large flat floor.  vehicles are suspended on a frictionless air carriage and are completely self-contained using rf communications, microprocessors, and a metrology system.  liquid nitrogen maintains cryogenic temperatures and batteries provide the power to the high-temperature superconductive (hts) coils.  the testbed has demonstrated control of the relative degrees of freedom (dof) in open loop and closed loop control using linearized controllers and a nonlinear sliding mode controller.   == awards == former space systems lab associate director dr. raymond sedwick (now at the university of maryland, college park) has been awarded the first bepi colombo prize for a paper on electromagnetic formation flight. according to aero-astro professor manuel martinez-sanchez, who worked with colombo and was a juror in the competition, ""the jury was unanimous in that ray\'s paper best represented \'bepi\' colombo\'s spirit of innovation and originality, combined with rigor.""   == collaborators == research on electromagnetic formation flight or similar projects is also ongoing at:  the institute of space and astronautical science / jaxa space research centre, polish academy of sciences michigan technogical university on colomb force spacecraft   == other journal articles == elias, laila m., kwon, daniel w., sedwick, raymond j., and miller, david w., ""electromagnetic formation flight dynamics including reaction wheel gyroscopic stiffening effects"" journal of guidance, control, and dynamics, vol. 30, no. 2, mar–apr. 2007, pp. 499–511.   == references ==   == external links == ssl\'s emff web page other research at the mit space systems laboratory mse spheres')"
75,"In naval architecture and aerospace engineering, the fineness ratio is the ratio of the length of a body to its maximum width. Shapes that are short and wide have a low fineness ratio, those that are long and narrow have high fineness ratios. Aircraft that spend time at supersonic speeds, e.g. the Concorde, generally have high fineness ratios.
At speeds below critical mach, one of the primary forms of drag is skin friction. As the name implies, this is drag caused by the interaction of the airflow with the aircraft's skin.  To minimize this drag, the aircraft should be designed to minimize the exposed skin area, or ""wetted surface"". One solution to this problem is constructing an ""egg shaped"" fuselage, for example as used on the home-built Questair Venture.
Theoretical ideal fineness ratios in subsonic aircraft fuselages are typically found at about 6:1, however this may be compromised by other design considerations such as seating or freight size requirements. Because a higher fineness fuselage can have reduced tail surfaces, this ideal ratio can practically be increased to 8:1.Most aircraft have fineness ratios significantly greater than this, however. This is often due to the competing need to place the tail control surfaces at the end of a longer moment arm to increase their effectiveness. Reducing the length of the fuselage would require larger controls, which would offset the drag savings from using the ideal fineness ratio. An example of a high-performance design with an imperfect fineness ratio is the Lancair. In other cases, the designer is forced to use a non-ideal design due to outside factors such as seating arrangements or cargo pallet sizes. Modern airliners often have fineness ratios much higher than ideal, a side effect of their cylindrical cross-section which is selected for strength, as well as providing a single width to simplify seating layout and air cargo handling.
As an aircraft approaches the speed of sound, shock waves form on areas of greater curvature. These shock waves radiate away energy that the engines must supply, energy that does not go into making the aircraft go faster. This appears to be a new form of drag—referred to as wave drag—which peaks at about three times the drag at speeds even slightly below the critical mach. In order to minimize the wave drag, the curvature of the aircraft should be kept to a minimum, which implies much higher fineness ratios. This is why high-speed aircraft have long pointed noses and tails, and cockpit canopies that are flush to the fuselage line.
More technically, the best possible performance for a supersonic design is typified by two ""perfect shapes"", the Sears-Haack body which is pointed at both ends, or the von Kármán ogive, which has a blunt tail. Examples of the latter design include the Concorde, F-104 Starfighter and XB-70 Valkyrie, although to some degree practically every post-World War II interceptor aircraft featured such a design. Missile designers are even less interested in low-speed performance, and missiles generally have higher fineness ratios than most aircraft.
The introduction of aircraft with higher fineness ratios also introduced a new form of instability, inertial coupling. As the engines and cockpit moved away from the aircraft's center of mass, the roll inertia of these masses grew to be able to overwhelm the power of the aerodynamic surfaces. A variety of methods are used to combat this effect, including oversized controls and stability augmentation systems.


== References ==

Form Factor
Basic Fluid Dynamics","pandas(index=75, _1=75, text='in naval architecture and aerospace engineering, the fineness ratio is the ratio of the length of a body to its maximum width. shapes that are short and wide have a low fineness ratio, those that are long and narrow have high fineness ratios. aircraft that spend time at supersonic speeds, e.g. the concorde, generally have high fineness ratios. at speeds below critical mach, one of the primary forms of drag is skin friction. as the name implies, this is drag caused by the interaction of the airflow with the aircraft\'s skin.  to minimize this drag, the aircraft should be designed to minimize the exposed skin area, or ""wetted surface"". one solution to this problem is constructing an ""egg shaped"" fuselage, for example as used on the home-built questair venture. theoretical ideal fineness ratios in subsonic aircraft fuselages are typically found at about 6:1, however this may be compromised by other design considerations such as seating or freight size requirements. because a higher fineness fuselage can have reduced tail surfaces, this ideal ratio can practically be increased to 8:1.most aircraft have fineness ratios significantly greater than this, however. this is often due to the competing need to place the tail control surfaces at the end of a longer moment arm to increase their effectiveness. reducing the length of the fuselage would require larger controls, which would offset the drag savings from using the ideal fineness ratio. an example of a high-performance design with an imperfect fineness ratio is the lancair. in other cases, the designer is forced to use a non-ideal design due to outside factors such as seating arrangements or cargo pallet sizes. modern airliners often have fineness ratios much higher than ideal, a side effect of their cylindrical cross-section which is selected for strength, as well as providing a single width to simplify seating layout and air cargo handling. as an aircraft approaches the speed of sound, shock waves form on areas of greater curvature. these shock waves radiate away energy that the engines must supply, energy that does not go into making the aircraft go faster. this appears to be a new form of drag—referred to as wave drag—which peaks at about three times the drag at speeds even slightly below the critical mach. in order to minimize the wave drag, the curvature of the aircraft should be kept to a minimum, which implies much higher fineness ratios. this is why high-speed aircraft have long pointed noses and tails, and cockpit canopies that are flush to the fuselage line. more technically, the best possible performance for a supersonic design is typified by two ""perfect shapes"", the sears-haack body which is pointed at both ends, or the von kármán ogive, which has a blunt tail. examples of the latter design include the concorde, f-104 starfighter and xb-70 valkyrie, although to some degree practically every post-world war ii interceptor aircraft featured such a design. missile designers are even less interested in low-speed performance, and missiles generally have higher fineness ratios than most aircraft. the introduction of aircraft with higher fineness ratios also introduced a new form of instability, inertial coupling. as the engines and cockpit moved away from the aircraft\'s center of mass, the roll inertia of these masses grew to be able to overwhelm the power of the aerodynamic surfaces. a variety of methods are used to combat this effect, including oversized controls and stability augmentation systems.   == references ==  form factor basic fluid dynamics')"
76,"Human-rating certification, also known as man-rating or crew-rating, is the certification of a spacecraft or launch vehicle as capable of safely transporting humans. There is no one particular standard for human-rating a spacecraft or launch vehicle, and the various entities that launch or plan to launch such spacecraft specify requirements for their particular systems to be human-rated.


== NASA ==
One entity that applies human rating is the US government civilian space agency, NASA. NASA's human-rating requires not just that a system be designed to be tolerant of failure and to protect the crew even if an unrecoverable failure occurs, but also that astronauts aboard a human-rated spacecraft have some control over it. This set of technical requirements and the associated certification process for crewed space systems are in addition to the standards and requirements for all of NASA's space flight programs.The development of the Space Shuttle and the International Space Station pre-dates the current NASA human-rating requirements. After the Challenger and Columbia accidents, the criteria used by NASA for human-rating spacecraft were made more stringent.


=== Commercial Crew Program (CCP) ===

The NASA CCP human-rating standards require that the probability of a loss on ascent does not exceed 1 in 500, and that the probability of a loss on descent did not exceed 1 in 500. The overall mission loss risk, which includes vehicle risk from micrometeorites and orbital debris while in orbit for up to 210 days, is required to be no more than 1 in 270. Maximum sustained acceleration is limited to 3 g.The United Launch Alliance (ULA) published a paper submitted to AIAA detailing the modifications to its Delta IV and Atlas V launch vehicles that would be needed to conform to NASA Standard 8705.2B. ULA has since been awarded $6.7 million under NASA's Commercial Crew Development (CCDev) program for development of an Emergency Detection System, one of the final pieces that would be needed to make these launchers suitable for human spaceflight.SpaceX is using Dragon 2, launched on a Falcon 9 rocket, to deliver crew to the ISS. Dragon 2 made its first uncrewed test flight in March 2019 and has been conducting crewed flights since Demo-2 in May 2020.Boeing is developing the Starliner spacecraft as part of the Commercial Crew Program.


== Other space agencies ==
The Russian state corporation Roscosmos, Indian space agency ISRO, Chinese space agency CNSA, and each private spaceflight system builder typically sets up their own specific criteria to be met before carrying humans on any space transport system.


== See also ==
FAA
List of human spaceflight programs


== References ==","pandas(index=76, _1=76, text=""human-rating certification, also known as man-rating or crew-rating, is the certification of a spacecraft or launch vehicle as capable of safely transporting humans. there is no one particular standard for human-rating a spacecraft or launch vehicle, and the various entities that launch or plan to launch such spacecraft specify requirements for their particular systems to be human-rated.   == nasa == one entity that applies human rating is the us government civilian space agency, nasa. nasa's human-rating requires not just that a system be designed to be tolerant of failure and to protect the crew even if an unrecoverable failure occurs, but also that astronauts aboard a human-rated spacecraft have some control over it. this set of technical requirements and the associated certification process for crewed space systems are in addition to the standards and requirements for all of nasa's space flight programs.the development of the space shuttle and the international space station pre-dates the current nasa human-rating requirements. after the challenger and columbia accidents, the criteria used by nasa for human-rating spacecraft were made more stringent. the nasa ccp human-rating standards require that the probability of a loss on ascent does not exceed 1 in 500, and that the probability of a loss on descent did not exceed 1 in 500. the overall mission loss risk, which includes vehicle risk from micrometeorites and orbital debris while in orbit for up to 210 days, is required to be no more than 1 in 270. maximum sustained acceleration is limited to 3 g.the united launch alliance (ula) published a paper submitted to aiaa detailing the modifications to its delta iv and atlas v launch vehicles that would be needed to conform to nasa standard 8705.2b. ula has since been awarded $6.7 million under nasa's commercial crew development (ccdev) program for development of an emergency detection system, one of the final pieces that would be needed to make these launchers suitable for human spaceflight.spacex is using dragon 2, launched on a falcon 9 rocket, to deliver crew to the iss. dragon 2 made its first uncrewed test flight in march 2019 and has been conducting crewed flights since demo-2 in may 2020.boeing is developing the starliner spacecraft as part of the commercial crew program.   == other space agencies == the russian state corporation roscosmos, indian space agency isro, chinese space agency cnsa, and each private spaceflight system builder typically sets up their own specific criteria to be met before carrying humans on any space transport system.   == see also == faa list of human spaceflight programs   == references =="")"
77,"Design/Build/Fly, or DBF, is a radio-controlled aircraft competition sponsored by the American institute of Aeronautics and Astronautics (AIAA), Cessna Aircraft Company, and Raytheon Missile Systems. The Office of Naval Research was also a sponsor until 2006. The competition is intended to challenge the AIAA student branches of each university to design, build, and fly a remote controlled airplane that can complete specific ground and flight missions. Additionally, the teams are required to submit a comprehensive design report detailing the most important aspects of their designs.
The competition rules change every year. Usually, rules are published in late August and the competition fly-off is held in April. The rules define a mathematical formula used to determine the score for an entry. Recent competitions' formulas have used a combination of design report score, mission score determined by performance conducting one or more mission tasks at the fly-off, and Rated Aircraft Cost, a variable used to define the complexity of the design.

1. The 2020 fly-off was cancelled due to the COVID-19 pandemic. Scores and rankings were solely based on the report scores.


== List of Top 5 Finish by University ==


== External links ==
Home page","pandas(index=77, _1=77, text=""design/build/fly, or dbf, is a radio-controlled aircraft competition sponsored by the american institute of aeronautics and astronautics (aiaa), cessna aircraft company, and raytheon missile systems. the office of naval research was also a sponsor until 2006. the competition is intended to challenge the aiaa student branches of each university to design, build, and fly a remote controlled airplane that can complete specific ground and flight missions. additionally, the teams are required to submit a comprehensive design report detailing the most important aspects of their designs. the competition rules change every year. usually, rules are published in late august and the competition fly-off is held in april. the rules define a mathematical formula used to determine the score for an entry. recent competitions' formulas have used a combination of design report score, mission score determined by performance conducting one or more mission tasks at the fly-off, and rated aircraft cost, a variable used to define the complexity of the design.  1. the 2020 fly-off was cancelled due to the covid-19 pandemic. scores and rankings were solely based on the report scores.   == list of top 5 finish by university ==   == external links == home page"")"
78,"If an aircraft in flight suffers a disturbance in pitch that causes an increase (or decrease) in angle of attack, it is desirable that the aerodynamic forces on the aircraft cause a decrease (or increase) in angle of attack so that the disturbance does not cause a continuous increase (or decrease) in angle of attack.  This is longitudinal static stability.
Static margin is a concept used to characterize the static longitudinal stability and controllability of aircraft and missiles.
In aircraft analysis, static margin is defined as the distance between the center of gravity and the neutral point of the aircraft, expressed as a percentage of the mean aerodynamic chord of the wing.  The greater this distance and the narrower the wing, the more stable the aircraft.  Conventionally, the neutral point is aft of the c.g., although in rare cases (computer controlled fighter aircraft) it may be forward of the c.g., i.e. slightly unstable, to obtain quickness of response in combat.  Too great longitudinal stability makes the aircraft ""stiff"" in pitch, resulting in such undesirable features as difficulty in obtaining the necessary stalled nose-up pitch when landing.
The position of the neutral point is found by taking the algebraic net moment of all horizontal surfaces, measured from the nose of the aircraft, in the same manner as the c.g. is determined, i.e. the sum of all such moments divided by their total area.  The stabilizer and elevator dominate this result, but it is necessary to account for all surfaces such as fuselage, landing gear, prop-normal, etc.  It is also necessary to take account of the center of pressure of the wing, which can move a good deal fore and aft as angle of attack of a flat-bottom wing section (Clark Y) changes, or not at all in the case of self-stabilizing sections such as the M6.
The neutral point in conventional aircraft is a short distance behind the c.g. (""The feathers of the arrow must be at the back""); but in unconventional aircraft such as canards and those with dual-wings, such as the Quickie, this will not be so.  The overall rule stated above must hold, i.e. the neutral point must be aft of the c.g., wherever that may be. 
The position of the center of gravity is determined by factors such as the positions of  loads, e.g. passengers, fuel, weapons, etc.; whether such loads can vary, e.g. presence or absence of luggage, ammunition, etc.; and how fuel is consumed during flight.  Additional information regarding the usual position of the neutral point aft of the center of gravity is at  longitudinal static stability. (For an aircraft this may be described as positive static margin.)  The response of an aircraft or missile to an angular disturbance such as a pitch disturbance is determined by its static margin.
In missile analysis, static margin is defined as the distance between the center of gravity and the center of pressure.  Missiles are symmetric vehicles and if they have airfoils they too are symmetric.  
For missiles, positive static margin implies that the complete vehicle makes a restoring moment for any angle of attack from the trim position.  If the center of pressure is behind the center of gravity then the moment will be restoring.  For missiles with symmetric airfoils, the neutral point and the center of pressure are coincident and the term neutral point is not used.


== Relationship to aircraft and missile stability and control ==
If the center of gravity (CG) of an aircraft is forward of the neutral point, or the CG of a missile is forward of the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that returns the angle of attack of the vehicle towards the angle that existed prior to the disturbance.
If the CG of an aircraft is behind the neutral point, or the CG of a missile is behind the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that continues to drive the angle of attack of the vehicle further away from the starting position.The first condition above is positive static stability.  In missile analysis this is described as positive static margin.  (In aircraft analysis it may be described as negative static margin.)
The second condition above is negative static stability.  In missile analysis this is defined as negative static margin.  (In aircraft analysis it may be described as positive static margin.)  
Depending on the static margin, humans may not be able to use control inputs to the elevators to control the pitch of the vehicle.  Typically, computer based autopilots are required to control the vehicle when it has negative static stability - usually described as negative static margin.
The purpose of the reduced stability (low static margin) is to make an aircraft more responsive to pilot inputs.  An aircraft with a large static margin will be very stable and slow to respond to the pilot inputs.  The amount of static margin is an important factor in determining the handling qualities of an aircraft.  For an unguided rocket, the vehicle must have a large positive static margin so the rocket shows minimum tendency to diverge from the direction of flight given to it at launch.  In contrast, guided missiles usually have a negative static margin for increased maneuverability.


== See also ==
Aerodynamic center
Longitudinal static stability
Center of pressure


== References ==","pandas(index=78, _1=78, text='if an aircraft in flight suffers a disturbance in pitch that causes an increase (or decrease) in angle of attack, it is desirable that the aerodynamic forces on the aircraft cause a decrease (or increase) in angle of attack so that the disturbance does not cause a continuous increase (or decrease) in angle of attack.  this is longitudinal static stability. static margin is a concept used to characterize the static longitudinal stability and controllability of aircraft and missiles. in aircraft analysis, static margin is defined as the distance between the center of gravity and the neutral point of the aircraft, expressed as a percentage of the mean aerodynamic chord of the wing.  the greater this distance and the narrower the wing, the more stable the aircraft.  conventionally, the neutral point is aft of the c.g., although in rare cases (computer controlled fighter aircraft) it may be forward of the c.g., i.e. slightly unstable, to obtain quickness of response in combat.  too great longitudinal stability makes the aircraft ""stiff"" in pitch, resulting in such undesirable features as difficulty in obtaining the necessary stalled nose-up pitch when landing. the position of the neutral point is found by taking the algebraic net moment of all horizontal surfaces, measured from the nose of the aircraft, in the same manner as the c.g. is determined, i.e. the sum of all such moments divided by their total area.  the stabilizer and elevator dominate this result, but it is necessary to account for all surfaces such as fuselage, landing gear, prop-normal, etc.  it is also necessary to take account of the center of pressure of the wing, which can move a good deal fore and aft as angle of attack of a flat-bottom wing section (clark y) changes, or not at all in the case of self-stabilizing sections such as the m6. the neutral point in conventional aircraft is a short distance behind the c.g. (""the feathers of the arrow must be at the back""); but in unconventional aircraft such as canards and those with dual-wings, such as the quickie, this will not be so.  the overall rule stated above must hold, i.e. the neutral point must be aft of the c.g., wherever that may be. the position of the center of gravity is determined by factors such as the positions of  loads, e.g. passengers, fuel, weapons, etc.; whether such loads can vary, e.g. presence or absence of luggage, ammunition, etc.; and how fuel is consumed during flight.  additional information regarding the usual position of the neutral point aft of the center of gravity is at  longitudinal static stability. (for an aircraft this may be described as positive static margin.)  the response of an aircraft or missile to an angular disturbance such as a pitch disturbance is determined by its static margin. in missile analysis, static margin is defined as the distance between the center of gravity and the center of pressure.  missiles are symmetric vehicles and if they have airfoils they too are symmetric. for missiles, positive static margin implies that the complete vehicle makes a restoring moment for any angle of attack from the trim position.  if the center of pressure is behind the center of gravity then the moment will be restoring.  for missiles with symmetric airfoils, the neutral point and the center of pressure are coincident and the term neutral point is not used.   == relationship to aircraft and missile stability and control == if the center of gravity (cg) of an aircraft is forward of the neutral point, or the cg of a missile is forward of the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that returns the angle of attack of the vehicle towards the angle that existed prior to the disturbance. if the cg of an aircraft is behind the neutral point, or the cg of a missile is behind the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that continues to drive the angle of attack of the vehicle further away from the starting position.the first condition above is positive static stability.  in missile analysis this is described as positive static margin.  (in aircraft analysis it may be described as negative static margin.) the second condition above is negative static stability.  in missile analysis this is defined as negative static margin.  (in aircraft analysis it may be described as positive static margin.) depending on the static margin, humans may not be able to use control inputs to the elevators to control the pitch of the vehicle.  typically, computer based autopilots are required to control the vehicle when it has negative static stability - usually described as negative static margin. the purpose of the reduced stability (low static margin) is to make an aircraft more responsive to pilot inputs.  an aircraft with a large static margin will be very stable and slow to respond to the pilot inputs.  the amount of static margin is an important factor in determining the handling qualities of an aircraft.  for an unguided rocket, the vehicle must have a large positive static margin so the rocket shows minimum tendency to diverge from the direction of flight given to it at launch.  in contrast, guided missiles usually have a negative static margin for increased maneuverability.   == see also == aerodynamic center longitudinal static stability center of pressure   == references ==')"
79,"In fluid dynamics, the Küssner effect describes the unsteady aerodynamic forces on an airfoil or hydrofoil caused by encountering a transverse gust. This is directly related to the Küssner function, used in describing the effect. Both the effect and function are named after Hans Georg Küssner (1900–1984), a German aerodynamics engineer.Küssner derived an approximate model for an airfoil encountering a sudden step-like change in the transverse gust velocity—or, equivalently, as seen from a frame of reference moving with the airfoil: a sudden change in the angle of attack. The airfoil is modelled as a flat plate in a potential flow, moving with constant horizontal velocity. For this case he derived the impulse response function—known as Küssner function—needed to compute the unsteady lift and moment exerted by the air on the airfoil.


== Notes ==


== References ==
H.G. Küssner (December 20, 1936), ""Zusammenfassender Bericht über den instationären Auftrieb von Flügeln (Summary report on the instationary lift of wings)"", Luftfahrtforschung (in German), 13 (12): 410–424
H.G. Küssner (1937), ""Flügel- und Leitwerkflattern"" (in German)
H.G. Küssner (1940), ""Der schwingende Flügel mit aerodynamisch ausgeglichenem Ruder"" (in German)
H.G. Küssner (1940), ""Allgemeine Tragflächentheorie"" (in German)
Ernst H. Hirschel; Horst Prem; Gero Madelung (2004), Aeronautical Research in Germany: From Lilienthal until Today, Springer, p. 287, ISBN 978-3-540-40645-7
Tuncer Cebeci (2005), Analysis of Low-speed Unsteady Airfoil Flows, Springer, pp. 15–16 & 52, ISBN 0-9668461-8-4
Raymond L. Bisplinghoff; Holt Ashley; Robert L. Halfman (1996), Aeroelasticity (revised ed.), Dover, pp. 281–286, ISBN 0-486-69189-6
John M. Eggleston (1956), Calculation of the forces and moments on a slender fuselage and vertical fin penetrating lateral gusts (PDF), NACA Technical Note 3805 Page 3
Beerinder Singh; Inderjit Chopra (September 2008), ""Insect-Based Hover-Capable Flapping Wings for Micro Air Vehicles: Experiments and Analysis"", AIAA Journal, 46 (9): 2115–2135, Bibcode:2008AIAAJ..46.2115S, doi:10.2514/1.28192
L.M. Laudanski (July 2000), ""Random disturbances, airplane loads and its fatigue life"", Probabilistic Engineering Mechanics, 15 (3): 233–240, doi:10.1016/S0266-8920(98)00020-4


== External links ==
E.C. Pike (Ed.) (1971), Manual on Aeroelasticity. Subject and author index (PDF), NATO AGARD Report 578CS1 maint: extra text: authors list (link) Page 13.
Küssner function, Georgia Institute of Technology, retrieved 10 March 2009","pandas(index=79, _1=79, text='in fluid dynamics, the küssner effect describes the unsteady aerodynamic forces on an airfoil or hydrofoil caused by encountering a transverse gust. this is directly related to the küssner function, used in describing the effect. both the effect and function are named after hans georg küssner (1900–1984), a german aerodynamics engineer.küssner derived an approximate model for an airfoil encountering a sudden step-like change in the transverse gust velocity—or, equivalently, as seen from a frame of reference moving with the airfoil: a sudden change in the angle of attack. the airfoil is modelled as a flat plate in a potential flow, moving with constant horizontal velocity. for this case he derived the impulse response function—known as küssner function—needed to compute the unsteady lift and moment exerted by the air on the airfoil.   == notes ==   == references == h.g. küssner (december 20, 1936), ""zusammenfassender bericht über den instationären auftrieb von flügeln (summary report on the instationary lift of wings)"", luftfahrtforschung (in german), 13 (12): 410–424 h.g. küssner (1937), ""flügel- und leitwerkflattern"" (in german) h.g. küssner (1940), ""der schwingende flügel mit aerodynamisch ausgeglichenem ruder"" (in german) h.g. küssner (1940), ""allgemeine tragflächentheorie"" (in german) ernst h. hirschel; horst prem; gero madelung (2004), aeronautical research in germany: from lilienthal until today, springer, p. 287, isbn 978-3-540-40645-7 tuncer cebeci (2005), analysis of low-speed unsteady airfoil flows, springer, pp. 15–16 & 52, isbn 0-9668461-8-4 raymond l. bisplinghoff; holt ashley; robert l. halfman (1996), aeroelasticity (revised ed.), dover, pp. 281–286, isbn 0-486-69189-6 john m. eggleston (1956), calculation of the forces and moments on a slender fuselage and vertical fin penetrating lateral gusts (pdf), naca technical note 3805 page 3 beerinder singh; inderjit chopra (september 2008), ""insect-based hover-capable flapping wings for micro air vehicles: experiments and analysis"", aiaa journal, 46 (9): 2115–2135, bibcode:2008aiaaj..46.2115s, doi:10.2514/1.28192 l.m. laudanski (july 2000), ""random disturbances, airplane loads and its fatigue life"", probabilistic engineering mechanics, 15 (3): 233–240, doi:10.1016/s0266-8920(98)00020-4   == external links == e.c. pike (ed.) (1971), manual on aeroelasticity. subject and author index (pdf), nato agard report 578cs1 maint: extra text: authors list (link) page 13. küssner function, georgia institute of technology, retrieved 10 march 2009')"
80,"In astronautics, a powered flyby, or Oberth maneuver, is a maneuver in which a spacecraft falls into a gravitational well and then uses its engines to further accelerate as it is falling, thereby achieving additional speed. The resulting maneuver is a more efficient way to gain kinetic energy than applying the same impulse outside of a gravitational well. The gain in efficiency is explained by the Oberth effect, wherein the use of a reaction engine at higher speeds generates a greater change in mechanical energy than its use at lower speeds. In practical terms, this means that the most energy-efficient method for a spacecraft to burn its fuel is at the lowest possible orbital periapsis, when its orbital velocity (and so, its kinetic energy) is greatest. In some cases, it is even worth spending fuel on slowing the spacecraft into a gravity well to take advantage of the efficiencies of the Oberth effect. The maneuver and effect are named after the person who first described them in 1927, Hermann Oberth, an Austro-Hungarian-born German physicist and a founder of modern rocketry.The Oberth effect is strongest at a point in orbit known as the periapsis, where the gravitational potential is lowest, and the speed is highest. This is because a given firing of a rocket engine at high speed causes a greater change in kinetic energy than when fired otherwise similarly at lower speed. 
Because the vehicle remains near periapsis only for a short time, for the Oberth maneuver to be most effective the vehicle must be able to generate as much impulse as possible in the shortest possible time. As a result the Oberth maneuver is much more useful for high-thrust rocket engines like liquid-propellant rockets, and less useful for low-thrust reaction engines such as ion drives, which take a long time to gain speed. The Oberth effect also can be used to understand the behavior of multi-stage rockets: the upper stage can generate much more usable kinetic energy than the total chemical energy of the propellants it carries.In terms of the energies involved, the Oberth effect is more effective at higher speeds because at high speed the propellant has significant kinetic energy in addition to its chemical potential energy. At higher speed the vehicle is able to employ the greater change (reduction) in kinetic energy of the propellant (as it is exhausted backwards and hence at reduced speed and hence reduced kinetic energy) to generate a greater increase in kinetic energy of the vehicle.


== Explanation in terms of momentum and kinetic energy ==
A rocket works by transferring momentum to its propellant. At a fixed exhaust velocity, this will be a fixed amount of momentum per unit of propellant. For a given mass of rocket (including remaining propellant), this implies a fixed change in velocity per unit of propellant. Because kinetic energy = 
  
    
      
        
          
            
              
                
                  
                    
                      1
                      2
                    
                  
                  m
                  
                    v
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{smallmatrix}{\frac {1}{2}}mv^{2}\end{smallmatrix}}}
  , this change in velocity imparts a greater increase in kinetic energy at a high velocity than it would at a low velocity. For example, considering a 2kg rocket: 

at 1 m/s, adding 1 m/s increases the  kinetic energy from 1J to 4J, for a gain of 3J.
at 10 m/s, starting with a kinetic energy of 100J, the rocket ends with 121J, for a net gain of 21J.This greater change in kinetic energy can then carry the rocket higher in the gravity well than if the propellant were burned at a lower speed.


== Description in terms of work ==
Rocket engines produce the same force regardless of their velocity. A rocket acting on a fixed object, as in a static firing, does no useful work at all; the rocket's stored energy is entirely expended on accelerating its propellant in the form of exhaust. But when the rocket moves, its thrust acts through the distance it moves. Force multiplied by distance is the definition of mechanical energy or work. So the farther the rocket and payload move during the burn (i.e. the faster they move), the greater the kinetic energy imparted to the rocket and its payload and the less to its exhaust.
This is shown as follows. The mechanical work done on the rocket (
  
    
      
        W
      
    
    {\displaystyle W}
  ) is defined as the dot product of the force of the engine's thrust (
  
    
      
        
          
            
              F
              →
            
          
        
      
    
    {\displaystyle {\vec {F}}}
  ) and the displacement it travels during the burn (
  
    
      
        
          
            
              s
              →
            
          
        
      
    
    {\displaystyle {\vec {s}}}
  ):

  
    
      
        W
        =
        
          
            
              F
              →
            
          
        
        ⋅
        
          
            
              s
              →
            
          
        
        .
      
    
    {\displaystyle W={\vec {F}}\cdot {\vec {s}}.}
  If the burn is made in the prograde direction, 
  
    
      
        
          
            
              F
              →
            
          
        
        ⋅
        
          
            
              s
              →
            
          
        
        =
        ‖
        F
        ‖
        ⋅
        ‖
        s
        ‖
        =
        F
        ⋅
        s
      
    
    {\displaystyle {\vec {F}}\cdot {\vec {s}}=\|F\|\cdot \|s\|=F\cdot s}
  . The work results in a change in kinetic energy

  
    
      
        Δ
        
          E
          
            k
          
        
        =
        F
        ⋅
        s
        .
      
    
    {\displaystyle \Delta E_{k}=F\cdot s.}
  Differentiating with respect to time, we obtain 

  
    
      
        
          
            
              
                d
              
              
                E
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        F
        ⋅
        
          
            
              
                d
              
              s
            
            
              
                d
              
              t
            
          
        
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} E_{k}}{\mathrm {d} t}}=F\cdot {\frac {\mathrm {d} s}{\mathrm {d} t}},}
  or

  
    
      
        
          
            
              
                d
              
              
                E
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        F
        ⋅
        v
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} E_{k}}{\mathrm {d} t}}=F\cdot v,}
  where 
  
    
      
        v
      
    
    {\displaystyle v}
   is the velocity. Dividing by the instantaneous mass 
  
    
      
        m
      
    
    {\displaystyle m}
   to express this in terms of specific energy (
  
    
      
        
          e
          
            k
          
        
      
    
    {\displaystyle e_{k}}
  ), we get 

  
    
      
        
          
            
              
                d
              
              
                e
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        
          
            F
            m
          
        
        ⋅
        v
        =
        a
        ⋅
        v
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} e_{k}}{\mathrm {d} t}}={\frac {F}{m}}\cdot v=a\cdot v,}
  where 
  
    
      
        a
      
    
    {\displaystyle a}
   is the acceleration vector.
Thus it can be readily seen that the rate of gain of specific energy of every part of the rocket is proportional to speed and, given this, the equation can be integrated (numerically or otherwise) to calculate the overall increase in specific energy of the rocket.


== Impulsive burn ==
Integrating the above energy equation is often unnecessary if the burn duration is short. Short burns of chemical rocket engines close to periapsis or elsewhere are usually mathematically modelled as impulsive burns, where the force of the engine dominates any other forces that might change the vehicle's energy over the burn.
For example, as a vehicle falls towards periapsis in any orbit (closed or escape orbits) the velocity relative to the central body increases. Briefly burning the engine (an “impulsive burn”) prograde at periapsis increases the velocity by the same increment as at any other time (
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  ). However, since the vehicle's kinetic energy is related to the square of its velocity, this increase in velocity has a non-linear effect on the vehicle's kinetic energy, leaving it with higher energy than if the burn were achieved at any other time.


=== Oberth calculation for a parabolic orbit ===
If an impulsive burn of Δv is performed at periapsis in a parabolic orbit, then the velocity at periapsis before the burn is equal to the escape velocity (Vesc), and the specific kinetic energy after the burn is

  
    
      
        
          
            
              
                
                  e
                  
                    k
                  
                
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                
                  V
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                (
                
                  V
                  
                    esc
                  
                
                +
                Δ
                v
                
                  )
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                
                  V
                  
                    esc
                  
                  
                    2
                  
                
                +
                Δ
                v
                
                  V
                  
                    esc
                  
                
                +
                
                  
                    
                      1
                      2
                    
                  
                
                Δ
                
                  v
                  
                    2
                  
                
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}e_{k}&={\tfrac {1}{2}}V^{2}\\&={\tfrac {1}{2}}(V_{\text{esc}}+\Delta v)^{2}\\&={\tfrac {1}{2}}V_{\text{esc}}^{2}+\Delta vV_{\text{esc}}+{\tfrac {1}{2}}\Delta v^{2},\end{aligned}}}
  where 
  
    
      
        V
        =
        
          V
          
            esc
          
        
        +
        Δ
        v
      
    
    {\displaystyle V=V_{\text{esc}}+\Delta v}
  .
When the vehicle leaves the gravity field, the loss of specific kinetic energy is

  
    
      
        
          
            
              1
              2
            
          
        
        
          V
          
            esc
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\tfrac {1}{2}}V_{\text{esc}}^{2},}
  so it retains the energy

  
    
      
        Δ
        v
        
          V
          
            esc
          
        
        +
        
          
            
              1
              2
            
          
        
        Δ
        
          v
          
            2
          
        
        ,
      
    
    {\displaystyle \Delta vV_{\text{esc}}+{\tfrac {1}{2}}\Delta v^{2},}
  which is larger than the energy from a burn outside the gravitational field (
  
    
      
        
          
            
              1
              2
            
          
        
        Δ
        
          v
          
            2
          
        
      
    
    {\displaystyle {\tfrac {1}{2}}\Delta v^{2}}
  ) by

  
    
      
        Δ
        v
        
          V
          
            esc
          
        
        .
      
    
    {\displaystyle \Delta vV_{\text{esc}}.}
  When the vehicle has left the gravity well, it is travelling at a speed

  
    
      
        V
        =
        Δ
        v
        
          
            1
            +
            
              
                
                  2
                  
                    V
                    
                      esc
                    
                  
                
                
                  Δ
                  v
                
              
            
          
        
        .
      
    
    {\displaystyle V=\Delta v{\sqrt {1+{\frac {2V_{\text{esc}}}{\Delta v}}}}.}
  For the case where the added impulse  Δv is small compared to escape velocity, the 1 can be ignored, and the effective Δv of the impulsive burn can be seen to be multiplied by a factor of simply

  
    
      
        
          
            
              
                2
                
                  V
                  
                    esc
                  
                
              
              
                Δ
                v
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {2V_{\text{esc}}}{\Delta v}}}}
  and one get 

  
    
      
        V
      
    
    {\displaystyle V}
   ≈ 
  
    
      
        
          
            
              2
              
                V
                
                  esc
                
              
            
            
              Δ
              v
            
          
        
        .
      
    
    {\displaystyle {\sqrt {{2V_{\text{esc}}}{\Delta v}}}.}
  Similar effects happen in closed and hyperbolic orbits.


=== Parabolic example ===
If the vehicle travels at velocity v at the start of a burn that changes the velocity by Δv, then the change in specific orbital energy (SOE) due to the new orbit is

  
    
      
        v
        
        Δ
        v
        +
        
          
            
              1
              2
            
          
        
        (
        Δ
        v
        
          )
          
            2
          
        
        .
      
    
    {\displaystyle v\,\Delta v+{\tfrac {1}{2}}(\Delta v)^{2}.}
  Once the spacecraft is far from the planet again, the SOE is entirely kinetic, since gravitational potential energy approaches zero. Therefore, the larger the v at the time of the burn, the greater the final kinetic energy, and the higher the final velocity.
The effect becomes more pronounced the closer to the central body, or more generally, the deeper in the gravitational field potential in which the burn occurs, since the velocity is higher there.
So if a spacecraft is on a parabolic flyby of Jupiter with a periapsis velocity of 50 km/s and performs a 5 km/s burn, it turns out that the final velocity change at great distance is 22.9 km/s, giving a multiplication of the burn by 4.58 times.


== Paradox ==
It may seem that the rocket is getting energy for free, which would violate conservation of energy. However, any gain to the rocket's kinetic energy is balanced by a relative decrease in the kinetic energy the exhaust is left with (the kinetic energy of the exhaust may still increase, but it does not increase as much). Contrast this to the situation of static firing, where the speed of the engine is fixed at zero. This means that its kinetic energy does not increase at all, and all the chemical energy released by the fuel is converted to the exhaust's kinetic energy (and heat).
At very high speeds the mechanical power imparted to the rocket can exceed the total power liberated in the combustion of the propellant; this may also seem to violate conservation of energy. But the propellants in a fast-moving rocket carry energy not only chemically, but also in their own kinetic energy, which at speeds above a few kilometres per second exceed the chemical component. When these propellants are burned, some of this kinetic energy is transferred to the rocket along with the chemical energy released by burning.The Oberth effect can therefore partly make up for what is extremely low efficiency early in the rocket's flight when it is moving only slowly. Most of the work done by a rocket early in flight is ""invested"" in the kinetic energy of the propellant not yet burned, part of which they will release later when they are burned.


== See also ==

Bi-elliptic transfer
Gravity assist
Propulsive efficiency


== References ==


== External links ==
Oberth effect
Explanation of the effect by Geoffrey Landis.Rocket propulsion, classical relativity, and the Oberth effectAnimation (MP4) of the Oberth effect in orbit from the Blanco and Mungan paper cited above.","pandas(index=80, _1=80, text='in astronautics, a powered flyby, or oberth maneuver, is a maneuver in which a spacecraft falls into a gravitational well and then uses its engines to further accelerate as it is falling, thereby achieving additional speed. the resulting maneuver is a more efficient way to gain kinetic energy than applying the same impulse outside of a gravitational well. the gain in efficiency is explained by the oberth effect, wherein the use of a reaction engine at higher speeds generates a greater change in mechanical energy than its use at lower speeds. in practical terms, this means that the most energy-efficient method for a spacecraft to burn its fuel is at the lowest possible orbital periapsis, when its orbital velocity (and so, its kinetic energy) is greatest. in some cases, it is even worth spending fuel on slowing the spacecraft into a gravity well to take advantage of the efficiencies of the oberth effect. the maneuver and effect are named after the person who first described them in 1927, hermann oberth, an austro-hungarian-born german physicist and a founder of modern rocketry.the oberth effect is strongest at a point in orbit known as the periapsis, where the gravitational potential is lowest, and the speed is highest. this is because a given firing of a rocket engine at high speed causes a greater change in kinetic energy than when fired otherwise similarly at lower speed. because the vehicle remains near periapsis only for a short time, for the oberth maneuver to be most effective the vehicle must be able to generate as much impulse as possible in the shortest possible time. as a result the oberth maneuver is much more useful for high-thrust rocket engines like liquid-propellant rockets, and less useful for low-thrust reaction engines such as ion drives, which take a long time to gain speed. the oberth effect also can be used to understand the behavior of multi-stage rockets: the upper stage can generate much more usable kinetic energy than the total chemical energy of the propellants it carries.in terms of the energies involved, the oberth effect is more effective at higher speeds because at high speed the propellant has significant kinetic energy in addition to its chemical potential energy. at higher speed the vehicle is able to employ the greater change (reduction) in kinetic energy of the propellant (as it is exhausted backwards and hence at reduced speed and hence reduced kinetic energy) to generate a greater increase in kinetic energy of the vehicle.   == explanation in terms of momentum and kinetic energy == a rocket works by transferring momentum to its propellant. at a fixed exhaust velocity, this will be a fixed amount of momentum per unit of propellant. for a given mass of rocket (including remaining propellant), this implies a fixed change in velocity per unit of propellant. because kinetic energy =           1 2   m  v  2           once the spacecraft is far from the planet again, the soe is entirely kinetic, since gravitational potential energy approaches zero. therefore, the larger the v at the time of the burn, the greater the final kinetic energy, and the higher the final velocity. the effect becomes more pronounced the closer to the central body, or more generally, the deeper in the gravitational field potential in which the burn occurs, since the velocity is higher there. so if a spacecraft is on a parabolic flyby of jupiter with a periapsis velocity of 50 km/s and performs a 5 km/s burn, it turns out that the final velocity change at great distance is 22.9 km/s, giving a multiplication of the burn by 4.58 times.   == paradox == it may seem that the rocket is getting energy for free, which would violate conservation of energy. however, any gain to the rocket\'s kinetic energy is balanced by a relative decrease in the kinetic energy the exhaust is left with (the kinetic energy of the exhaust may still increase, but it does not increase as much). contrast this to the situation of static firing, where the speed of the engine is fixed at zero. this means that its kinetic energy does not increase at all, and all the chemical energy released by the fuel is converted to the exhaust\'s kinetic energy (and heat). at very high speeds the mechanical power imparted to the rocket can exceed the total power liberated in the combustion of the propellant; this may also seem to violate conservation of energy. but the propellants in a fast-moving rocket carry energy not only chemically, but also in their own kinetic energy, which at speeds above a few kilometres per second exceed the chemical component. when these propellants are burned, some of this kinetic energy is transferred to the rocket along with the chemical energy released by burning.the oberth effect can therefore partly make up for what is extremely low efficiency early in the rocket\'s flight when it is moving only slowly. most of the work done by a rocket early in flight is ""invested"" in the kinetic energy of the propellant not yet burned, part of which they will release later when they are burned.   == see also ==  bi-elliptic transfer gravity assist propulsive efficiency   == references ==   == external links == oberth effect explanation of the effect by geoffrey landis.rocket propulsion, classical relativity, and the oberth effectanimation (mp4) of the oberth effect in orbit from the blanco and mungan paper cited above.')"
81,"GPS/INS is the use of GPS satellite signals to correct or calibrate a solution from an inertial navigation system (INS). The method is applicable for any GNSS/INS system.


== Overview ==


=== GPS/INS method ===
The GPS gives an absolute drift-free position value that can be used to reset the INS solution or can be blended with it by use of a mathematical algorithm, such as a Kalman filter. The angular orientation of the unit can be inferred from the series of position updates from the GPS. The change in the error in position relative to the GPS can be used to estimate the unknown angle error.
The benefits of using GPS with an INS are that the INS may be calibrated by the GPS signals and that the INS can provide position and angle updates at a quicker rate than GPS. For high dynamic vehicles, such as missiles and aircraft, INS fills in the gaps between GPS positions. Additionally, GPS may lose its signal and the INS can continue to compute the position and angle during the period of lost GPS signal. The two systems are complementary and are often employed together.


== Applications ==
GPS/INS is commonly used on aircraft for navigation purposes. Using GPS/INS allows for smoother position and velocity estimates that can be provided at a sampling rate faster than the GPS receiver.  This also allows for accurate estimation of the aircraft attitude (roll, pitch, and yaw) angles.  In general, GPS/INS sensor fusion is a nonlinear filtering problem, which is commonly approached using the extended Kalman filter (EKF) or the unscented Kalman filter (UKF).  The use of these two filters for GPS/INS has been compared in various sources, including a detailed sensitivity analysis. The EKF uses an analytical linearization approach using Jacobian matrices to linearize the system, while the UKF uses a statistical linearization approach called the unscented transform which uses a set of deterministically selected points to handle the nonlinearity. The UKF requires the calculation of a matrix square root of the state error covariance matrix, which is used to determine the spread of the sigma points for the unscented transform. There are various ways to calculate the matrix square root, which have been presented and compared within GPS/INS application. From this work it is recommended to use the Cholesky decomposition method.
In addition to aircraft applications, GPS/INS has also been studied for automobile applications such as autonomous navigation,  vehicle dynamics control, or sideslip, roll, and tire cornering stiffness estimation.


== See also ==
GNSS Augmentation


== References ==
US Patent No. 6900760","pandas(index=81, _1=81, text='gps/ins is the use of gps satellite signals to correct or calibrate a solution from an inertial navigation system (ins). the method is applicable for any gnss/ins system.   == overview == the gps gives an absolute drift-free position value that can be used to reset the ins solution or can be blended with it by use of a mathematical algorithm, such as a kalman filter. the angular orientation of the unit can be inferred from the series of position updates from the gps. the change in the error in position relative to the gps can be used to estimate the unknown angle error. the benefits of using gps with an ins are that the ins may be calibrated by the gps signals and that the ins can provide position and angle updates at a quicker rate than gps. for high dynamic vehicles, such as missiles and aircraft, ins fills in the gaps between gps positions. additionally, gps may lose its signal and the ins can continue to compute the position and angle during the period of lost gps signal. the two systems are complementary and are often employed together.   == applications == gps/ins is commonly used on aircraft for navigation purposes. using gps/ins allows for smoother position and velocity estimates that can be provided at a sampling rate faster than the gps receiver.  this also allows for accurate estimation of the aircraft attitude (roll, pitch, and yaw) angles.  in general, gps/ins sensor fusion is a nonlinear filtering problem, which is commonly approached using the extended kalman filter (ekf) or the unscented kalman filter (ukf).  the use of these two filters for gps/ins has been compared in various sources, including a detailed sensitivity analysis. the ekf uses an analytical linearization approach using jacobian matrices to linearize the system, while the ukf uses a statistical linearization approach called the unscented transform which uses a set of deterministically selected points to handle the nonlinearity. the ukf requires the calculation of a matrix square root of the state error covariance matrix, which is used to determine the spread of the sigma points for the unscented transform. there are various ways to calculate the matrix square root, which have been presented and compared within gps/ins application. from this work it is recommended to use the cholesky decomposition method. in addition to aircraft applications, gps/ins has also been studied for automobile applications such as autonomous navigation,  vehicle dynamics control, or sideslip, roll, and tire cornering stiffness estimation.   == see also == gnss augmentation   == references == us patent no. 6900760')"
82,"Hypersonic flight is flight through the atmosphere below about 90 km at speeds above Mach 5, a speed where dissociation of air begins to become significant and high heat loads exist.


== History ==
The first manufactured object to achieve hypersonic flight was the two-stage Bumper rocket, consisting of a WAC Corporal second stage set on top of a V-2 first stage. In February 1949, at White Sands, the rocket reached a speed of 8,288.12 km/h (5,150 mph), or approximately Mach 6.7. The vehicle, however, burned on atmospheric re-entry, and only charred remnants were found. In April 1961, Russian Major Yuri Gagarin became the first human to travel at hypersonic speed, during the world's first piloted orbital flight. Soon after, in May 1961, Alan Shepard became the first American and second person to achieve hypersonic flight when his capsule reentered the atmosphere at a speed above Mach 5 at the end of his suborbital flight over the Atlantic Ocean.In November 1961, Air Force Major Robert White flew the X-15 research airplane at speeds over Mach 6.
On 3 October 1967, in California, a X-15 reached Mach 6.7, but by the time the vehicle approached Edwards Air Force Base, intense heating associated with shock waves around the vehicle had partially melted the pylon that attached the ramjet engine to the fuselage.The reentry problem of a space vehicle was extensively studied. 
The NASA X-43A flew on scramjet for 10 seconds, and then glided for 10 minutes on its last flight in 2004.
The Boeing X-51 Waverider flew on scramjet for 210 seconds in 2013, finally reaching Mach 5.1 on its fourth flight test.
The hypersonic regime has since become the subject for further study during the 21st century, and strategic competition between China, India, Russia, and the U.S.


== Physics ==

The stagnation point of air flowing around a body is a point where its local velocity is zero. At this point the air flows around this location. A shock wave forms, which deflects the air from the stagnation point and insulates the flight body from the atmosphere. This can affect the lifting ability of a flight surface to counteract its drag and subsequent free fall. Ning describes a method for interrelating Reynolds number with Mach number.In order to maneuver in the atmosphere at faster speeds than supersonic, the forms of propulsion can still be airbreathing systems, but a ramjet no longer suffices for a system to attain Mach 5, as a ramjet slows down the airflow to subsonic. Some systems (waveriders) use a first stage rocket to boost a body into the hypersonic regime. Other systems (boost-glide vehicles) use scramjets after their initial boost, in which the speed of the air passing through the scramjet remains supersonic. Other systems (munitions) use a cannon for their initial boost.


=== High Temperature Effect ===
Hypersonic flow is a high energy flow. The ratio of kinetic energy to the internal energy of the gas increases as the square of the Mach number. When this flow enters a boundary layer, there are high viscous effects due to the friction between air and the high-speed object. In this case, the high kinetic energy is converted in part to internal energy and gas energy is proportional to the internal energy. Therefore, hypersonic boundary layers are high temperature regions due to the viscous dissipation of the flow's kinetic energy. 
Another region of high temperature flow is the shock layer behind the strong bow shock wave. In the case of the shock layer, the flows velocity decreases discontinuously as it passes through the shock wave. This results in a loss of kinetic energy and a gain of internal energy behind the shock wave. Due to high temperatures behind the shock wave, dissociation of molecules in the air becomes thermally active. For example, for air at T > 2000 K, dissociation of diatomic oxygen into oxygen radicals is active: O2 → 2OFor T > 4000 K, dissociation of diatomic nitrogen into N radicals is active: N2 → 2NConsequently, in this temperature range, molecular dissociation followed by recombination of oxygen and nitrogen radicals produces nitric oxide: N2 + O2 → 2NO, which then dissociates and recombines to form ions: N + O → NO+ + e−


=== Low Density Flow ===
At standard sea-level condition for air, the mean free path of air molecules is about 
  
    
      
        λ
        =
        68
        
          n
          m
        
      
    
    {\displaystyle \lambda =68\mathrm {nm} }
  . Low density air is much thinner. At an altitude of 104 km (342,000 ft) the mean free path is 
  
    
      
        λ
        =
        1
        
        f
        t
        =
        0.305
        
        m
      
    
    {\displaystyle \lambda =1\,ft=0.305\,m}
  . Because of this large free mean path aerodynamic concepts, equations, and results based on the assumption of a continuum begin to break down, therefore aerodynamics must be considered from kinetic theory. This regime of aerodynamics is called low-density flow.
For a given aerodynamic condition low-density effects depends on the value of a nondimensional parameter called the Knudsen number 
  
    
      
        
          K
          
            n
          
        
      
    
    {\displaystyle K_{n}}
  , defined as 
  
    
      
        
          K
          
            n
          
        
        =
        
          
            λ
            l
          
        
      
    
    {\displaystyle K_{n}={\frac {\lambda }{l}}}
   where 
  
    
      
        l
      
    
    {\displaystyle l}
   is the typical length scale of the object considered. The value of the Knudsen number based on nose radius, 
  
    
      
        
          K
          
            n
          
        
        =
        
          
            λ
            R
          
        
      
    
    {\displaystyle K_{n}={\frac {\lambda }{R}}}
  , can be near one.
Hypersonic vehicles frequently fly at very high altitudes and therefore encounter low-density conditions. Hence, the design and analysis of hypersonic vehicles sometimes require consideration of low-density flow. New generations of hypersonic airplanes may spend a considerable portion of their mission at high altitudes, and for these vehicles, low-density effects will become more significant.


=== Thin Shock Layer ===
The flow field between the shock wave and the body surface is called the shock layer. As the Mach number M increases, the angle of the resulting shock wave decreases. This Mach angle is described by the equation 
  
    
      
        μ
        =
        
          sin
          
            −
            1
          
        
        ⁡
        (
        a
        
          /
        
        v
        )
      
    
    {\displaystyle \mu =\sin ^{-1}(a/v)}
   where a is the speed of the sound wave and v is the flow velocity. Since M=v/a, the equation becomes 
  
    
      
        μ
        =
        
          sin
          
            −
            1
          
        
        ⁡
        (
        1
        
          /
        
        M
        )
      
    
    {\displaystyle \mu =\sin ^{-1}(1/M)}
  . Higher Mach numbers position the shock wave closer to the body surface, thus at hypersonic speeds, the shock wave lies extremely close to the body surface, resulting in a thin shock layer. At low Reynolds number, the boundary layer grows quite thick and merges with the shock wave, leading to a fully viscous shock layer.


=== Viscous Interaction ===
The compressible flow boundary layer increases proportionately to the square of the Mach number, and inversely to the square root of the Reynolds number.
At hypersonic speeds, this effect becomes much more pronounced, due to the exponential reliance on the Mach number. Since the boundary layer becomes so large, it interacts more viscously with the surrounding flow. The overall effect of this interaction is to create a much higher skin friction than normal, causing greater surface heat flow. Additionally, the surface pressure spikes, which results in a much larger aerodynamic drag coefficient. This effect is extreme at the leading edge and decreases as a function of length along the surface.


=== Entropy Layer ===
The entropy layer is a region of large velocity gradients caused by the strong curvature of the shock wave. The entropy layer begins at the nose of the aircraft and extends downstream close to the body surface. Downstream of the nose, the entropy layer interacts with the boundary layer which causes an increase in aerodynamic heating at the body surface. Although the shock wave at the nose at supersonic speeds is also curved, the entropy layer is only observed at hypersonic speeds because the magnitude of the curve is far greater at hypersonic speeds.


== Hypersonic weapons development ==

In the last year, China has tested more hypersonic weapons than we have in a decade. We've got to fix that.
Two main types of hypersonic weapons are hypersonic cruise missiles and hypersonic glide vehicles. Hypersonic weapons, by definition, travel five or more times the speed of sound. Hypersonic cruise missiles, which are powered by scramjet, are restricted below 100,000 feet; hypersonic glide vehicles can travel higher. Compared to a ballistic (parabolic) trajectory, a hypersonic vehicle would be capable of large-angle deviations from a parabolic trajectory. According to CNBC, Russia and China lead in hypersonic weapon development, trailed by the United States. India is also developing such weapons. France and Australia may also be pursuing the technology. Japan is acquiring both scramjet (Hypersonic Cruise Missile), and boost-glide weapons (Hyper Velocity Gliding Projectile).Waverider hypersonic weapons delivery is an avenue of development. 
China's XingKong-2 (星空二号, Starry-sky-2), a waverider, had its first flight 3 August 2018.In 2016, Russia is believed to have conducted two successful tests of Avangard, a hypersonic glide vehicle. The third known test, in 2017, failed. In 2018, an Avangard was launched at the Dombarovskiy missile base, reaching its target at the Kura shooting range, a distance of 3700 miles (5955 km).  Avangard uses new composite materials which are to withstand temperatures of up to 2,000 degrees Celsius (3,632 degrees Fahrenheit). The Avangard's environment at hypersonic speeds reaches such temperatures. Russia considered its carbon fiber solution to be unreliable, and replaced it with composite materials. Two Avangard hypersonic glide vehicles (HGVs) will first be mounted on SS-19 ICBMs; on 27 December 2019 the weapon was first fielded to the Yasnensky Missile Division, a unit in the Orenburg Oblast. In an earlier report, Franz-Stefan Gady named the unit as the 13th Regiment/Dombarovskiy Division (Strategic Missile Force).These tests have prompted US responses in weapons development per John Hyten's USSTRATCOM statement 05:03, 8 August 2018 (UTC). At least one vendor is developing ceramics to handle the temperatures of hypersonics systems. There are over a dozen US hypersonics projects as of 2018, notes the commander of USSTRATCOM; from which a future hypersonic cruise missile is sought, perhaps by Q4 FY2021. There are also privately developed hypersonic systems. 
DoD tested a Common Hypersonic Glide Body (C-HGB) in 2020.According to Air Force chief scientist, Dr. Greg Zacharias, the US anticipates having hypersonic weapons by the 2020s, hypersonic drones by the 2030s, and recoverable hypersonic drone aircraft by the 2040s. The focus of DoD development will be on air-breathing boost-glide hypersonics systems. Countering hypersonic weapons during their cruise phase will require radar with longer range, as well as space-based sensors, and systems for tracking and fire control.Rand Corporation (28 September 2017) estimates there is less than a decade to prevent Hypersonic Missile proliferation. 
In the same way that anti-ballistic missiles were developed as countermeasures to ballistic missiles, counter-countermeasures to hypersonics systems were not yet in development, as of 2019. But by 2019, $157.4 million was allocated in the FY2020 Pentagon budget for hypersonic defense, out of $2.6 billion for all hypersonic-related research. Both the US and Russia withdrew from the Intermediate-Range Nuclear Forces (INF) Treaty in February 2019. This will spur arms development, including hypersonic weapons, in FY2021 and forward.Australia and the US have begun joint development of air-launched hypersonic missiles, as announced by a Pentagon statement on 30 November 2020. The development will build on the $54 million Hypersonic International Flight Research Experimentation (HIFiRE) under which both nations collaborated on over a 15-year period. Small and large companies will all contribute to the development of these hypersonic missiles.


== Flown aircraft ==


=== Hypersonic aircraft ===
 Aerojet General X-8
 North American X-15 (crewed)
 Lockheed X-17
 NASA X-43
 Boeing X-51
 DF-ZF
 Avangard
 HSTDV


=== Spaceplanes ===
 Space Shuttle orbiter (crewed)
 Buran (human-rated, only flew without crew)
 RLV-TD
 Boeing X-37
 Shenlong
 IXV
 BOR-4
 Martin X-23 PRIME
 Martin X-24 (crewed)
 ASSET
 HYFLEX
 Chongfu Shiyong Shiyan Hangtian Qi (disputed)
 Jiageng-1


== Cancelled aircraft ==


=== Hypersonic aircraft ===
 Silbervogel (Sänger bomber)
 Keldysh bomber
 Tupolev Tu-360, follow-on to Tu-160
 Tupolev Tu-2000
 Lockheed L-301


=== Spaceplanes ===
 Boeing X-20 Dyna-Soar
 Rockwell X-30 (National Aerospace Plane)
 Orbital Sciences X-34
 Mikoyan-Gurevich MiG-105
 Tsien Spaceplane 1949
 HOPE-X
 XCOR Lynx
 Lockheed Martin X-33
 Hermes
 Prometheus
 HL-20 Personnel Launch System
 HL-42
 BAC Mustard
 Kliper
 HOTOL
 Valier Raketenschiff
 Rockwell C-1057


== Developing and proposed aircraft ==


=== Hypersonic aircraft ===
 I-Plane
 14-X
 Avatar (spacecraft)
 Advanced Technology Vehicle
 DARPA XS-1
 Dream Chaser
 NASA X-43
 HyperSoar
 HyperStar hypersonic passenger airliner
 Falcon HTV-2
 Boeing Commercial Airplanes hypersonic airliner Concept
 Lockheed Martin SR-72
 Tactical Boost Glide Vehicle
 Kholod
 Programme for Reusable In-orbit Demonstrator in Europe (PRIDE)
 Sänger II
 HyShot
 Hytex
 Horus
 SHEFEX
 Skylon
 Reaction Engines A2
 Spartan
 HEXAFLY
 SpaceLiner
 STRATOFLY
 Zero Emission Hyper Sonic Transport


=== Cruise missiles and warheads ===
 Advanced Hypersonic Weapon
 AGM-183A air launched rapid response weapon (ARRW, pronounced ""arrow"") Telemetry data has been successfully transmitted from ARRW —AGM-183A IMV-2 (Instrumented Measurement Vehicle) to the Point Mugu ground stations. Hundreds of ARRWs are being sought by the Air Force.
 Expendable Hypersonic Air-Breathing Multi-Mission Demonstrator (""Mayhem"") Based on HAWC and HSSW: ""solid rocket-boosted, air-breathing, hypersonic conventional cruise missile"", a follow-on to AGM-183A
 Hypersonic air-breathing weapon concept (HAWC, pronounced ""hawk"") It is easier to put a seeker on an air-breathing vehicle.
 Hypersonic conventional strike weapon (HCSW, pronounced ""hacksaw"") was cancelled 10 Feb 2020. HCSW was one of the boost-glide systems under development by the US.
 Kh-45 (cancelled)
 Avangard
 Kinzhal
 Zircon
 Hypersonic Technology Demonstrator Vehicle
HGV-202F Hypersonic glide vehicle

/ Brahmos-II
 DF-ZF


== See also ==
Hypersonic speed
Supersonic transport
Lifting body
Atmospheric entry
Boost-glide
Scramjet
Ramjet
List of X-planes
Thunderbird 1


== References ==


== External links ==
A comparative analysis of the performance of long-range hypervelocity vehicles
(2016) Joint Air Power Competence Centre (JAPCC)","pandas(index=82, _1=82, text='hypersonic flight is flight through the atmosphere below about 90 km at speeds above mach 5, a speed where dissociation of air begins to become significant and high heat loads exist.   == history == the first manufactured object to achieve hypersonic flight was the two-stage bumper rocket, consisting of a wac corporal second stage set on top of a v-2 first stage. in february 1949, at white sands, the rocket reached a speed of 8,288.12 km/h (5,150 mph), or approximately mach 6.7. the vehicle, however, burned on atmospheric re-entry, and only charred remnants were found. in april 1961, russian major yuri gagarin became the first human to travel at hypersonic speed, during the world\'s first piloted orbital flight. soon after, in may 1961, alan shepard became the first american and second person to achieve hypersonic flight when his capsule reentered the atmosphere at a speed above mach 5 at the end of his suborbital flight over the atlantic ocean.in november 1961, air force major robert white flew the x-15 research airplane at speeds over mach 6. on 3 october 1967, in california, a x-15 reached mach 6.7, but by the time the vehicle approached edwards air force base, intense heating associated with shock waves around the vehicle had partially melted the pylon that attached the ramjet engine to the fuselage.the reentry problem of a space vehicle was extensively studied. the nasa x-43a flew on scramjet for 10 seconds, and then glided for 10 minutes on its last flight in 2004. the boeing x-51 waverider flew on scramjet for 210 seconds in 2013, finally reaching mach 5.1 on its fourth flight test. the hypersonic regime has since become the subject for further study during the 21st century, and strategic competition between china, india, russia, and the u.s.   == physics ==  the stagnation point of air flowing around a body is a point where its local velocity is zero. at this point the air flows around this location. a shock wave forms, which deflects the air from the stagnation point and insulates the flight body from the atmosphere. this can affect the lifting ability of a flight surface to counteract its drag and subsequent free fall. ning describes a method for interrelating reynolds number with mach number.in order to maneuver in the atmosphere at faster speeds than supersonic, the forms of propulsion can still be airbreathing systems, but a ramjet no longer suffices for a system to attain mach 5, as a ramjet slows down the airflow to subsonic. some systems (waveriders) use a first stage rocket to boost a body into the hypersonic regime. other systems (boost-glide vehicles) use scramjets after their initial boost, in which the speed of the air passing through the scramjet remains supersonic. other systems (munitions) use a cannon for their initial boost. advanced hypersonic weapon agm-183a air launched rapid response weapon (arrw, pronounced ""arrow"") telemetry data has been successfully transmitted from arrw —agm-183a imv-2 (instrumented measurement vehicle) to the point mugu ground stations. hundreds of arrws are being sought by the air force. expendable hypersonic air-breathing multi-mission demonstrator (""mayhem"") based on hawc and hssw: ""solid rocket-boosted, air-breathing, hypersonic conventional cruise missile"", a follow-on to agm-183a hypersonic air-breathing weapon concept (hawc, pronounced ""hawk"") it is easier to put a seeker on an air-breathing vehicle. hypersonic conventional strike weapon (hcsw, pronounced ""hacksaw"") was cancelled 10 feb 2020. hcsw was one of the boost-glide systems under development by the us. kh-45 (cancelled) avangard kinzhal zircon hypersonic technology demonstrator vehicle hgv-202f hypersonic glide vehicle  / brahmos-ii df-zf   == see also == hypersonic speed supersonic transport lifting body atmospheric entry boost-glide scramjet ramjet list of x-planes thunderbird 1   == references ==   == external links == a comparative analysis of the performance of long-range hypervelocity vehicles (2016) joint air power competence centre (japcc)')"
83,"In aerodynamics, the normal shock tables are a series of tabulated data listing the various properties before and after the occurrence of a normal shock wave.  With a given upstream Mach number, the post-shock Mach number can be calculated along with the pressure, density, temperature, and stagnation pressure ratios.  Such tables are useful since the equations used to calculate the properties after a normal shock are cumbersome.
The tables below have been calculated using a heat capacity ratio, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , equal to 1.4.  The upstream Mach number, 
  
    
      
        
          M
          
            1
          
        
      
    
    {\displaystyle M_{1}}
  , begins at 1 and ends at 5.  Although the tables could be extended over any range of Mach numbers, stopping at Mach 5 is typical since assuming 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   to be 1.4 over the entire Mach number range leads to errors over 10% beyond Mach 5.


== Normal shock table equations ==
Given an upstream Mach number, 
  
    
      
        
          M
          
            1
          
        
      
    
    {\displaystyle M_{1}}
  , and the ratio of specific heats, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , the post normal shock Mach number, 
  
    
      
        
          M
          
            2
          
        
      
    
    {\displaystyle M_{2}}
  , can be calculated using the equation below.

  
    
      
        
          M
          
            2
          
        
        =
        
          
            
              
                
                  M
                  
                    1
                  
                  
                    2
                  
                
                
                  (
                  
                    γ
                    −
                    1
                  
                  )
                
                +
                2
              
              
                2
                γ
                
                  M
                  
                    1
                  
                  
                    2
                  
                
                −
                
                  (
                  
                    γ
                    −
                    1
                  
                  )
                
              
            
          
        
      
    
    {\displaystyle M_{2}={\sqrt {\frac {M_{1}^{2}\left(\gamma -1\right)+2}{2\gamma M_{1}^{2}-\left(\gamma -1\right)}}}}
  The next equation shows the relationship between the post normal shock pressure, 
  
    
      
        
          p
          
            2
          
        
      
    
    {\displaystyle p_{2}}
  , and the upstream ambient pressure, 
  
    
      
        
          p
          
            1
          
        
      
    
    {\displaystyle p_{1}}
  .

  
    
      
        
          
            
              p
              
                2
              
            
            
              p
              
                1
              
            
          
        
        =
        
          
            
              2
              γ
              
                M
                
                  1
                
                
                  2
                
              
            
            
              γ
              +
              1
            
          
        
        −
        
          
            
              γ
              −
              1
            
            
              γ
              +
              1
            
          
        
      
    
    {\displaystyle {\frac {p_{2}}{p_{1}}}={\frac {2\gamma M_{1}^{2}}{\gamma +1}}-{\frac {\gamma -1}{\gamma +1}}}
  The relationship between the post normal shock density, 
  
    
      
        
          ρ
          
            2
          
        
      
    
    {\displaystyle \rho _{2}}
  , and the upstream ambient density, 
  
    
      
        
          ρ
          
            1
          
        
      
    
    {\displaystyle \rho _{1}}
   is shown next in the tables.

  
    
      
        
          
            
              ρ
              
                2
              
            
            
              ρ
              
                1
              
            
          
        
        =
        
          
            
              
                (
                
                  γ
                  +
                  1
                
                )
              
              
                M
                
                  1
                
                
                  2
                
              
            
            
              
                (
                
                  γ
                  −
                  1
                
                )
              
              
                M
                
                  1
                
                
                  2
                
              
              +
              2
            
          
        
      
    
    {\displaystyle {\frac {\rho _{2}}{\rho _{1}}}={\frac {\left(\gamma +1\right)M_{1}^{2}}{\left(\gamma -1\right)M_{1}^{2}+2}}}
  Next, the equation below shows the relationship between the post normal shock temperature, 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
  , and the upstream ambient temperature, 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
  .

  
    
      
        
          
            
              T
              
                2
              
            
            
              T
              
                1
              
            
          
        
        =
        
          
            
              
                (
                
                  1
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
                )
              
              
                (
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        −
                        1
                      
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                  −
                  1
                
                )
              
            
            
              
                M
                
                  1
                
                
                  2
                
              
              
                (
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        −
                        1
                      
                    
                  
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\frac {T_{2}}{T_{1}}}={\frac {\left(1+{\frac {\gamma -1}{2}}M_{1}^{2}\right)\left({\frac {2\gamma }{\gamma -1}}M_{1}^{2}-1\right)}{M_{1}^{2}\left({\frac {2\gamma }{\gamma -1}}+{\frac {\gamma -1}{2}}\right)}}}
  Finally, the ratio of stagnation pressures is shown below where 
  
    
      
        
          p
          
            01
          
        
      
    
    {\displaystyle p_{01}}
   is the upstream stagnation pressure and 
  
    
      
        
          p
          
            02
          
        
      
    
    {\displaystyle p_{02}}
   occurs after the normal shock.  The ratio of stagnation temperatures remains constant across a normal shock since the process is adiabatic.

  
    
      
        
          
            
              p
              
                02
              
            
            
              p
              
                01
              
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      
                        γ
                        +
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
                
                  1
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
              
            
            )
          
          
            
              γ
              
                γ
                −
                1
              
            
          
        
        
          
            (
            
              
                1
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        +
                        1
                      
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                  −
                  
                    
                      
                        γ
                        −
                        1
                      
                      
                        γ
                        +
                        1
                      
                    
                  
                
              
            
            )
          
          
            
              1
              
                γ
                −
                1
              
            
          
        
      
    
    {\displaystyle {\frac {p_{02}}{p_{01}}}=\left({\frac {{\frac {\gamma +1}{2}}M_{1}^{2}}{1+{\frac {\gamma -1}{2}}M_{1}^{2}}}\right)^{\frac {\gamma }{\gamma -1}}\left({\frac {1}{{\frac {2\gamma }{\gamma +1}}M_{1}^{2}-{\frac {\gamma -1}{\gamma +1}}}}\right)^{\frac {1}{\gamma -1}}}
  


== The normal shock tables (for γ = 1.4) ==


== References ==


== See also ==
Normal shock
Mach number
Compressible flow


== External links ==
University of Cincinnati shock relations calculator","pandas(index=83, _1=83, text='in aerodynamics, the normal shock tables are a series of tabulated data listing the various properties before and after the occurrence of a normal shock wave.  with a given upstream mach number, the post-shock mach number can be calculated along with the pressure, density, temperature, and stagnation pressure ratios.  such tables are useful since the equations used to calculate the properties after a normal shock are cumbersome. the tables below have been calculated using a heat capacity ratio,    γ       == the normal shock tables (for γ = 1.4) ==   == references ==   == see also == normal shock mach number compressible flow   == external links == university of cincinnati shock relations calculator')"
84,"In flight dynamics, longitudinal static stability is the stability of an aircraft in the longitudinal, or pitching, plane under steady flight conditions. This characteristic is important in determining whether a human pilot will be able to control the aircraft in the pitching plane without requiring excessive attention or excessive strength.


== Static stability ==

As any vehicle moves it will be subjected to minor changes in the forces that act on it, and in its speed.

If such a change causes further changes that tend to restore the vehicle to its original speed and orientation, without human or machine input, the vehicle is said to be statically stable. The aircraft has positive stability.
If such a change causes further changes that tend to drive the vehicle away from its original speed and orientation, the vehicle is said to be statically unstable.  The aircraft has negative stability.
If such a change causes no tendency for the vehicle to be restored to its original speed and orientation, and no tendency for the vehicle to be driven away from its original speed and orientation, the vehicle is said to be neutrally stable.  The aircraft has zero stability.For a vehicle to possess positive static stability it is not necessary for its speed and orientation to return to exactly the speed and orientation that existed before the minor change that caused the upset.  It is sufficient that the speed and orientation do not continue to diverge but undergo at least a small change back towards the original speed and orientation.


== Longitudinal stability ==
The longitudinal stability of an aircraft, also called pitch stability, refers to the aircraft's stability in its plane of symmetry, about the lateral axis (the axis along the wingspan). One important aspect of the handling qualities of the aircraft, it is one of the main factors determining the ease with which the pilot is able to maintain trim.If an aircraft is longitudinally stable, a small increase in angle of attack will create a negative (nose-down) pitching moment on the aircraft so that the angle of attack decreases.  Similarly, a small decrease in angle of attack will create a positive (nose-up) pitching moment so that the angle of attack increases.Unlike motion about the other two axes and in the other degrees of freedom of the aircraft (sideslip translation, rotation in roll, rotation in yaw), which are usually heavily coupled, motion in the longitudinal degrees of freedom is planar and can be treated as two-dimensional.


== The pilot's task ==
The pilot of an aircraft with positive longitudinal stability, whether it is a human pilot or an autopilot, has an easy task to fly the aircraft and maintain the desired pitch attitude which, in turn, makes it easy to control the speed, angle of attack and fuselage angle relative to the horizon.  The pilot of an aircraft with negative longitudinal stability has a more difficult task to fly the aircraft.  It will be necessary for the pilot devote more effort, make more frequent inputs to the elevator control, and make larger inputs, in an attempt to maintain the desired pitch attitude.Most successful aircraft have positive longitudinal stability, providing the aircraft's center of gravity lies within the approved range.  Some aerobatic and combat aircraft have low-positive or neutral stability to provide high maneuverability.  Some advanced aircraft have a form of low-negative stability called relaxed stability to provide extra-high maneuverability.


== Center of gravity ==
The longitudinal static stability of an aircraft is significantly influenced by the distance (moment arm or lever arm) between the centre of gravity (c.g.) and the aerodynamic centre of the airplane.  The c.g. is established by the design of the airplane and influenced by its loading, as by payload, passengers, etc.  The aerodynamic centre (a.c.) of the airplane can be located approximately by taking the algebraic sum of the plan-view areas fore and aft of the c.g. multiplied by their blended moment arms and divided by their areas, in a manner analogous to the method of locating the c.g. itself.  In conventional aircraft, this point is aft of, but close to, the one-quarter-chord point of the wing.  In unconventional aircraft, e.g. the Quickie, it is between the two wings because the aft wing is so large.  The pitching moment at the a.c. is typically negative and constant.
The a.c. of an airplane typically does not change with loading or other changes; but the c.g. does, as noted above.  If the c.g. moves forward, the airplane becomes more stable (greater moment arm between the a.c. and the c.g.), and if too far forward will cause the airplane to be difficult for the pilot to bring nose-up as for landing.  If the c.g. is too far aft, the moment arm between it and the a.c. diminishes, reducing the inherent stability of the airplane and in the extreme going negative and rendering the airplane longitudinally unstable; see the diagram below.
Accordingly, the operating handbook for every airplane specifies the range over which the c.g. is permitted to move.  Inside this range, the airplane is considered to be inherently stable, which is to say that it will self-correct longitudinal (pitch) disturbances without pilot input.


== Analysis ==
Near the cruise condition most of the lift force is generated by the wings, with ideally only a small amount generated by the fuselage and tail. We may analyse the longitudinal static stability by considering the aircraft in equilibrium under wing lift, tail force, and weight.  The moment equilibrium condition is called trim, and we are generally interested in the longitudinal stability of the aircraft about this trim condition.

Equating forces in the vertical direction:

  
    
      
        W
        =
        
          L
          
            w
          
        
        +
        
          L
          
            t
          
        
      
    
    {\displaystyle W=L_{w}+L_{t}}
  where W is the weight, 
  
    
      
        
          L
          
            w
          
        
      
    
    {\displaystyle L_{w}}
   is the wing lift and 
  
    
      
        
          L
          
            t
          
        
      
    
    {\displaystyle L_{t}}
   is the tail force.
For a thin airfoil at low angle of attack, the wing lift is proportional to the angle of attack:

  
    
      
        
          L
          
            w
          
        
        =
        q
        
          S
          
            w
          
        
        
          
            
              ∂
              
                C
                
                  L
                
              
            
            
              ∂
              α
            
          
        
        (
        α
        +
        
          α
          
            0
          
        
        )
      
    
    {\displaystyle L_{w}=qS_{w}{\frac {\partial C_{L}}{\partial \alpha }}(\alpha +\alpha _{0})}
  where 
  
    
      
        
          S
          
            w
          
        
      
    
    {\displaystyle S_{w}}
   is the wing area 
  
    
      
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{L}}
   is the (wing) lift coefficient, 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is the angle of attack.  The term 
  
    
      
        
          α
          
            0
          
        
      
    
    {\displaystyle \alpha _{0}}
   is included to account for camber, which results in lift at zero angle of attack. Finally 
  
    
      
        q
      
    
    {\displaystyle q}
   is the dynamic pressure:

  
    
      
        q
        =
        
          
            1
            2
          
        
        ρ
        
          v
          
            2
          
        
      
    
    {\displaystyle q={\frac {1}{2}}\rho v^{2}}
  where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the air density and 
  
    
      
        v
      
    
    {\displaystyle v}
   is the speed.


=== Trim ===
The force from the tailplane is proportional to its angle of attack, including the effects of any elevator deflection and any adjustment the pilot has made to trim-out any stick force.  In addition, the tail is located in the flow field of the main wing, and consequently experiences  downwash, reducing its angle of attack.
In a statically stable aircraft of conventional (tail in rear) configuration, the tailplane force may act upward or downward depending on the design and the flight conditions. In a typical canard aircraft both fore and aft planes are lifting surfaces. The fundamental requirement for static stability is that the aft surface must have greater authority (leverage) in restoring a disturbance than the forward surface has in exacerbating it.  This leverage is a product of moment arm from the center of mass and surface area. Correctly balanced in this way, the partial derivative of pitching moment with respect to changes in angle of attack will be negative: a momentary pitch up to a larger angle of attack makes the resultant pitching moment tend to pitch the aircraft back down. (Here, pitch is used casually for the angle between the nose and the direction of the airflow; angle of attack.) This is the ""stability derivative"" d(M)/d(alpha), described below.
The tail force is, therefore:

  
    
      
        
          L
          
            t
          
        
        =
        q
        
          S
          
            t
          
        
        
          (
          
            
              
                
                  ∂
                  
                    C
                    
                      l
                    
                  
                
                
                  ∂
                  α
                
              
            
            
              (
              
                α
                −
                
                  
                    
                      ∂
                      ϵ
                    
                    
                      ∂
                      α
                    
                  
                
                α
              
              )
            
            +
            
              
                
                  ∂
                  
                    C
                    
                      l
                    
                  
                
                
                  ∂
                  η
                
              
            
            η
          
          )
        
      
    
    {\displaystyle L_{t}=qS_{t}\left({\frac {\partial C_{l}}{\partial \alpha }}\left(\alpha -{\frac {\partial \epsilon }{\partial \alpha }}\alpha \right)+{\frac {\partial C_{l}}{\partial \eta }}\eta \right)}
  where 
  
    
      
        
          S
          
            t
          
        
        
      
    
    {\displaystyle S_{t}\!}
   is the tail area, 
  
    
      
        
          C
          
            l
          
        
        
      
    
    {\displaystyle C_{l}\!}
   is the tail force coefficient, 
  
    
      
        η
        
      
    
    {\displaystyle \eta \!}
   is the elevator deflection, and 
  
    
      
        ϵ
        
      
    
    {\displaystyle \epsilon \!}
   is the downwash angle.
A canard aircraft may have its foreplane rigged at a high angle of incidence, which can be seen in a canard catapult glider from a toy store; the design puts the c.g. well forward, requiring nose-up lift.
Violations of the basic principle are exploited in some high performance ""relaxed static stability"" combat aircraft to enhance agility; artificial stability is supplied by active electronic means.
There are a few classical cases where this favorable response was not achieved, notably in T-tail configurations. A T-tail airplane has a higher horizontal tail that passes through the wake of the wing later (at a higher angle of attack) than a lower tail would, and at this point the wing has already stalled and has a much larger separated wake. Inside the separated wake, the tail sees little to no freestream and loses effectiveness. Elevator control power is also heavily reduced or even lost, and the pilot is unable to easily escape the stall. This phenomenon is known as 'deep stall'.
Taking moments about the center of gravity, the net nose-up moment is:

  
    
      
        M
        =
        
          L
          
            w
          
        
        
          x
          
            g
          
        
        −
        (
        
          l
          
            t
          
        
        −
        
          x
          
            g
          
        
        )
        
          L
          
            t
          
        
        
      
    
    {\displaystyle M=L_{w}x_{g}-(l_{t}-x_{g})L_{t}\!}
  where 
  
    
      
        
          x
          
            g
          
        
        
      
    
    {\displaystyle x_{g}\!}
   is the location of the center of gravity behind the aerodynamic center of the main wing, 
  
    
      
        
          l
          
            t
          
        
        
      
    
    {\displaystyle l_{t}\!}
   is the tail moment arm.
For trim, this moment must be zero.  For a given maximum elevator deflection, there is a corresponding limit on center of gravity position at which the aircraft can be kept in equilibrium.  When limited by control deflection this is known as a 'trim limit'.  In principle trim limits could determine the permissible forwards and rearwards shift of the centre of gravity, but usually it is only the forward cg limit which is determined by the available control, the aft limit is usually dictated by stability.
In a missile context 'trim limit' more usually refers to the maximum angle of attack, and hence lateral acceleration which can be generated.


=== Static stability ===
The nature of stability may be examined by considering the increment in pitching moment with change in angle of attack at the trim condition.  If this is nose up, the aircraft is longitudinally unstable; if nose down it is stable.  Differentiating the moment equation with respect to 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  :

  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
        =
        
          x
          
            g
          
        
        
          
            
              ∂
              
                L
                
                  w
                
              
            
            
              ∂
              α
            
          
        
        −
        (
        
          l
          
            t
          
        
        −
        
          x
          
            g
          
        
        )
        
          
            
              ∂
              
                L
                
                  t
                
              
            
            
              ∂
              α
            
          
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}=x_{g}{\frac {\partial L_{w}}{\partial \alpha }}-(l_{t}-x_{g}){\frac {\partial L_{t}}{\partial \alpha }}}
  Note: 
  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}}
   is a stability derivative.
It is convenient to treat total lift as acting at a distance h ahead of the centre of gravity, so that the moment equation may be written:

  
    
      
        M
        =
        h
        (
        
          L
          
            w
          
        
        +
        
          L
          
            t
          
        
        )
        
      
    
    {\displaystyle M=h(L_{w}+L_{t})\!}
  Applying the increment in angle of attack:

  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
        =
        h
        
          (
          
            
              
                
                  ∂
                  
                    L
                    
                      w
                    
                  
                
                
                  ∂
                  α
                
              
            
            +
            
              
                
                  ∂
                  
                    L
                    
                      t
                    
                  
                
                
                  ∂
                  α
                
              
            
          
          )
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}=h\left({\frac {\partial L_{w}}{\partial \alpha }}+{\frac {\partial L_{t}}{\partial \alpha }}\right)}
  Equating the two expressions for moment increment:

  
    
      
        h
        =
        
          x
          
            g
          
        
        −
        
          l
          
            t
          
        
        
          
            
              
                ∂
                
                  L
                  
                    t
                  
                
              
              
                ∂
                α
              
            
            
              
                
                  
                    ∂
                    
                      L
                      
                        w
                      
                    
                  
                  
                    ∂
                    α
                  
                
              
              +
              
                
                  
                    ∂
                    
                      L
                      
                        t
                      
                    
                  
                  
                    ∂
                    α
                  
                
              
            
          
        
      
    
    {\displaystyle h=x_{g}-l_{t}{\frac {\frac {\partial L_{t}}{\partial \alpha }}{{\frac {\partial L_{w}}{\partial \alpha }}+{\frac {\partial L_{t}}{\partial \alpha }}}}}
  The total lift 
  
    
      
        L
      
    
    {\displaystyle L}
   is the sum of 
  
    
      
        
          L
          
            w
          
        
      
    
    {\displaystyle L_{w}}
   and 
  
    
      
        
          L
          
            t
          
        
      
    
    {\displaystyle L_{t}}
   so the sum in the denominator can be simplified and written as the derivative of the total lift due to angle of attack, yielding:

  
    
      
        h
        =
        
          
            
              x
              
                g
              
            
            c
          
        
        −
        
          (
          
            1
            −
            
              
                
                  ∂
                  ϵ
                
                
                  ∂
                  α
                
              
            
          
          )
        
        
          
            
              
                ∂
                
                  C
                  
                    l
                  
                
              
              
                ∂
                α
              
            
            
              
                ∂
                
                  C
                  
                    L
                  
                
              
              
                ∂
                α
              
            
          
        
        
          
            
              
                l
                
                  t
                
              
              
                S
                
                  t
                
              
            
            
              c
              
                S
                
                  w
                
              
            
          
        
      
    
    {\displaystyle h={\frac {x_{g}}{c}}-\left(1-{\frac {\partial \epsilon }{\partial \alpha }}\right){\frac {\frac {\partial C_{l}}{\partial \alpha }}{\frac {\partial C_{L}}{\partial \alpha }}}{\frac {l_{t}S_{t}}{cS_{w}}}}
  Where c is the mean aerodynamic chord of the main wing.  The term:

  
    
      
        
          V
          
            t
          
        
        =
        
          
            
              
                l
                
                  t
                
              
              
                S
                
                  t
                
              
            
            
              c
              
                S
                
                  w
                
              
            
          
        
      
    
    {\displaystyle V_{t}={\frac {l_{t}S_{t}}{cS_{w}}}}
  is known as the tail volume ratio.  Its rather complicated coefficient, the ratio of the two lift derivatives, has values in the range of 0.50 to 0.65 for typical configurations, according to Piercy.  Hence the expression for h may be written more compactly, though somewhat approximately, as:

  
    
      
        h
        =
        
          x
          
            g
          
        
        −
        0.5
        c
        
          V
          
            t
          
        
        
      
    
    {\displaystyle h=x_{g}-0.5cV_{t}\!}
  h is known as the static margin.  For stability it must be negative. (However, for consistency of language, the static margin is sometimes taken as 
  
    
      
        −
        h
      
    
    {\displaystyle -h}
  , so that positive stability is associated with positive static margin.)


== Neutral point ==
A mathematical analysis of the longitudinal static stability of a complete aircraft (including horizontal stabilizer) yields the position of center of gravity at which stability is neutral.  This position is called the neutral point.  (The larger the area of the horizontal stabilizer, and the greater the moment arm of the horizontal stabilizer about the aerodynamic center, the further aft is the neutral point.)
The static center of gravity margin (c.g. margin) or static margin is the distance between the center of gravity (or mass) and the neutral point.  It is usually quoted as a percentage of the Mean Aerodynamic Chord.  The center of gravity must lie ahead of the neutral point for positive stability (positive static margin). If the center of gravity is behind the neutral point, the aircraft is longitudinally unstable (the static margin is negative), and active inputs to the control surfaces are required to maintain stable flight.  Some combat aircraft that are controlled by fly-by-wire systems are designed to be longitudinally unstable so they will be highly maneuverable.  Ultimately, the position of the center of gravity relative to the neutral point determines the stability, control forces, and controllability of the vehicle.For a tailless aircraft 
  
    
      
        
          V
          
            t
          
        
        =
        0
      
    
    {\displaystyle V_{t}=0}
  , the neutral point coincides with the aerodynamic center, and so for longitudinal static stability the center of gravity must lie ahead of the aerodynamic center.


== Longitudinal dynamic stability ==
An aircraft's static stability is an important, but not sufficient, measure of its handling characteristics, and whether it can be flown with ease and comfort by a human pilot. In particular, the longitudinal dynamic stability of a statically stable aircraft will determine whether or not it is finally able to return to its original position.


== See also ==
Directional stability
Flight dynamics
Handling qualities


== Notes ==


== References ==
Clancy, L.J. (1975), Aerodynamics, Pitman Publishing Limited, London.  ISBN 0-273-01120-0
Hurt, H.H. Jr, (1960), Aerodynamics for Naval Aviators Chapter 4, A National Flightshop Reprint, Florida.
Irving, F.G. (1966), An Introduction to the Longitudinal Static Stability of Low-Speed Aircraft, Pergamon Press, Oxford, UK.
McCormick, B.W., (1979), Aerodynamics, Aeronautics, and Flight Mechanics, Chapter 8, John Wiley and Sons, Inc., New York NY.
Perkins, C.D. and Hage, R.E., (1949), Airplane Performance Stability and Control, Chapter 5, John Wiley and Sons, Inc., New York NY.
Piercy, N.A.V. (1944), Elementary Aerodynamics, The English Universities Press Ltd., London.
Stengel R F: Flight Dynamics. Princeton University Press 2004, ISBN 0-691-11407-2.","pandas(index=84, _1=84, text='in flight dynamics, longitudinal static stability is the stability of an aircraft in the longitudinal, or pitching, plane under steady flight conditions. this characteristic is important in determining whether a human pilot will be able to control the aircraft in the pitching plane without requiring excessive attention or excessive strength.   == static stability ==  as any vehicle moves it will be subjected to minor changes in the forces that act on it, and in its speed.  if such a change causes further changes that tend to restore the vehicle to its original speed and orientation, without human or machine input, the vehicle is said to be statically stable. the aircraft has positive stability. if such a change causes further changes that tend to drive the vehicle away from its original speed and orientation, the vehicle is said to be statically unstable.  the aircraft has negative stability. if such a change causes no tendency for the vehicle to be restored to its original speed and orientation, and no tendency for the vehicle to be driven away from its original speed and orientation, the vehicle is said to be neutrally stable.  the aircraft has zero stability.for a vehicle to possess positive static stability it is not necessary for its speed and orientation to return to exactly the speed and orientation that existed before the minor change that caused the upset.  it is sufficient that the speed and orientation do not continue to diverge but undergo at least a small change back towards the original speed and orientation.   == longitudinal stability == the longitudinal stability of an aircraft, also called pitch stability, refers to the aircraft\'s stability in its plane of symmetry, about the lateral axis (the axis along the wingspan). one important aspect of the handling qualities of the aircraft, it is one of the main factors determining the ease with which the pilot is able to maintain trim.if an aircraft is longitudinally stable, a small increase in angle of attack will create a negative (nose-down) pitching moment on the aircraft so that the angle of attack decreases.  similarly, a small decrease in angle of attack will create a positive (nose-up) pitching moment so that the angle of attack increases.unlike motion about the other two axes and in the other degrees of freedom of the aircraft (sideslip translation, rotation in roll, rotation in yaw), which are usually heavily coupled, motion in the longitudinal degrees of freedom is planar and can be treated as two-dimensional.   == the pilot\'s task == the pilot of an aircraft with positive longitudinal stability, whether it is a human pilot or an autopilot, has an easy task to fly the aircraft and maintain the desired pitch attitude which, in turn, makes it easy to control the speed, angle of attack and fuselage angle relative to the horizon.  the pilot of an aircraft with negative longitudinal stability has a more difficult task to fly the aircraft.  it will be necessary for the pilot devote more effort, make more frequent inputs to the elevator control, and make larger inputs, in an attempt to maintain the desired pitch attitude.most successful aircraft have positive longitudinal stability, providing the aircraft\'s center of gravity lies within the approved range.  some aerobatic and combat aircraft have low-positive or neutral stability to provide high maneuverability.  some advanced aircraft have a form of low-negative stability called relaxed stability to provide extra-high maneuverability.   == center of gravity == the longitudinal static stability of an aircraft is significantly influenced by the distance (moment arm or lever arm) between the centre of gravity (c.g.) and the aerodynamic centre of the airplane.  the c.g. is established by the design of the airplane and influenced by its loading, as by payload, passengers, etc.  the aerodynamic centre (a.c.) of the airplane can be located approximately by taking the algebraic sum of the plan-view areas fore and aft of the c.g. multiplied by their blended moment arms and divided by their areas, in a manner analogous to the method of locating the c.g. itself.  in conventional aircraft, this point is aft of, but close to, the one-quarter-chord point of the wing.  in unconventional aircraft, e.g. the quickie, it is between the two wings because the aft wing is so large.  the pitching moment at the a.c. is typically negative and constant. the a.c. of an airplane typically does not change with loading or other changes; but the c.g. does, as noted above.  if the c.g. moves forward, the airplane becomes more stable (greater moment arm between the a.c. and the c.g.), and if too far forward will cause the airplane to be difficult for the pilot to bring nose-up as for landing.  if the c.g. is too far aft, the moment arm between it and the a.c. diminishes, reducing the inherent stability of the airplane and in the extreme going negative and rendering the airplane longitudinally unstable; see the diagram below. accordingly, the operating handbook for every airplane specifies the range over which the c.g. is permitted to move.  inside this range, the airplane is considered to be inherently stable, which is to say that it will self-correct longitudinal (pitch) disturbances without pilot input.   == analysis == near the cruise condition most of the lift force is generated by the wings, with ideally only a small amount generated by the fuselage and tail. we may analyse the longitudinal static stability by considering the aircraft in equilibrium under wing lift, tail force, and weight.  the moment equilibrium condition is called trim, and we are generally interested in the longitudinal stability of the aircraft about this trim condition.  equating forces in the vertical direction:     w =  l  wl  t      , the neutral point coincides with the aerodynamic center, and so for longitudinal static stability the center of gravity must lie ahead of the aerodynamic center.   == longitudinal dynamic stability == an aircraft\'s static stability is an important, but not sufficient, measure of its handling characteristics, and whether it can be flown with ease and comfort by a human pilot. in particular, the longitudinal dynamic stability of a statically stable aircraft will determine whether or not it is finally able to return to its original position.   == see also == directional stability flight dynamics handling qualities   == notes ==   == references == clancy, l.j. (1975), aerodynamics, pitman publishing limited, london.  isbn 0-273-01120-0 hurt, h.h. jr, (1960), aerodynamics for naval aviators chapter 4, a national flightshop reprint, florida. irving, f.g. (1966), an introduction to the longitudinal static stability of low-speed aircraft, pergamon press, oxford, uk. mccormick, b.w., (1979), aerodynamics, aeronautics, and flight mechanics, chapter 8, john wiley and sons, inc., new york ny. perkins, c.d. and hage, r.e., (1949), airplane performance stability and control, chapter 5, john wiley and sons, inc., new york ny. piercy, n.a.v. (1944), elementary aerodynamics, the english universities press ltd., london. stengel r f: flight dynamics. princeton university press 2004, isbn 0-691-11407-2.')"
85,"The Malewicki equations (or Fehskens–Malewicki equations) for sub-sonic endo-atmospheric rocket flight describe the maximum altitude and coast time of a vehicle such as a model rocket.  Aerospace engineer and inventor Douglas Malewicki first published them as a technical report by the model rocket company Estes Industries in 1967.


== References ==","pandas(index=85, _1=85, text='the malewicki equations (or fehskens–malewicki equations) for sub-sonic endo-atmospheric rocket flight describe the maximum altitude and coast time of a vehicle such as a model rocket.  aerospace engineer and inventor douglas malewicki first published them as a technical report by the model rocket company estes industries in 1967.   == references ==')"
86,"Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). BME is also traditionally known as ""bioengineering"", but this term has come to also refer to biological engineering. This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy. Also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a Biomedical Equipment Technician (BMET) or as clinical engineering.
Biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.


== Bioinformatics ==

Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.
Bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.


== Biomechanics ==

Biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.


== Biomaterial ==

A biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.


== Biomedical optics ==
Biomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.


== Tissue engineering ==

Tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with BME.
One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.


== Genetic engineering ==

Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.


== Neural engineering ==
Neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.


== Pharmaceutical engineering ==
Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.


== Medical devices ==

This is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
A medical device is intended for use in:

the diagnosis of disease or other conditions
in the cure, mitigation, treatment, or prevention of disease.Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.

Stereolithography is a practical example of medical modeling being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.
Medical devices are regulated and classified (in the US) as follows (see also Regulation):

Class I devices present minimal potential for harm to the user and are often simpler in design than Class II or Class III devices. Devices in this category include tongue depressors, bedpans, elastic bandages, examination gloves, and hand-held surgical instruments and other similar types of common equipment.
Class II devices are subject to special controls in addition to the general controls of Class I devices. Special controls may include special labeling requirements, mandatory performance standards, and postmarket surveillance. Devices in this class are typically non-invasive and include X-ray machines, PACS, powered wheelchairs, infusion pumps, and surgical drapes.
Class III devices generally require premarket approval (PMA) or premarket notification (510k), a scientific review to ensure the device's safety and effectiveness, in addition to the general controls of Class I. Examples include replacement heart valves, hip and knee joint implants, silicone gel-filled breast implants, implanted cerebellar stimulators, implantable pacemaker pulse generators and endosseous (intra-bone) implants.


=== Medical imaging ===

Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly ""view"" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.

Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.


=== Implants ===
An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.


=== Bionics ===

Artificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other tools.


=== Biomedical sensors ===
In recent years biomedical sensors based in microwave technology have gained more attention. Different sensors can be manufactured for specific uses in both diagnosing and monitoring disease conditions, for example microwave sensors can be used as a complementary technique to X-ray to monitor lower extremity trauma. The sensor monitor the dielectric properties and can thus notice change in tissue (bone, muscle, fat etc.) under the skin so when measuring at different times during the healing process the response from the sensor will change as the trauma heals.


== Clinical engineering ==

Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
Their inherent focus on practical implementation of technology has tended to keep them oriented more towards incremental-level redesigns and re configurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a ""bridge"" between the primary designers and the end-users, by combining the perspectives of being both close to the point-of-use, while also trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems. Clinical engineering department is constructed with a manager, supervisor, engineer and technician. One engineer per eighty beds in the hospital is the ratio. Clinical engineers is also authorized audit pharmaceutical and associated stores to monitor FDA recalls of invasive items.


== Rehabilitation engineering ==

Rehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.While some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have an undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility. Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.The rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote the inclusion of their users into the mainstream of society, commerce, and recreation.


== Regulatory issues ==
Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to ""a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death""Regardless of the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe and 2) effective and 3) for all the manufactured devices
A product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, ...) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.
A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecycle.
The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical devices, drugs, biologics, and combination products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for ""consumer"" use, such as physical therapy devices (which are also ""medical"" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K ""clearance"" (typically for Class 2 devices) or pre-market ""approval"" (typically for drugs and class 3 devices).
In the European context, safety effectiveness and quality is ensured through the ""Conformity Assessment"" that is defined as ""the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive"". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.

In the European Union, there are certifying entities named ""Notified Bodies"", accredited by the European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the optimal extent of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.


=== RoHS II ===
Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation ""Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices"" (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.


=== IEC 60601 ===
The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.


=== AS/NZS 3551:2012 ===
AS/ANS 3551:2012 is the Australian and New Zealand standards for the management of medical devices. The standard specifies the procedures required to maintain a wide range of medical assets in a clinical setting (e.g. Hospital). The standards are based on the IEC 606101 standards.
The standard covers a wide range of medical equipment management elements including, procurement, acceptance testing, maintenance (electrical safety and preventive maintenance testing) and decommissioning.


== Training and certification ==


=== Education ===
Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Bachelor's (B.Sc., B.S., B.Eng. or B.S.E.) or Master's (M.S., M.Sc., M.S.E., or M.Eng.) or a doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Sc., B.S., B.Eng. or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as its own discipline rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a ""pre-med"" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.In Canada and Australia, accredited graduate programs in biomedical engineering are common. For example, McMaster University offers an M.A.Sc, an MD/PhD, and a PhD in Biomedical engineering. The first Canadian undergraduate BME program was offered at Ryerson University as a four-year B.Eng. program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering as is Flinders University.As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees is also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.
Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or another engineering discipline (plus certain life science coursework), or life science (plus certain engineering coursework).
Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.


=== Licensure/certification ===

As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but, in US, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of American engineers). The US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division. The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.
The Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.


== Career prospects ==
In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. Biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.


== Notable figures ==
Earl Bakken - Invented the first transistorised pacemaker, co-founder of Medtronic.
Forrest Bird (deceased) – aviator and pioneer in the invention of mechanical ventilators
Y.C. Fung (deceased) – professor emeritus at the University of California, San Diego, considered by many to be the founder of modern biomechanics
Leslie Geddes (deceased) – professor emeritus at Purdue University, electrical engineer, inventor, and educator of over 2000 biomedical engineers, received a National Medal of Technology in 2006 from President George Bush for his more than 50 years of contributions that have spawned innovations ranging from burn treatments to miniature defibrillators, ligament repair to tiny blood pressure monitors for premature infants, as well as a new method for performing cardiopulmonary resuscitation (CPR).
Willem Johan Kolff (deceased) – pioneer of hemodialysis as well as in the field of artificial organs
Robert Langer – Institute Professor at MIT, runs the largest BME laboratory in the world, pioneer in drug delivery and tissue engineering
John Macleod (deceased) – one of the co-discoverers of insulin at Case Western Reserve University.
Alfred E. Mann – Physicist, entrepreneur and philanthropist. A pioneer in the field of Biomedical Engineering.
J. Thomas Mortimer – Emeritus professor of biomedical engineering at Case Western Reserve University. Pioneer in Functional Electrical Stimulation (FES)
Robert M. Nerem – professor emeritus at Georgia Institute of Technology. Pioneer in regenerative tissue, biomechanics, and author of over 300 published works. His works have been cited more than 20,000 times cumulatively.
P. Hunter Peckham – Donnell Professor of Biomedical Engineering and Orthopaedics at Case Western Reserve University. Pioneer in Functional Electrical Stimulation (FES)
Nicholas A. Peppas – Chaired Professor in Engineering, University of Texas at Austin, pioneer in drug delivery, biomaterials, hydrogels and nanobiotechnology.
Robert Plonsey – professor emeritus at Duke University, pioneer of electrophysiology
Otto Schmitt (deceased) – biophysicist with significant contributions to BME, working with biomimetics
Ascher Shapiro (deceased) – Institute Professor at MIT, contributed to the development of the BME field, medical devices (e.g. intra-aortic balloons)
Gordana Vunjak-Novakovic – University Professor at Columbia University, pioneer in tissue engineering and bioreactor design
John G. Webster – professor emeritus at the University of Wisconsin–Madison, a pioneer in the field of instrumentation amplifiers for the recording of electrophysiological signals
Fred Weibell, coauthor of Biomedical Instrumentation and Measurements
U.A. Whitaker (deceased) – provider of the Whitaker Foundation, which supported research and education in BME by providing over $700 million to various universities, helping to create 30 BME programs and helping finance the construction of 13 buildings


== See also ==

Biomedicine – Branch of medical science that applies biological and physiological principles to clinical practice
Cardiophysics
Computational anatomy
Medical physics
Physiome
Biomedical engineering jobs
Biomedical Engineering and Instrumentation Program (BEIP)


== References ==


== Further reading ==
Bronzino, Joseph D. (April 2006). The Biomedical Engineering Handbook (Third ed.). [CRC Press]. ISBN 978-0-8493-2124-5. Archived from the original on 2015-02-24. Retrieved 2009-06-22.
Villafane, Carlos (June 2009). Biomed: From the Student's Perspective (First ed.). [Techniciansfriend.com]. ISBN 978-1-61539-663-4.


== External links ==
 Media related to Biomedical engineering at Wikimedia Commons
Biomedical Engineering at Curlie","pandas(index=86, _1=86, text='biomedical engineering (bme) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). bme is also traditionally known as ""bioengineering"", but this term has come to also refer to biological engineering. this field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy. also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. this involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a biomedical equipment technician (bmet) or as clinical engineering. biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as mris and ekg/ecgs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.   == bioinformatics ==  bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. as an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. common uses of bioinformatics include the identification of candidate genes and nucleotides (snps). often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. in a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.   == biomechanics ==  biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.   == biomaterial ==  a biomaterial is any matter, surface, or construct that interacts with living systems. as a science, biomaterials is about fifty years old. the study of biomaterials is called biomaterials science or biomaterials engineering. it has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.   == biomedical optics == biomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.   == tissue engineering ==  tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with bme. one of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. biomedical engineers are currently researching methods of creating such organs. researchers have grown solid jawbones and tracheas from human stem cells towards this end. several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.   == genetic engineering ==  genetic engineering, recombinant dna technology, genetic modification/manipulation (gm) and gene splicing are terms that apply to the direct manipulation of an organism\'s genes. unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. genetic engineering techniques have found success in numerous applications. some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.   == neural engineering == neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.   == pharmaceutical engineering == pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of chemical engineering, and pharmaceutical analysis. it may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.   == medical devices ==  this is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism. a medical device is intended for use in:  the diagnosis of disease or other conditions in the cure, mitigation, treatment, or prevention of disease.some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.  stereolithography is a practical example of medical modeling being used to create physical objects. beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases. medical devices are regulated and classified (in the us) as follows (see also regulation):  class i devices present minimal potential for harm to the user and are often simpler in design than class ii or class iii devices. devices in this category include tongue depressors, bedpans, elastic bandages, examination gloves, and hand-held surgical instruments and other similar types of common equipment. class ii devices are subject to special controls in addition to the general controls of class i devices. special controls may include special labeling requirements, mandatory performance standards, and postmarket surveillance. devices in this class are typically non-invasive and include x-ray machines, pacs, powered wheelchairs, infusion pumps, and surgical drapes. class iii devices generally require premarket approval (pma) or premarket notification (510k), a scientific review to ensure the device\'s safety and effectiveness, in addition to the general controls of class i. examples include replacement heart valves, hip and knee joint implants, silicone gel-filled breast implants, implanted cerebellar stimulators, implantable pacemaker pulse generators and endosseous (intra-bone) implants. as with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered professional engineer (pe), but, in us, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of american engineers). the us model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. this is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine. biomedical engineering is regulated in some countries, such as australia, but registration is typically only recommended and not required.in the uk, mechanical engineers working in the areas of medical engineering, bioengineering or biomedical engineering can gain chartered engineer status through the institution of mechanical engineers. the institution also runs the engineering in medicine and health division. the institute of physics and engineering in medicine (ipem) has a panel for the accreditation of msc courses in biomedical engineering and chartered engineering status can also be sought through ipem. the fundamentals of engineering exam – the first (and more general) of two licensure examinations for most u.s. jurisdictions—does now cover biology (although technically not bme). for the second exam, called the principles and practices, part 2, or the professional engineering exam, candidates may select a particular engineering discipline\'s content to be tested on; there is currently not an option for bme with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). however, the biomedical engineering society (bmes) is, as of 2009, exploring the possibility of seeking to implement a bme-specific version of this exam to facilitate biomedical engineers pursuing licensure. beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. one such example is the certified clinical engineer (cce) certification for clinical engineers.   == career prospects == in 2012 there were about 19,400 biomedical engineers employed in the us, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.   == notable figures == earl bakken - invented the first transistorised pacemaker, co-founder of medtronic. forrest bird (deceased) – aviator and pioneer in the invention of mechanical ventilators y.c. fung (deceased) – professor emeritus at the university of california, san diego, considered by many to be the founder of modern biomechanics leslie geddes (deceased) – professor emeritus at purdue university, electrical engineer, inventor, and educator of over 2000 biomedical engineers, received a national medal of technology in 2006 from president george bush for his more than 50 years of contributions that have spawned innovations ranging from burn treatments to miniature defibrillators, ligament repair to tiny blood pressure monitors for premature infants, as well as a new method for performing cardiopulmonary resuscitation (cpr). willem johan kolff (deceased) – pioneer of hemodialysis as well as in the field of artificial organs robert langer – institute professor at mit, runs the largest bme laboratory in the world, pioneer in drug delivery and tissue engineering john macleod (deceased) – one of the co-discoverers of insulin at case western reserve university. alfred e. mann – physicist, entrepreneur and philanthropist. a pioneer in the field of biomedical engineering. j. thomas mortimer – emeritus professor of biomedical engineering at case western reserve university. pioneer in functional electrical stimulation (fes) robert m. nerem – professor emeritus at georgia institute of technology. pioneer in regenerative tissue, biomechanics, and author of over 300 published works. his works have been cited more than 20,000 times cumulatively. p. hunter peckham – donnell professor of biomedical engineering and orthopaedics at case western reserve university. pioneer in functional electrical stimulation (fes) nicholas a. peppas – chaired professor in engineering, university of texas at austin, pioneer in drug delivery, biomaterials, hydrogels and nanobiotechnology. robert plonsey – professor emeritus at duke university, pioneer of electrophysiology otto schmitt (deceased) – biophysicist with significant contributions to bme, working with biomimetics ascher shapiro (deceased) – institute professor at mit, contributed to the development of the bme field, medical devices (e.g. intra-aortic balloons) gordana vunjak-novakovic – university professor at columbia university, pioneer in tissue engineering and bioreactor design john g. webster – professor emeritus at the university of wisconsin–madison, a pioneer in the field of instrumentation amplifiers for the recording of electrophysiological signals fred weibell, coauthor of biomedical instrumentation and measurements u.a. whitaker (deceased) – provider of the whitaker foundation, which supported research and education in bme by providing over $700 million to various universities, helping to create 30 bme programs and helping finance the construction of 13 buildings   == see also ==  biomedicine – branch of medical science that applies biological and physiological principles to clinical practice cardiophysics computational anatomy medical physics physiome biomedical engineering jobs biomedical engineering and instrumentation program (beip)   == references ==   == further reading == bronzino, joseph d. (april 2006). the biomedical engineering handbook (third ed.). [crc press]. isbn 978-0-8493-2124-5. archived from the original on 2015-02-24. retrieved 2009-06-22. villafane, carlos (june 2009). biomed: from the student\'s perspective (first ed.). [techniciansfriend.com]. isbn 978-1-61539-663-4.   == external links == media related to biomedical engineering at wikimedia commons biomedical engineering at curlie')"
87,"In medicine, a prosthesis (plural: prostheses; from Ancient Greek prosthesis, ""addition, application, attachment"") or prosthetic implant is an artificial device that replaces a missing body part, which may be lost through trauma, disease, or a condition present at birth (congenital disorder). Prostheses are intended to restore the normal functions of the missing body part. Amputee rehabilitation is primarily coordinated by a physiatrist as part of an inter-disciplinary team consisting of physiatrists, prosthetists, nurses, physical therapists, and occupational therapists. Prostheses can be created by hand or with computer-aided design (CAD), a software interface that helps creators design and analyze the creation with computer-generated 2-D and 3-D graphics as well as analysis and optimization tools.


== Types ==
A person's prosthesis should be designed and assembled according to the person's appearance and functional needs. For instance, a person may need a transradial prosthesis, but need to choose between an aesthetic functional device, a myoelectric device, a body-powered device, or an activity specific device. The person's future goals and economical capabilities may help them choose between one or more devices.
Craniofacial prostheses include intra-oral and extra-oral prostheses. Extra-oral prostheses are further divided into hemifacial, auricular (ear), nasal, orbital and ocular. Intra-oral prostheses include dental prostheses such as dentures, obturators, and dental implants.
Prostheses of the neck include larynx substitutes, trachea and upper esophageal replacements,
Somato prostheses of the torso include breast prostheses which may be either single or bilateral, full breast devices or nipple prostheses.
Penile prostheses are used to treat erectile dysfunction, correct penile deformity, perform phalloplasty and metoidioplasty procedures in biological men, and to build a new penis in female-to-male gender reassignment surgeries.


=== Limb prostheses ===

Limb prostheses include both upper- and lower-extremity prostheses.
Upper-extremity prostheses are used at varying levels of amputation: forequarter, shoulder disarticulation, transhumeral prosthesis, elbow disarticulation, transradial prosthesis, wrist disarticulation, full hand, partial hand, finger, partial finger. A transradial prosthesis is an artificial limb that replaces an arm missing below the elbow.
Upper limb prostheses can be categorized in three main categories: Passive devices, Body Powered devices, Externally Powered (myoelectric) devices. Passive devices can either be passive hands, mainly used for cosmetic purpose, or passive tools, mainly used for specific activities (e.g. leisure or vocational). An extensive overview and classification of passive devices can be found in a literature review by Maat et.al. A passive device can be static, meaning the device has no movable parts, or it can be adjustable, meaning its configuration can be adjusted (e.g. adjustable hand opening). Despite the absence of active grasping, passive devices are very useful in bimanual tasks that require fixation or support of an object, or for gesticulation in social interaction. According to scientific data a third of the upper limb amputees worldwide use a passive prosthetic hand. Body Powered or cable operated limbs work by attaching a harness and cable around the opposite shoulder of the damaged arm. The third category of prosthetic devices available are myoelectric arms. These work by sensing, via electrodes, when the muscles in the upper arm move, causing an artificial hand to open or close. In the prosthetics industry, a trans-radial prosthetic arm is often referred to as a ""BE"" or below elbow prosthesis.
Lower-extremity prostheses provide replacements at varying levels of amputation. These include hip disarticulation, transfemoral prosthesis, knee disarticulation, transtibial prosthesis, Syme's amputation, foot, partial foot, and toe. The two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency) and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency).
A transfemoral prosthesis is an artificial limb that replaces a leg missing above the knee. Transfemoral amputees can have a very difficult time regaining normal movement. In general, a transfemoral amputee must use approximately 80% more energy to walk than a person with two whole legs. This is due to the complexities in movement associated with the knee. In newer and more improved designs, hydraulics, carbon fiber, mechanical linkages, motors, computer microprocessors, and innovative combinations of these technologies are employed to give more control to the user. In the prosthetics industry a trans-femoral prosthetic leg is often referred to as an ""AK"" or above the knee prosthesis.
A transtibial prosthesis is an artificial limb that replaces a leg missing below the knee. A transtibial amputee is usually able to regain normal movement more readily than someone with a transfemoral amputation, due in large part to retaining the knee, which allows for easier movement.  Lower extremity prosthetics describes artificially replaced limbs located at the hip level or lower. In the prosthetics industry a trans-tibial prosthetic leg is often referred to as a ""BK"" or below the knee prosthesis.
Physical therapists are trained to teach a person to walk with a leg prosthesis. To do so, the physical therapist may provide verbal instructions and may also help guide the person using touch or tactile cues. This may be done in a clinic or home. There is some research suggesting that such training in the home may be more successful if the treatment includes the use of a treadmill. Using a treadmill, along with the physical therapy treatment, helps the person to experience many of the challenges of walking with a prosthesis.
In the United Kingdom, 75% of lower limb amputations are performed due to inadequate circulation (dysvascularity). This condition is often associated with many other medical conditions (co-morbidities) including diabetes and heart disease that may make it a challenge to recover and use a prosthetic limb to regain mobility and independence. For people who have inadequate circulation and have lost a lower limb, there is insufficient evidence due to a lack of research, to inform them regarding their choice of prosthetic rehabilitation approaches.

Lower extremity prostheses are often categorized by the level of amputation or after the name of a surgeon:
Transfemoral (Above-knee)
Transtibial (Below-knee)
Ankle disarticulation (e.g.: Syme amputation)
Knee disarticulation
Hemi-pelvictomy (Hip disarticulation)
Partial foot amputations (Pirogoff, Talo-Navicular and Calcaneo-cuboid (Chopart), Tarso-metatarsal (Lisfranc), Trans-metatarsal, Metatarsal-phalangeal, Ray amputations, toe amputations).
Van Nes rotationplasty


==== Prosthetic raw materials ====
Prosthetic are made lightweight for better convenience for the amputee. Some of these materials include:

Plastics:
Polyethylene
Polypropylene
Acrylics
Polyurethane
Wood (early prosthetics)
Rubber (early prosthetics)
Lightweight metals:
Titanium
Aluminum
Composites:
Carbon fiber reinforced polymersWheeled prostheses have also been used extensively in the rehabilitation of injured domestic animals, including dogs, cats, pigs, rabbits, and turtles.


== History ==

Prosthetics originate from the ancient Egypt Near East circa 3000 BCE, with the earliest evidence of prosthetics appearing in ancient Egypt and Iran. The earliest recorded mention of eye prosthetics is from the Egyptian story of the Eye of Horus dates circa 3000 BC, which involves the left eye of Horus being plucked out and then restored by Thoth. Circa 3000-2800 BC, the earliest archaeological evidence of prosthetics is found in ancient Iran, where an eye prosthetic is found buried with a woman in Shahr-i Shōkhta. It was likely made of bitumen paste that was covered with a thin layer of gold. The Egyptians were also early pioneers of foot prosthetics, as shown by the wooden toe found on a body from the New Kingdom circa 1000 BC. Another early recorded mention is found in South Asia circa 1200 BC, involving the warrior queen Vishpala in the Rigveda. Roman bronze crowns have also been found, but their use could have been more aesthetic than medical.An early mention of a prosthetic comes from the Greek historian Herodotus, who tells the story of Hegesistratus, a Greek diviner who cut off his own foot to escape his Spartan captors and replaced it with a wooden one.


=== Wood and metal prosthetics ===

Pliny the Elder also recorded the tale of a Roman general, Marcus Sergius, whose right hand was cut off while campaigning and had an iron hand made to hold his shield so that he could return to battle. A famous and quite refined historical prosthetic arm was that of Götz von Berlichingen, made at the beginning of the 16th century.  The first confirmed use of a prosthetic device, however, is from 950 to 710 BC. In 2000, research pathologists discovered a mummy from this period buried in the Egyptian necropolis near ancient Thebes that possessed an artificial big toe.  This toe, consisting of wood and leather, exhibited evidence of use.  When reproduced by bio-mechanical engineers in 2011, researchers discovered that this ancient prosthetic enabled its wearer to walk both barefoot and in Egyptian style sandals.  Previously, the earliest discovered prosthetic was an artificial leg from Capua.Around the same time, François de la Noue is also reported to have had an iron hand, as is, in the 17th Century, René-Robert Cavalier de la Salle. Henri de Tonti had a prosthetic hook for a hand. During the Middle Ages, prosthetic remained quite basic in form. Debilitated knights would be fitted with prosthetics so they could hold up a shield, grasp a lance or a sword, or stabilize a mounted warrior. Only the wealthy could afford anything that would assist in daily life.One notable prosthesis was that belonging to an Italian man, who scientists estimate replaced his amputated right hand with a knife. Scientists investigating the skeleton, which was found in a Longobard cemetery in Povegliano Veronese, estimated that the man had lived sometime between the 6th and 8th centuries AD. Materials found near the man's body suggest that the knife prosthesis was attached with a leather strap, which he repeatedly tightened with his teeth.During the Renaissance, prosthetics developed with the use of iron, steel, copper, and wood. Functional prosthetics began to make an appearance in the 1500s.


=== Technology progress before the 20th century ===
An Italian surgeon recorded the existence of an amputee who had an arm that allowed him to remove his hat, open his purse, and sign his name. Improvement in amputation surgery and prosthetic design came at the hands of Ambroise Paré. Among his inventions was an above-knee device that was a kneeling peg leg and foot prosthesis with a fixed position, adjustable harness, and knee lock control. The functionality of his advancements showed how future prosthetics could develop.
Other major improvements before the modern era:

Pieter Verduyn – First non-locking below-knee (BK) prosthesis.
James Potts – Prosthesis made of a wooden shank and socket, a steel knee joint and an articulated foot that was controlled by catgut tendons from the knee to the ankle. Came to be known as ""Anglesey Leg"" or ""Selpho Leg"".
Sir James Syme – A new method of ankle amputation that did not involve amputating at the thigh.
Benjamin Palmer – Improved upon the Selpho leg. Added an anterior spring and concealed tendons to simulate natural-looking movement.
Dubois Parmlee – Created prosthetic with a suction socket, polycentric knee, and multi-articulated foot.
Marcel Desoutter & Charles Desoutter – First aluminium prosthesis
Henry Heather Bigg, and his son Henry Robert Heather Bigg, won the Queen's command to provide ""surgical appliances"" to wounded soldiers after Crimea War. They developed arms that allowed a double arm amputee to crochet, and a hand that felt natural to others based on ivory, felt and leather.At the end of World War II, the NAS (National Academy of Sciences) began to advocate better research and development of prosthetics. Through government funding, a research and development program was developed within the Army, Navy, Air Force, and the Veterans Administration.


=== Lower extremity modern history ===

After the Second World War a team at the University of California, Berkeley including James Foort and C.W. Radcliff helped to develop the quadrilateral socket by developing a jig fitting system for amputations above the knee. Socket technology for lower extremity limbs saw a further revolution during the 1980s when John Sabolich C.P.O., invented the Contoured Adducted Trochanteric-Controlled Alignment Method (CATCAM) socket, later to evolve into the Sabolich Socket. He followed the direction of Ivan Long and Ossur Christensen as they developed alternatives to the quadrilateral socket, which in turn followed the open ended plug socket, created from wood. The advancement was due to the difference in the socket to patient contact model. Prior to this, sockets were made in the shape of a square shape with no specialized containment for muscular tissue. New designs thus help to lock in the bony anatomy, locking it into place and distributing the weight evenly over the existing limb as well as the musculature of the patient. Ischial containment is well known and used today by many prosthetist to help in patient care. Variations of the ischial containment socket thus exists and each socket is tailored to the specific needs of the patient. Others who contributed to socket development and changes over the years include Tim Staats, Chris Hoyt, and Frank Gottschalk. Gottschalk disputed the efficacy of the CAT-CAM socket- insisting the surgical procedure done by the amputation surgeon was most important to prepare the amputee for good use of a prosthesis of any type socket design.The first microprocessor-controlled prosthetic knees became available in the early 1990s. The Intelligent Prosthesis was the first commercially available microprocessor-controlled prosthetic knee. It was released by Chas. A. Blatchford & Sons, Ltd., of Great Britain, in 1993 and made walking with the prosthesis feel and look more natural. An improved version was released in 1995 by the name Intelligent Prosthesis Plus. Blatchford released another prosthesis, the Adaptive Prosthesis, in 1998. The Adaptive Prosthesis utilized hydraulic controls, pneumatic controls, and a microprocessor to provide the amputee with a gait that was more responsive to changes in walking speed. Cost analysis reveals that a sophisticated above-knee prosthesis will be about $1 million in 45 years, given only annual cost of living adjustments.In 2019, a project under AT2030 was launched in which bespoke sockets are made using a thermoplastic, rather than through a plaster cast. This is faster to do and significantly less expensive. The sockets were called Amparo Confidence sockets.


=== Upper extremity modern history ===
In 2005, DARPA started the Revolutionizing Prosthetics program.


== Patient procedure ==
A prosthesis is a functional replacement for an amputated or congenitally malformed or missing limb. Prosthetists are responsible for the prescription, design, and management of a prosthetic device.
In most cases, the prosthetist begins by taking a plaster cast of the patient's affected limb. Lightweight, high-strength thermoplastics are custom-formed to this model of the patient. Cutting-edge materials such as carbon fiber, titanium and Kevlar provide strength and durability while making the new prosthesis lighter. More sophisticated prostheses are equipped with advanced electronics, providing additional stability and control.


== Current technology and manufacturing ==

Over the years, there have been advancements in artificial limbs. New plastics and other materials, such as carbon fiber, have allowed artificial limbs to be stronger and lighter, limiting the amount of extra energy necessary to operate the limb. This is especially important for trans-femoral amputees. Additional materials have allowed artificial limbs to look much more realistic, which is important to trans-radial and transhumeral amputees because they are more likely to have the artificial limb exposed.

In addition to new materials, the use of electronics has become very common in artificial limbs. Myoelectric limbs, which control the limbs by converting muscle movements to electrical signals, have become much more common than cable operated limbs. Myoelectric signals are picked up by electrodes, the signal gets integrated and once it exceeds a certain threshold, the prosthetic limb control signal is triggered which is why inherently, all myoelectric controls lag. Conversely, cable control is immediate and physical, and through that offers a certain degree of direct force feedback that myoelectric control does not. Computers are also used extensively in the manufacturing of limbs. Computer Aided Design and Computer Aided Manufacturing are often used to assist in the design and manufacture of artificial limbs.Most modern artificial limbs are attached to the residual limb (stump) of the amputee by belts and cuffs or by suction. The residual limb either directly fits into a socket on the prosthetic, or—more commonly today—a liner is used that then is fixed to the socket either by vacuum (suction sockets) or a pin lock. Liners are soft and by that, they can create a far better suction fit than hard sockets. Silicone liners can be obtained in standard sizes, mostly with a circular (round) cross section, but for any other residual limb shape, custom liners can be made. The socket is custom made to fit the residual limb and to distribute the forces of the artificial limb across the area of the residual limb (rather than just one small spot), which helps reduce wear on the residual limb.


=== Production of prosthetic socket ===
The production of a prosthetic socket begins with capturing the geometry of the residual limb, this process is called shape capture. The goal of this process is to create an accurate representation of the residual limb, which is critical to achieve good socket fit. The custom socket is created by taking a plaster cast of the residual limb or, more commonly today, of the liner worn over their residual limb, and then making a mold from the plaster cast. The commonly used compound is called Plaster of Paris. In recent years, various digital shape capture systems have been developed which can be input directly to a computer allowing for a more sophisticated design. In general, the shape capturing process begins with the digital acquisition of three-dimensional (3D) geometric data from the amputee's residual limb. Data are acquired with either a probe, laser scanner, structured light scanner, or a photographic-based 3D scanning system.After shape capture, the second phase of the socket production is called rectification, which is the process of modifying the model of the residual limb by adding volume to bony prominence and potential pressure points and remove volume from load bearing area. This can be done manually by adding or removing plaster to the positive model, or virtually by manipulating the computerized model in the software. Lastly, the fabrication of the prosthetic socket begins once the model has been rectified and finalized. The prosthetists would wrap the positive model with a semi-molten plastic sheet or carbon fiber coated with epoxy resin to construct the prosthetic socket. For the computerized model, it can be 3D printed using a various of material with different flexibility and mechanical strength.Optimal socket fit between the residual limb and socket is critical to the function and usage of the entire prosthesis. If the fit between the residual limb and socket attachment is too loose, this will reduce the area of contact between the residual limb and socket or liner, and increase pockets between residual limb skin and socket or liner. Pressure then is higher, which can be painful. Air pockets can allow sweat to accumulate that can soften the skin. Ultimately, this is a frequent cause for itchy skin rashes. Over time, this can lead to breakdown of the skin. On the other hand, a very tight fit may excessively increase the interface pressures that may also lead to skin breakdown after prolonged use.Artificial limbs are typically manufactured using the following steps:
Measurement of the residual limb
Measurement of the body to determine the size required for the artificial limb
Fitting of a silicone liner
Creation of a model of the liner worn over the residual limb
Formation of thermoplastic sheet around the model – This is then used to test the fit of the prosthetic
Formation of permanent socket
Formation of plastic parts of the artificial limb – Different methods are used, including vacuum forming and injection molding
Creation of metal parts of the artificial limb using die casting
Assembly of entire limb


=== Body-powered arms ===
Current technology allows body-powered arms to weigh around one-half to one-third of what a myoelectric arm does.


==== Sockets ====
Current body-powered arms contain sockets that are built from hard epoxy or carbon fiber. These sockets or ""interfaces"" can be made more comfortable by lining them with a softer, compressible foam material that provides padding for the bone prominences. A self-suspending or supra-condylar socket design is useful for those with short to mid-range below elbow absence. Longer limbs may require the use of a locking roll-on type inner liner or more complex harnessing to help augment suspension.


==== Wrists ====
Wrist units are either screw-on connectors featuring the UNF 1/2-20 thread (USA) or quick-release connector, of which there are different models.


==== Voluntary opening and voluntary closing ====
Two types of body-powered systems exist, voluntary opening ""pull to open"" and voluntary closing ""pull to close"". Virtually all ""split hook"" prostheses operate with a voluntary opening type system.
More modern ""prehensors"" called GRIPS utilize voluntary closing systems. The differences are significant. Users of voluntary opening systems rely on elastic bands or springs for gripping force, while users of voluntary closing systems rely on their own body power and energy to create gripping force.
Voluntary closing users can generate prehension forces equivalent to the normal hand, up to or exceeding one hundred pounds. Voluntary closing GRIPS require constant tension to grip, like a human hand, and in that property, they do come closer to matching human hand performance. Voluntary opening split hook users are limited to forces their rubber or springs can generate which usually is below 20 pounds.


==== Feedback ====
An additional difference exists in the biofeedback created that allows the user to ""feel"" what is being held. Voluntary opening systems once engaged provide the holding force so that they operate like a passive vice at the end of the arm. No gripping feedback is provided once the hook has closed around the object being held. Voluntary closing systems provide directly proportional control and biofeedback so that the user can feel how much force that they are applying.
A recent study showed that by stimulating the median and ulnar nerves, according to the information provided by the artificial sensors from a hand prosthesis, physiologically appropriate (near-natural) sensory information could be provided to an amputee. This feedback enabled the participant to effectively modulate the grasping force of the prosthesis with no visual or auditory feedback.In February 2013, researchers from École Polytechnique Fédérale de Lausanne in Switzerland and the Scuola Superiore Sant'Anna in Italy, implanted electrodes into an amputee's arm, which gave the patient sensory feedback and allowed for real time control of the prosthetic. With wires linked to nerves in his upper arm, the Danish patient was able to handle objects and instantly receive a sense of touch through the special artificial hand that was created by Silvestro Micera and researchers both in Switzerland and Italy.In July 2019, this technology was expanded on even further by researchers from the University of Utah, lead by Jacob George. The group of researchers implanted electrodes into the patient's arm to map out several sensory precepts. They would then stimulate each electrode to figure out how each sensory precept was triggered, then proceed to map the sensory information onto the prosthetic. This would allow the researchers to get a good approximation of the same kind of information that the patient would receive from their natural hand. Unfortunately, the arm is too expensive for the average user to acquire, however, Jacob mentioned that insurance companies could cover the costs of the prosthetic.


==== Terminal devices ====
Terminal devices contain a range of hooks, prehensors, hands or other devices.


===== Hooks =====
Voluntary opening split hook systems are simple, convenient, light, robust, versatile and relatively affordable.
A hook does not match a normal human hand for appearance or overall versatility, but its material tolerances can exceed and surpass the normal human hand for mechanical stress (one can even use a hook to slice open boxes or as a hammer whereas the same is not possible with a normal hand), for thermal stability (one can use a hook to grip items from boiling water, to turn meat on a grill, to hold a match until it has burned down completely) and for chemical hazards (as a metal hook withstands acids or lye, and does not react to solvents like a prosthetic glove or human skin).


===== Hands =====

Prosthetic hands are available in both voluntary opening and voluntary closing versions and because of their more complex mechanics and cosmetic glove covering require a relatively large activation force, which, depending on the type of harness used, may be uncomfortable. A recent study by the Delft University of Technology, The Netherlands, showed that the development of mechanical prosthetic hands has been neglected during the past decades. The study showed that the pinch force level of most current mechanical hands is too low for practical use. The best tested hand was a prosthetic hand developed around 1945. In 2017 however, a research has been started with bionic hands by Laura Hruby of the Medical University of Vienna. A few open-hardware 3-d printable bionic hands have also become available. Some companies are also producing robotic hands with integrated forearm, for fitting unto a patient's upper arm and in 2020, at the Italian Institute of Technology (IIT), another robotic hand with integrated forearm (Soft Hand Pro) was developed.


==== Commercial providers and materials ====
Hosmer and Otto Bock are major commercial hook providers. Mechanical hands are sold by Hosmer and Otto Bock as well; the Becker Hand is still manufactured by the Becker family. Prosthetic hands may be fitted with standard stock or custom-made cosmetic looking silicone gloves. But regular work gloves may be worn as well. Other terminal devices include the V2P Prehensor, a versatile robust gripper that allows customers to modify aspects of it, Texas Assist Devices (with a whole assortment of tools) and TRS that offers a range of terminal devices for sports. Cable harnesses can be built using aircraft steel cables, ball hinges, and self-lubricating cable sheaths. Some prosthetics have been designed specifically for use in salt water.


=== Lower-extremity prosthetics ===

Lower-extremity prosthetics describes artificially replaced limbs located at the hip level or lower. Concerning all ages Ephraim et al. (2003) found a worldwide estimate of all-cause lower-extremity amputations of 2.0–5.9 per 10,000 inhabitants. For birth prevalence rates of congenital limb deficiency they found an estimate between 3.5 and 7.1 cases per 10,000 births.The two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency), and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency). In the prosthetic industry, a trans-tibial prosthetic leg is often referred to as a ""BK"" or below the knee prosthesis while the trans-femoral prosthetic leg is often referred to as an ""AK"" or above the knee prosthesis.
Other, less prevalent lower extremity cases include the following:

Hip disarticulations – This usually refers to when an amputee or congenitally challenged patient has either an amputation or anomaly at or in close proximity to the hip joint.
Knee disarticulations – This usually refers to an amputation through the knee disarticulating the femur from the tibia.
Symes – This is an ankle disarticulation while preserving the heel pad.


==== Socket ====
The socket serves as an interface between the residuum and the prosthesis, ideally allowing comfortable weight-bearing, movement control and proprioception. Socket issues, such as discomfort and skin breakdown, are rated among the most important issues faced by lower-limb amputees.


==== Shank and connectors ====
This part creates distance and support between the knee-joint and the foot (in case of an upper-leg prosthesis) or between the socket and the foot. The type of connectors that are used between the shank and the knee/foot determines whether the prosthesis is modular or not. Modular means that the angle and the displacement of the foot in respect to the socket can be changed after fitting. In developing countries prosthesis mostly are non-modular, in order to reduce cost. When considering children modularity of angle and height is important because of their average growth of 1.9 cm annually.


==== Foot ====
Providing contact to the ground, the foot provides shock absorption and stability during stance. Additionally it influences gait biomechanics by its shape and stiffness. This is because the trajectory of the center of pressure (COP) and the angle of the ground reaction forces is determined by the shape and stiffness of the foot and needs to match the subject's build in order to produce a normal gait pattern. Andrysek (2010) found 16 different types of feet, with greatly varying results concerning durability and biomechanics. The main problem found in current feet is durability, endurance ranging from 16 to 32 months These results are for adults and will probably be worse for children due to higher activity levels and scale effects. Evidence comparing different types of feet and ankle prosthetic devices is not strong enough to determine if one mechanism of ankle/foot is superior to another. When deciding on a device, the cost of the device, a person's functional need, and the availability of a particular device should be considered.


==== Knee joint ====
In case of a trans-femoral (above knee) amputation, there also is a need for a complex connector providing articulation, allowing flexion during swing-phase but not during stance. As its purpose is to replace the knee, the prosthetic knee joint is the most critical component of the prosthesis for trans-femoral amputees. The function of the good prosthetic knee joint is to mimic the function of the normal knee, such as providing structural support and stability during stance phase but able to flex in a controllable manner during swing phase. Hence it allows users to have a smooth and energy efficient gait and minimize the impact of amputation. The prosthetic knee is connected to the prosthetic foot by the shank, which is usually made of an aluminum or graphite tube.
One of the most important aspect of a prosthetic knee joint would be its stance-phase control mechanism. The function of stance-phase control is to prevent the leg from buckling when the limb is loaded during weight acceptance. This ensures the stability of the knee in order to support the single limb support task of stance phase and provides a smooth transition to the swing phase. Stance phase control can be achieved in several ways including the mechanical locks, relative alignment of prosthetic components, weight activated friction control, and polycentric mechanisms.


===== Microprocessor control =====
To mimic the knee's functionality during gait, microprocessor-controlled knee joints have been developed that control the flexion of the knee. Some examples are Otto Bock’s C-leg, introduced in 1997, Ossur's Rheo Knee, released in 2005, the Power Knee by Ossur, introduced in 2006, the Plié Knee from Freedom Innovations and DAW Industries’ Self Learning Knee (SLK).The idea was originally developed by Kelly James, a Canadian engineer, at the University of Alberta.A microprocessor is used to interpret and analyze signals from knee-angle sensors and moment sensors. The microprocessor receives signals from its sensors to determine the type of motion being employed by the amputee. Most microprocessor controlled knee-joints are powered by a battery housed inside the prosthesis.
The sensory signals computed by the microprocessor are used to control the resistance generated by hydraulic cylinders in the knee-joint. Small valves control the amount of hydraulic fluid that can pass into and out of the cylinder, thus regulating the extension and compression of a piston connected to the upper section of the knee.The main advantage of a microprocessor-controlled prosthesis is a closer approximation to an amputee's natural gait. Some allow amputees to walk near walking speed or run. Variations in speed are also possible and are taken into account by sensors and communicated to the microprocessor, which adjusts to these changes accordingly. It also enables the amputees to walk downstairs with a step-over-step approach, rather than the one step at a time approach used with mechanical knees. There is some research suggesting that people with microprocessor-controlled prostheses report greater satisfaction and improvement in functionality, residual limb health, and safety. People may be able to perform everyday activities at greater speeds, even while multitasking, and reduce their risk of falls.However, some have some significant drawbacks that impair its use. They can be susceptible to water damage and thus great care must be taken to ensure that the prosthesis remains dry.


=== Myoelectric ===
A myoelectric prosthesis uses the electrical tension generated every time a muscle contracts, as information. This tension can be captured from voluntarily contracted muscles by electrodes applied on the skin to control the movements of the prosthesis, such as elbow flexion/extension, wrist supination/pronation (rotation) or opening/closing of the fingers. A prosthesis of this type utilizes the residual neuromuscular system of the human body to control the functions of an electric powered prosthetic hand, wrist, elbow or foot. This is different from an electric switch prosthesis, which requires straps and/or cables actuated by body movements to actuate or operate switches that control the movements of the prosthesis. There is no clear evidence concluding that myoelectric upper extremity prostheses function better than body-powered prostheses. Advantages to using a myoelectric upper extremity prosthesis include the potential for improvement in cosmetic appeal (this type of prosthesis may have a more natural look), may be better for light everyday activities, and may be beneficial for people experiencing phantom limb pain. When compared to a body-powered prosthesis, a myoelectric prosthesis may not be as durable, may have a longer training time, may require more adjustments, may need more maintenance, and does not provide feedback to the user.The USSR was the first to develop a myoelectric arm in 1958, while the first myoelectric arm became commercial in 1964 by the Central Prosthetic Research Institute of the USSR, and distributed by the Hangar Limb Factory of the UK.


=== Robotic prostheses ===

Robots can be used to generate objective measures of patient's impairment and therapy outcome, assist in diagnosis, customize therapies based on patient's motor abilities, and assure compliance with treatment regimens and maintain patient's records. It is shown in many studies that there is a significant improvement in upper limb motor function after stroke using robotics for upper limb rehabilitation.
In order for a robotic prosthetic limb to work, it must have several components to integrate it into the body's function: Biosensors detect signals from the user's nervous or muscular systems. It then relays this information to a controller located inside the device, and processes feedback from the limb and actuator, e.g., position or force, and sends it to the controller. Examples include surface electrodes that detect electrical activity on the skin, needle electrodes implanted in muscle, or solid-state electrode arrays with nerves growing through them. One type of these biosensors are employed in myoelectric prostheses.
A device known as the controller is connected to the user's nerve and muscular systems and the device itself. It sends intention commands from the user to the actuators of the device and interprets feedback from the mechanical and biosensors to the user. The controller is also responsible for the monitoring and control of the movements of the device.
An actuator mimics the actions of a muscle in producing force and movement. Examples include a motor that aids or replaces original muscle tissue.
Targeted muscle reinnervation (TMR) is a technique in which motor nerves, which previously controlled muscles on an amputated limb, are surgically rerouted such that they reinnervate a small region of a large, intact muscle, such as the pectoralis major. As a result, when a patient thinks about moving the thumb of his missing hand, a small area of muscle on his chest will contract instead. By placing sensors over the reinnervated muscle, these contractions can be made to control the movement of an appropriate part of the robotic prosthesis.A variant of this technique is called targeted sensory reinnervation (TSR). This procedure is similar to TMR, except that sensory nerves are surgically rerouted to skin on the chest, rather than motor nerves rerouted to muscle. Recently, robotic limbs have improved in their ability to take signals from the human brain and translate those signals into motion in the artificial limb. DARPA, the Pentagon's research division, is working to make even more advancements in this area. Their desire is to create an artificial limb that ties directly into the nervous system.


==== Robotic arms ====
Advancements in the processors used in myoelectric arms have allowed developers to make gains in fine-tuned control of the prosthetic. The Boston Digital Arm is a recent artificial limb that has taken advantage of these more advanced processors. The arm allows movement in five axes and allows the arm to be programmed for a more customized feel. Recently the I-LIMB Hand, invented in Edinburgh, Scotland, by David Gow has become the first commercially available hand prosthesis with five individually powered digits. The hand also possesses a manually rotatable thumb which is operated passively by the user and allows the hand to grip in precision, power, and key grip modes.Another neural prosthetic is Johns Hopkins University Applied Physics Laboratory Proto 1. Besides the Proto 1, the university also finished the Proto 2 in 2010. Early in 2013, Max Ortiz Catalan and Rickard Brånemark of the Chalmers University of Technology, and Sahlgrenska University Hospital in Sweden, succeeded in making the first robotic arm which is mind-controlled and can be permanently attached to the body (using osseointegration).An approach that is very useful is called arm rotation which is common for unilateral amputees which is an amputation that affects only one side of the body; and also essential for bilateral amputees, a person who is missing or has had amputated either both arms or legs, to carry out activities of daily living. This involves inserting a small permanent magnet into the distal end of the residual bone of subjects with upper limb amputations. When a subject rotates the residual arm, the magnet will rotate with the residual bone, causing a change in magnetic field distribution. EEG (electroencephalogram) signals, detected using small flat metal discs attached to the scalp, essentially decoding human brain activity used for physical movement, is used to control the robotic limbs. This allows the user to control the part directly.


==== Robotic transtibial prostheses ====
The research of robotic legs has made some advancement over time, allowing exact movement and control.
Researchers at the Rehabilitation Institute of Chicago announced in September 2013 that they have developed a robotic leg that translates neural impulses from the user's thigh muscles into movement, which is the first prosthetic leg to do so. It is currently in testing.Hugh Herr, head of the biomechatronics group at MIT's Media Lab developed a robotic transtibial leg (PowerFoot BiOM).The Icelandic company Össur has also created a robotic transtibial leg with motorized ankle that moves through algorithms and sensors that automatically adjust the angle of the foot during different points in its wearer's stride. Also there are brain-controlled bionic legs that allow an individual to move his limbs with a wireless transmitter.


===== Prosthesis design =====
The main goal of a robotic prosthesis is to provide active actuation during gait to improve the biomechanics of gait, including, among other things, stability, symmetry, or energy expenditure for amputees. There are several powered prosthetic legs currently on the market, including fully powered legs, in which actuators directly drive the joints, and semi-active legs, which use small amounts of energy and a small actuator to change the mechanical properties of the leg but do not inject net positive energy into gait.  Specific examples include The emPOWER from BionX, the Proprio Foot from Ossur, and the Elan Foot from Endolite. Various research groups have also experimented with robotic legs over the last decade. Central issues being researched include designing the behavior of the device during stance and swing phases, recognizing the current ambulation task, and various mechanical design problems such as robustness, weight, battery-life/efficiency, and noise-level. However, scientists from Stanford University and Seoul National University has developed artificial nerves system that will help prosthetic limbs feel. This synthetic nerve system enables prosthetic limbs sense braille, feel the sense of touch and respond to the environment.


=== Use of recycled materials ===
Prosthetics are being made from recycled plastic bottles and lids around the world.


== Attachment to the body ==
Most prostheses can be attached to the exterior of the body, in a non-permanent way. Some others however can be attached in a permanent way. One such example are exoprostheses (see below).


=== Direct bone attachment and osseointegration ===

Osseointegration is a method of attaching the artificial limb to the body. This method is also sometimes referred to as exoprosthesis (attaching an artificial limb to the bone), or endo-exoprosthesis.
The stump and socket method can cause significant pain in the amputee, which is why the direct bone attachment has been explored extensively. The method works by inserting a titanium bolt into the bone at the end of the stump. After several months the bone attaches itself to the titanium bolt and an abutment is attached to the titanium bolt. The abutment extends out of the stump and the (removable) artificial limb is then attached to the abutment. Some of the benefits of this method include the following:

Better muscle control of the prosthetic.
The ability to wear the prosthetic for an extended period of time; with the stump and socket method this is not possible.
The ability for transfemoral amputees to drive a car.The main disadvantage of this method is that amputees with the direct bone attachment cannot have large impacts on the limb, such as those experienced during jogging, because of the potential for the bone to break.


== Cosmesis ==
Cosmetic prosthesis has long been used to disguise injuries and disfigurements. With advances in modern technology, cosmesis, the creation of lifelike limbs made from silicone or PVC, has been made possible. Such prosthetics, including artificial hands, can now be designed to simulate the appearance of real hands, complete with freckles, veins, hair, fingerprints and even tattoos.
Custom-made cosmeses are generally more expensive (costing thousands of U.S. dollars, depending on the level of detail), while standard cosmeses come premade in a variety of sizes, although they are often not as realistic as their custom-made counterparts. Another option is the custom-made silicone cover, which can be made to match a person's skin tone but not details such as freckles or wrinkles. Cosmeses are attached to the body in any number of ways, using an adhesive, suction, form-fitting, stretchable skin, or a skin sleeve.


== Cognition ==

Unlike neuromotor prostheses, neurocognitive prostheses would sense or modulate neural function in order to physically reconstitute or augment cognitive processes such as executive function, attention, language, and memory. No neurocognitive prostheses are currently available but the development of implantable neurocognitive brain-computer interfaces has been proposed to help treat conditions such as stroke, traumatic brain injury, cerebral palsy, autism, and Alzheimer's disease.
The recent field of Assistive Technology for Cognition concerns the development of technologies to augment human cognition.  Scheduling devices such as Neuropage remind users with memory impairments when to perform certain activities, such as visiting the doctor. Micro-prompting devices such as PEAT, AbleLink and Guide have been used to aid users with memory and executive function problems perform activities of daily living.


== Prosthetic enhancement ==

In addition to the standard artificial limb for everyday use, many amputees or congenital patients have special limbs and devices to aid in the participation of sports and recreational activities.
Within science fiction, and, more recently, within the scientific community, there has been consideration given to using advanced prostheses to replace healthy body parts with artificial mechanisms and systems to improve function. The morality and desirability of such technologies are being debated by transhumanists, other ethicists, and others in general. Body parts such as legs, arms, hands, feet, and others can be replaced.
The first experiment with a healthy individual appears to have been that by the British scientist Kevin Warwick. In 2002, an implant was interfaced directly into Warwick's nervous system. The electrode array, which contained around a hundred electrodes, was placed in the median nerve. The signals produced were detailed enough that a robot arm was able to mimic the actions of Warwick's own arm and provide a form of touch feedback again via the implant.The DEKA company of Dean Kamen developed the ""Luke arm"", an advanced nerve-controlled prosthetic. Clinical trials began in 2008, with FDA approval in 2014 and commercial manufacturing by the Universal Instruments Corporation expected in 2017. The price offered at retail by Mobius Bionics is expected to be around $100,000.Further research in April 2019, there have been improvements towards prosthetic function and comfort of 3D-printed personalized wearable systems. Instead of manual integration after printing, integrating electronic sensors at the intersection between a prosthetic and the wearer's tissue can gather information such as pressure across wearer's tissue, that can help improve further iteration of these types of prosthetic.


=== Oscar Pistorius ===
In early 2008, Oscar Pistorius, the ""Blade Runner"" of South Africa, was briefly ruled ineligible to compete in the 2008 Summer Olympics because his transtibial prosthesis limbs were said to give him an unfair advantage over runners who had ankles. One researcher found that his limbs used twenty-five percent less energy than those of an able-bodied runner moving at the same speed.  This ruling was overturned on appeal, with the appellate court stating that the overall set of advantages and disadvantages of Pistorius' limbs had not been considered.
Pistorius did not qualify for the South African team for the Olympics, but went on to sweep the 2008 Summer Paralympics, and has been ruled eligible to qualify for any future Olympics. He qualified for the 2011 World Championship in South Korea and reached the semi-final where he ended last timewise, he was 14th in the first round, his personal best at 400m would have given him 5th place in the finals. At the 2012 Summer Olympics in London, Pistorius became the first amputee runner to compete at an Olympic Games. He ran in the 400 metres race semi-finals, and the 4 × 400 metres relay race finals. He also competed in 5 events in the 2012 Summer Paralympics in London.


== Design considerations ==
There are multiple factors to consider when designing a transtibial prosthesis. Manufacturers must make choices about their priorities regarding these factors.


=== Performance ===
Nonetheless, there are certain elements of socket and foot mechanics that are invaluable for the athlete, and these are the focus of today's high-tech prosthetics companies:

Fit – athletic/active amputees, or those with bony residua, may require a carefully detailed socket fit; less-active patients may be comfortable with a 'total contact' fit and gel liner
Energy storage and return – storage of energy acquired through ground contact and utilization of that stored energy for propulsion
Energy absorption – minimizing the effect of high impact on the musculoskeletal system
Ground compliance – stability independent of terrain type and angle
Rotation – ease of changing direction
Weight – maximizing comfort, balance and speed
Suspension – how the socket will join and fit to the limb


=== Other ===
The buyer is also concerned with numerous other factors:

Cosmetics
Cost
Ease of use
Size availability


== Cost and source freedom ==


=== High-cost ===
In the USA a typical prosthetic limb costs anywhere between $15,000 and $90,000, depending on the type of limb desired by the patient.  With medical insurance, a patient will typically pay 10%–50% of the total cost of a prosthetic limb, while the insurance company will cover the rest of the cost.  The percent that the patient pays varies on the type of insurance plan, as well as the limb requested by the patient. In the United Kingdom, much of Europe, Australia and New Zealand the entire cost of prosthetic limbs is met by state funding or statutory insurance. For example, in Australia prostheses are fully funded by state schemes in the case of amputation due to disease, and by workers compensation or traffic injury insurance in the case of most traumatic amputations. The National Disability Insurance Scheme, which is being rolled out nationally between 2017 and 2020 also pays for prostheses.
Transradial (below the elbow amputation) and transtibial prostheses (below the knee amputation) typically cost between US $6,000 and $8,000, while transfemoral (above the knee amputation) and transhumeral prosthetics (above the elbow amputation) cost approximately twice as much with a range of $10,000 to $15,000 and can sometimes reach costs of $35,000. The cost of an artificial limb often recurs, while a limb typically needs to be replaced every 3–4 years due to wear and tear of everyday use.  In addition, if the socket has fit issues, the socket must be replaced within several months from the onset of pain. If height is an issue, components such as pylons can be changed.Not only does the patient need to pay for their multiple prosthetic limbs, but they also need to pay for physical and occupational therapy that come along with adapting to living with an artificial limb.  Unlike the reoccurring cost of the prosthetic limbs, the patient will typically only pay the $2000 to $5000 for therapy during the first year or two of living as an amputee.  Once the patient is strong and comfortable with their new limb, they will not be required to go to therapy anymore. Throughout one's life, it is projected that a typical amputee will go through $1.4 million worth of treatment, including surgeries, prosthetics, as well as therapies.


=== Low-cost ===

Low-cost above-knee prostheses often provide only basic structural support with limited function. This function is often achieved with crude, non-articulating, unstable, or manually locking knee joints. A limited number of organizations, such as the International Committee of the Red Cross (ICRC), create devices for developing countries. Their device which is manufactured by CR Equipments is a single-axis, manually operated locking polymer prosthetic knee joint.Table. List of knee joint technologies based on the literature review.

A plan for a low-cost artificial leg, designed by Sébastien Dubois, was featured at the 2007 International Design Exhibition and award show in Copenhagen, Denmark, where it won the Index: Award. It would be able to create an energy-return prosthetic leg for US $8.00, composed primarily of fiberglass.Prior to the 1980s, foot prostheses merely restored basic walking capabilities. These early devices can be characterized by a simple artificial attachment connecting one's residual limb to the ground.
The introduction of the Seattle Foot (Seattle Limb Systems) in 1981 revolutionized the field, bringing the concept of an Energy Storing Prosthetic Foot (ESPF) to the fore. Other companies soon followed suit, and before long, there were multiple models of energy storing prostheses on the market. Each model utilized some variation of a compressible heel. The heel is compressed during initial ground contact, storing energy which is then returned during the latter phase of ground contact to help propel the body forward.
Since then, the foot prosthetics industry has been dominated by steady, small improvements in performance, comfort, and marketability.
With 3D printers, it is possible to manufacture a single product without having to have metal molds, so the costs can be drastically reduced.Jaipur Foot, an artificial limb from Jaipur, India, costs about US$40.


=== Open-source robotic prothesis ===

There is currently an open-design Prosthetics forum known as the ""Open Prosthetics Project"".  The group employs collaborators and volunteers to advance Prosthetics technology while attempting to lower the costs of these necessary devices. Open Bionics is a company that is developing open-source robotic prosthetic hands. It uses 3D printing to manufacture the devices and low-cost 3D scanners to fit them, with the aim of lowering the cost of fabricating custom prosthetics. A review study on a wide range of printed prosthetic hands, found that although 3D printing technology holds a promise for individualised prosthesis design, it is not necessarily cheaper when all costs are included. The same study also found that evidence on the functionality, durability and user acceptance of 3D printed hand prostheses is still lacking.


== Low-cost prosthetics for children ==

In the USA an estimate was found of 32,500 children (<21 years) that suffer from major paediatric amputation, with 5,525 new cases each year, of which 3,315 congenital.Carr et al. (1998) investigated amputations caused by landmines for Afghanistan, Bosnia and Herzegovina, Cambodia and Mozambique among children (<14 years), showing estimates of respectively 4.7, 0.19, 1.11 and 0.67 per 1000 children. Mohan (1986) indicated in India a total of 424,000 amputees (23,500 annually), of which 10.3% had an onset of disability below the age of 14, amounting to a total of about 43,700 limb deficient children in India alone.Few low-cost solutions have been created specially for children. Examples of low-cost prosthetic devices include:


=== Pole and crutch ===
This hand-held pole with leather support band or platform for the limb is one of the simplest and cheapest solutions found. It serves well as a short-term solution, but is prone to rapid contracture formation if the limb is not stretched daily through a series of range-of motion (RoM) sets.


=== Bamboo, PVC or plaster limbs ===
This also fairly simple solution comprises a plaster socket with a bamboo or PVC pipe at the bottom, optionally attached to a prosthetic foot. This solution prevents contractures because the knee is moved through its full RoM. The David Werner Collection, an online database for the assistance of disabled village children, displays manuals of production of these solutions.


=== Adjustable bicycle limb ===
This solution is built using a bicycle seat post up side down as foot, generating flexibility and (length) adjustability. It is a very cheap solution, using locally available materials.


=== Sathi Limb ===
It is an endoskeletal modular lower limb from India, which uses thermoplastic parts. Its main advantages are the small weight and adaptability.


=== Monolimb ===
Monolimbs are non-modular prostheses and thus require more experienced prosthetist for correct fitting, because alignment can barely be changed after production. However, their durability on average is better than low-cost modular solutions.


== Cultural and social theory perspectives ==
A number of theorists have explored the meaning and implications of prosthetic extension of the body. Elizabeth Grosz writes, ""Creatures use tools, ornaments, and appliances to augment their bodily capacities. Are their bodies lacking something, which they need to replace with artificial or substitute organs?...Or conversely, should prostheses be understood, in terms of aesthetic reorganization and proliferation, as the consequence of an inventiveness that functions beyond and perhaps in defiance of pragmatic need?"" Elaine Scarry argues that every artifact recreates and extends the body. Chairs supplement the skeleton, tools append the hands, clothing augments the skin. In Scarry's thinking, ""furniture and houses are neither more nor less interior to the human body than the food it absorbs, nor are they fundamentally different from such sophisticated prosthetics as artificial lungs, eyes and kidneys. The consumption of manufactured things turns the body inside out, opening it up to and as the culture of objects."" Mark Wigley, a professor of architecture, continues this line of thinking about how architecture supplements our natural capabilities, and argues that ""a blurring of identity is produced by all prostheses."" Some of this work relies on Freud's earlier characterization of man's relation to objects as one of extension.


== Notable users of prosthetic devices ==
Marie Moentmann (1900–1974), child survivor of industrial accident
Terry Fox (1958–1981), Canadian athlete, humanitarian, and cancer research activist
Oscar Pistorius (1986– ), South African former professional sprinter


== See also ==


== References ==


=== Citations ===


=== Sources ===


== External links ==
Afghan amputees tell their stories at Texas gathering, Fayetteville Observer
Can modern prosthetics actually help reclaim the sense of touch?, PBS Newshour
A hand for Rick, Fayetteville Observer
What is prosthesis , prosthetic limb and its various component","pandas(index=87, _1=87, text='in medicine, a prosthesis (plural: prostheses; from ancient greek prosthesis, ""addition, application, attachment"") or prosthetic implant is an artificial device that replaces a missing body part, which may be lost through trauma, disease, or a condition present at birth (congenital disorder). prostheses are intended to restore the normal functions of the missing body part. amputee rehabilitation is primarily coordinated by a physiatrist as part of an inter-disciplinary team consisting of physiatrists, prosthetists, nurses, physical therapists, and occupational therapists. prostheses can be created by hand or with computer-aided design (cad), a software interface that helps creators design and analyze the creation with computer-generated 2-d and 3-d graphics as well as analysis and optimization tools.   == types == a person\'s prosthesis should be designed and assembled according to the person\'s appearance and functional needs. for instance, a person may need a transradial prosthesis, but need to choose between an aesthetic functional device, a myoelectric device, a body-powered device, or an activity specific device. the person\'s future goals and economical capabilities may help them choose between one or more devices. craniofacial prostheses include intra-oral and extra-oral prostheses. extra-oral prostheses are further divided into hemifacial, auricular (ear), nasal, orbital and ocular. intra-oral prostheses include dental prostheses such as dentures, obturators, and dental implants. prostheses of the neck include larynx substitutes, trachea and upper esophageal replacements, somato prostheses of the torso include breast prostheses which may be either single or bilateral, full breast devices or nipple prostheses. penile prostheses are used to treat erectile dysfunction, correct penile deformity, perform phalloplasty and metoidioplasty procedures in biological men, and to build a new penis in female-to-male gender reassignment surgeries. == external links == afghan amputees tell their stories at texas gathering, fayetteville observer can modern prosthetics actually help reclaim the sense of touch?, pbs newshour a hand for rick, fayetteville observer what is prosthesis , prosthetic limb and its various component')"
88,"A cardiac pacemaker (or artificial pacemaker, so as not to be confused with the natural pacemaker of the heart), is a medical device that generates electrical impulses delivered by electrodes to cause the heart muscle chambers (the upper, or atria and/or the lower, or ventricles) to contract and therefore pump blood; by doing so this device replaces and/or regulates the function of the electrical conduction system of the heart.
The primary purpose of a pacemaker is to maintain an adequate heart rate, either because the heart's natural pacemaker is not fast enough, or because there is a block in the heart's electrical conduction system. Modern pacemakers are externally programmable and allow a cardiologist, particularly a cardiac electrophysiologist to select the optimal pacing modes for individual patients. A specific type of pacemaker called a defibrillator combines pacemaker and defibrillator functions in a single implantable device, which should be called a defibrillator, for clarity. Others, called biventricular pacemakers have multiple electrodes stimulating differing positions within the lower heart chambers to improve synchronization of the ventricles, the lower chambers of the heart.


== Methods of pacing ==


=== Percussive pacing ===
Percussive pacing, also known as transthoracic mechanical pacing, is the use of the closed fist, usually on the left lower edge of the sternum over the right ventricle in the vena cava, striking from a distance of 20 – 30 cm to induce a ventricular beat (the British Journal of Anaesthesia suggests this must be done to raise the ventricular pressure to 10–15 mmHg to induce electrical activity). This is an old procedure used only as a life saving means until an electrical pacemaker is brought to the patient.


=== Transcutaneous pacing ===

Transcutaneous pacing (TCP), also called external pacing, is recommended for the initial stabilization of hemodynamically significant bradycardias of all types. The procedure is performed by placing two pacing pads on the patient's chest, either in the anterior/lateral position or the anterior/posterior position. The rescuer selects the pacing rate, and gradually increases the pacing current (measured in mA) until electrical capture (characterized by a wide QRS complex with a tall, broad T wave on the ECG) is achieved, with a corresponding pulse. Pacing artifact on the ECG and severe muscle twitching may make this determination difficult. External pacing should not be relied upon for an extended period of time. It is an emergency procedure that acts as a bridge until transvenous pacing or other therapies can be applied.


=== Epicardial pacing (temporary) ===

Temporary epicardial pacing is used during open heart surgery should the surgical procedure create atrio-ventricular block. The electrodes are placed in contact with the outer wall of the ventricle (epicardium) to maintain satisfactory cardiac output until a temporary transvenous electrode has been inserted.


=== Transvenous pacing (temporary) ===

Transvenous pacing, when used for temporary pacing, is an alternative to transcutaneous pacing. A pacemaker wire is placed into a vein, under sterile conditions, and then passed into either the right atrium or right ventricle. The pacing wire is then connected to an external pacemaker outside the body. Transvenous pacing is often used as a bridge to permanent pacemaker placement. It can be kept in place until a permanent pacemaker is implanted or until there is no longer a need for a pacemaker and then it is removed.


=== Permanent transvenous pacing ===
Permanent pacing with an implantable pacemaker involves transvenous placement of one or more pacing electrodes within a chamber, or chambers, of the heart, while the pacemaker is implanted inside the skin under the clavicle. The procedure is performed by incision of a suitable vein into which the electrode lead is inserted and passed along the vein, through the valve of the heart, until positioned in the chamber. The procedure is facilitated by fluoroscopy which enables the physician to view the passage of the electrode lead. After satisfactory lodgement of the electrode is confirmed, the opposite end of the electrode lead is connected to the pacemaker generator.
There are three basic types of permanent pacemakers, classified according to the number of chambers involved and their basic operating mechanism:
Single-chamber pacemaker. In this type, only one pacing lead is placed into a chamber of the heart, either the atrium or the ventricle.
Dual-chamber pacemaker. Here, wires are placed in two chambers of the heart. One lead paces the atrium and one paces the ventricle. This type more closely resembles the natural pacing of the heart by assisting the heart in coordinating the function between the atria and ventricles.
Biventricular pacemaker. This pacemaker has three wires placed in three chambers of the heart. One in the atrium and two in either ventricle. It is more complicated to implant.
Rate-responsive pacemaker. This pacemaker has sensors that detect changes in the patient's physical activity and automatically adjust the pacing rate to fulfill the body's metabolic needs.The pacemaker generator is a hermetically sealed device containing a power source, usually a lithium battery, a sensing amplifier which processes the electrical manifestation of naturally occurring heart beats as sensed by the heart electrodes, the computer logic for the pacemaker and the output circuitry which delivers the pacing impulse to the electrodes.
Most commonly, the generator is placed below the subcutaneous fat of the chest wall, above the muscles and bones of the chest. However, the placement may vary on a case by case basis.
The outer casing of pacemakers is so designed that it will rarely be rejected by the body's immune system. It is usually made of titanium, which is inert in the body.


=== Leadless pacing ===
Leadless pacemakers are devices that are small enough to allow the generator to be placed within the heart, therefore avoiding the need for pacing leads.    As pacemaker leads can fail over time, a pacing system that avoids these components offers theoretical advantages.  Leadless pacemakers can be implanted into the heart using a steerable catheter fed into the femoral vein via an incision in the groin.


== Basic function ==

Modern pacemakers usually have multiple functions. The most basic form monitors the heart's native electrical rhythm. When the pacemaker wire or ""lead"" does not detect heart electrical activity in the chamber - atrium or ventricle - within a normal beat-to-beat time period - most commonly one second - it will stimulate either the atrium or the ventricle with a short low voltage pulse. If it does sense electrical activity, it will hold off stimulating. This sensing and stimulating activity continues on a beat by beat basis and is called ""demand pacing"". In the case of a dual chamber device, when the upper chambers have a spontaneous or stimulated activation, the device starts a countdown to ensure that in an acceptable - and programmable - interval, there is an activation of the ventricle, otherwise again an impulse will be delivered.
The more complex forms include the ability to sense and/or stimulate both the atrial and ventricular chambers.

From this the basic ventricular ""on demand"" pacing mode is VVI or with automatic rate adjustment for exercise VVIR – this mode is suitable when no synchronization with the atrial beat is required, as in atrial fibrillation. The equivalent atrial pacing mode is AAI or AAIR which is the mode of choice when atrioventricular conduction is intact but the natural pacemaker the sinoatrial node is unreliable – sinus node disease (SND) or sick sinus syndrome. Where the problem is atrioventricular block (AVB) the pacemaker is required to detect (sense) the atrial beat and after a normal delay (0.1–0.2 seconds) trigger a ventricular beat, unless it has already happened – this is VDD mode and can be achieved with a single pacing lead with electrodes in the right atrium (to sense) and ventricle (to sense and pace). These modes AAIR and VDD are unusual in the US but widely used in Latin America and Europe. The DDDR mode is most commonly used as it covers all the options though the pacemakers require separate atrial and ventricular leads and are more complex, requiring careful programming of their functions for optimal results.


== Biventricular pacing ==

Cardiac resynchronization therapy (CRT) is used for people with heart failure in whom the left and right ventricles do not contract simultaneously (ventricular dyssynchrony), which occurs in approximately 25–50% of heart failure patients. To achieve CRT, a biventricular pacemaker (BVP) is used, which can pace both the septal and lateral walls of the left ventricle. By pacing both sides of the left ventricle, the pacemaker can resynchronize the ventricular contractions.
CRT devices have at least two leads, one passing through the vena cava and the right atrium into the right ventricle to stimulate the septum, and another passing through the vena cava and the right atrium and inserted through the coronary sinus to pace the epicardial wall of the left ventricle. Often, for patients in normal sinus rhythm, there is also a lead in the right atrium to facilitate synchrony with the atrial contraction. Thus, timing between the atrial and ventricular contractions, as well as between the septal and lateral walls of the left ventricle can be adjusted to achieve optimal cardiac function.
CRT devices have been shown to reduce mortality and improve quality of life in patients with heart failure symptoms; a LV ejection fraction less than or equal to 35% and QRS duration on EKG of 120 ms or greater.Biventricular pacing alone is referred to as CRT-P (for pacing). For selected patients at risk of arrhythmias, CRT can be combined with an implantable cardioverter-defibrillator (ICD): such devices, known as CRT-D (for defibrillation), also provide effective protection against life-threatening arrhythmias.


== His bundle pacing ==
Conventional placement of ventricular leads in or around the tip or apex of the right ventricle, or RV apical pacing, can have negative effects on heart function. Indeed, it has been associated with increased risk of atrial fibrillation, heart failure, weakening of the heart muscle and potentially shorter life expectancy. His bundle pacing (HBP) leads to a more natural or perfectly natural ventricular activation and has generated strong research and clinical interest. By stimulating the His–Purkinje fiber network directly with a special lead and placement technique, HBP causes a synchronized and therefore more effective ventricular activation and avoid long term heart muscle disease. HBP in some cases can also correct bundle branch block patterns.


== Advancements in function ==

A major step forward in pacemaker function has been to attempt to mimic nature by utilizing various inputs to produce a rate-responsive pacemaker using parameters such as the QT interval, pO2 – pCO2 (dissolved oxygen or carbon dioxide levels) in the arterial-venous system, physical activity as determined by an accelerometer, body temperature, ATP levels, adrenaline, etc.
Instead of producing a static, predetermined heart rate, or intermittent control, such a pacemaker, a 'Dynamic Pacemaker', could compensate for both actual respiratory loading and potentially anticipated respiratory loading. The first dynamic pacemaker was invented by Anthony Rickards of the National Heart Hospital, London, UK, in 1982.Dynamic pacemaking technology could also be applied to future artificial hearts. Advances in transitional tissue welding would support this and other artificial organ/joint/tissue replacement efforts. Stem cells may be of interest in transitional tissue welding.Many advancements have been made to improve the control of the pacemaker once implanted. Many of these have been made possible by the transition to microprocessor controlled pacemakers. Pacemakers that control not only the ventricles but the atria as well have become common. Pacemakers that control both the atria and ventricles are called dual-chamber pacemakers. Although these dual-chamber models are usually more expensive, timing the contractions of the atria to precede that of the ventricles improves the pumping efficiency of the heart and can be useful in congestive heart failure.
Rate responsive pacing allows the device to sense the physical activity of the patient and respond appropriately by increasing or decreasing the base pacing rate via rate response algorithms.
The DAVID trials have shown that unnecessary pacing of the right ventricle can exacerbate heart failure and increases the incidence of atrial fibrillation. The newer dual chamber devices can keep the amount of right ventricle pacing to a minimum and thus prevent worsening of the heart disease.


== Considerations ==


=== Insertion ===
A pacemaker may be implanted whilst a person is awake using local anesthetic to numb the skin with or without sedation, or asleep using a general anesthetic. An antibiotic is usually given to reduce the risk of infection.  Pacemakers are generally implanted in the front of the chest in the region of the left or right shoulder.  The skin is prepared by clipping or shaving any hair over the implant site before cleaning the skin with a disinfectant such as chlorhexidine.  An incision is made below the collar bone and a space or pocket is created under the skin to house the pacemaker generator.  This pocket is usually created just above the pectoralis major muscle (prepectoral), but in some cases the device may be inserted beneath the muscle (submuscular). The lead or leads are fed into the heart through a large vein guided by X-ray imaging (fluoroscopy). The tips of the leads may be positioned within the right ventricle, the right atrium, or the coronary sinus, depending on the type of pacemaker required. Surgery is typically completed within 30 to 90 minutes.  Following implantation, the surgical wound should be kept clean and dry until it has healed.  Care should be taken to avoid excessive movement of the shoulder within the first few weeks to reduce the risk of dislodging the pacemaker leads.The batteries within a pacemaker generator typically last 5 to 10 years. When the batteries are nearing the end of life, the generator is replaced in a procedure that is usually simpler than a new implant. Replacement involves making an incision to remove the existing device, disconnecting the leads from the old device and reconnecting them to a new generator, reinserting the new device and closing the skin.


==== Periodic pacemaker checkups ====

Once the pacemaker is implanted, it is periodically checked to ensure the device is operational and performing appropriately. Depending on the frequency set by the following physician, the device can be checked as often as is necessary. Routine pacemaker checks are typically done in-office every six (6) months, though will vary depending upon patient/device status and remote monitoring availability. Newer pacemaker models can also be interrogated remotely, with the patient transmitting their pacemaker data using an at-home transmitter connected to their geographical cellular network. This data can then be accessed by the technician through the device manufacturer's web portal.
At the time of in-office follow-up, the device will be interrogated to perform diagnostic testing. These tests include:

Sensing: the ability of the device to ""see"" intrinsic cardiac activity (Atrial and ventricular depolarization).
Impedance: A test to measure lead integrity. Large and/or sudden increases in impedance can be indicative of a lead fracture while large and/or sudden decreases in impedance can signify a breach in lead insulation.
Threshold amplitude: The minimum amount of energy (generally in hundredths of volts) required in order to pace the atrium or ventricle connected to the lead.
Threshold duration: The amount of time that the device requires at the preset amplitude to reliably pace the atrium or ventricle connected to the lead.
Percentage of pacing: Defines how dependent the patient is on the device, the percentage of time that the pacemaker has been actively pacing since the previous device interrogation.
Estimated battery life at current rate: As modern pacemakers are ""on-demand"", meaning that they only pace when necessary, device longevity is affected by how much it is utilized. Other factors affecting device longevity include programmed output and algorithms (features) causing a higher level of current drain from the battery.
Any events that were stored since the last follow-up, in particular arrhythmias such as atrial fibrillation. These are typically stored based on specific criteria set by the physician and specific to the patient. Some devices have the availability to display intracardiac electrograms of the onset of the event as well as the event itself. This is especially helpful in diagnosing the cause or origin of the event and making any necessary programming changes.


=== Magnetic fields, MRIs, and other lifestyle issues ===
A patient's lifestyle is usually not modified to any great degree after insertion of a pacemaker. There are a few activities that are unwise such as full contact sports and activities that involve intense magnetic fields.
The pacemaker patient may find that some types of everyday actions need to be modified. For instance, the shoulder harness of a vehicle seatbelt may be uncomfortable if the harness should fall across the pacemaker insertion site.
If the patient does wish to practice any type of sport or physical activity, special pacemaker protection can be worn to prevent possible physical injuries or damage to the pacemaker leads.
Any kind of an activity that involves intense electro-magnetic fields should be avoided. This includes activities such as arc welding possibly, with certain types of equipment, or maintaining heavy equipment that may generate intense magnetic fields (such as a magnetic resonance imaging (MRI) machine).
However, in February 2011 the FDA approved a new pacemaker device from Medtronic called the Revo MRI SureScan which was the first to be labeled as conditional for MRI use. There are several limitations to its use including certain patients' qualifications and scan settings. An MRI conditional device has to be reprogrammed right before and right after MRI scanning.  All the 5 most common cardiac pacing device manufacturers (covering more than 99% of the US market) now have FDA-approved MR-conditional pacemakers.A 2008 US study has found that the magnetic field created by some headphones included with portable music players or cell phones, when placed within inches of pacemakers, may cause interference.
In addition, according to the American Heart Association, some home devices have a remote potential to cause interference by occasionally inhibiting a single beat. Cellphones available in the United States (less than 3 watts) do not seem to damage pulse generators or affect how the pacemaker works.Having a pacemaker does not imply that a patient requires the use of antibiotics to be administered before procedures such as dental work. The patient should inform all medical personnel that he or she has a pacemaker. The use of MRI may be ruled out by the patient having a pacemaker manufactured before MRI conditional devices became common, or by the patient having old pacing wires abandoned inside the heart, no longer connected to their pacemaker.


=== Turning off the pacemaker ===
A panel of The Heart Rhythm Society, a specialist organization based in Washington, DC found that it was legal and ethical to honor requests by patients, or by those with legal authority to make decisions for patients, to deactivate implanted cardiac devices. Lawyers say that the legal situation is similar to removing a feeding tube, though there is currently no legal precedent involving pacemakers in the United States of America. A patient in the United States is thought to have a right to refuse or discontinue treatment, including a pacemaker that keeps him or her alive. Physicians have a right to refuse to turn it off, but are advised by the HRS panel that they should refer the patient to a physician who will. Some patients believe that hopeless, debilitating conditions, like those brought on by severe strokes or late-stage dementia, can cause so much suffering that they would prefer not to prolong their lives with supportive measures, such as cardiac devices.


=== Privacy and security ===
Security and privacy concerns have been raised with pacemakers that allow wireless communication. Unauthorized third parties may be able to read patient records contained in the pacemaker, or reprogram the devices, as has been demonstrated by a team of researchers. The demonstration worked at short range; they did not attempt to develop a long range antenna. The proof of concept exploit helps demonstrate the need for better security and patient alerting measures in remotely accessible medical implants. In response to this threat, Purdue University and Princeton University researchers have developed a prototype firewall device, called MedMon, which is designed to protect wireless medical devices such as pacemakers and insulin pumps from attackers.


=== Complications ===

Complications from having surgery to implant a pacemaker are uncommon (each 1-3 % approximately), but could include: infection where the pacemaker is implanted or in the bloodstream; allergic reaction to the dye or anesthesia used during the procedure; swelling, bruising or bleeding at the generator site, or around the heart, especially if the patient is taking blood thinners, elderly, of thin frame or otherwise on chronic steroids use. A possible complication of dual-chamber artificial pacemakers is 'pacemaker-mediated tachycardia' (PMT), a form of reentrant tachycardia. In PMT, the artificial pacemaker forms the anterograde (atrium to ventricle) limb of the circuit and the atrioventricular (AV) node forms the retrograde limb (ventricle to atrium) of the circuit. Treatment of PMT typically involves reprogramming the pacemaker.Another possible complication is ""pacemaker-tracked tachycardia,"" where a supraventricular tachycardia such as atrial fibrillation or atrial flutter is tracked by the pacemaker and produces beats from a ventricular lead. This is becoming exceedingly rare as newer devices are often programmed to recognize supraventricular tachycardias and switch to non-tracking modes.
Sometimes the leads, which are small diameter wires, from the pacemaker to the implantation site in the heart muscle will need to be removed. The most common reason for lead removal is infection, however over time leads can degrade due to a number of reasons such as lead flexing. Changes to programming of the pacemaker may overcome lead degradation to some extent. However, a patient who has several pacemaker replacements over a decade or two in which the leads were reused may require a lead replacement surgery.
Lead replacement may be done in one of two ways. Insert a new set of leads without removing the current leads (not recommended as it provides additional obstruction to blood flow and heart valve function) or remove the current leads and then insert replacements. The lead removal technique will vary depending on the surgeon's estimation of the probability that simple traction will suffice to more complex procedures. Leads can normally be disconnected from the pacemaker easily which is why device replacement usually entails simple surgery to access the device and replace it by simply unhooking the leads from the device to replace and hooking the leads to the new device. The possible complications, such as perforation of the heart wall, come from removing the lead{s} from the patient's body.
The other end of a pacemaker lead is actually implanted into the heart muscle with a miniature screw or anchored with small plastic hooks called tines. In addition, the longer the leads have been implanted starting from a year or two, the more likely that they will have attachments to the patient's body at various places in the pathway from device to heart muscle, since the human body tends to incorporate foreign devices into tissue. In some cases, for a lead that has been inserted for a short amount of time, removal may involve simple traction to pull the lead from the body. Removal in other cases is typically done with a laser or cutting device which threads like a cannula with a cutting edge over the lead and is moved down the lead to remove any organic attachments with tiny cutting lasers or similar device.
Pacemaker lead malposition in various locations has been described in the literature. Depending on the location of the pacer lead and symptoms treatment varies.Another possible complication called twiddler's syndrome occurs when a patient manipulates the pacemaker and causes the leads to be removed from their intended location and causes possible stimulation of other nerves.


== Other devices ==
Sometimes devices resembling pacemakers, called implantable cardioverter-defibrillators (ICDs) are implanted. These devices are often used in the treatment of patients at risk from sudden cardiac death. An ICD has the ability to treat many types of heart rhythm disturbances by means of pacing, cardioversion, or defibrillation. Some ICD devices can distinguish between ventricular fibrillation and ventricular tachycardia (VT), and may try to pace the heart faster than its intrinsic rate in the case of VT, to try to break the tachycardia before it progresses to ventricular fibrillation. This is known as fast-pacing, overdrive pacing, or anti-tachycardia pacing (ATP). ATP is only effective if the underlying rhythm is ventricular tachycardia, and is never effective if the rhythm is ventricular fibrillation.


== History ==


=== Origin ===
In 1889, John Alexander MacWilliam reported in the British Medical Journal (BMJ) of his experiments in which application of an electrical impulse to the human heart in asystole caused a ventricular contraction and that a heart rhythm of 60–70 beats per minute could be evoked by impulses applied at spacings equal to 60–70/minute.In 1926, Mark C Lidwill of the Royal Prince Alfred Hospital of Sydney, supported by physicist Edgar H. Booth of the University of Sydney, devised a portable apparatus which ""plugged into a lighting point"" and in which ""One pole was applied to a skin pad soaked in strong salt solution"" while the other pole ""consisted of a needle insulated except at its point, and was plunged into the appropriate cardiac chamber"". ""The pacemaker rate was variable from about 80 to 120 pulses per minute, and likewise the voltage variable from 1.5 to 120 volts"". In 1928, the apparatus was used to revive a stillborn infant at Crown Street Women's Hospital, Sydney whose heart continued ""to beat on its own accord"", ""at the end of 10 minutes"" of stimulation.In 1932, American physiologist Albert Hyman, with the help of his brother, described an electro-mechanical instrument of his own, powered by a spring-wound hand-cranked motor. Hyman himself referred to his invention as an ""artificial pacemaker"", the term continuing in use to this day.An apparent hiatus in publication of research conducted between the early 1930s and World War II may be attributed to the public perception of interfering with nature by ""reviving the dead"". For example, ""Hyman did not publish data on the use of his pacemaker in humans because of adverse publicity, both among his fellow physicians, and due to newspaper reporting at the time. Lidwell may have been aware of this and did not proceed with his experiments in humans"".


=== Transcutaneous ===
In 1950, Canadian electrical engineer John Hopps designed and built the first external pacemaker based upon observations by cardio-thoracic surgeons Wilfred Gordon Bigelow and John Callaghan at Toronto General Hospital, although the device was first tested on a dog at the University of Toronto's Banting Institute. A substantial external device using vacuum tube technology to provide transcutaneous pacing, it was somewhat crude and painful to the patient in use and, being powered from an AC wall socket, carried a potential hazard of electrocution of the patient and inducing ventricular fibrillation.
A number of innovators, including Paul Zoll, made smaller but still bulky transcutaneous pacing devices from 1952 using a large rechargeable battery as the power supply.In 1957, William L. Weirich published the results of research performed at the University of Minnesota. These studies demonstrated the restoration of heart rate, cardiac output and mean aortic pressures in animal subjects with complete heart block through the use of a myocardial electrode.In 1958 Colombian doctor Alberto Vejarano Laverde and Colombian electrical engineer Jorge Reynolds Pombo constructed an external pacemaker, similar to those of Hopps and Zoll, weighing 45 kg and powered by a 12 volt car lead–acid battery, but connected to electrodes attached to the heart. This apparatus was successfully used to sustain a 70-year-old priest, Gerardo Florez.The development of the silicon transistor and its first commercial availability in 1956 was the pivotal event that led to rapid development of practical cardiac pacemaking.


=== Wearable ===
In 1958, engineer Earl Bakken of Minneapolis, Minnesota, produced the first wearable external pacemaker for a patient of C. Walton Lillehei. This transistorized pacemaker, housed in a small plastic box, had controls to permit adjustment of pacing heart rate and output voltage and was connected to electrode leads which passed through the skin of the patient to terminate in electrodes attached to the surface of the myocardium of the heart.
One of the earliest patients to receive this Lucas pacemaker device was a woman in her early 30s in an operation carried out in 1964 at the Radcliffe Infirmary in Oxford by cardiac surgeon Alf Gunning from South Africa and later Professor Gunning who was a student of Christiaan Barnard. This pioneering operation was carried out under the guidance of cardiac consultant Peter Sleight at the Radcliffe Infirmary in Oxford and his cardiac research team at St George's Hospital in London. Sleight later became Professor of Cardiovascular Medicine at Oxford University.


=== Implantable ===

The first clinical implantation into a human of a fully implantable pacemaker was in 1958 at the Karolinska Institute in Solna, Sweden, using a pacemaker designed by inventor Rune Elmqvist and surgeon Åke Senning (in collaboration with Elema-Schönander AB, later Siemens-Elema AB), connected to electrodes attached to the myocardium of the heart by thoracotomy. The device failed after three hours. A second device was then implanted which lasted for two days. The world's first implantable pacemaker patient, Arne Larsson, went on to receive 26 different pacemakers during his lifetime. He died in 2001, at the age of 86, outliving the inventor as well as the surgeon.In 1959, temporary transvenous pacing was first demonstrated by Seymour Furman and John Schwedel, whereby the catheter electrode was inserted via the patient's basilic vein.In February 1960, an improved version of the Swedish Elmqvist design was implanted in Montevideo, Uruguay in the Casmu 1 Hospital by Doctors Orestes Fiandra and Roberto Rubio. That device lasted until the patient died of other ailments, nine months later. The early Swedish-designed devices used rechargeable batteries, which were charged by an induction coil from the outside. It was the first pacemaker implanted in America.
Implantable pacemakers constructed by engineer Wilson Greatbatch entered use in humans from April 1960 following extensive animal testing. The Greatbatch innovation varied from the earlier Swedish devices in using primary cells (mercury battery) as the energy source. The first patient lived for a further 18 months.
The first use of transvenous pacing in conjunction with an implanted pacemaker was by Parsonnet in the United States, Lagergren in Sweden and Jean-Jacques Welti in France in 1962–63.
The transvenous, or pervenous, procedure involved incision of a vein into which was inserted the catheter electrode lead under fluoroscopic guidance, until it was lodged within the trabeculae of the right ventricle. This method was to become the method of choice by the mid-1960s.
Cardiothoracic surgeon Leon Abrams and medical engineer Ray Lightwood developed and implanted the first patient-controlled variable-rate heart pacemaker in 1960 at Birmingham University. The first implant took place in March 1960, with two further implants the following month. These three patients made good recoveries and returned to a high quality of life. By 1966, 56 patients had undergone implantation with one surviving for over ​5 1⁄2 years.


=== Lithium battery ===

The preceding implantable devices all suffered from the unreliability and short lifetime of the available primary cell technology which was mainly that of the mercury battery. In the late 1960s, several companies, including ARCO in the USA, developed isotope-powered pacemakers, but this development was overtaken by the development in 1971 of the lithium iodide cell battery by Wilson Greatbatch. Lithium-iodide or lithium anode cells became the standard for future pacemaker designs.
A further impediment to reliability of the early devices was the diffusion of water vapour from the body fluids through the epoxy resin encapsulation affecting the electronic circuitry. This phenomenon was overcome by encasing the pacemaker generator in a hermetically sealed metal case, initially by Telectronics of Australia in 1969 followed by Cardiac Pacemakers Inc of Minneapolis in 1972. This technology, using titanium as the encasing metal, became the standard by the mid-1970s.
On July 9, 1974, Manuel A. Villafaña and Anthony Adducci founders of Cardiac Pacemakers, Inc. (Guidant) in St. Paul, Minnesota, manufactured the world's first pacemaker with a lithium anode and a lithium-iodide electrolyte solid-state battery.


=== Intra-cardial ===
In 2013, multiple firms announced devices that could be inserted via a leg catheter rather than invasive surgery. The devices are roughly the size and shape of a pill, much smaller than the size of a traditional pacemaker. Once implanted, the device's prongs contact the muscle and stabilize heartbeats. Engineers and scientists are currently working on this type of device. In November 2014 a patient, Bill Pike of Fairbanks, Alaska, received a Medtronic Micra pacemaker in Providence St Vincent Hospital in Portland Oregon. D. Randolph Jones was the EP doctor. In 2014 also St. Jude Medical Inc. announced the first enrollments in the company’s leadless Pacemaker Observational Study evaluating the Nanostim leadless pacing technology. The Nanostim pacemaker received CE marking in 2013. The post-approval implants have occurred in Europe. The European study was recently stopped, after there were reports of six perforations that led to two patient deaths. After investigations St Jude Medical restarted the study. But in the United States this therapy is still not approved by the FDA. While the St Jude Nanostim and the Medtronic Micra are just single-chamber pacemakers it is anticipated that leadless dual-chamber pacing for patients with atrioventricular block will become possible with further development.


=== Reusable pacemakers ===
Thousands of pacemakers are removed by funeral home personnel each year all over the world. They have to be removed postmortem from bodies that are going to be cremated to avoid explosions. It is a fairly simple procedure that can be carried out by a mortician. Pacemakers with significant battery life are potentially life-saving devices for people in low and middle income countries (LMICs). The Institute of Medicine, a United States non-governmental organization, has reported that inadequate access to advanced cardiovascular technologies is one of the major contributors to cardiovascular disease morbidity and mortality in LMICs. Ever since the 1970s, multiple studies all over the world have reported on the safety and efficacy of pacemaker reuse. As of 2016, widely acceptable standards for safe pacemaker and ICD reuse have not been developed, and there continue to be legal and regulatory barriers to widespread adoption of medical device reuse.


== Manufacturers ==
Current and prior manufacturers of implantable pacemakers

Biotronik (Germany)
Boston Scientific (USA)
Guidant (USA) (now owned by Boston Scientific)
Intermedics (USA)
Lepu Medical (China)
Medico (Italy)
Medtronic (USA)
Sorin Group (Italy) (merged with Cyberonics to form LivaNova)
St. Jude Medical (USA) (now owned by Abbott Laboratories)


== See also ==
Biological pacemaker
Button cell
Electrical conduction system of the heart
Implantable cardioverter-defibrillator
Infective endocarditis
Pacemaker syndrome


== References ==


== External links ==
Detecting and Distinguishing Cardiac Pacing Artifacts
Implantable Cardioverter Defibrillator from National Heart, Lung and Blood Institute
Current indications for CRT-P and CRT-D: Webinar from the European Heart Rhythm Association (EHRA)","pandas(index=88, _1=88, text='a cardiac pacemaker (or artificial pacemaker, so as not to be confused with the natural pacemaker of the heart), is a medical device that generates electrical impulses delivered by electrodes to cause the heart muscle chambers (the upper, or atria and/or the lower, or ventricles) to contract and therefore pump blood; by doing so this device replaces and/or regulates the function of the electrical conduction system of the heart. the primary purpose of a pacemaker is to maintain an adequate heart rate, either because the heart\'s natural pacemaker is not fast enough, or because there is a block in the heart\'s electrical conduction system. modern pacemakers are externally programmable and allow a cardiologist, particularly a cardiac electrophysiologist to select the optimal pacing modes for individual patients. a specific type of pacemaker called a defibrillator combines pacemaker and defibrillator functions in a single implantable device, which should be called a defibrillator, for clarity. others, called biventricular pacemakers have multiple electrodes stimulating differing positions within the lower heart chambers to improve synchronization of the ventricles, the lower chambers of the heart.   == methods of pacing == thousands of pacemakers are removed by funeral home personnel each year all over the world. they have to be removed postmortem from bodies that are going to be cremated to avoid explosions. it is a fairly simple procedure that can be carried out by a mortician. pacemakers with significant battery life are potentially life-saving devices for people in low and middle income countries (lmics). the institute of medicine, a united states non-governmental organization, has reported that inadequate access to advanced cardiovascular technologies is one of the major contributors to cardiovascular disease morbidity and mortality in lmics. ever since the 1970s, multiple studies all over the world have reported on the safety and efficacy of pacemaker reuse. as of 2016, widely acceptable standards for safe pacemaker and icd reuse have not been developed, and there continue to be legal and regulatory barriers to widespread adoption of medical device reuse.   == manufacturers == current and prior manufacturers of implantable pacemakers  biotronik (germany) boston scientific (usa) guidant (usa) (now owned by boston scientific) intermedics (usa) lepu medical (china) medico (italy) medtronic (usa) sorin group (italy) (merged with cyberonics to form livanova) st. jude medical (usa) (now owned by abbott laboratories)   == see also == biological pacemaker button cell electrical conduction system of the heart implantable cardioverter-defibrillator infective endocarditis pacemaker syndrome   == references ==   == external links == detecting and distinguishing cardiac pacing artifacts implantable cardioverter defibrillator from national heart, lung and blood institute current indications for crt-p and crt-d: webinar from the european heart rhythm association (ehra)')"
89,"Tissue engineering is a biomedical engineering discipline that uses a combination of cells, engineering, materials methods, and suitable biochemical and physicochemical factors to restore, maintain, improve, or replace different types of biological tissues. Tissue engineering often involves the use of cells placed on tissue scaffolds in the formation of new viable tissue for a medical purpose but is not limited to applications involving cells and tissue scaffolds. While it was once categorized as a sub-field of biomaterials, having grown in scope and importance it can be considered as a field in its own.

While most definitions of tissue engineering cover a broad range of applications, in practice the term is closely associated with applications that repair or replace portions of or whole tissues (i.e., bone, cartilage, blood vessels, bladder, skin, muscle etc.). Often, the tissues involved require certain mechanical and structural properties for proper functioning. The term has also been applied to efforts to perform specific biochemical functions using cells within an artificially-created support system (e.g. an artificial pancreas, or a bio artificial liver). The term regenerative medicine is often used synonymously with tissue engineering, although those involved in regenerative medicine place more emphasis on the use of stem cells or progenitor cells to produce tissues.


== Overview ==

A commonly applied definition of tissue engineering, as stated by Langer and Vacanti, is ""an interdisciplinary field that applies the principles of engineering and life sciences toward the development of biological substitutes that restore, maintain, or improve [Biological tissue] function or a whole organ"". In addition, Langer and Vacanti also state that there are three main types of tissue engineering: cells, tissue-inducing substances, and a cells + matrix approach (often referred to as a scaffold).Tissue engineering has also been defined as ""understanding the principles of tissue growth, and applying this to produce functional replacement tissue for clinical use"". A further description goes on to say that an ""underlying supposition of tissue engineering is that the employment of natural biology of the system will allow for greater success in developing therapeutic strategies aimed at the replacement, repair, maintenance, or enhancement of tissue function"".Developments in the multidisciplinary field of tissue engineering have yielded a novel set of tissue replacement parts and implementation strategies. Scientific advances in biomaterials, stem cells, growth and differentiation factors, and biomimetic environments have created unique opportunities to fabricate or improve existing tissues in the laboratory from combinations of engineered extracellular matrices (""scaffolds""), cells, and biologically active molecules. Among the major challenges now facing tissue engineering is the need for more complex functionality, biomechanical stability, and vascularization in laboratory-grown tissues destined for transplantation. The continued success of tissue engineering and the eventual development of true human replacement parts will grow from the convergence of engineering and basic research advances in tissue, matrix, growth factor, stem cell, and developmental biology, as well as materials science and bioinformatics.
In 2003, the NSF published a report entitled ""The Emergence of Tissue Engineering as a Research Field"", which gives a thorough description of the history of this field.


== Etymology ==
The historic origins of the term are unclear as the definition of the word has changed throughout the past decades. The term first appeared in a 1984 publication that described the organization of an endothelium-like membrane on the surface of a long-implanted, synthetic ophthalmic prosthesis The first modern use of the term as recognized today was in 1985 by the researcher, physiologist and bioengineer Y.C Fung of the Engineering Research Center. He proposed the joining of the terms tissue (in reference to the fundamental relationship between cells and organs) and engineering (in reference to the field of modification of said tissues). The term was officially adopted in 1987.


== History ==


=== Ancient Era (Pre-17th Century) ===
A rudimentary understanding of the inner workings of human tissues may date back further than most would expect. As early as the Neolithic period, sutures were being used to close wounds and aid in healing. Later on, societies such as ancient Egypt developed better materials for sewing up wounds such as linen sutures. Around 2500 BC in ancient India, skin grafts were developed by cutting skin from the buttock and suturing it to wound sites in the ear, nose, or lips. Ancient Egyptians often would graft skin from corpses onto living humans and even attempted to use honey as a type of antibiotic and grease as a protective barrier to prevent infection. In the 1st and 2nd centuries AD, Gallo-Romans developed wrought iron implants and dental implants could be found in ancient Mayans.
Enlightenment (17th Century-19th Century)
While these ancient societies had developed techniques that were way ahead of their time, they still lacked a mechanistic understanding of how the body was reacting to these procedures. This mechanistic approach came along in tandem with the development of the empirical method of science pioneered by Rene Descartes. Sir Isaac Newton began to describe the body as a “physiochemical machine” and postured that disease was a breakdown in the machine. In the 17th century, Robert Hooke discovered the cell and a letter from Benedict de Spinoza brought forward the idea of the homeostasis between the dynamic processes in the body. Hydra experiments performed by Abraham Trembley in the 18th century began to delve into the regenerative capabilities of cells. During the 19th century, a better understanding of how different metals reacted with the body led to the development of better sutures and a shift towards screw and plate implants in bone fixation. Further, it was first hypothesized in the mid-1800’s that cell-environment interactions and cell proliferation were vital for tissue regeneration.


=== Modern Era (20th and  21st Centuries) ===
As time progresses and technology advances, there is a constant need for change in the approach researchers take in their studies. Tissue engineering has continued to evolve over centuries. In the beginning people used to look at and use samples directly from human or animal cadavers. Now, tissue engineers have the ability to remake many of the tissues in the body through the use of modern techniques such as microfabrication and three-dimensional bioprinting in conjunction with native tissue cells/stem cells. These advances have allowed researchers to generate new tissues in a much more efficient manner. For example, these techniques allow for more personalization which allow for better biocompatibility, decreased immune response, cellular integration, and longevity. There is no doubt that these techniques will continue to evolve, as we have continued to see microfabrication and bioprinting evolve over the past decade.
In 1960, Wichterle and Lim were the first to publish experiments on hydrogels for biomedical applications by using them in contact lens construction. Work on the field developed slowly over the next two decades, but later found traction when hydrogels were repurposed for drug delivery. In 1984, Charles Hull developed bioprinting by converting a Hewlett-Packard inkjet printer into a device capable of depositing cells in 2D. 3D printing is a type of additive manufacturing which has since found various applications in Medical engineering, due to its high precision and efficiency. With Biologist James Thompson’s development of first human stem cell lines in 1998 followed by transplantation of first laboratory-grown internal organs in 1999 and creation of the first bioprinter in 2003 by the University of Missouri when they printed spheroids without the need of scaffolds, 3D bioprinting became more conventionally used in medical field than ever before. 
So far, scientists have been able to print mini organoids and organs-on-chips that have rendered practical insights into the functions of a human body. Pharmaceutical companies are using these models to test drugs before moving on to animal studies. However, a fully functional and structurally similar organ hasn’t been printed yet. A team at University of Utah has reportedly printed ears and successfully transplanted those onto children born with defects that left their ears partially developed.
Today hydrogels are considered the preferred choice of bioinks for 3D bioprinting since they mimic cells’ natural ECM while also containing strong mechanical properties capable of sustaining 3D structures. Furthermore, hydrogels in conjunction with 3D bioprinting allow researchers to produce different scaffolds which can be used to form new tissues or organs.
3-D printed tissues still face many challenges such as adding vasculature. Meanwhile, 3-D printing parts of tissues definitely will improve our understanding of the human body, thus accelerating both basic and clinical research.


== Examples ==

As defined by Langer and Vacanti, examples of tissue engineering fall into one or more of three categories: ""just cells,"" ""cells and scaffold,"" or ""tissue-inducing factors."" 

In vitro meat: Edible artificial animal muscle tissue cultured in vitro.
Bioartificial liver device, “Temporary Liver”, Extracorporeal Liver Assist Device (ELAD): The human hepatocyte cell line (C3A line) in a hollow fiber bioreactor can mimic the hepatic function of the liver for acute instances of liver failure. A fully capable ELAD would temporarily function as an individual’s liver, thus avoiding transplantation and allowing regeneration of their own liver.
Artificial pancreas: Research involves using islet cells to regulate the body’s blood sugar, particularly in cases of diabetes . Biochemical factors may be used to cause human pluripotent stem cells to differentiate (turn into) cells that function similarly to beta cells, which are in an islet cell in charge of producing insulin.
Artificial bladders: Anthony Atala (Wake Forest University) has successfully implanted artificial bladders, constructed of cultured cells seeded onto a bladder-shaped scaffold, into seven out of approximately 20 human test subjects as part of a long-term experiment.
Cartilage: lab-grown cartilage, cultured in vitro on a scaffold, was successfully used as an autologous transplant to repair patients' knees.
Scaffold-free cartilage: Cartilage generated without the use of exogenous scaffold material. In this methodology, all material in the construct is cellular produced directly by the cells.
bioartificial heart: Doris Taylor's lab constructed a biocompatible rat heart by re-cellularizing a de-cellularized rat heart. This scaffold and cells were placed in a bioreactor, where it matured to become a partially or fully transplantable organ. the work was called a ""landmark"". The lab first stripped the cells away from a rat heart (a process called ""decellularization"") and then injected rat stem cells into the decellularized rat heart.
Tissue-engineered airway: A donor trachea was successfully decellularized and recellularized with autologous cells and transplanted into the recipient.
Tissue-engineered blood vessels: Blood vessels that have been grown in a lab and can be used to repair damaged blood vessels without eliciting an immune response.
Artificial skin constructed from human skin cells embedded in a hydrogel, such as in the case of bio-printed constructs for battlefield burn repairs.
Artificial bone marrow: Bone marrow cultured in vitro to be transplanted serves as a ""just cells"" approach to tissue engineering.
Tissue engineered bone: A structural matrix can be composed of metals such as titanium, polymers of varying degradation rates, or certain types of ceramics. Materials are often chosen to recruit osteoblasts to aid in reforming the bone and returning biological function. Various types of cells can be added directly into the matrix to expediate the process.
Laboratory-grown penis: Decellularized scaffolds of rabbit penises were recellularized with smooth muscle and endothelial cells. The organ was then transplanted to live rabbits and functioned comparably to the native organ, suggesting potential as treatment for genital trauma.
Oral mucosa tissue engineering uses a cells and scaffold approach to replicate the 3 dimensional structure and function of oral mucosa.


== Cells as building blocks ==

Cells are one of the main components for the success of tissue engineering approaches. Tissue engineering uses cells as strategies for creation/replacement of new tissue. Examples include fibroblasts used for skin repair or renewal, chondrocytes used for cartilage repair (MACI -FDA approved product), and hepatocytes used in liver support systems
Cells can be used alone or with support matrices for tissue engineering applications. An adequate environment for promoting cell growth, differentiation, and integration with the existing tissue is a critical factor for cell-based building blocks. Manipulation of any of these cell processes create alternative avenues for the development of new tissue (e.g., reprogramming of somatic cells, vascularization).


=== Isolation ===
Techniques for cell isolation depend on the cell source. Centrifugation and apheresis are techniques used for extracting cells from biofluids (e.g., blood). Whereas digestion processes, typically using enzymes to remove the extracellular matrix (ECM), are required prior to centrifugation or apheresis techniques to extract cells from tissues/organs. Trypsin and collagenase are the most common enzymes used for tissue digestion. While trypsin is temperature dependent, collagenase is less sensitive to changes in temperature.


=== Cell sources ===

Primary cells are those directly isolated from host tissue. These cells provide an ex-vivo model of cell behavior without any genetic, epigenetic, or developmental changes; making them a closer replication of in-vivo conditions than cells derived from other methods. This constraint however, can also make studying them difficult. These are mature cells, often terminally differentiated, meaning that for many cell types proliferation is difficult or impossible. Additionally, the microenvironments these cells exist in are highly specialized, often making replication of these conditions difficult.Secondary cells A portion of cells from a primary culture is moved to a new repository/vessel to continue being cultured. Medium from the primary culture is removed, the cells that are desired to be transferred are obtained, and then cultured in a new vessel with fresh growth medium. A secondary cell culture is useful in order to ensure that cells have both the room and nutrients that they require to grow. Secondary cultures are most notably used in any scenario in which a larger quantity of cells than can be found in the primary culture is desired. Secondary cells share the constraints of primary cells (see above) but have an added risk of contamination when transferring to a new vessel.


=== Genetic classifications of cells ===
Autologous: The donor and the recipient of the cells are the same individual. Cells are harvested, cultured or stored, and then reintroduced to the host. As a result of the host’s own cells being reintroduced, an antigenic response is not elicited. The body’s immune system recognizes these reimplanted cells as its own, and does not target them for attack. Autologous cell dependence on host cell health and donor site morbidity may be deterrents to their use. Adipose-derived and bone marrow-derived mesenchymal stem cells are commonly autologous in nature, and can be used in a myriad of ways, from helping repair skeletal tissue to replenishing beta cells in diabetic patients.Allogenic: Cells are obtained from the body of a donor of the same species as the recipient. While there are some ethical constraints to the use of human cells for in vitro studies (ie. human brain tissue chimerae development ), the employment of dermal fibroblasts from human foreskin demonstrates an immunologically safe and thus a viable choice for allogenic tissue engineering of the skin.
Xenogenic: These cells are derived isolated cells from alternate species from the recipient. A notable example of xenogenic tissue utilization is cardiovascular implant construction via animal cells. Chimeric human-animal farming raises ethical concerns around the potential for improved consciousness from implanting human organs in animals.Syngenic or isogenic: These cells describe those borne from identical genetic code. This imparts an immunologic benefit similar to autologous cell lines (see above). Autologous cells can be considered syngenic, but the classification also extends to non-autologously derived cells such as those from an identical twin, from genetically identical (cloned) research models, or induced stem cells (iSC)  as related to the donor.


=== Stem cells ===
Stem cells are undifferentiated cells with the ability to divide in culture and give rise to different forms of specialized cells. Stem cells are divided into ""adult"" and ""embryonic"" stem cells according to their source. While there is still a large ethical debate related to the use of embryonic stem cells, it is thought that another alternative source-- induced pluripotent stem cells—may be useful for the repair of diseased or damaged tissues, or may be used to grow new organs.
Totipotent cells are stem cells which can divide into further stem cells or differentiate into any cell type in the body, including extra-embryonic tissue.
Pluripotent cells are stem cells which can differentiate into any cell type in the body except extra-embryonic tissue. induced pluripotent stem cells (iPSCs) are  subclass of pluripotent stem cells resembling embryonic stem cells (ESCs) that have been derived from adult differentiated cells. iPSCs are created by altering the expression of transcriptional factors in adult cells until they become like embryonic stem cells. As of November 2020, a popular method is to use modified retroviruses to introduce specific genes into the genome of adult cells to induce them to an embryonic stem cell-like state.Multipotent stem cells can be differentiated into any cell within the same class, such as blood or bone. A common example of multipotent cells is Mesenchymal stem cells (MSCs).


== Scaffolds ==
Scaffolds are materials that have been engineered to cause desirable cellular interactions to contribute to the formation of new functional tissues for medical purposes. Cells are often 'seeded' into these structures capable of supporting three-dimensional tissue formation. Scaffolds mimic the extracellular matrix of the native tissue, recapitulating the in vivo milieu and allowing cells to influence their own microenvironments. They usually serve at least one of the following purposes: allow cell attachment and migration, deliver and retain cells and biochemical factors, enable diffusion of vital cell nutrients and expressed products, exert certain mechanical and biological influences to modify the behaviour of the cell phase.
In 2009, an interdisciplinary team led by the thoracic surgeon Thorsten Walles implanted the first bioartificial transplant that provides an innate vascular network for post-transplant graft supply successfully into a patient awaiting tracheal reconstruction.

To achieve the goal of tissue reconstruction, scaffolds must meet some specific requirements. High porosity and adequate pore size are necessary to facilitate cell seeding and diffusion throughout the whole structure of both cells and nutrients. Biodegradability is often an essential factor since scaffolds should preferably be absorbed by the surrounding tissues without the necessity of surgical removal. The rate at which degradation occurs has to coincide as much as possible with the rate of tissue formation: this means that while cells are fabricating their own natural matrix structure around themselves, the scaffold is able to provide structural integrity within the body and eventually it will break down leaving the newly formed tissue which will take over the mechanical load. Injectability is also important for clinical uses.
Recent research on organ printing is showing how crucial a good control of the 3D environment is to ensure reproducibility of experiments and offer better results.


=== Materials ===
Material selection is an essential aspect of producing a scaffold.  The materials utilized can be natural or synthetic and can be biodegradable or non-biodegradable. Additionally, they must be biocompatible, meaning that they don’t cause any adverse effects to cells. Silicone, for example, is a synthetic, non-biodegradable material commonly used as a drug delivery material, while gelatin is a biodegradable, natural material commonly used in cell-culture scaffoldsThe material needed for each application is different, and dependent the desired mechanical properties of the material. Tissue engineering of bone, for example, will require a much more rigid scaffold compared to a scaffold for skin regeneration.There are a few versatile synthetic materials used for many different scaffold applications. One of these commonly used materials is polylactic acid (PLA), a synthetic polymer. PLA - polylactic acid. This is a polyester which degrades within the human body to form lactic acid, a naturally occurring chemical which is easily removed from the body. Similar materials are polyglycolic acid (PGA) and polycaprolactone (PCL): their degradation mechanism is similar to that of PLA,  but PCL degrades slower and PGA degrades faster. PLA is commonly combined with PGA to create poly-lactic-co-glycolic acid (PLGA). This is especially useful because the degradation of PLGA can be tailored by altering the weight percentages of PLA and PGA: More PLA – slower degradation, more PLA – faster degradation. This tunability, along with its biocompatibility, makes it an extremely useful material for scaffold creation.Scaffolds may also be constructed from natural materials: in particular different derivatives of the extracellular matrix have been studied to evaluate their ability to support cell growth. Protein based materials - such as collagen, or fibrin, and polysaccharidic materials- like chitosan or glycosaminoglycans (GAGs), have all proved suitable in terms of cell compatibility. Among GAGs, hyaluronic acid, possibly in combination with cross linking agents (e.g. glutaraldehyde, water-soluble carbodiimide, etc.), is one of the possible choices as scaffold material. Another form of scaffold is decellularized tissue. This is a process where chemicals are used to extracts cells from tissues, leaving just the extracellular matrix. This has the benefit of a foully formed matrix specific to the desired tissue type. However, the decellurized scaffold may present immune problems with future introduced cells.


=== Synthesis ===

A number of different methods have been described in the literature for preparing porous structures to be employed as tissue engineering scaffolds. Each of these techniques presents its own advantages, but none are free of drawbacks.


==== Nanofiber self-assembly ====
Molecular self-assembly is one of the few methods for creating biomaterials with properties similar in scale and chemistry to that of the natural in vivo extracellular matrix (ECM), a crucial step toward tissue engineering of complex tissues. Moreover, these hydrogel scaffolds have shown superiority in in vivo toxicology and biocompatibility compared to traditional macroscaffolds and animal-derived materials.


==== Textile technologies ====
These techniques include all the approaches that have been successfully employed for the preparation of non-woven meshes of different polymers. In particular, non-woven polyglycolide structures have been tested for tissue engineering applications: such fibrous structures have been found useful to grow different types of cells. The principal drawbacks are related to the difficulties in obtaining high porosity and regular pore size.


==== Solvent casting and particulate leaching ====
Solvent casting and particulate leaching (SCPL) allows for the preparation of structures with regular porosity, but with limited thickness. First, the polymer is dissolved into a suitable organic solvent (e.g. polylactic acid could be dissolved into dichloromethane), then the solution is cast into a mold filled with porogen particles. Such porogen can be an inorganic salt like sodium chloride, crystals of saccharose, gelatin spheres or paraffin spheres. The size of the porogen particles will affect the size of the scaffold pores, while the polymer to porogen ratio is directly correlated to the amount of porosity of the final structure. After the polymer solution has been cast the solvent is allowed to fully evaporate, then the composite structure in the mold is immersed in a bath of a liquid suitable for dissolving the porogen: water in the case of sodium chloride, saccharose and gelatin or an aliphatic solvent like hexane for use with paraffin. Once the porogen has been fully dissolved, a porous structure is obtained. Other than the small thickness range that can be obtained, another drawback of SCPL lies in its use of organic solvents which must be fully removed to avoid any possible damage to the cells seeded on the scaffold.


==== Gas foaming ====
To overcome the need to use organic solvents and solid porogens, a technique using gas as a porogen has been developed. First, disc-shaped structures made of the desired polymer are prepared by means of compression molding using a heated mold. The discs are then placed in a chamber where they are exposed to high pressure CO2 for several days. The pressure inside the chamber is gradually restored to atmospheric levels. During this procedure the pores are formed by the carbon dioxide molecules that abandon the polymer, resulting in a sponge-like structure. The main problems resulting from such a technique are caused by the excessive heat used during compression molding (which prohibits the incorporation of any temperature labile material into the polymer matrix) and by the fact that the pores do not form an interconnected structure.


==== Emulsification freeze-drying ====
This technique does not require the use of a solid porogen like SCPL. First, a synthetic polymer is dissolved into a suitable solvent (e.g. polylactic acid in dichloromethane) then water is added to the polymeric solution and the two liquids are mixed in order to obtain an emulsion. Before the two phases can separate, the emulsion is cast into a mold and quickly frozen by means of immersion into liquid nitrogen. The frozen emulsion is subsequently freeze-dried to remove the dispersed water and the solvent, thus leaving a solidified, porous polymeric structure. While emulsification and freeze-drying allow for a faster preparation when compared to SCPL (since it does not require a time-consuming leaching step), it still requires the use of solvents. Moreover, pore size is relatively small and porosity is often irregular. Freeze-drying by itself is also a commonly employed technique for the fabrication of scaffolds. In particular, it is used to prepare collagen sponges: collagen is dissolved into acidic solutions of acetic acid or hydrochloric acid that are cast into a mold, frozen with liquid nitrogen and then lyophilized.


==== Thermally induced phase separation ====
Similar to the previous technique, the TIPS phase separation procedure requires the use of a solvent with a low melting point that is easy to sublime. For example, dioxane could be used to dissolve polylactic acid, then phase separation is induced through the addition of a small quantity of water: a polymer-rich and a polymer-poor phase are formed. Following cooling below the solvent melting point and some days of vacuum-drying to sublime the solvent, a porous scaffold is obtained. Liquid-liquid phase separation presents the same drawbacks of emulsification/freeze-drying.


==== Electrospinning ====
Electrospinning is a highly versatile technique that can be used to produce continuous fibers ranging in diameter from a few microns to a few nanometers. In a typical electrospinning set-up, the desired scaffold material is dissolved within a solvent and placed within a syringe. This solution is fed through a needle and a high voltage is applied to the tip and to a conductive collection surface. The buildup of electrostatic forces within the solution causes it to eject a thin fibrous stream towards the oppositely charged or grounded collection surface. During this process the solvent evaporates, leaving solid fibers leaving a highly porous network. This technique is highly tunable, with variation to solvent, voltage, working distance (distance from the needle to collection surface), flow rate of solution, solute concentration, and collection surface. This allows for precise control of fiber morphology.
On a commercial level however, due to scalability reasons, there are 40 or sometimes 96 needles involved operating at once. The bottle-necks in such set-ups are: 1) Maintaining the aforementioned variables uniformly for all of the needles and 2) formation of ""beads"" in single fibers that we as engineers, want to be of a uniform diameter. By modifying variables such as the distance to collector, magnitude of applied voltage, or solution flow rate—researchers can dramatically change the overall scaffold architecture.
Historically, research on electrospun fibrous scaffolds dates back to at least the late 1980s when Simon showed that electrospinning could be used to produced nano- and submicron-scale fibrous scaffolds from polymer solutions specifically intended for use as in vitro cell and tissue substrates. This early use of electrospun lattices for cell culture and tissue engineering showed that various cell types would adhere to and proliferate upon polycarbonate fibers. It was noted that as opposed to the flattened morphology typically seen in 2D culture, cells grown on the electrospun fibers exhibited a more rounded 3-dimensional morphology generally observed of tissues in vivo.


==== CAD/CAM technologies ====
Because most of the above techniques are limited when it comes to the control of porosity and pore size, computer assisted design and manufacturing techniques have been introduced to tissue engineering. First, a three-dimensional structure is designed using CAD software. The porosity can be tailored using algorithms within the software. The scaffold is then realized by using ink-jet printing of polymer powders or through Fused Deposition Modeling of a polymer melt.A 2011 study by El-Ayoubi et al. investigated ""3D-plotting technique to produce (biocompatible and biodegradable) poly-L-Lactide macroporous scaffolds with two different pore sizes"" via solid free-form fabrication (SSF) with computer-aided-design (CAD), to explore therapeutic articular cartilage replacement as an ""alternative to conventional tissue repair"". The study found the smaller the pore size paired with mechanical stress in a bioreactor (to induce in vivo-like conditions), the higher the cell viability in potential therapeutic functionality via decreasing recovery time and increasing transplant effectiveness.


==== Laser-assisted bioprinting ====
In a 2012 study, Koch et al. focused on whether Laser-assisted BioPrinting (LaBP) can be used to build multicellular 3D patterns in natural matrix, and whether the generated constructs are functioning and forming tissue. LaBP arranges small volumes of living cell suspensions in set high-resolution patterns. The investigation was successful, the researchers foresee that ""generated tissue constructs might be used for in vivo testing by implanting them into animal models"" (14). As of this study, only human skin tissue has been synthesized, though researchers project that ""by integrating further cell types (e.g. melanocytes, Schwann cells, hair follicle cells) into the printed cell construct, the behavior of these cells in a 3D in vitro microenvironment similar to their natural one can be analyzed"", which is useful for drug discovery and toxicology studies.


==== Self-assembled recombinant spider silk nanomembranes ====
Gustafsson et al. demonstrated free‐standing, bioactive membranes of cm-sized area, but only 250 nm thin, that were formed by self‐assembly of spider silk at the interface of an aqueous solution. The membranes uniquely combine nanoscale thickness, biodegradability, ultrahigh strain and strength, permeability to proteins and promote rapid cell adherence and proliferation. They demonstrated growing a coherent layer of keratinocytes.


== Assembly methods ==
A persistent problem within tissue engineering is mass transport limitations. Engineered tissues generally lack an initial blood supply, thus making it difficult for any implanted cells to obtain sufficient oxygen and nutrients to survive, or function properly.


=== Self-assembly ===
Self-assembly methods have been shown to be promising methods for tissue engineering. Self-assembly methods have the advantage of allowing tissues to develop their own extracellular matrix, resulting in tissue that better recapitulates biochemical and biomechanical properties of native tissue. Self-assembling engineered articular cartilage was introduced by Jerry Hu and Kyriacos A. Athanasiou in 2006 and applications of the process have resulted in engineered cartilage approaching the strength of native tissue. Self-assembly is a prime technology to get cells grown in a lab to assemble into three-dimensional shapes. To break down tissues into cells, researchers first have to dissolve the extracellular matrix that normally binds them together. Once cells are isolated, they must form the complex structures that make up our natural tissues.


=== Liquid-based template assembly ===
The air-liquid surface established by Faraday waves is explored as a template to assemble biological entities for bottom-up tissue engineering. This liquid-based template can be dynamically reconfigured in a few seconds, and the assembly on the template can be achieved in a scalable and parallel manner. Assembly of microscale hydrogels, cells, neuron-seeded micro-carrier beads, cell spheroids into various symmetrical and periodic structures was demonstrated with good cell viability. Formation of 3D neural network was achieved after 14-day tissue culture.


=== Additive manufacturing ===

It might be possible to print organs, or possibly entire organisms using additive manufacturing techniques. A recent innovative method of construction uses an ink-jet mechanism to print precise layers of cells in a matrix of thermoreversible gel. Endothelial cells, the cells that line blood vessels, have been printed in a set of stacked rings. When incubated, these fused into a tube. This technique has been referred to as “bioprinting” within the field as it involves the printing of biological components in a structure resembling the organ of focus.
The field of three-dimensional and highly accurate models of biological systems is pioneered by multiple projects and technologies including a rapid method for creating tissues and even whole organs involve a 3D printer that can bioprint the scaffolding and cells layer by layer into a working tissue sample or organ. The device is presented in a TED talk by Dr. Anthony Atala, M.D. the Director of the Wake Forest Institute for Regenerative Medicine, and the W.H. Boyce Professor and Chair of the Department of Urology at Wake Forest University, in which a kidney is printed on stage during the seminar and then presented to the crowd. It is anticipated that this technology will enable the production of livers in the future for transplantation and theoretically for toxicology and other biological studies as well.
Recently Multi-Photon Processing (MPP) was employed for in vivo experiments by engineering artificial cartilage constructs. An ex vivo histological examination showed that certain pore geometry and the pre-growing of chondrocytes (Cho) prior to implantation significantly improves the performance of the created 3D scaffolds. The achieved biocompatibility was comparable to the commercially available collagen membranes. The successful outcome of this study supports the idea that hexagonal-pore-shaped hybrid organic-inorganic microstructured scaffolds in combination with Cho seeding may be successfully implemented for cartilage tissue engineering.


=== Scaffolding ===
In 2013, using a 3-d scaffolding of Matrigel in various configurations, substantial pancreatic organoids was produced in vitro. Clusters of small numbers of cells proliferated into 40,000 cells within one week. The clusters transform into cells that make either digestive enzymes or hormones like insulin, self-organizing into branched pancreatic organoids that resemble the pancreas.The cells are sensitive to the environment, such as gel stiffness and contact with other cells. Individual cells do not thrive; a minimum of four proximate cells was required for subsequent organoid development. Modifications to the medium composition produced either hollow spheres mainly composed of pancreatic progenitors, or complex organoids that spontaneously undergo pancreatic morphogenesis and differentiation. Maintenance and expansion of pancreatic progenitors require active Notch and FGF signaling, recapitulating in vivo niche signaling interactions.The organoids were seen as potentially offering mini-organs for drug testing and for spare insulin-producing cells.Aside from Matrigel 3-D scaffolds, other collagen gel systems have been developed. Collagen/hyaluronic acid scaffolds have been used for modeling the mammary gland In Vitro while co-coculturing epithelial and adipocyte cells. The HyStem kit is another 3-D platform containing ECM components and hyaluronic acid that has been used for cancer research. Additionally, hydrogel constituents can be chemically modified to assist in crosslinking and enhance their mechanical properties.


== Tissue culture ==
In many cases, creation of functional tissues and biological structures in vitro requires extensive culturing to promote survival, growth and inducement of functionality. In general, the basic requirements of cells must be maintained in culture, which include oxygen, pH, humidity, temperature, nutrients and osmotic pressure maintenance.
Tissue engineered cultures also present additional problems in maintaining culture conditions. In standard cell culture, diffusion is often the sole means of nutrient and metabolite transport. However, as a culture becomes larger and more complex, such as the case with engineered organs and whole tissues, other mechanisms must be employed to maintain the culture, such as the creation of capillary networks within the tissue.

Another issue with tissue culture is introducing the proper factors or stimuli required to induce functionality. In many cases, simple maintenance culture is not sufficient. Growth factors, hormones, specific metabolites or nutrients, chemical and physical stimuli are sometimes required. For example, certain cells respond to changes in oxygen tension as part of their normal development, such as chondrocytes, which must adapt to low oxygen conditions or hypoxia during skeletal development. Others, such as endothelial cells, respond to shear stress from fluid flow, which is encountered in blood vessels. Mechanical stimuli, such as pressure pulses seem to be beneficial to all kind of cardiovascular tissue such as heart valves, blood vessels or pericardium.


=== Bioreactors ===

In tissue engineering, a bioreactor is a device that attempts to simulate a physiological environment in order to promote cell or tissue growth in vitro. A physiological environment can consist of many different parameters such as temperature, pressure, oxygen or carbon dioxide concentration, or osmolality of fluid environment, and it can extend to all kinds of biological, chemical or mechanical stimuli. Therefore, there are systems that may include the application of forces such as electromagnetic forces, mechanical pressures, or fluid pressures to the tissue. These systems can be two- or three-dimensional setups. Bioreactors can be used in both academic and industry applications. General-use and application-specific bioreactors are also commercially available, which may provide static chemical stimulation or a combination of chemical and mechanical stimulation.
Cell proliferation and differentiation are largely influenced by mechanical and biochemical cues in the surrounding extracellular matrix environment. Bioreactors are typically developed to replicate the specific physiological environment of the tissue being grown (e.g., flex and fluid shearing for heart tissue growth). This can allow specialized cell lines to thrive in cultures replicating their native environments, but it also makes bioreactors attractive tools for culturing stem cells. A successful stem-cell-based bioreactor is effective at expanding stem cells with uniform properties and/or promoting controlled, reproducible differentiation into selected mature cell types.There are a variety of bioreactors designed for 3D cell cultures. There are small plastic cylindrical chambers, as well as glass chambers, with regulated internal humidity and moisture specifically engineered for the purpose of growing cells in three dimensions. The bioreactor uses bioactive synthetic materials such as polyethylene terephthalate membranes to surround the spheroid cells in an environment that maintains high levels of nutrients. They are easy to open and close, so that cell spheroids can be removed for testing, yet the chamber is able to maintain 100% humidity throughout. This humidity is important to achieve maximum cell growth and function. The bioreactor chamber is part of a larger device that rotates to ensure equal cell growth in each direction across three dimensions.QuinXell Technologies now under Quintech Life Sciences from Singapore has developed a bioreactor known as the TisXell Biaxial Bioreactor which is specially designed for the purpose of tissue engineering. It is the first bioreactor in the world to have a spherical glass chamber with biaxial rotation; specifically to mimic the rotation of the fetus in the womb; which provides a conducive environment for the growth of tissues.Multiple forms of mechanical stimulation have also been combined into a single bioreactor. Using gene expression analysis, one academic study found that applying a combination of cyclic strain and ultrasound stimulation to pre-osteoblast cells in a bioreactor accelerated matrix maturation and differentiation. The technology of this combined stimulation bioreactor could be used to grow bone cells more quickly and effectively in future clinical stem cell therapies.MC2 Biotek has also developed a bioreactor known as ProtoTissue that uses gas exchange to maintain high oxygen levels within the cell chamber; improving upon previous bioreactors, since the higher oxygen levels help the cell grow and undergo normal cell respiration.Active areas of research on bioreactors includes increasing production scale and refining the physiological environment, both of which could improve the efficiency and efficacy of bioreactors in research or clinical use. Bioreactors are currently used to study, among other things, cell and tissue level therapies, cell and tissue response to specific physiological environment changes, and development of disease and injury.


=== Long fiber generation ===
In 2013, a group from the University of Tokyo developed cell laden fibers up to a meter in length and on the order of 100 µm in size. These fibers were created using a microfluidic device that forms a double coaxial laminar flow. Each 'layer' of the microfluidic device (cells seeded in ECM, a hydrogel sheath, and finally a calcium chloride solution). The seeded cells culture within the hydrogel sheath for several days, and then the sheath is removed with viable cell fibers. Various cell types were inserted into the ECM core, including myocytes, endothelial cells, nerve cell fibers, and epithelial cell fibers. This group then showed that these fibers can be woven together to fabricate tissues or organs in a mechanism similar to textile weaving. Fibrous morphologies are advantageous in that they provide an alternative to traditional scaffold design, and many organs (such as muscle) are composed of fibrous cells.


=== Bioartificial organs ===

An artificial organ is an engineered device that can be extra corporeal or implanted to support impaired or failing organ systems. Bioartificial organs are typically created with the intent to restore critical biological functions like in the replacement of diseased hearts and lungs, or provide drastic quality of life improvements like in the use of engineered skin on burn victims. While some examples of bioartificial organs are still in the research stage of development due to the limitations involved with creating functional organs, others are currently being used in clinical settings experimentally and commercially.


==== Lung ====
Extracorporeal membrane oxygenation (ECMO) machines, otherwise known as heart and lung machines, are an adaptation of cardiopulmonary bypass techniques that provide heart and lung support. It is used primarily to support the lungs for a prolonged but still temporary timeframe (1–30 days) and allow for recovery from reversible diseases. Robert Bartlett is known as the father of ECMO and performed the first treatment of a newborn using an EMCO machine in 1975.Skin
Tissue-engineered skin is a type of bioartificial organ that is often used to treat burns, diabetic foot ulcers, or other large wounds that cannot heal well on their own. Artificial skin can be made from autografts, allografts, and xenografts. Autografted skin comes from a patient’s own skin, which allows the dermis to have a faster healing rate, and the donor site can be reharvested a few times. Allograft skin often comes from cadaver skin and is mostly used to treat burn victims. Lastly, xenografted skin comes from animals and provides a temporary healing structure for the skin. They assist in dermal regeneration, but cannot become part of the host skin. Tissue-engineered skin is now available in commercial products. Integra, originally used to only treat burns, consists of a collagen matrix and chondroitin sulfate that can be used as a skin replacement. The chondroitin sulfate functions as a component of proteoglycans, which helps to form the extracellular matrix. Integra can be repopulated and revascularized while maintaining its dermal collagen architecture, making it a bioartificial organ  Dermagraft, another commercial-made tissue-engineered skin product, is made out of living fibroblasts. These fibroblasts proliferate and produce growth factors, collagen, and ECM proteins, that help build granulation tissue.


==== Heart ====
Since the number of patients awaiting a heart transplant is continuously increasing over time, and the number of patients on the waiting list surpasses the organ availability, artificial organs used as replacement therapy for terminal heart failure would help alleviate this difficulty.  Artificial hearts are usually used to bridge the heart transplantation or can be applied as replacement therapy for terminal heart malfunction. The total artificial heart (TAH), first introduced by Dr. Vladimir P. Demikhov in 1937, emerged as an ideal alternative. Since then it has been developed and improved as a mechanical pump that provides long-term circulatory support and replaces diseased or damaged heart ventricles that cannot properly pump the blood, restoring thus the pulmonary and systemic flow. Some of the current TAHs include AbioCor, an FDA-approved device that comprises two artificial ventricles and their valves, and does not require subcutaneous connections, and is indicated for patients with biventricular heart failure. In 2010 SynCardia released the portable freedom driver that allows patients to have a portable device without being confined to the hospital.


==== Kidney ====
While kidney transplants are possible, renal failure is more often treated using an artificial kidney. The first artificial kidneys and the majority of those currently in use are extracorporeal, such as with hemodialysis, which filters blood directly, or peritoneal dialysis, which filters via a fluid in the abdomen. In order to contribute to the biological functions of a kidney such as producing metabolic factors or hormones, some artificial kidneys incorporate renal cells. There has been progress in the way of making these devices smaller and more transportable, or even implantable . One challenge still to be faced in these smaller devices is countering the limited volume and therefore limited filtering capabilities.


=== Biomimetics ===

Biomimetics is a field that aims to produce materials and systems that replicate those present in nature. In the context of tissue engineering, this is a common approach used by engineers to create materials for these applications that are comparable to native tissues in terms of their structure, properties, and biocompatibility. Material properties are largely dependent on physical, structural, and chemical characteristics of that material. Subsequently, a biomimetic approach to system design will become significant in material integration, and a sufficient understanding of biological processes and interactions will be necessary. Replication of biological systems and processes may also be used in the synthesis of bio-inspired materials to achieve conditions that produce the desired biological material. Therefore, if a material is synthesized having the same characteristics of biological tissues both structurally and chemically, then ideally the synthesized material will have similar properties. This technique has an extensive history originating from the idea of using natural phenomenon as design inspiration for solutions to human problems. Many modern advancements in technology have been inspired by nature and natural systems, including aircraft, automobiles, architecture, and even industrial systems. Advancements in nanotechnology initiated the application of this technique to micro- and nano-scale problems, including tissue engineering. This technique has been used to develop synthetic bone tissues, vascular technologies, scaffolding materials and integration techniques, and functionalized nanoparticles.


== Constructing neural networks in soft material ==
In 2018, scientists at Brandeis University reported their research on soft material embedded with chemical networks which can mimic the smooth and coordinated behavior of neural tissue. This research was funded by the U.S. Army Research Laboratory. The researchers presented an experimental system of neural networks, theoretically modeled as reaction-diffusion systems. Within the networks was an array of patterned reactors, each performing the Belousov-Zhabotinsky (BZ) reaction. These reactors could function on a nanoliter scale.The researchers state that the inspiration for their project was the movement of the blue ribbon eel. The eel's movements are controlled by electrical impulses determined by a class of neural networks called the central pattern generator.  Central Pattern Generators function within the autonomic nervous system to control bodily functions such as respiration, movement, and peristalsis.Qualities of the reactor that were designed were the network topology, boundary conditions, initial conditions, reactor volume, coupling strength, and the synaptic polarity of the reactor (whether its behavior is inhibitory or excitatory). A BZ emulsion system with a solid elastomer polydimethylsiloxane (PDMS) was designed. Both light and bromine permeable PDMS have been reported as viable methods to create a pacemaker for neural networks.


== Market ==
The history of the tissue engineering market can be divided into three major parts. The time before the crash of the biotech market in the early 2000s, the crash and the time afterward.


=== Beginning ===
Most early progress in tissue engineering research was done in the US. This is due to less strict regulations regarding stem cell research and more available funding than in other countries. This leads to the creation of academic startups many of them coming from Harvard or MIT. Examples are BioHybridTechnologies whose founder, Bill Chick,  went to Harvard Medical School and focused on the creation of artificial pancreas. Another example would be Organogenesis Inc. whose founder went to MIT and worked on skin engineering products. Other companies with links to the MIT are TEI Biosciences, Therics and Guilford Pharmaceuticals. The renewed interest in biotechnologies in the 1980s leads to many private investors investing in these new technologies even though the business models of these early startups were often not very clear and did not present a path to long term profitability. Government sponsors were more restraint in their funding as tissue engineering was considered a high-risk investment.In the UK the market got off to a slower start even though the regulations on stem cell research were not strict as well. This is mainly due to more investors being less willing to invest in these new technologies which were considered to be high-risk investments. Another problem faced by British companies was getting the NHS to pay for their products. This especially because the NHS runs a cost-effectiveness analysis on all supported products. Novel technologies often do not do well in this respect.In Japan, the regulatory situation was quite different. First cell cultivation was only allowed in a hospital setting and second academic scientists employed by state-owned universities were not allowed outside employment until 1998. Moreover, the Japanese authorities took longer to approve new drugs and treatments than there US and European counterparts.For these reasons in the early days of the Japanese market, the focus was mainly on getting products that were already approved elsewhere in Japan and selling them. Contrary to the US market the early actors in Japan were mainly big firms or sub-companies of such big firms, such as J-TEC, Menicon and Terumo,  and not small startups. After regulatory changes in 2014, which allowed cell cultivation outside of a hospital setting, the speed of research in Japan increased and Japanese companies also started to develop their own products.Japanese companies, such as ReproCell and iPS Academia Japan,  are currently working on iPS cell-related products.


=== Crash ===
Soon after the big boom, the first problems started to appear. There were problems getting products approved by the FDA and if they got approved there were often difficulties in getting insurance providers to pay for the products and getting it accepted by health care providers.For example, organogenesis ran into problems marketing its product and integrating its product in the health system. This partially due to the difficulties of handling living cells and the increased difficulties faced by physicians in using these products over conventional methods.Another example would be Advanced Tissue Sciences Dermagraft skin product which could not create a high enough demand without reimbursements from insurance providers. Reasons for this were $4000 price-tag and the circumstance that Additionally Advanced Tissue Sciences struggled to get their product known by physicians.The above examples demonstrate how companies struggled to make profit. This, in turn, lead investors to lose patience and stoping further funding. In consequence, several Tissue Engineering companies such as Organogenesis and Advanced Tissue Sciences filed for bankruptcy in the early 2000s. At this time, these were the only ones having commercial skin products on the market.


=== Reemergence ===
The technologies of the bankrupt or struggling companies were often bought by other companies which continued the development under more conservative business models. Examples of companies who sold their products after folding were Curis and Intercytex.Many of the companies abandoned their long term goals of developing fully functional organs in favour of products and technologies that could turn a profit in the short run. Examples of these kinds of products are products in the cosmetic and testing industry.
In other cases such as in the case of Advanced Tissue Sciences, the founders started new companies.In the 2010s the regulatory framework also started to facilitate faster time to market especially in the USA as new centres and pathways were created by the FDA specifically aimed at products coming from living cells such as the Center for Biologics Evaluation and Research.The first tissue engineering products started to get commercially profitable in the 2010s.


== Regulation ==
In Europe, regulation is currently split into three areas of regulation: medical devices, medicinal products, and biologics. Tissue Engineering products are often of hybrid nature, as they are often composed of cells and a supporting structure. While some products can be approved as medicinal products, others need to gain approval as medical devices. Derksen explains in her thesis that Tissue Engineering researchers are sometimes confronted with regulation that does not fit the characteristics of Tissue Engineering.New regulatory regimes have been observed in Europe that tackle these issues. An explanation for the difficulties in finding regulatory consensus in this matter is given by a survey conducted in the UK. The authors attribute these problems to the close relatedness and overlap with other technologies such as xenotransplantation. It can therefore not be handled separately by regulatory bodies. Regulation is further complicated by the ethical controversies associated with this and related fields of research (e.g. stem cells controversy, ethics of organ transplantation). The same survey as mentioned above  shows on the example of autologous cartilage transplantation that a specific technology can be regarded as ‘pure’ or ‘polluted’ by the same social actor.
Two regulatory movements are most relevant to tissue engineering in the European Union. These are the Directive on standards of quality and safety for the sourcing and processing of human tissues  which was adopted by the European Parliament in 2004 and a proposed cells and the Human Tissue- Engineered Products regulation. The latter was developed under the abscise of the European Commission DG Enterprise and presented in Brussels in 2004.


== See also ==


== Notes ==


== References ==


== External links ==
Clinical Tissue Engineering Center State of Ohio Initiative for Tissue Engineering (National Center for Regenerative Medicine)
Organ Printing Multi-site NSF-funded initiative
LOEX Center Université Laval Initiative for Tissue Engineering","pandas(index=89, _1=89, text='tissue engineering is a biomedical engineering discipline that uses a combination of cells, engineering, materials methods, and suitable biochemical and physicochemical factors to restore, maintain, improve, or replace different types of biological tissues. tissue engineering often involves the use of cells placed on tissue scaffolds in the formation of new viable tissue for a medical purpose but is not limited to applications involving cells and tissue scaffolds. while it was once categorized as a sub-field of biomaterials, having grown in scope and importance it can be considered as a field in its own.  while most definitions of tissue engineering cover a broad range of applications, in practice the term is closely associated with applications that repair or replace portions of or whole tissues (i.e., bone, cartilage, blood vessels, bladder, skin, muscle etc.). often, the tissues involved require certain mechanical and structural properties for proper functioning. the term has also been applied to efforts to perform specific biochemical functions using cells within an artificially-created support system (e.g. an artificial pancreas, or a bio artificial liver). the term regenerative medicine is often used synonymously with tissue engineering, although those involved in regenerative medicine place more emphasis on the use of stem cells or progenitor cells to produce tissues.   == overview ==  a commonly applied definition of tissue engineering, as stated by langer and vacanti, is ""an interdisciplinary field that applies the principles of engineering and life sciences toward the development of biological substitutes that restore, maintain, or improve [biological tissue] function or a whole organ"". in addition, langer and vacanti also state that there are three main types of tissue engineering: cells, tissue-inducing substances, and a cellsmatrix approach (often referred to as a scaffold).tissue engineering has also been defined as ""understanding the principles of tissue growth, and applying this to produce functional replacement tissue for clinical use"". a further description goes on to say that an ""underlying supposition of tissue engineering is that the employment of natural biology of the system will allow for greater success in developing therapeutic strategies aimed at the replacement, repair, maintenance, or enhancement of tissue function"".developments in the multidisciplinary field of tissue engineering have yielded a novel set of tissue replacement parts and implementation strategies. scientific advances in biomaterials, stem cells, growth and differentiation factors, and biomimetic environments have created unique opportunities to fabricate or improve existing tissues in the laboratory from combinations of engineered extracellular matrices (""scaffolds""), cells, and biologically active molecules. among the major challenges now facing tissue engineering is the need for more complex functionality, biomechanical stability, and vascularization in laboratory-grown tissues destined for transplantation. the continued success of tissue engineering and the eventual development of true human replacement parts will grow from the convergence of engineering and basic research advances in tissue, matrix, growth factor, stem cell, and developmental biology, as well as materials science and bioinformatics. in 2003, the nsf published a report entitled ""the emergence of tissue engineering as a research field"", which gives a thorough description of the history of this field.   == etymology == the historic origins of the term are unclear as the definition of the word has changed throughout the past decades. the term first appeared in a 1984 publication that described the organization of an endothelium-like membrane on the surface of a long-implanted, synthetic ophthalmic prosthesis the first modern use of the term as recognized today was in 1985 by the researcher, physiologist and bioengineer y.c fung of the engineering research center. he proposed the joining of the terms tissue (in reference to the fundamental relationship between cells and organs) and engineering (in reference to the field of modification of said tissues). the term was officially adopted in 1987.   == history == the technologies of the bankrupt or struggling companies were often bought by other companies which continued the development under more conservative business models. examples of companies who sold their products after folding were curis and intercytex.many of the companies abandoned their long term goals of developing fully functional organs in favour of products and technologies that could turn a profit in the short run. examples of these kinds of products are products in the cosmetic and testing industry. in other cases such as in the case of advanced tissue sciences, the founders started new companies.in the 2010s the regulatory framework also started to facilitate faster time to market especially in the usa as new centres and pathways were created by the fda specifically aimed at products coming from living cells such as the center for biologics evaluation and research.the first tissue engineering products started to get commercially profitable in the 2010s.   == regulation == in europe, regulation is currently split into three areas of regulation: medical devices, medicinal products, and biologics. tissue engineering products are often of hybrid nature, as they are often composed of cells and a supporting structure. while some products can be approved as medicinal products, others need to gain approval as medical devices. derksen explains in her thesis that tissue engineering researchers are sometimes confronted with regulation that does not fit the characteristics of tissue engineering.new regulatory regimes have been observed in europe that tackle these issues. an explanation for the difficulties in finding regulatory consensus in this matter is given by a survey conducted in the uk. the authors attribute these problems to the close relatedness and overlap with other technologies such as xenotransplantation. it can therefore not be handled separately by regulatory bodies. regulation is further complicated by the ethical controversies associated with this and related fields of research (e.g. stem cells controversy, ethics of organ transplantation). the same survey as mentioned above  shows on the example of autologous cartilage transplantation that a specific technology can be regarded as ‘pure’ or ‘polluted’ by the same social actor. two regulatory movements are most relevant to tissue engineering in the european union. these are the directive on standards of quality and safety for the sourcing and processing of human tissues  which was adopted by the european parliament in 2004 and a proposed cells and the human tissue- engineered products regulation. the latter was developed under the abscise of the european commission dg enterprise and presented in brussels in 2004.   == see also ==   == notes ==   == references ==   == external links == clinical tissue engineering center state of ohio initiative for tissue engineering (national center for regenerative medicine) organ printing multi-site nsf-funded initiative loex center université laval initiative for tissue engineering')"
90,"A heart rate monitor (HRM) is a personal monitoring device that allows one to measure/display heart rate in real time or record the heart rate for later study. It is largely used to gather heart rate data while performing various types of physical exercise. Measuring electrical heart information is referred to as Electrocardiography (ECG or EKG).
Medical heart rate monitoring used in hospitals is usually wired and usually multiple sensors are used. Portable medical units are referred to as a Holter monitor. Consumer heart rate monitors are designed for everyday use and do not use wires to connect.


== History ==
Early models consisted of a monitoring box with a set of electrode leads which attached to the chest. The first wireless EKG heart rate monitor was invented in 1977 by Polar Electro as a training aid for the Finnish National Cross Country Ski team. As ""intensity training"" became a popular concept in athletic circles in the mid-80s, retail sales of wireless personal heart monitors started in 1983.


== Technologies ==

Modern heart rate monitors commonly use one of two different methods to record heart signals (electrical and optical). Both types of signals can provide the same basic heart rate data, using fully automated algorithms to measure heart rate, such as the Pan-Tompkins algorithm.ECG (Electrocardiography) sensors measure the bio-potential generated by electrical signals that control the expansion and contraction of heart chambers, typically implemented in medical devices.
PPG (Photoplethysmography) sensors use a light-based technology to measure the blood volume controlled by the heart's pumping action.


=== Electrical ===
The electrical monitors consist of two elements: a monitor/transmitter, which is worn on a chest strap, and a receiver. When a heartbeat is detected a radio signal is transmitted, which the receiver uses to display/determine the current heart rate. This signal can be a simple radio pulse or a unique coded signal from the chest strap (such as Bluetooth, ANT, or other low-power radio links). Newer technology prevents one user's receiver from using signals from other nearby transmitters (known as cross-talk interference) or eavesdropping. Note the older Polar 5.1 kHz radio transmission technology is usable underwater. Both Bluetooth and Ant+ use the 2.4  GHz radio band, which cannot send signals underwater.


=== Optical ===

More recent devices use optics to measure heart rate by shining light from an LED through the skin and measuring how it scatters off blood vessels. In addition to measuring the heart rate, some devices using this technology are able to measure blood oxygen saturation (SpO2). Some recent optical sensors can also transmit data as mentioned above.
Newer devices such as cell phones or watches can be used to display and/or collect the information. Some devices can simultaneously monitor heart rate, oxygen saturation, and other parameters. These may include sensors such as accelerometers, gyroscopes, and GPS to detect speed, location and distance.
In recent years, it has been common for smartwatches to include heart rate monitors, which has greatly increased popularity.
Some smart watches, smart bands and cell phones often use PPG sensors.


=== Fitness Metrics ===
Garmin, Polar Electro, Suunto and Fitbit are vendors selling consumer heart rate products. Most companies use their own proprietary heart rate algorithms.


== Accuracy ==
The newer, wrist based heart rate monitors have achieved almost identical levels of accuracy as their chest strap counterparts with independent tests showing up to 95% accuracy, but sometimes more than 30% error can persist for several minutes. Optical devices can be less accurate when used during vigorous activity or when used underwater.
Currently heart rate variability is less available on optical devices. Apple introduced HRV data collection to the Apple Watch devices in 2018.


== See also ==
Apple Watch
GPS watch
Activity tracker
Pedometer
eHealth
E-textiles


== References ==


== External links ==
 Media related to Heart rate monitors at Wikimedia Commons","pandas(index=90, _1=90, text='a heart rate monitor (hrm) is a personal monitoring device that allows one to measure/display heart rate in real time or record the heart rate for later study. it is largely used to gather heart rate data while performing various types of physical exercise. measuring electrical heart information is referred to as electrocardiography (ecg or ekg). medical heart rate monitoring used in hospitals is usually wired and usually multiple sensors are used. portable medical units are referred to as a holter monitor. consumer heart rate monitors are designed for everyday use and do not use wires to connect.   == history == early models consisted of a monitoring box with a set of electrode leads which attached to the chest. the first wireless ekg heart rate monitor was invented in 1977 by polar electro as a training aid for the finnish national cross country ski team. as ""intensity training"" became a popular concept in athletic circles in the mid-80s, retail sales of wireless personal heart monitors started in 1983.   == technologies ==  modern heart rate monitors commonly use one of two different methods to record heart signals (electrical and optical). both types of signals can provide the same basic heart rate data, using fully automated algorithms to measure heart rate, such as the pan-tompkins algorithm.ecg (electrocardiography) sensors measure the bio-potential generated by electrical signals that control the expansion and contraction of heart chambers, typically implemented in medical devices. ppg (photoplethysmography) sensors use a light-based technology to measure the blood volume controlled by the heart\'s pumping action. garmin, polar electro, suunto and fitbit are vendors selling consumer heart rate products. most companies use their own proprietary heart rate algorithms.   == accuracy == the newer, wrist based heart rate monitors have achieved almost identical levels of accuracy as their chest strap counterparts with independent tests showing up to 95% accuracy, but sometimes more than 30% error can persist for several minutes. optical devices can be less accurate when used during vigorous activity or when used underwater. currently heart rate variability is less available on optical devices. apple introduced hrv data collection to the apple watch devices in 2018.   == see also == apple watch gps watch activity tracker pedometer ehealth e-textiles   == references ==   == external links == media related to heart rate monitors at wikimedia commons')"
91,"Strategies for Engineered Negligible Senescence (SENS) is the term coined by British biogerontologist Aubrey de Grey for the diverse range of regenerative medical therapies, either planned or currently in development, for the periodical repair of all age-related damage to human tissue with the ultimate purpose of maintaining a state of negligible senescence in the patient, thereby postponing age-associated disease for as long as the therapies are reapplied.The term ""negligible senescence"" was first used in the early 1990s by professor Caleb Finch to describe organisms such as lobsters and hydras, which do not show symptoms of aging. The term ""engineered negligible senescence"" first appeared in print in Aubrey de Grey's 1999 book The Mitochondrial Free Radical Theory of Aging. De Grey called SENS a ""goal-directed rather than curiosity-driven"" approach to the science of aging, and ""an effort to expand regenerative medicine into the territory of aging"".While many biogerontologists find it ""worthy of discussion"", some contend that the ultimate goals of de Grey's programme are too speculative given the current state of technology, referring to it as ""fantasy rather than science"".


== Framework ==

The ultimate objective of SENS is the eventual elimination of age-related diseases and infirmity by repeatedly reducing the state of senescence in the organism. The SENS project consists in implementing a series of periodic medical interventions designed to repair, prevent or render irrelevant all the types of molecular and cellular damage that cause age-related pathology and degeneration, in order to avoid debilitation and death from age-related causes.


=== The Strategies ===
The following table as transcribed from the SENS's website details the following major ailments and preventative strategies:


== Scientific controversy ==
While some fields mentioned as branches of SENS are broadly supported by the medical research community, e.g., stem cell research (RepleniSENS), anti-Alzheimers research (AmyloSENS) and oncogenomics (OncoSENS), the SENS programme as a whole has been a highly controversial proposal, with many critics arguing that the SENS agenda is fanciful and the highly complicated biomedical phenomena involved in the aging process contain too many unknowns for SENS to be fully implementable in the foreseeable future. Cancer may well deserve special attention as an aging-associated disease (OncoSENS), but the SENS claim that nuclear DNA damage only matters for aging because of cancer has been challenged in the literature as well as by material in the article DNA damage theory of aging.
In November 2005, 28 biogerontologists published a statement of criticism in EMBO Reports, ""Science fact and the SENS agenda: what can we reasonably expect from ageing research?,"" arguing ""each one of the specific proposals that comprise the SENS agenda is, at our present stage of ignorance, exceptionally optimistic,"" and that some of the specific proposals ""will take decades of hard work [to be medically integrated], if [they] ever prove to be useful."" The researchers argue that while there is ""a rationale for thinking that we might eventually learn how to postpone human illnesses to an important degree,"" increased basic research, rather than the goal-directed approach of SENS, is presently the scientifically appropriate goal.
More recently, biogerontologist Marios Kyriazis has sharply criticised the clinical applicability of SENS claiming that such therapies, even if developed in the laboratory, would be practically unusable by the general public. De Grey responded to one such criticism.


=== Technology Review controversy ===
In February 2005, Technology Review, which is owned by the Massachusetts Institute of Technology, published an article by Sherwin Nuland, a Clinical Professor of Surgery at Yale University and the author of ""How We Die"", that drew a skeptical portrait of SENS, at the time de Grey was a computer associate in the Flybase Facility of the Department of Genetics at the University of Cambridge.
During June 2005, David Gobel, CEO and Co-founder of Methuselah Foundation offered Technology Review $20,000 to fund a prize competition to publicly clarify the viability of the SENS approach. In July 2005, Pontin announced a $20,000 prize, funded 50/50 by Methuselah Foundation and MIT Technology Review, open to any molecular biologist, with a record of publication in biogerontology, who could prove that the alleged benefits of SENS were ""so wrong that it is unworthy of learned debate."" Technology Review received five submissions to its Challenge. In March 2006, Technology Review announced that it had chosen a panel of judges for the Challenge: Rodney Brooks, Anita Goel, Nathan Myhrvold, Vikram Sheel Kumar, and Craig Venter. Three of the five submissions met the terms of the prize competition. They were published by Technology Review on June 9, 2006. On July 11, 2006, Technology Review published the results of the SENS Challenge.In the end, no one won the $20,000 prize. The judges felt that no submission met the criterion of the challenge and discredited SENS, although they unanimously agreed that one submission, by Preston Estep and his colleagues, was the most eloquent. Craig Venter succinctly expressed the prevailing opinion: ""Estep et al. ... have not demonstrated that SENS is unworthy of discussion, but the proponents of SENS have not made a compelling case for it."" Summarizing the judges' deliberations, Pontin wrote that SENS is ""highly speculative"" and that many of its proposals could not be reproduced with the scientific technology of that period. Myhrvold described SENS as belonging to a kind of ""antechamber of science"" where they wait until technology and scientific knowledge advance to the point where it can be tested.


== SENS Research Foundation ==

The SENS Research Foundation is a non-profit organization co-founded by Michael Kope, Aubrey de Grey, Jeff Hall, Sarah Marr and Kevin Perrott, which is based in California, United States. Its activities include SENS-based research programs and public relations work for the acceptance of and interest in related research.


== See also ==


== References ==","pandas(index=91, _1=91, text='strategies for engineered negligible senescence (sens) is the term coined by british biogerontologist aubrey de grey for the diverse range of regenerative medical therapies, either planned or currently in development, for the periodical repair of all age-related damage to human tissue with the ultimate purpose of maintaining a state of negligible senescence in the patient, thereby postponing age-associated disease for as long as the therapies are reapplied.the term ""negligible senescence"" was first used in the early 1990s by professor caleb finch to describe organisms such as lobsters and hydras, which do not show symptoms of aging. the term ""engineered negligible senescence"" first appeared in print in aubrey de grey\'s 1999 book the mitochondrial free radical theory of aging. de grey called sens a ""goal-directed rather than curiosity-driven"" approach to the science of aging, and ""an effort to expand regenerative medicine into the territory of aging"".while many biogerontologists find it ""worthy of discussion"", some contend that the ultimate goals of de grey\'s programme are too speculative given the current state of technology, referring to it as ""fantasy rather than science"".   == framework ==  the ultimate objective of sens is the eventual elimination of age-related diseases and infirmity by repeatedly reducing the state of senescence in the organism. the sens project consists in implementing a series of periodic medical interventions designed to repair, prevent or render irrelevant all the types of molecular and cellular damage that cause age-related pathology and degeneration, in order to avoid debilitation and death from age-related causes. in february 2005, technology review, which is owned by the massachusetts institute of technology, published an article by sherwin nuland, a clinical professor of surgery at yale university and the author of ""how we die"", that drew a skeptical portrait of sens, at the time de grey was a computer associate in the flybase facility of the department of genetics at the university of cambridge. during june 2005, david gobel, ceo and co-founder of methuselah foundation offered technology review $20,000 to fund a prize competition to publicly clarify the viability of the sens approach. in july 2005, pontin announced a $20,000 prize, funded 50/50 by methuselah foundation and mit technology review, open to any molecular biologist, with a record of publication in biogerontology, who could prove that the alleged benefits of sens were ""so wrong that it is unworthy of learned debate."" technology review received five submissions to its challenge. in march 2006, technology review announced that it had chosen a panel of judges for the challenge: rodney brooks, anita goel, nathan myhrvold, vikram sheel kumar, and craig venter. three of the five submissions met the terms of the prize competition. they were published by technology review on june 9, 2006. on july 11, 2006, technology review published the results of the sens challenge.in the end, no one won the $20,000 prize. the judges felt that no submission met the criterion of the challenge and discredited sens, although they unanimously agreed that one submission, by preston estep and his colleagues, was the most eloquent. craig venter succinctly expressed the prevailing opinion: ""estep et al. ... have not demonstrated that sens is unworthy of discussion, but the proponents of sens have not made a compelling case for it."" summarizing the judges\' deliberations, pontin wrote that sens is ""highly speculative"" and that many of its proposals could not be reproduced with the scientific technology of that period. myhrvold described sens as belonging to a kind of ""antechamber of science"" where they wait until technology and scientific knowledge advance to the point where it can be tested.   == sens research foundation ==  the sens research foundation is a non-profit organization co-founded by michael kope, aubrey de grey, jeff hall, sarah marr and kevin perrott, which is based in california, united states. its activities include sens-based research programs and public relations work for the acceptance of and interest in related research.   == see also ==   == references ==')"
92,"Fantastic Voyage: Live Long Enough to Live Forever (Rodale Books, ISBN 1-57954-954-3) is a book authored by Ray Kurzweil and Terry Grossman published in 2004. The basic premise of the book is that if middle aged people can live long enough, until approximately 120 years, they will be able to live forever—as humanity overcomes all diseases and old age itself. This might also be considered a break-even scenario where developments made during a year increase life expectancy by more than one year. Biogerontologist Aubrey de Grey called this the ""Longevity escape velocity"" in a 2005 TED talk.The book focuses primarily on health topics such as heart disease, cancer, and type 2 diabetes. It promotes lifestyle changes such as a low glycemic index diet, calorie restriction, exercise, drinking green tea and alkalinized water, and other changes to daily living. They also promote aggressive supplementation to make up for nutrient deficiencies they believe are common in Western society.  In contrast to his previous book The 10% Solution for a Healthy Life, in which he recommended a diet with 10% of calories from fat, in this book, Kurzweil recommends consuming less than one third of calories from carbohydrates (and less than one sixth of calories in his low-carbohydrate diet) and consuming 25% of calories from fat.The book states that the purpose of these changes is to obtain and maintain idyllic health so that an individual can extend his or her life as long as possible. The authors believe that within the next 20 to 50 years technology will advance to the point where much of the aging process will be conquered, and degenerative diseases eliminated. The book is peppered with side notes on these futuristic topics, showing how current research is leading us toward life extension, and explaining how future technologies such as nanotechnology and bioengineering might change the way humans live their lives. Ray Kurzweil discusses these topics at further length in his 2005 book The Singularity Is Near.
A follow-up on Fantastic Voyage, Transcend: Nine Steps to Living Well Forever, was released on April 28, 2009.


== Organization ==
Chapter 1: You can live long enough to live forever
Chapter 2: The bridges to come
Chapter 3: Our personal journeys
Chapter 4: Food and water
Chapter 5: Carbohydrates and the glycemic load
Chapter 6: Fat and protein
Chapter 7: You are what you digest
Chapter 8: Change your weight for life in one day
Chapter 9: The problem with sugar (and insulin)
Chapter 10: Ray's personal program
Chapter 11: The promise of genomics
Chapter 12: Inflammation—the latest ""smoking gun""
Chapter 13: Methylation—critically important to your health
Chapter 14: Cleaning up the mess: Toxins and detoxification
Chapter 15: The real cause of heart disease and how to prevent it
Chapter 16: The prevention and early detection of cancer
Chapter 17: Terry's personal program
Chapter 18: Your brain: The power of thinking...and of ideas
Chapter 19: Hormones and aging, hormones of youth
Chapter 20: Other hormones of youth: Sex hormones
Chapter 21: Aggressive supplementation
Chapter 22: Keep moving: The power of exercise
Chapter 23: Stress and balance
Epilogue


== Criticisms ==
One claim in the book has been called pseudoscience. Dr. Stephen Lower, retired Professor of Chemistry at Simon Fraser University, disputes some of the book's statements about alkaline water on his web site. Kurzweil and Grossman counter this specific criticism directly in their Reader Q&A.


== See also ==
Nutrition
Simulated reality
Technological singularity


== References ==


== External links ==
Fantastic-voyage.net
Short Guide - lifestyle changes from the book in bullet point format
Reader Q&A - response to criticism of alkaline water claim","pandas(index=92, _1=92, text='fantastic voyage: live long enough to live forever (rodale books, isbn 1-57954-954-3) is a book authored by ray kurzweil and terry grossman published in 2004. the basic premise of the book is that if middle aged people can live long enough, until approximately 120 years, they will be able to live forever—as humanity overcomes all diseases and old age itself. this might also be considered a break-even scenario where developments made during a year increase life expectancy by more than one year. biogerontologist aubrey de grey called this the ""longevity escape velocity"" in a 2005 ted talk.the book focuses primarily on health topics such as heart disease, cancer, and type 2 diabetes. it promotes lifestyle changes such as a low glycemic index diet, calorie restriction, exercise, drinking green tea and alkalinized water, and other changes to daily living. they also promote aggressive supplementation to make up for nutrient deficiencies they believe are common in western society.  in contrast to his previous book the 10% solution for a healthy life, in which he recommended a diet with 10% of calories from fat, in this book, kurzweil recommends consuming less than one third of calories from carbohydrates (and less than one sixth of calories in his low-carbohydrate diet) and consuming 25% of calories from fat.the book states that the purpose of these changes is to obtain and maintain idyllic health so that an individual can extend his or her life as long as possible. the authors believe that within the next 20 to 50 years technology will advance to the point where much of the aging process will be conquered, and degenerative diseases eliminated. the book is peppered with side notes on these futuristic topics, showing how current research is leading us toward life extension, and explaining how future technologies such as nanotechnology and bioengineering might change the way humans live their lives. ray kurzweil discusses these topics at further length in his 2005 book the singularity is near. a follow-up on fantastic voyage, transcend: nine steps to living well forever, was released on april 28, 2009.   == organization == chapter 1: you can live long enough to live forever chapter 2: the bridges to come chapter 3: our personal journeys chapter 4: food and water chapter 5: carbohydrates and the glycemic load chapter 6: fat and protein chapter 7: you are what you digest chapter 8: change your weight for life in one day chapter 9: the problem with sugar (and insulin) chapter 10: ray\'s personal program chapter 11: the promise of genomics chapter 12: inflammation—the latest ""smoking gun"" chapter 13: methylation—critically important to your health chapter 14: cleaning up the mess: toxins and detoxification chapter 15: the real cause of heart disease and how to prevent it chapter 16: the prevention and early detection of cancer chapter 17: terry\'s personal program chapter 18: your brain: the power of thinking...and of ideas chapter 19: hormones and aging, hormones of youth chapter 20: other hormones of youth: sex hormones chapter 21: aggressive supplementation chapter 22: keep moving: the power of exercise chapter 23: stress and balance epilogue   == criticisms == one claim in the book has been called pseudoscience. dr. stephen lower, retired professor of chemistry at simon fraser university, disputes some of the book\'s statements about alkaline water on his web site. kurzweil and grossman counter this specific criticism directly in their reader q&a.   == see also == nutrition simulated reality technological singularity   == references ==   == external links == fantastic-voyage.net short guide - lifestyle changes from the book in bullet point format reader q&a - response to criticism of alkaline water claim')"
93,"Sensory substitution is a change of the characteristics of one sensory modality into stimuli of another sensory modality.
A sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. The sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. In case the sensor obtains signals of a kind not originally available to the bearer it is a case of sensory augmentation. Sensory substitution concerns human perception and the plasticity of the human brain; and therefore, allows us to study these aspects of neuroscience more through neuroimaging.
Sensory substitution systems may help people by restoring their ability to perceive  certain defective sensory modality by using sensory information from a functioning sensory modality.


== History ==
The idea of sensory substitution was introduced in the '80s by Paul Bach-y-Rita as a means of using one sensory modality, mainly taction, to gain environmental information to be used by another sensory modality, mainly vision. Thereafter, the entire field was discussed by Chaim-Meyer Scheff in ""Experimental model for the study of changes in the organization of human sensory information processing through the design and testing of non-invasive prosthetic devices for sensory impaired people"". The first sensory substitution system was developed by Bach-y-Rita et al. as a means of brain plasticity in congenitally blind individuals. After this historic invention, sensory substitution has been the basis of many studies investigating perceptive and cognitive neuroscience. Since then, sensory substitution has contributed to the study of brain function, human cognition and rehabilitation.


== Physiology ==
When a person becomes blind or deaf they generally do not lose the ability to hear or see; they simply lose their ability to transmit the sensory signals from the periphery (retina for visions and cochlea for hearing) to brain. Since the vision processing pathways are still intact, a person who has lost the ability to retrieve data from the retina can still see subjective images by using data gathered from other sensory modalities such as touch or audition.In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive sight. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, one can determine which parts of the brain are activated during sensory perception. In blind persons, it is seen that while they are only receiving tactile information, their visual cortex is also activated as they perceive sight objects. Touch-to-touch sensory substitution is also possible, wherein information from touch receptors of one region of the body can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita,touch perception was able to be restored in a patient who lost peripheral sensation due to leprosy.


=== Technological support ===
In order to achieve sensory substitution and stimulate the brain without intact sensory organs to relay the information, machines can be used to do the signal transduction, rather than the sensory organs. This brain–machine interface collects external signals and transforms them into electrical signals for the brain to interpret. Generally, a camera or a microphone is used to collect visual or auditory stimuli that are used to replace lost sight and hearing, respectively. The visual or auditory data collected from the sensors is transformed into tactile stimuli that are then relayed to the brain for visual and auditory perception. This and all types of sensory substitution are only possible due to neuroplasticity.


=== Brain plasticity ===
Brain plasticity refers to the brain's ability to adapt to a changing environment, for instance to the absence or deterioration of a sense. It is conceivable that cortical remapping or reorganization in response to the loss of one sense may be an evolutionary mechanism that allows people to adapt and compensate by using other senses better. Functional imaging of congenitally blind patients showed a cross-modal recruitment of the occipital cortex during perceptual tasks such as Braille reading, tactile perception, tactual object recognition, sound localization, and sound discrimination. This may suggest that blind people can use their occipital lobe, generally used for vision, to perceive objects through the use of other sensory modalities. This cross modal plasticity may explain the often described tendency of blind people to show enhanced ability in the other senses.


=== Perception versus sensing ===
While considering the physiological aspects of sensory substitution, it is essential to distinguish between sensing and perceiving. The general question posed by this differentiation is: Are blind people seeing or perceiving to see by putting together different sensory data? While sensation comes in one modality – visual, auditory, tactile etc. – perception due to sensory substitution is not one modality but a result of cross-modal interactions. It is therefore concluded that while sensory substitution for vision induces visual-like perception in sighted individuals, it induces auditory or tactile perception in blind individuals. In short, blind people perceive to see through touch and audition with sensory substitution.


== Different applications ==
Applications are not restricted to handicapped persons, but also include artistic presentations, games, and augmented reality. Some examples are substitution of visual stimuli to audio or tactile, and of audio stimuli to tactile. Some of the most popular are probably Paul Bach-y-Rita's Tactile Vision Sensory Substitution (TVSS), developed with Carter Collins at Smith-Kettlewell Institute and Peter Meijer's Seeing with Sound approach (The vOICe). Technical developments, such as miniaturization and electrical stimulation help the advance of sensory substitution devices.
In sensory substitution systems, we generally have sensors that collect the data from the external environment. This data is then relayed to a coupling system that interprets and transduces the information and then replays it to a stimulator. This stimulator ultimately stimulates a functioning sensory modality. After training, people learn to use the information gained from this stimulation to experience a perception of the sensation they lack instead of the actually stimulated sensation. For example, a leprosy patient, whose perception of peripheral touch was restored, was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead.


=== Tactile systems ===
To understand tactile sensory substitution it is essential to understand some basic physiology of the tactile receptors of the skin. There are five basic types of tactile receptors: Pacinian corpuscle, Meissner's corpuscle, Ruffini endings, Merkel nerve endings, and free nerve endings. These receptors are mainly characterized by which type of stimuli best activates them, and by their rate of adaptation to sustained stimuli. Because of the rapid adaptation of some of these receptors to sustained stimuli, those receptors require rapidly changing tactile stimulation systems in order to be optimally activated. Among all these mechanoreceptors Pacinian corpuscle offers the highest sensitivity to high frequency vibration starting from few 10s of Hz to a few kHz with the help of its specialized mechanotransduction mechanism.There have been two different types of stimulators: electrotactile or vibrotactile. Electrotactile stimulators use direct electrical stimulation of the nerve ending in the skin to initiate the action potentials; the sensation triggered, burn, itch, pain, pressure etc. depends on the stimulating voltage. Vibrotactile stimulators use pressure and the properties of the mechanoreceptors of the skin to initiate action potentials. There are advantages and disadvantages for both these stimulation systems. With the electrotactile stimulating systems a lot of factors affect the sensation triggered: stimulating voltage, current, waveform, electrode size, material, contact force, skin location, thickness and hydration. Electrotactile stimulation may involve the direct stimulation of the nerves (percutaneous), or through the skin (transcutaneous). Percutaneous application causes additional distress to the patient, and is a major disadvantage of this approach. Furthermore, stimulation of the skin without insertion leads to the need for high voltage stimulation because of the high impedance of the dry skin, unless the tongue is used as a receptor, which requires only about 3% as much voltage. This latter technique is undergoing clinical trials for various applications, and been approved for assistance to the blind in the UK. Alternatively, the roof of the mouth has been proposed as another area where low currents can be felt.Electrostatic arrays are explored as human-computer interaction devices for touch screens. These are based on a phenomenon called electrovibration, which allows microamperre-level currents to be felt as roughness on a surface.Vibrotactile systems use the properties of mechanoreceptors in the skin so they have fewer parameters that need to be monitored as compared to electrotactile stimulation. However, vibrotactile stimulation systems need to account for the rapid adaptation of the tactile sense.
Another important aspect of tactile sensory substitution systems is the location of the tactile stimulation. Tactile receptors are abundant on the fingertips, face, and tongue while sparse on the back, legs and arms. It is essential to take into account the spatial resolution of the receptor as it has a major effect on the resolution of the sensory substitution. A high resolution pin-arrayed display is able to present spatial information via tactile symbols, such as city maps and obstacle maps.Below you can find some descriptions of current tactile substitution systems.


==== Tactile–visual ====
One of the earliest and most well known form of sensory substitution devices was Paul Bach-y-Rita's TVSS that converted the image from a video camera into a tactile image and coupled it to the tactile receptors on the back of his blind subject. Recently, several new systems have been developed that interface the tactile image to tactile receptors on different areas of the body such as the on the chest, brow, fingertip, abdomen, and forehead. The tactile image is produced by hundreds of activators placed on the person. The activators are solenoids of one millimeter diameter. In experiments, blind (or blindfolded) subjects equipped with the TVSS can learn to detect shapes and to orient themselves. In the case of simple geometric shapes, it took around 50 trials to achieve 100 percent correct recognition. To identify objects in different orientations requires several hours of learning.
A system using the tongue as the human-machine interface is most practical.  The tongue-machine interface is both protected by the closed mouth and the saliva in the mouth provides a good electrolytic environment that ensures good electrode contact. Results from a study by Bach-y-Rita et al. show that electrotactile stimulation of the tongue required 3% of the voltage required to stimulate the finger. Also, since it is more practical to wear an orthodontic retainer holding the stimulation system than an apparatus strapped to other parts of the body, the tongue-machine interface is more popular among TVSS systems.
This tongue TVSS system works by delivering electrotactile stimuli to the dorsum of the tongue via a flexible electrode array placed in the mouth. This electrode array is connected to a Tongue Display Unit [TDU] via a ribbon cable passing out of the mouth. A video camera records a picture, transfers it to the TDU for conversion into a tactile image. The tactile image is then projected onto the tongue via the ribbon cable where the tongue's receptors pick up the signal. After training, subjects are able to associate certain types of stimuli to certain types of visual images. In this way, tactile sensation can be used for visual perception.
Sensory substitutions have also been successful with the emergence of wearable haptic actuators like vibrotactile motors, solenoids, peltier diodes, etc. At the Center for Cognitive Ubiquitous Computing at Arizona State University, researchers have developed technologies that enable people who are blind to perceive social situational information using wearable vibrotactile belts (Haptic Belt) and gloves (VibroGlove). Both technologies use miniature cameras that are mounted on a pair of glasses worn by the user who is blind. The Haptic Belt provides vibrations that convey the direction and distance at which a person is standing in front of a user, while the VibroGlove uses spatio-temporal mapping of vibration patterns to convey facial expressions of the interaction partner. Alternatively, it has been shown that even very simple cues indicating the presence or absence of obstacles (through small vibration modules located at strategic places in the body) can be useful for navigation, gait stabilization and reduced anxiety when evolving in an unknown space. This approach, called the ""Haptic Radar"" has been studied since 2005 by researchers at the University of Tokyo in collaboration with the University of Rio de Janeiro. Similar products include the Eyeronman vest and belt, and the forehead retina system.


==== Tactile–auditory ====
Neuroscientist David Eagleman presented a new device for sound-to-touch hearing at TED in 2015; his laboratory research then expanded into a company based in Palo Alto, California, called Neosensory.  Neosensory devices capture sound and turn them into high-dimensional patterns of touch on the skin.Experiments by Schurmann et al. show that tactile senses can activate the human auditory cortex. Currently vibrotactile stimuli can be used to facilitate hearing in normal and hearing-impaired people. To test for the auditory areas activated by touch, Schurmann et al. tested subjects while stimulating their fingers and palms with vibration bursts and their fingertips with tactile pressure. They found that tactile stimulation of the fingers lead to activation of the auditory belt area, which suggests that there is a relationship between audition and tactition. Therefore, future research can be done to investigate the likelihood of a tactile–auditory sensory substitution system. One promising invention is the 'Sense organs synthesizer' which aims at delivering a normal hearing range of nine octaves via 216 electrodes to sequential touch nerve zones, next to the spine.


==== Tactile–vestibular ====
Some people with balance disorders or adverse reactions to antibiotics suffer from bilateral vestibular damage (BVD). They experience difficulty maintaining posture, unstable gait, and oscillopsia. Tyler et al. studied the restitution of postural control through a tactile for vestibular sensory substitution. Because BVD patients cannot integrate visual and tactile cues, they have a lot of difficulty standing. Using a head-mounted accelerometer and a brain-machine interface that employs electrotactile stimulation on the tongue, information about head-body orientation was relayed to the patient so that a new source of data is available to orient themselves and maintain good posture.


==== Tactile–tactile to restore peripheral sensation ====
Touch to touch sensory substitution is where information from touch receptors of one region can be used to perceive touch in another. For example, in one experiment by Bach-y-Rita, the touch perception was restored in a patient who lost peripheral sensation from leprosy. For example, this leprosy patient was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead. After two days of training one of the leprosy subjects reported ""the wonderful sensation of touching his wife, which he had been unable to experience for 20 years.""


==== Tactile feedback system for prosthetic limbs ====
The development of new technologies has now made it plausible to provide patients with prosthetic arms with tactile and kinesthetic sensibilities. While this is not purely a sensory substitution system, it uses the same principles to restore perception of senses. Some tactile feedback methods of restoring a perception of touch to amputees would be direct or micro stimulation of the tactile nerve afferents.Other applications of sensory substitution systems can be seen in function robotic prostheses for patients with high level quadriplegia. These robotic arms have several mechanisms of slip detection, vibration and texture detection that they relay to the patient through feedback.  After more research and development, the information from these arms can be used by patients to perceive that they are holding and manipulating objects while their robotic arm actually accomplishes the task.


=== Auditory systems ===
Auditory sensory substitution systems like the tactile sensory substitution systems aim to use one sensory modality to compensate for the lack of another in order to gain a perception of one that is lacking. With auditory sensory substitution, visual or tactile sensors detect and store information about the external environment. This information is then transformed by interfaces into sound. Most systems are auditory-vision substitutions aimed at using the sense of hearing to convey visual information to the blind.


==== The vOICe Auditory Display ====
""The vOICe"" converts live camera views from a video camera into soundscapes, patterns of scores of different tones at different volumes and pitches emitted simultaneously. The technology of the vOICe was invented in the 1990s by Peter Meijer and uses general video to audio mapping by associating height to pitch and brightness with loudness in a left-to-right scan of any video frame.


==== EyeMusic ====
The EyeMusic user wears a miniature camera connected to a small computer (or smartphone) and stereo headphones. The images are converted into ""soundscapes"". The high locations on the image are projected as high-pitched musical notes on a pentatonic scale, and low vertical locations as low-pitched musical notes.
The EyeMusic conveys color information by using different musical instruments for each of the following five colors: white, blue, red, green, yellow. The EyeMusic employs an intermediate resolution of 30×50 pixels.


==== LibreAudioView ====
This project, presented in 2015, proposes a new versatile mobile device and a sonification method specifically designed to the pedestrian locomotion of the visually impaired. It sonifies in real-time spatial information from a video stream acquired at a standard frame rate. The device is composed of a miniature camera integrated into a glasses frame which is connected to a battery-powered minicomputer worn around the neck with a strap. The audio signal is transmitted to the user via running headphones. This system has two operating modes. With the first mode, when the user is static, only the edges of the moving objects are sonified. With the second mode, when the user is moving, the edges of both static and moving objects are sonified. Thus, the video stream is simplified by extracting only the edges of objects that can become dangerous obstacles. The system enables the localization of moving objects, the estimation of trajectories, and the detection of approaching objects.


==== PSVA ====
Another successful visual-to-auditory sensory substitution device is the Prosthesis Substituting Vision for Audition (PSVA). This system utilizes a head-mounted TV camera that allows real-time, online translation of visual patterns into sound. While the patient moves around, the device captures visual frames at a high frequency and generates the corresponding complex sounds that allow recognition. Visual stimuli are transduced into auditory stimuli with the use of a system that uses pixel to frequency relationship and couples a rough model of the human retina with an inverse model of the cochlea.


==== The Vibe ====
The sound produced by this software is a mixture of sinusoidal sounds produced by virtual ""sources"", corresponding each to a ""receptive field"" in the image. Each receptive field is a set of localized pixels. The sound's amplitude is determined by the mean luminosity of the pixels of the corresponding receptive field. The frequency and the inter-aural disparity are determined by the center of gravity of the co-ordinates of the receptive field's pixels in the image (see ""There is something out there: distal attribution in sensory substitution, twenty years later""; Auvray M., Hanneton S., Lenay C., O'Regan K. Journal of Integrative Neuroscience 4 (2005) 505-21). The Vibe is an Open Source project hosted by Sourceforge.


==== Other systems ====
Other approaches to the substitution of hearing for vision use binaural directional cues, much as natural human echolocation does.  An example of the latter approach is the ""SeeHear"" chip from Caltech.Other visual-auditory substitution devices deviate from the vOICe's greyscale mapping of images. Zach Capalbo's Kromophone uses a basic color spectrum correlating to different sounds and timbres to give users perceptual information beyond the vOICe's capabilities.


=== Nervous system implants ===
By means of stimulating electrodes implanted into the human nervous system, it is possible to apply current pulses to be learned and reliably recognized by the recipient. It has been shown successfully in experimentation, by Kevin Warwick, that signals can be employed from force/touch indicators on a robot hand as a means of communication.


== Criticism ==
It has been argued that the term ""substitution"" is misleading, as it is merely an ""addition"" or ""supplementation"" not a substitution of a sensory modality.


== Sensory augmentation ==

Building upon the research conducted on sensory substitution, investigations into the possibility of augmenting the body's sensory apparatus are now beginning. The intention is to extend the body's ability to sense aspects of the environment that are not normally perceivable by the body in its natural state.
Active work in this direction is being conducted by, among others, the e-sense project of the Open University and Edinburgh University, the feelSpace project of the University of Osnabrück, and the hearSpace project at University of Paris.
The findings of research into sensory augmentation (as well as sensory substitution in general) that investigate the emergence of perceptual experience (qualia) from the activity of neurons have implications for the understanding of consciousness.


== See also ==
Biological neural network
Brain implant
Human echolocation, blind people navigating by listening to the echo of sounds


== References ==


== External links ==
Tongue display for sensory substitution
The vOICe auditory display for sensory substitution.
Artificial Retinas
Sensory Substitution:limits and perspectives C. Lenay et al.
The Vibe
feelSpace - The Magnetic Perception Group of the University of Osnabrück
The Kromophone
Sensory Substitution For Blind (Nihat Erim İnceoğlu)
Sensory augmentation: integration of an auditory compass signal into human perception of space","pandas(index=93, _1=93, text='sensory substitution is a change of the characteristics of one sensory modality into stimuli of another sensory modality. a sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. the sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. in case the sensor obtains signals of a kind not originally available to the bearer it is a case of sensory augmentation. sensory substitution concerns human perception and the plasticity of the human brain; and therefore, allows us to study these aspects of neuroscience more through neuroimaging. sensory substitution systems may help people by restoring their ability to perceive  certain defective sensory modality by using sensory information from a functioning sensory modality.   == history == the idea of sensory substitution was introduced in the \'80s by paul bach-y-rita as a means of using one sensory modality, mainly taction, to gain environmental information to be used by another sensory modality, mainly vision. thereafter, the entire field was discussed by chaim-meyer scheff in ""experimental model for the study of changes in the organization of human sensory information processing through the design and testing of non-invasive prosthetic devices for sensory impaired people"". the first sensory substitution system was developed by bach-y-rita et al. as a means of brain plasticity in congenitally blind individuals. after this historic invention, sensory substitution has been the basis of many studies investigating perceptive and cognitive neuroscience. since then, sensory substitution has contributed to the study of brain function, human cognition and rehabilitation.   == physiology == when a person becomes blind or deaf they generally do not lose the ability to hear or see; they simply lose their ability to transmit the sensory signals from the periphery (retina for visions and cochlea for hearing) to brain. since the vision processing pathways are still intact, a person who has lost the ability to retrieve data from the retina can still see subjective images by using data gathered from other sensory modalities such as touch or audition.in a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. because it is the brain that is responsible for the final perception, sensory substitution is possible. during sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive sight. with sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. for example, through fmri, one can determine which parts of the brain are activated during sensory perception. in blind persons, it is seen that while they are only receiving tactile information, their visual cortex is also activated as they perceive sight objects. touch-to-touch sensory substitution is also possible, wherein information from touch receptors of one region of the body can be used to perceive touch in another region. for example, in one experiment by bach-y-rita,touch perception was able to be restored in a patient who lost peripheral sensation due to leprosy. by means of stimulating electrodes implanted into the human nervous system, it is possible to apply current pulses to be learned and reliably recognized by the recipient. it has been shown successfully in experimentation, by kevin warwick, that signals can be employed from force/touch indicators on a robot hand as a means of communication.   == criticism == it has been argued that the term ""substitution"" is misleading, as it is merely an ""addition"" or ""supplementation"" not a substitution of a sensory modality.   == sensory augmentation ==  building upon the research conducted on sensory substitution, investigations into the possibility of augmenting the body\'s sensory apparatus are now beginning. the intention is to extend the body\'s ability to sense aspects of the environment that are not normally perceivable by the body in its natural state. active work in this direction is being conducted by, among others, the e-sense project of the open university and edinburgh university, the feelspace project of the university of osnabrück, and the hearspace project at university of paris. the findings of research into sensory augmentation (as well as sensory substitution in general) that investigate the emergence of perceptual experience (qualia) from the activity of neurons have implications for the understanding of consciousness.   == see also == biological neural network brain implant human echolocation, blind people navigating by listening to the echo of sounds   == references ==   == external links == tongue display for sensory substitution the voice auditory display for sensory substitution. artificial retinas sensory substitution:limits and perspectives c. lenay et al. the vibe feelspace - the magnetic perception group of the university of osnabrück the kromophone sensory substitution for blind (nihat erim i̇nceoğlu) sensory augmentation: integration of an auditory compass signal into human perception of space')"
94,"An implant is a medical device manufactured to replace a missing biological structure, support a damaged biological structure, or enhance an existing biological structure. Medical implants are man-made devices, in contrast to a transplant, which is a transplanted biomedical tissue. The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone, or apatite depending on what is the most functional. In some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.


== Applications ==
Implants can roughly be categorized into groups by application:


=== Sensory and neurological ===
Sensory and neurological implants are used for disorders affecting the major senses and the brain, as well as other neurological disorders. They are predominately used in the treatment of conditions such as cataract, glaucoma, keratoconus, and other visual impairments; otosclerosis and other hearing loss issues, as well as middle ear diseases such as otitis media; and neurological diseases such as epilepsy, Parkinson's disease, and treatment-resistant depression. Examples include the intraocular lens, intrastromal corneal ring segment, cochlear implant, tympanostomy tube, and neurostimulator.


=== Cardiovascular ===
Cardiovascular medical devices are implanted in cases where the heart, its valves, and the rest of the circulatory system is in disorder. They are used to treat conditions such as heart failure, cardiac arrhythmia, ventricular tachycardia, valvular heart disease, angina pectoris, and atherosclerosis. Examples include the artificial heart, artificial heart valve, implantable cardioverter-defibrillator, cardiac pacemaker, and coronary stent.


=== Orthopedic ===
Orthopaedic implants help alleviate issues with the bones and joints of the body. They're used to treat bone fractures, osteoarthritis, scoliosis, spinal stenosis, and chronic pain. Examples include a wide variety of pins, rods, screws, and plates used to anchor fractured bones while they heal.Metallic glasses based on magnesium with zinc and calcium addition are tested as the potential metallic biomaterials for biodegradable medical implants.Patient with orthopaedic implants sometimes need to be put under magnetic resonance imaging (MRI) machine for detailed musculoskeletal study. Therefore, concerns have been raised regarding the loosening and migration of implant, heating of the implant metal which could cause thermal damage to surrounding tissues, and distortion of the MRI scan that affects the imaging results. A study of orthopaedic implants in 2005 has shown that majority of the orthopaedic implants does not react with magnetic fields under the 1.0 Tesla MRI scanning machine with the exception of external fixator clamps. However, at 7.0 Tesla, several orthopaedic implants would show significant interaction with the MRI magnetic fields, such as heel and fibular implant.


=== Electric ===
Electrical implants are being used to relieve pain and suffering from rheumatoid arthritis. The electric implant is embedded in the neck of patients with rheumatoid arthritics, the implant sends electrical signals to electrodes in the vagus nerve. The application of this device is being tested an alternative to medicating sufferers of rheumatoid arthritis for their lifetime.


=== Contraception ===
Contraceptive implants are primarily used to prevent unintended pregnancy and treat conditions such as non-pathological forms of menorrhagia. Examples include copper- and hormone-based intrauterine devices.


=== Cosmetic ===
Cosmetic implants — often prosthetics — attempt to bring some portion of the body back to an acceptable aesthetic norm. They are used as a follow-up to mastectomy due to breast cancer, for correcting some forms of disfigurement, and modifying aspects of the body (as in buttock augmentation and chin augmentation). Examples include the breast implant, nose prosthesis, ocular prosthesis, and injectable filler.


=== Other organs and systems ===
Other types of organ dysfunction can occur in the systems of the body, including the gastrointestinal, respiratory, and urological systems. Implants are used in those and other locations to treat conditions such as gastroesophageal reflux disease, gastroparesis, respiratory failure, sleep apnea, urinary and fecal incontinence, and erectile dysfunction. Examples include the LINX, implantable gastric stimulator, diaphragmatic/phrenic nerve stimulator, neurostimulator, surgical mesh, artificial urinary sphincter and penile implant.


== Classification ==


=== United States classification ===
Medical devices are classified by the US Food and Drug Administration (FDA) under three different classes depending on the risks the medical device may impose on the user. According to 21CFR 860.3, Class I devices are considered to pose the least amount of risk to the user and require the least amount of control. Class I devices include simple devices such as arm slings and hand-held surgical instruments. Class II devices are considered to need more regulation than Class I devices and are required to undergo specific requirements before FDA approval. Class II devices include X-ray systems and physiological monitors. Class III devices require the most regulatory controls since the device supports or sustains human life or may not be well tested. Class III devices include replacement heart valves and implanted cerebellar stimulators. Many implants typically fall under Class II and Class III devices.


== Materials ==


=== Commonly implanted metals ===
A variety of minimally bioreactive metals are routinely implanted.  The most commonly implanted form of stainless steel is 316L. Cobalt-chromium and titanium-based implant alloys are also permanently implanted.  All of these are made passive by a thin layer of oxide on their surface. A consideration, however, is that metal ions diffuse outward through the oxide and end up in the surrounding tissue.  Bioreaction to metal implants includes the formation of a small envelope of fibrous tissue.  The thickness of this layer is determined by the products being dissolved, and the extent to which the implant moves around within the enclosing tissue.  Pure titanium may have only a minimal fibrous encapsulation.  Stainless steel, on the other hand, may elicit encapsulation of as much as 2 mm.


=== List of implantable metal alloys ===


==== Stainless Steel ====
ASTM F138/F139 316L
ASTM F1314 22Cr-13Ni–5Mn


==== Titanium Alloy ====
ASTM F67 Unalloyed (Commercially Pure) Titanium
ASTM F136 Ti-6Al-4V-ELI
ASTM F1295 Ti-6Al-7Nb
ASTM F1472 Ti-6Al-4V


==== Cobalt Chrome Alloy ====
ASTM F90 Co-20Cr-15W-10Ni
ASTM F562 Co-35Ni-20Cr-10Mo
ASTM F1537 Co-28Cr-6Mo


==== Tantalum ====
ASTM F560 Unalloyed Tantalum


== Complications ==

Under ideal conditions, implants should initiate the desired host response. Ideally, the implant should not cause any undesired reaction from neighboring or distant tissues. However, the interaction between the implant and the tissue surrounding the implant can lead to complications. The process of implantation of medical devices is subjected to the same complications that other invasive medical procedures can have during or after surgery. Common complications include infection, inflammation, and pain. Other complications that can occur include risk of rejection from implant-induced coagulation and allergic foreign body response. Depending on the type of implant, the complications may vary.When the site of an implant becomes infected during or after surgery, the surrounding tissue becomes infected by microorganisms. Three main categories of infection can occur after operation. Superficial immediate infections are caused by organisms that commonly grow near or on skin. The infection usually occurs at the surgical opening. Deep immediate infection, the second type, occurs immediately after surgery at the site of the implant. Skin-dwelling and airborne bacteria cause deep immediate infection. These bacteria enter the body by attaching to the implant's surface prior to implantation. Though not common, deep immediate infections can also occur from dormant bacteria from previous infections of the tissue at the implantation site that have been activated from being disturbed during the surgery. The last type, late infection, occurs months to years after the implantation of the implant. Late infections are caused by dormant blood-borne bacteria attached to the implant prior to implantation. The blood-borne bacteria colonize on the implant and eventually get released from it. Depending on the type of material used to make the implant, it may be infused with antibiotics to lower the risk of infections during surgery. However, only certain types of materials can be infused with antibiotics, the use of antibiotic-infused implants runs the risk of rejection by the patient since the patient may develop a sensitivity to the antibiotic, and the antibiotic may not work on the bacteria.Inflammation, a common occurrence after any surgical procedure, is the body's response to tissue damage as a result of trauma, infection, intrusion of foreign materials, or local cell death, or as a part of an immune response. Inflammation starts with the rapid dilation of local capillaries to supply the local tissue with blood. The inflow of blood causes the tissue to become swollen and may cause cell death. The excess blood, or edema, can activate pain receptors at the tissue. The site of the inflammation becomes warm from local disturbances of fluid flow and the increased cellular activity to repair the tissue or remove debris from the site.Implant-induced coagulation is similar to the coagulation process done within the body to prevent blood loss from damaged blood vessels. However, the coagulation process is triggered from proteins that become attached to the implant surface and lose their shapes. When this occurs, the protein changes conformation and different activation sites become exposed, which may trigger an immune system response where the body attempts to attack the implant to remove the foreign material. The trigger of the immune system response can be accompanied by inflammation. The immune system response may lead to chronic inflammation where the implant is rejected and has to be removed from the body. The immune system may encapsulate the implant as an attempt to remove the foreign material from the site of the tissue by encapsulating the implant in fibrinogen and platelets. The encapsulation of the implant can lead to further complications, since the thick layers of fibrous encapsulation may prevent the implant from performing the desired functions. Bacteria may attack the fibrous encapsulation and become embedded into the fibers. Since the layers of fibers are thick, antibiotics may not be able to reach the bacteria and the bacteria may grow and infect the surrounding tissue. In order to remove the bacteria, the implant would have to be removed. Lastly, the immune system may accept the presence of the implant and repair and remodel the surrounding tissue. Similar responses occur when the body initiates an allergic foreign body response. In the case of an allergic foreign body response, the implant would have to be removed.


== Failures ==

The many examples of implant failure include rupture of silicone breast implants, hip replacement joints, and artificial heart valves, such as the Bjork–Shiley valve, all of which have caused FDA intervention. The consequences of implant failure depend on the nature of the implant and its position in the body. Thus, heart valve failure is likely to threaten the life of the individual, while breast implant or hip joint failure is less likely to be life-threatening.Devices implanted directly in the grey matter of the brain produce the highest quality signals, but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.In 2018, Implant files, an investigation made by ICIJ revealed that medical devices that are unsafe and have not been adequately tested were implanted in patients' bodies. In United Kingdom, Prof Derek Alderson, president of the Royal College of Surgeons, concludes: ""All implantable devices should be registered and tracked to monitor efficacy and patient safety in the long-term.""


== See also ==
Biofunctionalisation
Implantable devices
List of orthopedic implants
Medical device
Prosthesis
(in French)Implant Files scandal by ICIJ, November 2018.


== References ==


== External links ==
AAOMS - Dental Implant Surgery
ACOG - IUDs and Birth Control Implants: Resource Overview
FDA - Implants and Prosthetics
International Medical Devices Database – Recalls, Safety Alerts and Field Safety Notices of medical devices – International Consortium of Investigative Journalists
Implant-Register","pandas(index=94, _1=94, text='an implant is a medical device manufactured to replace a missing biological structure, support a damaged biological structure, or enhance an existing biological structure. medical implants are man-made devices, in contrast to a transplant, which is a transplanted biomedical tissue. the surface of implants that contact the body might be made of a biomedical material such as titanium, silicone, or apatite depending on what is the most functional. in some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.   == applications == implants can roughly be categorized into groups by application: astm f560 unalloyed tantalum   == complications ==  under ideal conditions, implants should initiate the desired host response. ideally, the implant should not cause any undesired reaction from neighboring or distant tissues. however, the interaction between the implant and the tissue surrounding the implant can lead to complications. the process of implantation of medical devices is subjected to the same complications that other invasive medical procedures can have during or after surgery. common complications include infection, inflammation, and pain. other complications that can occur include risk of rejection from implant-induced coagulation and allergic foreign body response. depending on the type of implant, the complications may vary.when the site of an implant becomes infected during or after surgery, the surrounding tissue becomes infected by microorganisms. three main categories of infection can occur after operation. superficial immediate infections are caused by organisms that commonly grow near or on skin. the infection usually occurs at the surgical opening. deep immediate infection, the second type, occurs immediately after surgery at the site of the implant. skin-dwelling and airborne bacteria cause deep immediate infection. these bacteria enter the body by attaching to the implant\'s surface prior to implantation. though not common, deep immediate infections can also occur from dormant bacteria from previous infections of the tissue at the implantation site that have been activated from being disturbed during the surgery. the last type, late infection, occurs months to years after the implantation of the implant. late infections are caused by dormant blood-borne bacteria attached to the implant prior to implantation. the blood-borne bacteria colonize on the implant and eventually get released from it. depending on the type of material used to make the implant, it may be infused with antibiotics to lower the risk of infections during surgery. however, only certain types of materials can be infused with antibiotics, the use of antibiotic-infused implants runs the risk of rejection by the patient since the patient may develop a sensitivity to the antibiotic, and the antibiotic may not work on the bacteria.inflammation, a common occurrence after any surgical procedure, is the body\'s response to tissue damage as a result of trauma, infection, intrusion of foreign materials, or local cell death, or as a part of an immune response. inflammation starts with the rapid dilation of local capillaries to supply the local tissue with blood. the inflow of blood causes the tissue to become swollen and may cause cell death. the excess blood, or edema, can activate pain receptors at the tissue. the site of the inflammation becomes warm from local disturbances of fluid flow and the increased cellular activity to repair the tissue or remove debris from the site.implant-induced coagulation is similar to the coagulation process done within the body to prevent blood loss from damaged blood vessels. however, the coagulation process is triggered from proteins that become attached to the implant surface and lose their shapes. when this occurs, the protein changes conformation and different activation sites become exposed, which may trigger an immune system response where the body attempts to attack the implant to remove the foreign material. the trigger of the immune system response can be accompanied by inflammation. the immune system response may lead to chronic inflammation where the implant is rejected and has to be removed from the body. the immune system may encapsulate the implant as an attempt to remove the foreign material from the site of the tissue by encapsulating the implant in fibrinogen and platelets. the encapsulation of the implant can lead to further complications, since the thick layers of fibrous encapsulation may prevent the implant from performing the desired functions. bacteria may attack the fibrous encapsulation and become embedded into the fibers. since the layers of fibers are thick, antibiotics may not be able to reach the bacteria and the bacteria may grow and infect the surrounding tissue. in order to remove the bacteria, the implant would have to be removed. lastly, the immune system may accept the presence of the implant and repair and remodel the surrounding tissue. similar responses occur when the body initiates an allergic foreign body response. in the case of an allergic foreign body response, the implant would have to be removed.   == failures ==  the many examples of implant failure include rupture of silicone breast implants, hip replacement joints, and artificial heart valves, such as the bjork–shiley valve, all of which have caused fda intervention. the consequences of implant failure depend on the nature of the implant and its position in the body. thus, heart valve failure is likely to threaten the life of the individual, while breast implant or hip joint failure is less likely to be life-threatening.devices implanted directly in the grey matter of the brain produce the highest quality signals, but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.in 2018, implant files, an investigation made by icij revealed that medical devices that are unsafe and have not been adequately tested were implanted in patients\' bodies. in united kingdom, prof derek alderson, president of the royal college of surgeons, concludes: ""all implantable devices should be registered and tracked to monitor efficacy and patient safety in the long-term.""   == see also == biofunctionalisation implantable devices list of orthopedic implants medical device prosthesis (in french)implant files scandal by icij, november 2018.   == references ==   == external links == aaoms - dental implant surgery acog - iuds and birth control implants: resource overview fda - implants and prosthetics international medical devices database – recalls, safety alerts and field safety notices of medical devices – international consortium of investigative journalists implant-register')"
95,"Porous silicon (abbreviated as ""PS"" or ""pSi"") is a form of the chemical element silicon that has introduced nanopores in its microstructure, rendering a large surface to volume ratio in the order of 500 m2/cm3.


== History ==
Porous silicon was discovered by accident in 1956 by Arthur Uhlir Jr. and Ingeborg Uhlir at the Bell Labs in the U.S. At the time, the Ulhirs were in the process of developing a technique for polishing and shaping the surfaces of silicon and germanium. However, it was found that under several conditions a crude product in the form of thick black, red or brown film were formed on the surface of the material. At the time, the findings were not taken further and were only mentioned in Bell Lab's technical notes.Despite the discovery of porous silicon in the 1950s, the scientific community was not interested in porous silicon until the late 1980s. At the time, Leigh Canham – while working at the Defence Research Agency in England – reasoned that the porous silicon may display quantum confinement effects. The intuition was followed by successful experimental results published in 1990. In the published experiment, it was revealed that silicon wafers can emit light if subjected to electrochemical and chemical dissolution.
The published result stimulated the interest of the scientific community in its non-linear optical and electrical properties. The growing interest was evidenced in the number of published work concerning the properties and potential applications of porous silicon. In an article published in 2000, it was found that the number of published work grew exponentially in between 1991 and 1995.In 2001, a team of scientists at the Technical University of Munich inadvertently discovered that hydrogenated porous silicon reacts explosively with oxygen at cryogenic temperatures, releasing several times as much energy as an equivalent amount of TNT, at a much greater speed. (An abstract of the study can be found below.) Explosion occurs because the oxygen, which is in a liquid state at the necessary temperatures, is able to oxidize through the porous molecular structure of the silicon extremely rapidly, causing a very quick and efficient detonation. Although hydrogenated porous silicon would probably not be effective as a weapon, due to its functioning only at low temperatures, other uses are being explored for its explosive properties, such as providing thrust for satellites.


== Fabrication of porous silicon ==
Anodization and stain-etching are the two most common methods used for fabrication of porous silicon; however, there are almost twenty other methods to fabricate this material.  Drying and surface modification might be needed afterwards.  If anodization in an aqueous solution is used to form microporous silicon, the material is commonly treated in ethanol immediately after fabrication, to avoid damage to the structure that results due to the stresses of the capillary effect of the aqueous solution.


=== Anodization ===

One method of introducing pores in silicon is through the use of an anodization cell. A possible anodization cell employs platinum cathode and silicon wafer anode immersed in hydrogen fluoride (HF) electrolyte. Recently, inert diamond cathodes are used to avoid metallic impurities in the electrolyte and inert diamond anodes form an improved electrical back plate contact to the silicon wafers. Corrosion of the anode is produced by running electric current through the cell. It is noted that the running of constant DC is usually implemented to ensure steady tip-concentration of HF resulting in a more homogeneous porosity layer although pulsed current is more appropriate for the formation of thick silicon wafers bigger than 50 µm.It was noted by Halimaoui that hydrogen evolution occurs during the formation of porous silicon. 

When purely aqueous HF solutions are used for the PS formation, the hydrogen bubbles stick to the surface and induce lateral and in-depth inhomogeneity
The hydrogen evolution is normally treated with absolute ethanol in concentration exceeding 15%. It was found that the introduction of ethanol eliminates hydrogen and ensures complete infiltration of HF solution within the pores. Subsequently, uniform distribution of porosity and thickness is improved.


=== Stain etching ===
It is possible to obtain porous silicon through stain-etching with hydrofluoric acid, nitric acid and water. A publication in 1957 revealed that stain films can be grown in dilute solutions of nitric acid in concentrated hydrofluoric acid. Porous silicon formation by stain-etching is particularly attractive because of its simplicity and the presence of readily available corrosive reagents; namely nitric acid (HNO3) and hydrogen fluoride (HF). Furthermore, stain-etching is useful if one needs to produce a very thin porous Si films. A publication in 1960 by R. J. Archer revealed that it is possible to create stain films as thin as 25 Å through stain-etching with HF-HNO3 solution.


=== Bottom-Up Synthesis ===
Porous silicon can be synthesized chemically from silicon tetrachloride, using self-forming salt byproducts as templates for pore formation. The salt templates are later removed with water.


== Drying of porous silicon ==
Porous silicon is systematically prone to presence of cracks when the water is evaporated. The cracks are particularly evident in thick or highly porous silicon layers. The origin of the cracks has been attributed to the large capillary stress due to the minute size of the pores. In particular, it has been known that cracks will appear for porous silicon samples with thickness larger than a certain critical value. Bellet concluded that it was impossible to avoid cracking in thick porous silicon layers under normal evaporating conditions. Hence, several appropriate techniques have been developed to minimize the risk of cracks formed during drying.

Supercritical dryingSupercritical drying is reputed to be the most efficient drying technique but is rather expensive and difficult to implement. It was first implemented by Canham in 1994 and involves superheating the liquid pore above the critical point to avoid interfacial tension.
Freeze dryingFreeze drying procedure was first documented around 1996. After the formation of porous silicon, the sample is frozen at a temperature of about 200 K and sublimed under vacuum.
Pentane dryingThe technique uses pentane as the drying liquid instead of water. In doing so the capillary stress is reduced because pentane has a lower surface tension than water.
Slow evaporationSlow evaporating technique can be implemented following the water or ethanol rinsing. It was found that slow evaporation decreased the trap density


== Surface modification of porous silicon ==
The surface of porous silicon may be modified to exhibit different properties. Often, freshly etched porous silicon may be unstable due to the rate of its oxidation by the atmosphere or unsuitable for cell attachment purposes. Therefore, it can be surface modified to improve stability and cell attachment


=== Surface modification improving stability ===
Following the formation of porous silicon, its surface is covered with covalently bonded hydrogen. Although the hydrogen coated surface is sufficiently stable when exposed to inert atmosphere for a short period of time, prolonged exposure render the surface prone to oxidation by atmospheric oxygen. The oxidation promotes instability in the surface and is undesirable for many applications. Thus, several methods were developed to promote the surface stability of porous silicon.
An approach that can be taken is through thermal oxidation. The process involves heating the silicon to a temperature above 1000 C to promote full oxidation of silicon. The method reportedly produced samples with good stability to aging and electronic surface passivation.Porous silicon exhibits a high degree of biocompatibility. The large surface area enables organic molecules to adhere well. It degrades to Orthosillicic acid (H4SiO4), which causes no harm to the body. This has opened potential applications in medicine such as a framework of the growth of bone.


=== Surface modification improving cell adhesion ===
Surface modification can also affect properties that promote cell adhesion. One particular research in 2005 studied the mammalian cell adhesion on the modified surfaces of porous silicon. The research used rat PC12 cells and Human Lens Epithelial (HLE) cells cultured for four hours on the surface modified porous silicon. Cells were then stained with vital dye FDA and observed under fluorescence microscopy. The research concluded that ""amino silanisation and coating the pSi surface with collagen enhanced cell attachment and spreading"".


== Classification of porous silicon ==


=== Porosity ===
Porosity is defined as the fraction of void within the pSi layer and can be determined easily by weight measurement. During formation of porous silicon layer through anodization, the porosity of a wafer can be increased through increasing current density, decreasing HF concentration and thicker silicon layer. The porosity of porous silicon may range from 4% for macroporous layers to 95% for mesoporous layers. A study by Canham in 1995 found that ""a 1 µm thick layer of high porosity silicon completely dissolved within a day of in-vitro exposure to a simulated body fluid"". It was also found that a silicon wafer with medium to low porosity displayed more stability. Hence, the porosity of porous silicon is varied depending on its potential application areas.


=== Pore size ===
The porosity value of silicon is a macroscopic parameter and doesn’t yield any information regarding the microstructure of the layer. It is proposed that the properties of a sample are more accurately predicted if the pore size and its distribution within the sample can be obtained. Therefore, porous silicon has been divided into three categories based on the size of its pores; macroporous, mesoporous, and microporous.


== Key characteristic of porous silicon ==


=== Highly controllable properties ===
Porous silicon studies conducted in 1995 showed that the behavior of porous silicon can be altered in between ""bio-inert"", ""bioactive"" and ""resorbable"" by varying the porosity of the silicon sample. The in-vitro study used simulated body fluid containing ion concentration similar to the human blood and tested the activities of porous silicon sample when exposed to the fluids for prolonged period of time. It was found that high porosity mesoporous layers were completely removed by the simulated body fluids within a day. In contrast, low to medium porosity microporous layers displayed more stable configurations and induced hydroxyapatite growth.


=== Bioactive ===
The first sign of porous silicon as a bioactive material was found in 1995. In the conducted study, it was found that hydroxyapatite growth was occurring on porous silicon areas. It was then suggested that ""hydrated microporous Si could be a bioactive form of the semiconductor and suggest that Si itself should be seriously considered for development as a material for widespread in vivo applications."" Another paper published the finding that porous silicon may be used a substrate for hydroxyapatite growth either by simple soaking process or laser-liquid-solid interaction process.Since then, in-vitro studies have been conducted to evaluate the interaction of cells with porous silicon. A 1995 study of the interaction of B50 rat hippocampal cells with porous silicon found that B50 cells have clear preference for adhesion to porous silicon over untreated surface. The study indicated that porous silicon can be suitable for cell culturing purposes and can be used to control cell growth pattern.


=== Non-toxic waste product ===
Another positive attribute of porous silicon is the degradation of porous silicon into monomeric silicic acid (SiOH4). Silicic acid is reputed to be the most natural form of element in the environment and is readily removed by kidneys.
The human blood plasma contains monomeric silicic acid at levels of less than 1 mg Si/l, corresponding to the average dietary intake of 20–50 mg/day. It was proposed that the small thickness of silicon coatings presents minimal risk to a toxic concentration being reached. The proposal was supported by an experiment involving volunteers and silicic-acid drinks. It was found that concentration of the acid rose only briefly above the normal 1 mg Si/l level and was efficiently expelled by urine excretion.


=== Superhydrophobicity ===
The simple adjustment of pore morphology and geometry of porous silicon also offers a convenient way to control its wetting behavior. Stable ultra- and superhydrophobic states on porous silicon can be fabricated and used in lab-on-a-chip, microfluidic devices for the improved surface-based bioanalysis.


=== Optical properties ===
pSi demonstrates optical properties based on porosity and the medium inside the pores. The effective refractive index of pSi is determined by the porosity and refractive index of the medium inside the pores. If the refractive index of the medium inside pores is high, the effective refractive index of pSi will be high as well. This phenomenon causes the spectrum to shift towards longer wavelength.


== See also ==
Nanocrystalline silicon
Silicon
Porosity
Quantum wire
Etching (microfabrication)


== References ==


== Further reading ==
Feng Z.C.; Tsu R., eds. (1994). Porous Silicon. Singapore: World Scientific. ISBN 978-981-02-1634-4.
Kovalev D.; Timoshenko V. Y.; Künzner N.; Gross E.; Koch F. (August 2001). ""Strong explosive interaction of hydrogenated porous silicon with oxygen at cryogenic temperatures"". Phys. Rev. Lett. 87 (6): 068301. Bibcode:2001PhRvL..87f8301K. doi:10.1103/PhysRevLett.87.068301. PMID 11497868.","pandas(index=95, _1=95, text='porous silicon (abbreviated as ""ps"" or ""psi"") is a form of the chemical element silicon that has introduced nanopores in its microstructure, rendering a large surface to volume ratio in the order of 500 m2/cm3.   == history == porous silicon was discovered by accident in 1956 by arthur uhlir jr. and ingeborg uhlir at the bell labs in the u.s. at the time, the ulhirs were in the process of developing a technique for polishing and shaping the surfaces of silicon and germanium. however, it was found that under several conditions a crude product in the form of thick black, red or brown film were formed on the surface of the material. at the time, the findings were not taken further and were only mentioned in bell lab\'s technical notes.despite the discovery of porous silicon in the 1950s, the scientific community was not interested in porous silicon until the late 1980s. at the time, leigh canham – while working at the defence research agency in england – reasoned that the porous silicon may display quantum confinement effects. the intuition was followed by successful experimental results published in 1990. in the published experiment, it was revealed that silicon wafers can emit light if subjected to electrochemical and chemical dissolution. the published result stimulated the interest of the scientific community in its non-linear optical and electrical properties. the growing interest was evidenced in the number of published work concerning the properties and potential applications of porous silicon. in an article published in 2000, it was found that the number of published work grew exponentially in between 1991 and 1995.in 2001, a team of scientists at the technical university of munich inadvertently discovered that hydrogenated porous silicon reacts explosively with oxygen at cryogenic temperatures, releasing several times as much energy as an equivalent amount of tnt, at a much greater speed. (an abstract of the study can be found below.) explosion occurs because the oxygen, which is in a liquid state at the necessary temperatures, is able to oxidize through the porous molecular structure of the silicon extremely rapidly, causing a very quick and efficient detonation. although hydrogenated porous silicon would probably not be effective as a weapon, due to its functioning only at low temperatures, other uses are being explored for its explosive properties, such as providing thrust for satellites.   == fabrication of porous silicon == anodization and stain-etching are the two most common methods used for fabrication of porous silicon; however, there are almost twenty other methods to fabricate this material.  drying and surface modification might be needed afterwards.  if anodization in an aqueous solution is used to form microporous silicon, the material is commonly treated in ethanol immediately after fabrication, to avoid damage to the structure that results due to the stresses of the capillary effect of the aqueous solution. psi demonstrates optical properties based on porosity and the medium inside the pores. the effective refractive index of psi is determined by the porosity and refractive index of the medium inside the pores. if the refractive index of the medium inside pores is high, the effective refractive index of psi will be high as well. this phenomenon causes the spectrum to shift towards longer wavelength.   == see also == nanocrystalline silicon silicon porosity quantum wire etching (microfabrication)   == references ==   == further reading == feng z.c.; tsu r., eds. (1994). porous silicon. singapore: world scientific. isbn 978-981-02-1634-4. kovalev d.; timoshenko v. y.; künzner n.; gross e.; koch f. (august 2001). ""strong explosive interaction of hydrogenated porous silicon with oxygen at cryogenic temperatures"". phys. rev. lett. 87 (6): 068301. bibcode:2001phrvl..87f8301k. doi:10.1103/physrevlett.87.068301. pmid 11497868.')"
96,"Retinal prostheses for restoration of sight to patients blinded by retinal degeneration are being developed by a number of private companies and research institutions worldwide.  The system is meant to partially restore useful vision to people who have lost their photoreceptors due to retinal diseases such as retinitis pigmentosa (RP) or age-related macular degeneration (AMD).  Three types of retinal implants are currently in clinical trials: epiretinal (on the retina), subretinal (behind the retina), and suprachoroidal (between the choroid and the sclera).  Retinal implants introduce visual information into the retina by electrically stimulating the surviving retinal neurons.  So far, elicited percepts had rather low resolution, and may be suitable for light perception and recognition of simple objects.


== History ==
Foerster was the first to discover that electrical stimulation of the occipital cortex could be used to create visual percepts, phosphenes.  The first application of an implantable stimulator for vision restoration was developed by Drs.  Brindley and Lewin in 1968.    This experiment demonstrated the viability of creating visual percepts using direct electrical stimulation, and it motivated the development of several other implantable devices for stimulation of the visual pathway, including retinal implants.  Retinal stimulation devices, in particular, have become a focus of research as approximately half of all cases of blindness are caused by retinal damage.  The development of retinal implants has also been motivated in part by the advancement and success of cochlear implants, which has demonstrated that humans can regain significant sensory function with limited input.The Argus II retinal implant, manufactured by Second Sight Medical Products received market approval in the US in Feb 2013 and in Europe in Feb 2011, becoming the first approved implant. The device may help adults with RP who have lost the ability to perceive shapes and movement to be more mobile and to perform day-to-day activities. The epiretinal device is known as the Retina Implant and was originally developed in Germany by Retina Implant AG. It completed a multi-centre clinical trial in Europe and was awarded a CE Mark in 2013, making it the first wireless epiretinal electronic device to gain approval.


== Candidates ==
Optimal candidates for retinal implants have retinal diseases, such as retinitis pigmentosa or age-related macular degeneration.  These diseases cause blindness by affecting the photoreceptor cells in the outer layer of the retina, while leaving the inner and middle retinal layers intact.  Minimally, a patient must have an intact ganglion cell layer in order to be a candidate for a retinal implant. This can be assessed non-invasively using optical coherence tomography (OCT) imaging.  Other factors, including the amount of residual vision, overall health, and family commitment to rehabilitation, are also considered when determining candidates for retinal implants.  In subjects with age-related macular degeneration, who may have intact peripheral vision, retinal implants could result in a hybrid form of vision.  In this case the implant would supplement the remaining peripheral vision with central vision information.


== Types ==
There are two main types of retinal implants by placement. Epiretinal implants are placed in the internal surface of the retina, while subretinal implants are placed between the outer retinal layer and the retinal pigment epithelium.


=== Epiretinal implants ===


==== Design principles ====
Epiretinal implants are placed on top of the retinal surface, above the nerve fiber layer, directly stimulating ganglion cells and bypassing all other retinal layers. Array of electrodes is stabilized on the retina using micro tacks which penetrate into the sclera.  Typically, external video camera onto eyeglasses acquires images and transmits processed video information to the stimulating electrodes via wireless telemetry.  An external transmitter is also required to provide power to the implant via radio-frequency induction coils or infrared lasers.  The real-time image processing involves reducing the resolution, enhancing contrast, detecting the edges in the image and converting it into a spatio-temporal pattern of stimulation delivered to the electrode array on the retina. The majority of electronics can be incorporated into the associated external components, allowing for a smaller implant and simpler upgrades without additional surgery.  The external electronics provides full control over the image processing for each patient.


==== Advantages ====
Epiretinal implants directly stimulate the retinal ganglion cells, thereby bypassing all other retinal layers. Therefore, in principle, epiretinal implants could provide visual perception to individuals even if all other retinal layers have been damaged.


==== Disadvantages ====
Since the nerve fiber layer has similar stimulation threshold to that of the retinal ganglion cells, axons passing under the epiretinal electrodes are stimulated, creating arcuate percepts, and thereby distorting the retinotopic map. So far, none of the epiretinal implants had light-sensitive pixels, and hence they rely on external camera for capturing the visual information. Therefore, unlike natural vision, eye movements do not shift the transmitted image on the retina, which creates a perception of the moving object  when person with such an implant changes the direction of gaze. Therefore, patients with such implants are asked to not move their eyes, but rather scan the visual field with their head. Additionally, encoding visual information at the ganglion cell layer requires very sophisticated image processing techniques in order to account for various types of the retinal ganglion cells encoding different features of the image.


==== Clinical study ====
The first epiretinal implant, the ARGUS device, included a silicon platinum array with 16 electrodes.  The Phase I clinical trial of ARGUS began in 2002 by implanting six participants with the device.  All patients reported gaining a perception of light and discrete phosphenes, with the visual function of some patients improving significantly over time.  Future versions of the ARGUS device are being developed with increasingly dense electrode arrays, allowing for improved spatial resolution.  The most recent ARGUS II device contains 60 electrodes, and a 200 electrode device is under development by ophthalmologists and engineers at the USC Eye Institute.  The ARGUS II device received marketing approval in February 2011 (CE Mark demonstrating safety and performance), and it is available in Germany, France, Italy, and UK. Interim results on 30 patients long term trials were published in Ophthalmology in 2012. Argus II received approval from the US FDA on April 14, 2013 FDA Approval.
Another epiretinal device, the Learning Retinal Implant, has been developed by IIP technologies GmbH, and has begun to be evaluated in clinical trials.  A third epiretinal device, EPI-RET, has been developed and progressed to clinical testing in six patients. The EPI-RET device contains 25 electrodes and requires the crystalline lens to be replaced with a receiver chip.  All subjects have demonstrated the ability to discriminate between different spatial and temporal patterns of stimulation.


=== Subretinal implants ===


==== Design principles ====
Subretinal implants sit on the outer surface of the retina, between the photoreceptor layer and the retinal pigment epithelium, directly stimulating retinal cells and relying on the normal processing of the inner and middle retinal layers.   Adhering a subretinal implant in place is relatively simple, as the implant is mechanically constrained by the minimal distance between the outer retina and the retinal pigment epithelium.  A subretinal implant consists of a silicon wafer containing light sensitive microphotodiodes, which generate signals directly from the incoming light.  Incident light passing through the retina generates currents within the microphotodiodes, which directly inject the resultant current into the underlying retinal cells via arrays of microelectrodes.  The pattern of microphotodiodes activated by incident light therefore stimulates a pattern of bipolar, horizontal, amacrine, and ganglion cells, leading to a visual perception representative of the original incident image.  In principle, subretinal implants do not require any external hardware beyond the implanted microphotodiodes array.  However, some subretinal implants require power from external circuitry to enhance the image signal.


==== Advantages ====
A subretinal implant is advantageous over an epiretinal implant in part because of its simpler design.  The light acquisition, processing, and stimulation are all carried out by microphotodiodes mounted onto a single chip, as opposed to the external camera, processing chip, and implanted electrode array associated with an epiretinal implant.  The subretinal placement is also more straightforward, as it places the stimulating array directly adjacent to the damaged photoreceptors.  By relying on the function of the remaining retinal layers, subretinal implants allow for normal inner retinal processing, including amplification, thus resulting in an overall lower threshold for a visual response.  Additionally, subretinal implants enable subjects to use normal eye movements to shift their gaze.  The retinotopic stimulation from subretinal implants is inherently more accurate, as the pattern of incident light on the microphotodiodes is a direct reflection of the desired image.  Subretinal implants require minimal fixation, as the subretinal space is mechanically constrained and the retinal pigment epithelium creates negative pressure within the subretinal space.


==== Disadvantages ====
The main disadvantage of subretinal implants is the lack of sufficient incident light to enable the microphotodiodes to generate adequate current.  Thus, subretinal implants often incorporate an external power source to amplify the effect of incident light.  The compact nature of the subretinal space imposes significant size constraints on the implant.  The close proximity between the implant and the retina also increases the possibility of thermal damage to the retina from heat generated by the implant.  Subretinal implants require intact inner and middle retinal layers, and therefore are not beneficial for retinal diseases extending beyond the outer photoreceptor layer.  Additionally, photoreceptor loss can result in the formation of a membrane at the boundary of the damaged photoreceptors, which can impede stimulation and increase the stimulation threshold.


==== Clinical studies ====
Optobionics was the first company to develop a subretinal implant and evaluate the design in a clinical trial.  Initial reports indicated that the implantation procedure was safe, and all subjects reported some perception of light and mild improvement in visual function.  The current version of this device has been implanted in 10 patients, who have each reported improvements in the perception of visual details, including contrast, shape, and movement.  Retina Implant AG in Germany has also developed a subretinal implant, which has undergone clinical testing in nine patients. Trial was put on hold due to repeated failures.  The Retina Implant AG device contains 1500 microphotodiodes, allowing for increased spatial resolution, but requires an external power source. Retina implant AG reported 12 months results on the Alpha IMS study in Feb 2013 showing that six out of nine patients had a device failure in the nine months post implant Proceedings of the royal society B, and that five of the eight subjects reported various implant-mediated visual perceptions in daily life. One had optic nerve damage and did not perceive stimulation.  The Boston Subretinal Implant Project has also developed several iterations of a functional subretinal implant, and focused on short term analysis of implant function.  Results from all clinical trials to date indicate that patients receiving subretinal implants report perception of phosphenes, with some gaining the ability to perform basic visual tasks, such as shape recognition and motion detection.


== Spatial resolution ==
The quality of vision expected from a retinal implant is largely based on the maximum spatial resolution of the implant.  Current prototypes of retinal implants are capable of providing low resolution, pixelated images.
""State-of-the-art"" retinal implants incorporate 60-100 channels, sufficient for basic object discrimination and recognition tasks.  However, simulations of the resultant pixelated images assume that all electrodes on the implant are in contact with the desired retinal cell; in reality the expected spatial resolution is lower, as a few of the electrodes may not function optimally.  Tests of reading performance indicated that a 60-channel implant is sufficient to restore some reading ability, but only with significantly enlarged text.  Similar experiments evaluating room navigation ability with pixelated images demonstrated that 60 channels were sufficient for experienced subjects, while naïve subjects required 256 channels.  This experiment, therefore, not only demonstrated the functionality provided by low resolution visual feedback, but also the ability for subjects to adapt and improve over time.  However, these experiments are based merely on simulations of low resolution vision in normal subjects, rather than clinical testing of implanted subjects.  The number of electrodes necessary for reading or room navigation may differ in implanted subjects, and further testing needs to be conducted within this clinical population to determine the required spatial resolution for specific visual tasks.
Simulation results indicate that 600-1000 electrodes would be required to enable subjects to perform a wide variety of tasks, including reading, face recognition, and navigating around rooms. Thus, the available spatial resolution of retinal implants needs to increase by a factor of 10, while remaining small enough to implant, to restore sufficient visual function for those tasks. It is worth to note high-density stimulation is not equal to high visual acuity (resolution), which requires a lot of factors in both hardware (electrodes and coatings) and software (stimulation strategies based on surgical results).


== Current status and future developments ==
Clinical reports to date have demonstrated mixed success, with all patients report at least some sensation of light from the electrodes, and a smaller proportion gaining more detailed visual function, such as identifying patterns of light and dark areas.  The clinical reports indicate that, even with low resolution, retinal implants are potentially useful in providing crude vision to individuals who otherwise would not have any visual sensation.  However, clinical testing in implanted subjects is somewhat limited and the majority of spatial resolution simulation experiments have been conducted in normal controls.  It remains unclear whether the low level vision provided by current retinal implants is sufficient to balance the risks associated with the surgical procedure, especially for subjects with intact peripheral vision.  Several other aspects of retinal implants need to be addressed in future research, including the long term stability of the implants and the possibility of retinal neuron plasticity in response to prolonged stimulation.The Manchester Royal Infirmary and Prof Paulo E Stanga announced on July 22, 2015 the first successful implantation of Second Sight's Argus II in patients suffering from severe Age Related Macular Degeneration. These results are very impressive as it appears that the patients integrate the residual vision and the artificial vision. It potentially opens the use of retinal implants to millions of patients suffering from AMD.


== See also ==
Retinal regeneration


== References ==


== External links ==
Japan Retinal Implant Project
- The Retinal Implant Project - rle.mit.edu
National Eye Institute of the National Institutes of Heath (NIH)","pandas(index=96, _1=96, text='retinal prostheses for restoration of sight to patients blinded by retinal degeneration are being developed by a number of private companies and research institutions worldwide.  the system is meant to partially restore useful vision to people who have lost their photoreceptors due to retinal diseases such as retinitis pigmentosa (rp) or age-related macular degeneration (amd).  three types of retinal implants are currently in clinical trials: epiretinal (on the retina), subretinal (behind the retina), and suprachoroidal (between the choroid and the sclera).  retinal implants introduce visual information into the retina by electrically stimulating the surviving retinal neurons.  so far, elicited percepts had rather low resolution, and may be suitable for light perception and recognition of simple objects.   == history == foerster was the first to discover that electrical stimulation of the occipital cortex could be used to create visual percepts, phosphenes.  the first application of an implantable stimulator for vision restoration was developed by drs.  brindley and lewin in 1968.    this experiment demonstrated the viability of creating visual percepts using direct electrical stimulation, and it motivated the development of several other implantable devices for stimulation of the visual pathway, including retinal implants.  retinal stimulation devices, in particular, have become a focus of research as approximately half of all cases of blindness are caused by retinal damage.  the development of retinal implants has also been motivated in part by the advancement and success of cochlear implants, which has demonstrated that humans can regain significant sensory function with limited input.the argus ii retinal implant, manufactured by second sight medical products received market approval in the us in feb 2013 and in europe in feb 2011, becoming the first approved implant. the device may help adults with rp who have lost the ability to perceive shapes and movement to be more mobile and to perform day-to-day activities. the epiretinal device is known as the retina implant and was originally developed in germany by retina implant ag. it completed a multi-centre clinical trial in europe and was awarded a ce mark in 2013, making it the first wireless epiretinal electronic device to gain approval.   == candidates == optimal candidates for retinal implants have retinal diseases, such as retinitis pigmentosa or age-related macular degeneration.  these diseases cause blindness by affecting the photoreceptor cells in the outer layer of the retina, while leaving the inner and middle retinal layers intact.  minimally, a patient must have an intact ganglion cell layer in order to be a candidate for a retinal implant. this can be assessed non-invasively using optical coherence tomography (oct) imaging.  other factors, including the amount of residual vision, overall health, and family commitment to rehabilitation, are also considered when determining candidates for retinal implants.  in subjects with age-related macular degeneration, who may have intact peripheral vision, retinal implants could result in a hybrid form of vision.  in this case the implant would supplement the remaining peripheral vision with central vision information.   == types == there are two main types of retinal implants by placement. epiretinal implants are placed in the internal surface of the retina, while subretinal implants are placed between the outer retinal layer and the retinal pigment epithelium. optobionics was the first company to develop a subretinal implant and evaluate the design in a clinical trial.  initial reports indicated that the implantation procedure was safe, and all subjects reported some perception of light and mild improvement in visual function.  the current version of this device has been implanted in 10 patients, who have each reported improvements in the perception of visual details, including contrast, shape, and movement.  retina implant ag in germany has also developed a subretinal implant, which has undergone clinical testing in nine patients. trial was put on hold due to repeated failures.  the retina implant ag device contains 1500 microphotodiodes, allowing for increased spatial resolution, but requires an external power source. retina implant ag reported 12 months results on the alpha ims study in feb 2013 showing that six out of nine patients had a device failure in the nine months post implant proceedings of the royal society b, and that five of the eight subjects reported various implant-mediated visual perceptions in daily life. one had optic nerve damage and did not perceive stimulation.  the boston subretinal implant project has also developed several iterations of a functional subretinal implant, and focused on short term analysis of implant function.  results from all clinical trials to date indicate that patients receiving subretinal implants report perception of phosphenes, with some gaining the ability to perform basic visual tasks, such as shape recognition and motion detection.   == spatial resolution == the quality of vision expected from a retinal implant is largely based on the maximum spatial resolution of the implant.  current prototypes of retinal implants are capable of providing low resolution, pixelated images. ""state-of-the-art"" retinal implants incorporate 60-100 channels, sufficient for basic object discrimination and recognition tasks.  however, simulations of the resultant pixelated images assume that all electrodes on the implant are in contact with the desired retinal cell; in reality the expected spatial resolution is lower, as a few of the electrodes may not function optimally.  tests of reading performance indicated that a 60-channel implant is sufficient to restore some reading ability, but only with significantly enlarged text.  similar experiments evaluating room navigation ability with pixelated images demonstrated that 60 channels were sufficient for experienced subjects, while naïve subjects required 256 channels.  this experiment, therefore, not only demonstrated the functionality provided by low resolution visual feedback, but also the ability for subjects to adapt and improve over time.  however, these experiments are based merely on simulations of low resolution vision in normal subjects, rather than clinical testing of implanted subjects.  the number of electrodes necessary for reading or room navigation may differ in implanted subjects, and further testing needs to be conducted within this clinical population to determine the required spatial resolution for specific visual tasks. simulation results indicate that 600-1000 electrodes would be required to enable subjects to perform a wide variety of tasks, including reading, face recognition, and navigating around rooms. thus, the available spatial resolution of retinal implants needs to increase by a factor of 10, while remaining small enough to implant, to restore sufficient visual function for those tasks. it is worth to note high-density stimulation is not equal to high visual acuity (resolution), which requires a lot of factors in both hardware (electrodes and coatings) and software (stimulation strategies based on surgical results).   == current status and future developments == clinical reports to date have demonstrated mixed success, with all patients report at least some sensation of light from the electrodes, and a smaller proportion gaining more detailed visual function, such as identifying patterns of light and dark areas.  the clinical reports indicate that, even with low resolution, retinal implants are potentially useful in providing crude vision to individuals who otherwise would not have any visual sensation.  however, clinical testing in implanted subjects is somewhat limited and the majority of spatial resolution simulation experiments have been conducted in normal controls.  it remains unclear whether the low level vision provided by current retinal implants is sufficient to balance the risks associated with the surgical procedure, especially for subjects with intact peripheral vision.  several other aspects of retinal implants need to be addressed in future research, including the long term stability of the implants and the possibility of retinal neuron plasticity in response to prolonged stimulation.the manchester royal infirmary and prof paulo e stanga announced on july 22, 2015 the first successful implantation of second sight\'s argus ii in patients suffering from severe age related macular degeneration. these results are very impressive as it appears that the patients integrate the residual vision and the artificial vision. it potentially opens the use of retinal implants to millions of patients suffering from amd.   == see also == retinal regeneration   == references ==   == external links == japan retinal implant project - the retinal implant project - rle.mit.edu national eye institute of the national institutes of heath (nih)')"
97,"Health technology is defined by the World Health Organization as the ""application of organized knowledge and skills in the form of devices, medicines, vaccines, procedures, and systems developed to solve a health problem and improve quality of lives"". This includes pharmaceuticals, devices, procedures, and organizational systems used in the healthcare industry, as well as computer-supported information systems. In the United States, these technologies involve standardized physical objects, as well as traditional and designed social means and methods to treat or care for patients.


== Development ==


=== Pre-digital Era ===
During a pre-digital era, patients suffered from inefficient and faulty clinical systems, processes, and conditions. Many medical errors happened in the past due to undeveloped health technologies. Some examples of these medical errors included adverse drug events and alarm fatigue. Alarm fatigue is caused when an alarm is repeatedly triggered or activated and one becomes desensitized to them. As the alarms were sometimes triggered by unimportant events in the past, nurses thought the alarm was not significant. Alarm fatigue is dangerous because it could lead to death and dangerous situations. With technological development, an intelligent program of integration and physiologic sense-making was developed and helped reduce the number of false alarms.Also, with greater investment in health technologies, fewer medical errors happened. Outdated paper records were replaced in many healthcare organizations by electronic health records (EHR). According to studies, this change has brought a lot of changes to healthcare. Drug administration has improved, healthcare providers can now access medical information easier, provide better treatments and faster results, and save more costs.


=== Improvement ===
To help promote and expand the adoption of health information technology, Congress passed the HITECH act as part of the American Recovery and Reinvestment Act of 2009. HITECH stands for Health Information Technology for Economic and Clinical Health Act. It gave the department of health and human services the authority to improve healthcare quality and efficiency through the promotion of health IT. The act provided financial incentives or penalties to organizations to motivate healthcare providers to improve healthcare. The purpose of the act was to improve quality, safety, efficiency, and ultimately to reduce health disparities.One of the main parts of the HITECH act was setting the meaningful use requirement, which required EHRs to allow for the electronic exchange of health information and to submit clinical information. The purpose of HITECH is to ensure the sharing of electronic information with patients and other clinicians are secure. HITECH also aimed to help healthcare providers have more efficient operations and reduce medical errors. The program consisted of three phases. Phase one aimed to improve healthcare quality, safety and efficiency. Phase two expanded on phase one and focused on clinical processes and ensuring the meaningful use of EHRs. Lastly, phase three focused on using Certified Electronic Health Record Technology (CEHRT) to improve health outcomes.In 2014, the implementation of electronic records in US hospitals rose from a low percentage of 10% to a high percentage of 70%.At the beginning of 2018, healthcare providers who participated in the Medicare Promoting Interoperability Program needed to report on Quality Payment Program requirements. The program focused more on interoperability and aimed to improve patient access to health information.


=== Privacy of Health Data ===
Phones that can track one's whereabouts, steps and more can serve as medical devices, and medical devices have much the same effect as these phones. In the research article, Privacy Attitudes among Early Adopters of Emerging Health Technologies by Cynthia Cheung, Matthew Bietz, Kevin Patrick and Cinnamon Bloss discovered people were willing to share personal data for scientific advancements, although they still expressed uncertainty about who would have access to their data. People are naturally cautious about giving out sensitive personal information. Phones add an extra level of threat according to the research article Security in Cloud-Computing-Based Mobile Health. Mobile devices continue to increase in popularity each year. The addition of mobile devices serving as medical devices increases the chances for an attacker to gain unauthorized information.In 2015 the Medical Access and CHIP Reauthorization Act (MACRA) was passed which will be put into play in 2018 pushing towards electronic health records. Health Information Technology: Integration, Patient Empowerment, and Security by K. Marvin provided multiple different polls based on people's views on different types of technology entering the medical field most answers were responded with somewhat likely and very few completely disagreed on the technology being used in medicine. Marvin discusses the maintenance required to protect medical data and technology against cyber attacks as well as providing a proper data backup system for the information.Patient Protection and Affordable Care Act (ACA) also known as Obamacare and health information technology health care is entering the digital era. Although with this development it needs to be protected. Both health information and financial information now made digital within the health industry might become a larger target for cyber-crime. Even with multiple different types of safeguards hackers somehow still find their way in so the security that is in place needs to constantly be updated to prevent these breaches.


==== Policy ====
With the increased use of IT systems, privacy violations were increasing rapidly due to the easier access and poor management. As such, the concern of privacy has become an important topic in healthcare. Privacy breaches happen when organizations do not protect the privacy of people's data. There are four types of privacy breaches, which include unintended disclosure by authorized personnel, intended disclosure by authorized personnel, privacy data loss or theft, and virtual hacking. It became more important to protect the privacy and security of patients’ data because of the high negative impact on both individuals and organizations. Stolen personal information can be used to open credit cards or other unethical behaviors. Also, individuals have to spend a large amount of money to rectify the issue. The exposure of sensitive health information also can cause negative impacts on individuals’ relationships, jobs, or other personal areas. For the organization, the privacy breach can cause loss of trust, customers, legal actions, and monetary fines.

HIPAA stands for the Health Insurance Portability and Accountability Act of 1996. It is a U.S. healthcare legislation to direct how patient data is used and includes two major rules which are privacy and security of data. The privacy rule protects people's rights to privacy and security rule determines how to protect people's privacy.According to the HIPAA Security Rule, it ensures that protected health information has three characteristics. They are confidentiality, availability, and integrity. Confidentiality indicates keeping the data confidential to prevent data loss or individuals who are unauthorized to access that protected health information. Availability allows people who are authorized to access the systems and networks when and where that information is in fact needed, such as natural disasters. In cases like this, protected health information is mostly backed up on to a separate server or printed out in paper copies, so people can access it. Lastly, Integrity ensures not using inaccurate information and improperly modified data due to a bad design system or process to protect the permanence of the patient data. The consequences of using inaccurate or improperly modified data could become useless or even dangerous.Health Organizations of HIPAA also created administrative safeguards, physical safeguards, technical safeguards, to help protect the privacy of patients. Administrative safeguards typically include security management process, security personnel, information access management, workforce training and management, and evaluation of security policies and procedures. Security management processes are one of the important administrative safeguards’ examples. It is essential to reduce the risks and vulnerabilities of the system. The processes are mostly the standard operating procedures written out as training manuals. The purpose is to educate people on how to handle protected health information in proper behavior.Physical safeguards include lock and key, card swipe, positioning of screens, confidential envelopes, and shredding of paper copies. Lock and key are common examples of physical safeguards. They can limit physical access to facilities. Lock and key are simple, but they can prevent individuals from stealing medical records. Individuals must have an actual key to access to the lock.Lastly, technical safeguards include access control, audit controls, integrity controls, and transmission security. The access control mechanism is a common example of technical safeguards. It allows the access of authorized personnel. The technology includes authentication and authorization. Authentication is the proof of identity that handles confidential information like username and password, while authorization is the act of determining whether a particular user is allowed to access certain data and perform activities in a system like add and delete.


=== Assessment ===
The concept of health technology assessment (HTA) was first coined in 1967 by the U.S. Congress in response to the increasing need to address the unintended and potential consequences of health technology, along with its prominent role in society. It was further institutionalized with the establishment of the congressional Office of Technology Assessment (OTA) in 1972–1973. HTA is defined as a comprehensive form of policy research that examines short- and long-term consequences of the application of technology, including benefits, costs, and risks. Due to the broad scope of technology assessment, it requires the participation of individuals besides scientists and health care practitioners such as managers and even the consumers.Several American organizations provide health technology assessments and these include the Centers for Medicare and Medicaid Services (CMS) and the Veterans Administration through its VA Technology Assessment Program (VATAP). The models adopted by these institutions vary, although they focus on whether a medical technology being offered is therapeutically relevant. A study conducted in 2007 noted that the assessments still did not use formal economic analyses.Aside from its development, however, assessment in the health technology industry has been viewed as sporadic and fragmented Issues such as the determination of products that needed to be developed, cost, and access, among others, also emerged. These - some argue - need to be included in the assessment since health technology is never purely a matter of science but also of beliefs, values, and ideologies. One of the mechanisms being suggested – either as an element of- or an alternative to the current TAs is bioethics, which is also referred to as the ""fourth-generation"" evaluation framework. There are at least two dimensions to an ethical HTA. The first involves the incorporation of ethics in the methodological standards employed to assess technologies while the second is concerned with the use of ethical framework in research and judgment on the part of the researchers who produce information used in the industry.


=== Health technology in the future ===

The practice of medicine in the United States is currently in a major transition. This transition is due to many factors, but primarily because of the implementation and integration of health technologies into healthcare. In recent years, the widespread adoption of electronic health records (EHR) has caused a big impact on healthcare. ""The Digital Doctor: Hope, Hype, and Harm at the Dawn of Medicine's Computer Age,"" by Robert Wachter, aims to inform readers about this transition. Dr. Wachter has reviewed and made points about the future of health technologies in the book. He states that there will be fewer hospitals in the future. Due to the advancement of technologies, people will be more likely to go to hospitals for major surgeries or critical illness. In the future, nurse call buttons will not be needed in hospitals. Instead, robots will deliver medication, take care of patients, and administer the system. In the future, the electronic health record will look different. Healthcare providers will be able to enter the notes via speech-to-text transcriptions in real-time.Dr. Wachter stated that information will be edited collaboratively across the patient-care team to improve the quality. Also, natural language processing will be more developed to help parse out keywords. In the future, patient data will reside in the cloud. Patients will be able to access their data from any device or location. The data is also accessible for authorized providers and individuals. In the future, big data analysis will constantly be improving. Artificial Intelligence and machine learning will be constantly improving and developing as it receives new data. Alerts will also be more intelligent and efficient than the current systems.


== Medical technology ==
Medical technology, or ""Medtech"", encompasses a wide range of healthcare products and is used to treat diseases and medical conditions affecting humans. Such technologies are intended to improve the quality of healthcare delivered through earlier diagnosis, less invasive treatment options and reduction in hospital stays and rehabilitation times. Recent advances in medical technology have also focused on cost reduction. Medical technology may broadly include medical devices, information technology, biotech, and healthcare services.
The impacts of medical technology involve social and ethical issues. For example, physicians can seek objective information from technology rather than read subjective patient reports.A major driver of the sector's growth is the consumerization of Medtech. Supported by the widespread availability of smartphones and tablets, providers can reach a large audience at low cost, a trend that stands to be consolidated as wearable technologies spread throughout the market.In the years 2010–2015, venture funding has grown 200%, allowing US$11.7 billion to flow into health tech businesses from over 30,000 investors in the space.


=== Types of Technology ===
Medical technology has evolved into smaller portable devices, for instance, smartphones, touchscreens, tablets, laptops, digital ink, voice and face recognition and more. With this technology, innovations like electronic health records (EHR), health information exchange (HIE), Nationwide Health Information Network (NwHIN), personal health records (PHRs), patient portals, nanomedicine, genome-based personalized medicine, Geographical Positioning System (GPS), radio frequency identification (RFID), telemedicine, clinical decision support (CDS), mobile home health care and cloud computing came to exist.Medical imaging and Magnetic resonance imaging (MRI) have been long used and proven Medical Technologies for medical research, patient reviewing, and treatment analyzing. With the advancement of imagining technologies, including the use of faster and more data, higher resolution images, and specialist automation software, the capabilities of medical imaging technology are growing and yielding better results. As the imaging hardware and software evolve this means that patients will need to use less contrasting agents, and also spend less time and money.3D printing is another major development in healthcare. It can be used to produce specialized splints, prostheses, parts for medical devices and inert implants. The end goal of 3D printing is being able to print out customized replaceable body parts. In the following section, it will explain more about 3D printing in healthcare. New types of technologies also include artificial intelligence and robots.


==== 3D printing ====

3D printing is the use of specialized machines, software programs and materials to automate the process of building certain objects. It is having a rapid growth in the prosthesis, medical implants, novel drug formulations and the bioprinting of human tissues and organs.Companies such as Surgical Theater, provide new technology that is capable of capturing 3D virtual images of patients' brains to use as practice for operations. 3D printing allows medical companies to produce prototypes to practice before an operation created with artificial tissue.3D printing technologies are great for bio-medicine because the materials that are used to make allow the fabrication with control over many design features. 3D printing also has the benefits of affordable customization, more efficient designs, and saving more time. 3D printing is precise to design pills to house several drugs due to different release times. The technology allows the pills to transport to the targeted area and degrade safely in the body. As such, pills can be designed more efficiently and conveniently. In the future, doctors might be giving a digital file of printing instructions instead of a prescription.Besides, 3D printing will be more useful in medical implants. An example includes a surgical team that has designed a tracheal splint made by 3D printing to improve the respiration of a patient. This example shows the potential of 3D printing, which allows physicians to develop new implant and instrument designs easily.Overall, in the future of medicine, 3D printing will be crucial as it can be used in surgical planning, artificial and prosthetic devices, drugs, and medical implants.


==== Artificial Intelligence ====

Artificial Intelligence (AI) is a program that enables computers to sense, reason, act and adapt. AI is not new, but it is growing rapidly and tremendously. AI can now deal with large data sets, solve problems, and provide more efficient operation. AI will be more potential in healthcare because it provides easier accessibility of information, improves healthcare, and reduce cost. There are different factors that drive AI in healthcare, but the two most important are economics and the advent of big data analytics. Costs, new payment options, and people's desire to improve health outcomes are the primary economic drivers of the AI. Based on the reading, AI can save $150 million annually in the US by 2026. Also, AI growth is expected to reach $6.6 million by 2021. Big data analytics is another big driver because we are in the age of big data. The data is extremely helpful to assist the integration of AI in healthcare because it ensures the execution of complex tasks, quality, and efficiency.


===== Applications of Artificial Intelligence =====
AI brings many benefits to the healthcare industry. AI helps to detect diseases, administer chronic conditions, deliver health services, and discover the drug. Also, AI has the potential to address important health challenges. In healthcare organizations, AI is able to plan and relocate resources. AI is able to match patients with healthcare providers that meet their needs. AI also helps improve the healthcare experience by using an app to identify patients' anxieties. In medical research, AI helps to analyze and evaluate the patterns and complex data. For instance, AI is important in drug discovery because it can search relevant studies and analyze different kinds of data. In clinical care, AI helps to detect diseases, analyze clinical data, publications, and guidelines. As such, AI aids to find the best treatments for the patients. Other uses of AI in clinical care include medical imaging, echocardiography, screening, and surgery.


===== Education =====
Medical virtual reality provides doctors multiple surgical scenarios that could happen and allows them to practice and prepare themselves for these situations. It also permits medical students a hands-on experience of different procedures without the consequences of making potential mistakes. ORamaVR is one of the leading companies that employ such medical virtual reality technologies to transform medical education (knowledge) and training (skills) to improve patient outcomes, reduce surgical errors and training time and democratize medical education and training.


==== Robots ====
Modern robotics have made huge progress and contribution to healthcare. Robots can help doctors in performing  variety tasks. Robotics adoption is increasing tremendously in hospitals . The following are different ways to improve healthcare by using robots:

Surgical robots are one of the robotic systems, which allows a surgeon to bend and rotate tissues in a  more flexible and efficient way. The system is equipped with a 3D magnification vision system that can translate the hand movements of the surgeon to be precise in-order to perform a surgery with minimal incisions. Other robotics systems include the ability to diagnose and treat cancers. Many scientists began working on creating a next-generation robot system to assist the surgeon in performing knee and hip replacement surgeries.Assistant robots will also be important to help reduce the workload for regular medical staff. They can help nurses with simple and time-consuming tasks like carrying multiple racks of medicines, lab specimen or other sensitive materials.Shortly, robotic pills are expected to reduce the number of surgeries. They can be moved inside a patient and delivered to the desired area. In addition, they can conduct biopsies, film the area and clear clogged arteries.
Overall, medical robots are extremely useful in assisting physicians; however, it might take time to be professionally trained working with medical robots and for the robots to respond to a clinician's instructions. As such, many researchers and startups were working constantly to provide solutions to these challenges.


=== Assistive Technologies ===
Assistive technologies are products designed to provide accessibility to individuals who have physical or cognitive problems or disabilities. They aim to improve the quality of life with assistive technologies. The range of assistive technologies is broad, ranging from low-tech solutions to physical hardware, to technical devices. There are four areas of assistive technologies, which include visual impairment, hearing impairment, physical limitations, cognitive limitations. There are many benefits of assistive technologies. They enable individuals to care for themselves, work, study, access information easily, improve independence and communication, and lastly participate fully in community life.


=== Consumer-driven healthcare software ===
As part of an ongoing trend towards consumer-driven healthcare, websites or apps which provide more information on health care quality and price to help patients choose their providers have grown. As of 2017, the sites with the most number of reviews in descending order included Healthgrades, Vitals.com, and RateMDs.com. Yelp, Google, and Facebook also host reviews with a large amount of traffic, although as of 2017 they had fewer medical reviews per doctor. Disputes around online reviews can lead to websites by health professionals alleging defamation.Patient safety organizations and government programs which have historically assessed quality have made their data more accessible over the internet; notable examples include the HospitalCompare by CMS and the LeapFrog Group's hospitalsafetygrade.org.Patient-oriented software may also help in other ways, including general education and appointments.Disclosure of legal disputes including medical license complaints or malpractice lawsuits has also been made easier. Every state discloses license status and at least some disciplinary action to the public, but as of 2018, this was not accessible via the internet for a few states. Consumers can look up medical licenses in a national database, DocInfo.org, maintained by the medical licensing organizations which contains limited details. Other tools include DocFinder at docfinder.docboard.org and certificationmatters.org from the American Board of Medical Specialties. In some cases more information is available from a mailed or walk-in request than the internet; for example, the Medical Board of California removes dismissed accusations from website profiles, but these are still available from a written or walk-in request, or a lookup in a separate database. The trend to disclosure is controversial and generate significant public debate, particularly about opening up the National Practitioner Data Bank. In 1996, Massachusetts became the first state to require detailed disclosure of malpractice claims.


=== Self-Monitoring ===
Smartphones, tablets, and wearable computers have allowed people to monitor their health. These devices run numerous applications that are designed to provide simple health services and the monitoring of one's health.  An example of this is Fitbit, a fitness tracker that is worn on the user's wrist. This wearable technology allows people to track their steps, heart rate, floors climbed, miles walked, active minutes, and even sleep patterns. The data collected and analyzed allow users not just to keep track of their health but also help manage it, particularly through its capability to identify health risk factors.There is also the case of the Internet, which serves as a repository of information and expert content that can be used to ""self-diagnose""  instead of going to their doctor. For instance, one need only enumerate symptoms as search parameters at Google and the search engine could identify the illness from the list of contents uploaded to the World Wide Web, particularly those provided by expert/medical sources. These advances may eventually have some effect on doctor visits from patients and change the role of the health professionals from ""gatekeeper to secondary care to facilitator of information interpretation and decision-making."" Apart from basic services provided by Google in Search, there are also companies such as WebMD that already offer dedicated symptom-checking apps.


=== Technology testing ===
All medical equipment introduced commercially must meet both United States and international regulations. The devices are tested on their material, effects on the human body, all components including devices that have other devices included with them, and the mechanical aspects.The Medical Device User Fee and Modernization Act of 2002 was created to speed up the FDA's approval process of medical technology by introducing sponsor user fees for a faster review time with predetermined performance targets for review time. In addition, 36 devices and apps were approved by the FDA in 2016.


== Careers ==
There are numerous careers to choose from in health technology in the USA. Listed below are some job titles and average salaries.

Athletic Trainer,Mean Salary: $41,340. Athletic trainers treat athletes and other individuals who have sustained injuries. They also teach people how to prevent injuries. They perform their job under the supervision of physicians.
Dental Hygienist,  Mean Salary: $67,340. Dental hygienists provide preventive dental care and teach patients how to maintain good oral health. They usually work under dentists' supervision.
Clinical Laboratory Scientists, Technicians, and Technologists, Mean Salary: $51,770. Lab technicians and technologists perform laboratory tests and procedures. Technicians work under the supervision of a laboratory technologist or laboratory manager.
Nuclear Medicine Technologist, Mean Salary: $67,910. Nuclear medicine technologists prepare and administer radiopharmaceuticals, radioactive drugs, to patients to treat or diagnose diseases.
Pharmacy Technician, Mean Salary: $28,070. Pharmacy technicians assist pharmacists with the preparation of prescription medications for customers.


=== Allied Professions ===
The term medical technology may also refer to the duties performed by clinical laboratory professionals or medical technologists in various settings within the public and private sectors. The work of these professionals encompasses clinical applications of chemistry, genetics, hematology, immunohematology (blood banking), immunology, microbiology, serology, urinalysis, and miscellaneous body fluid analysis. Depending on location, educational level, and certifying body, these professionals may be referred to as biomedical scientists, medical laboratory scientists (MLS), medical technologists (MT), medical laboratory technologists and medical laboratory technicians.


== References ==","pandas(index=97, _1=97, text='health technology is defined by the world health organization as the ""application of organized knowledge and skills in the form of devices, medicines, vaccines, procedures, and systems developed to solve a health problem and improve quality of lives"". this includes pharmaceuticals, devices, procedures, and organizational systems used in the healthcare industry, as well as computer-supported information systems. in the united states, these technologies involve standardized physical objects, as well as traditional and designed social means and methods to treat or care for patients.   == development == the term medical technology may also refer to the duties performed by clinical laboratory professionals or medical technologists in various settings within the public and private sectors. the work of these professionals encompasses clinical applications of chemistry, genetics, hematology, immunohematology (blood banking), immunology, microbiology, serology, urinalysis, and miscellaneous body fluid analysis. depending on location, educational level, and certifying body, these professionals may be referred to as biomedical scientists, medical laboratory scientists (mls), medical technologists (mt), medical laboratory technologists and medical laboratory technicians.   == references ==')"
98,"A needle remover is a device used to physically remove a needle from a syringe. In developing countries, there is still a need for improvements in needle safety in hospital settings as most of the needle removal processes are done manually and under severe risk of hazard from needles puncturing skin risking infection. These countries cannot afford needles with individual safety devices attached, so needle-removers must be used to remove the needle from the syringe. This lowers possible pathogen spread by preventing the reuse of the syringes, reducing incidents of accidental needle-sticks, and facilitating syringe disposal.


== Background ==
In regions surveyed by the World Health Organization (WHO), the reported number of needle-stick injuries in developing world countries ranged from .93 to 4.68 injuries per person and per year, which is five times higher than in industrialized nations.  Needle-stick injuries are further complicated by disease transmission, such as Hepatitis B, Hepatitis C and HIV. In Ghana, a study of 803 schoolchildren revealed that 61.2% had at least one marker of hepatitis B virus.  As a result, health care workers, patients, and the community in developing nations are at an increased risk of contracting blood-borne pathogens via the reuse and improper disposal of needles, and accidental needle-sticks.In the U.S., the Needlestick Safety Act signed in 2000 and the 2001 Bloodborne Pathogens Standard both mandated the use of safety devices and needle-removers with any sharps or needles.  As a result, there was a large increase in research, development, and marketing of needle safety devices and needle-remover.  In most hospital and medical settings in the U.S., needle safety regulations are maintained through individual needle safety devices and needle disposal boxes.


== Existing solutions ==
One of the most common causes of needle-stick injuries, which the Needlestick Act and Bloodborne Pathogens Standard were attempting to decrease, was two-handed recapping.  As a result, a one-handed capping mechanism was added to insulin and tuberculin syringes.  The cap is attached to the syringe via a hinge, which allows the cap to be snapped onto the needle using one hand. The disadvantage to the hinge system is that the cap can get caught by jewelry and clothing, can get bumped when used, and the fixed position can be a hindrance during low angle injection.  So Becton Dickinson (BD) has recently come out with a variation on this safety: instead of a hinge, the device slides over the needle and fully covers the tip of the needle, so accidental needle-sticks do not occur.However, the rest of the world does not have similar needle and syringe regulations.  For instance, the WHO is only able to regulate vaccinations in developing countries by ensuring that all vaccination syringes sent to these countries have autodisable features, since the major concern is the reuse of contaminated needles and syringes.  These autodisable features allow the syringes to only be used once, so they cannot be reused.  These mechanisms could be teeth that interlock to prevent the plunger from being pulled back for another use or a bag prefilled with the vaccine to stop reuse.  For example, the SoloShot has a metal clip that locks the plunger down after one use.  The BD Uniject is a prefilled vaccine syringe that uses a plastic bulb instead of a plunger and has a disc valve to prevent reuse.Still, over 90% of syringes worldwide do not have autodisable features.  Individual protection devices are expensive, and regular needles are much more prevalent.  Consequently, many developing world countries use needle-removers to reduce the risk of disease transmission via these exposed.


== Benefits of needle-removers ==
Needle-removers minimize the occurrence of accidental needle-sticks because they allow immediate removal and containment of the needles, especially if the device is near the area of use.  Reuse of syringes is prevented because the needle-remover physically separates the needle from the syringe, making the syringe useless.  They also improve waste disposal by decreasing both the amount of infectious waste and the amount of safety boxes needed for the waste, since safety boxes can pack syringes 20-60% more compactly without the needles.  Additionally, these devices are cost-efficient since one device can handle several hundred needles.  Many developing world countries do not have the resources to afford auto-disable syringes, so with needle-removers, the hospitals can continue to use cheap syringes, while only paying a one-time fee to buy a needle-remover that has a life-span of about 200-500 needles.


== Social and ethical implications ==
A significant ethical issue for the project is whether or not the needle-remover will cause more harm than its potential benefits.  Engineers are obliged to use their skills and knowledge to improve the safety, health, and welfare of the public.  The main concern is for the operator of the device; no engineer should create a device that could injure the operator.  Another concern is that children may gain access to the device and accidentally hurt themselves.  If a device design could potentially cause either of these problems, the team would be ethically obligated to reexamine that design, and it would either have to be improved or abandoned. When the device functions effectively and safely, it will serve to protect the welfare of the community.  In developing countries, the risk of disease transmission is elevated due to the high percentage of needle-stick injuries, which is a result of inadequate needle collection devices.  Increased pathogen transmission also occurs from the reuse of contaminated needles when supplies are low.  The device will prevent reuse of needles and facilitate needle collection and disposal, and thus will improve the health and safety of hospital workers and the community.
The social and economic effects of the device also need to be recognized.  In developing countries, the lack of proper needle collection devices leads to an increase in the number of occupational needle-sticks by health care workers via contaminated needles.  Occupational needle-sticks account for 40%-65% of Hepatitis B and C infections in health care workers.  As a result, more health care workers have to undergo post-exposure testing and treatment, both of which cost money for the hospitals and the countries.  There is also the manpower cost associated with losing trained health care workers to infections acquired on the job.  With fewer than 10 doctors for every 100,000 individuals in sub-Saharan nations, any loss of hospital staff puts a strain of hospital resources.  In addition, developing countries have made significant investments in training their health care workers, which is lost when occupational needle-sticks cause health care workers to leave the medical field.The economic considerations are not just limited to costs associated with health care workers.  Due to the high cost of needle-disposal containers and the fact that the containers usually have to be shipped overseas, unsafe and dangerous substitutes are used instead.  This practice can potentially lead to needle-sticks by health care workers and individuals in the community, as well as needle reuse by members of the community, which can increase the potential spread of diseases.


== Possible designs ==
The easiest needle-removers to operate are electrically powered, and either melt the needle or cut the needles at multiple sections.  One patented design involves a syringe falling down into a chamber where powered movable blades advance the syringe onto fixed blades on the opposite side, at which point the syringe is cut with a shearing motion at multiple points.  There are other patents that use electricity between electrodes or between rotating gears to short-circuit the needle and melt it off the syringe.  A more complex design involves a hammer mill and grinder to break up and grind up the plastic and metal parts of the syringes, after which, the pieces are heated and cooled.  The end result is metal particles encapsulated in a piece of plastic.However, electricity in developing countries is not a dependable source, so hand-powered needle-cutters would be preferred.  Some designs use the squeezing force from a hand to force one or two blades to shear across each other and hence cut the needle between the blades.  There are other designs in which a twisting motion brings a shearing blade in contact with the needle and thus cuts it.  Another design has a stationary outer surface that the syringe body rests against and a cylindrical inner cutting body with a bore for the needle to pass through.  A lever rotates the inner body, which shears the needle from the syringe and dumps the needle into a container.  A crank system can be used to power a similar design, which also uses a cylindrical inner body.  However instead of cutting the needle, the device pulls the needle completely out of the syringe, which deforms the needle, and dumps it into a container.  A more complicated design actually pulls the needle and collar from the barrel of the syringe without a rotational motion: the downward motion of putting the syringe into the device powers two arms to pull the needle off the syringe.  The interesting aspect of this device is that it appears to be one-handed.  Another one-handed device uses a downward motion to cause rotating gears to unscrew the needle and collar from the syringe.  This design is very complex to implement, so an improvement of this design involves pegs that grip and rotate the needle collar instead of gears.  The downward force is transferred into moving the pegs in helical slots, which causes the collar to rotate and the needle to be removed from the syringe.In 2006 a cheap and simple solution utilizing old cola or beer cans to dispose needles and specially developed lid to safely seal them was designed by Yellowone and given the name Antivirus. The lid snaps onto the top of the can permanently sealing it without using any glue or tools. The ’collar’ of the cap is protecting the user during the needle separation process. The insertion hole is designed to separate needle and syringe at the point of use. No finger can pass through the opening. Each can securely contains 150-200 used needles (Yellowone).


== Commercial models ==
There are several electrically powered needle-removers on the market now.  The Disintegrator Needle Destruction Device, offered by American Scientific Resources (ASFX), uses plasma arch technology to destroy the needle, kill pathogens and blunt the syringe. Designed to be used with only one hand, this device completely eliminates the sharp. One model from Techno Fab uses a regular electrical short-circuit to melt the needle, while another needle-remover, seen at CarePathways.com, uses a plasma arc to melt the needle.  A unique needle-remover design is the Needle Remover Device, designed by the Program for Appropriate Technology in Health (PATH).  It uses two handles that are squeezed together to slide two circular blades across each other, which cuts the hub from the syringe.  It is also reusable, and its target cost is about $15.  Another needle-removers currently on the market is Advanced Care Products's Clip&Stor, which uses a hand-powered clipper action to remove the needle.  The cost of the Clip&Stor is about seven dollars.  There is also the BD Hub Cutter, which uses a squeezing hand motion to cut the syringe.  The edges of the squeezable parts have blades that do the actual cutting.  However, unlike a regular needle-remover, the BD Hub Cutter cuts the syringe at the hub so the needle is completely separated from the syringe.  As a result, the risk of a contaminated puncture is completely eliminated because no needle shards remain on the syringe.  The Hub Cutter is not reusable though, and disposal of the whole unit must occur.  The cost of the Hub Cutter is about four dollars.


== Limitations ==
Most of these current needle-removers require the use of two hands; one to hold the needle in place and the other to activate the mechanism.  This form of operation can cause problems because if hospital personnel are busy, especially in a developing world country, they may not have the time or hands needed to operate the device.  As a result, the needle will remain exposed on the syringe, posing a risk to both health care workers and patients.
Furthermore, many of these existing needle-removers do not make use of cheap and readily available materials, like used motor oil jugs, for containers, which raises the price of the device and requires that the hospital continuously buys more containers from the company.  A typical 3-gallon Bemis sharps container with a rotating lid costs about $8 without including shipping costs.  If these containers must be shipped overseas, the price of the device can far exceed the available resources of many hospitals in developing countries, which causes them not to buy needle-remover


== See also ==
Occupational Safety and Health Administration
Hypodermic needle
Injection (medicine)
International Council of Nurses
Biomedical technology
Biomedical engineering
Needle-exchange programme


== References ==


== External links ==
Becton Dickinson Corporate Website
International Health Care Worker Safety Center","pandas(index=98, _1=98, text=""a needle remover is a device used to physically remove a needle from a syringe. in developing countries, there is still a need for improvements in needle safety in hospital settings as most of the needle removal processes are done manually and under severe risk of hazard from needles puncturing skin risking infection. these countries cannot afford needles with individual safety devices attached, so needle-removers must be used to remove the needle from the syringe. this lowers possible pathogen spread by preventing the reuse of the syringes, reducing incidents of accidental needle-sticks, and facilitating syringe disposal.   == background == in regions surveyed by the world health organization (who), the reported number of needle-stick injuries in developing world countries ranged from .93 to 4.68 injuries per person and per year, which is five times higher than in industrialized nations.  needle-stick injuries are further complicated by disease transmission, such as hepatitis b, hepatitis c and hiv. in ghana, a study of 803 schoolchildren revealed that 61.2% had at least one marker of hepatitis b virus.  as a result, health care workers, patients, and the community in developing nations are at an increased risk of contracting blood-borne pathogens via the reuse and improper disposal of needles, and accidental needle-sticks.in the u.s., the needlestick safety act signed in 2000 and the 2001 bloodborne pathogens standard both mandated the use of safety devices and needle-removers with any sharps or needles.  as a result, there was a large increase in research, development, and marketing of needle safety devices and needle-remover.  in most hospital and medical settings in the u.s., needle safety regulations are maintained through individual needle safety devices and needle disposal boxes.   == existing solutions == one of the most common causes of needle-stick injuries, which the needlestick act and bloodborne pathogens standard were attempting to decrease, was two-handed recapping.  as a result, a one-handed capping mechanism was added to insulin and tuberculin syringes.  the cap is attached to the syringe via a hinge, which allows the cap to be snapped onto the needle using one hand. the disadvantage to the hinge system is that the cap can get caught by jewelry and clothing, can get bumped when used, and the fixed position can be a hindrance during low angle injection.  so becton dickinson (bd) has recently come out with a variation on this safety: instead of a hinge, the device slides over the needle and fully covers the tip of the needle, so accidental needle-sticks do not occur.however, the rest of the world does not have similar needle and syringe regulations.  for instance, the who is only able to regulate vaccinations in developing countries by ensuring that all vaccination syringes sent to these countries have autodisable features, since the major concern is the reuse of contaminated needles and syringes.  these autodisable features allow the syringes to only be used once, so they cannot be reused.  these mechanisms could be teeth that interlock to prevent the plunger from being pulled back for another use or a bag prefilled with the vaccine to stop reuse.  for example, the soloshot has a metal clip that locks the plunger down after one use.  the bd uniject is a prefilled vaccine syringe that uses a plastic bulb instead of a plunger and has a disc valve to prevent reuse.still, over 90% of syringes worldwide do not have autodisable features.  individual protection devices are expensive, and regular needles are much more prevalent.  consequently, many developing world countries use needle-removers to reduce the risk of disease transmission via these exposed.   == benefits of needle-removers == needle-removers minimize the occurrence of accidental needle-sticks because they allow immediate removal and containment of the needles, especially if the device is near the area of use.  reuse of syringes is prevented because the needle-remover physically separates the needle from the syringe, making the syringe useless.  they also improve waste disposal by decreasing both the amount of infectious waste and the amount of safety boxes needed for the waste, since safety boxes can pack syringes 20-60% more compactly without the needles.  additionally, these devices are cost-efficient since one device can handle several hundred needles.  many developing world countries do not have the resources to afford auto-disable syringes, so with needle-removers, the hospitals can continue to use cheap syringes, while only paying a one-time fee to buy a needle-remover that has a life-span of about 200-500 needles.   == social and ethical implications == a significant ethical issue for the project is whether or not the needle-remover will cause more harm than its potential benefits.  engineers are obliged to use their skills and knowledge to improve the safety, health, and welfare of the public.  the main concern is for the operator of the device; no engineer should create a device that could injure the operator.  another concern is that children may gain access to the device and accidentally hurt themselves.  if a device design could potentially cause either of these problems, the team would be ethically obligated to reexamine that design, and it would either have to be improved or abandoned. when the device functions effectively and safely, it will serve to protect the welfare of the community.  in developing countries, the risk of disease transmission is elevated due to the high percentage of needle-stick injuries, which is a result of inadequate needle collection devices.  increased pathogen transmission also occurs from the reuse of contaminated needles when supplies are low.  the device will prevent reuse of needles and facilitate needle collection and disposal, and thus will improve the health and safety of hospital workers and the community. the social and economic effects of the device also need to be recognized.  in developing countries, the lack of proper needle collection devices leads to an increase in the number of occupational needle-sticks by health care workers via contaminated needles.  occupational needle-sticks account for 40%-65% of hepatitis b and c infections in health care workers.  as a result, more health care workers have to undergo post-exposure testing and treatment, both of which cost money for the hospitals and the countries.  there is also the manpower cost associated with losing trained health care workers to infections acquired on the job.  with fewer than 10 doctors for every 100,000 individuals in sub-saharan nations, any loss of hospital staff puts a strain of hospital resources.  in addition, developing countries have made significant investments in training their health care workers, which is lost when occupational needle-sticks cause health care workers to leave the medical field.the economic considerations are not just limited to costs associated with health care workers.  due to the high cost of needle-disposal containers and the fact that the containers usually have to be shipped overseas, unsafe and dangerous substitutes are used instead.  this practice can potentially lead to needle-sticks by health care workers and individuals in the community, as well as needle reuse by members of the community, which can increase the potential spread of diseases.   == possible designs == the easiest needle-removers to operate are electrically powered, and either melt the needle or cut the needles at multiple sections.  one patented design involves a syringe falling down into a chamber where powered movable blades advance the syringe onto fixed blades on the opposite side, at which point the syringe is cut with a shearing motion at multiple points.  there are other patents that use electricity between electrodes or between rotating gears to short-circuit the needle and melt it off the syringe.  a more complex design involves a hammer mill and grinder to break up and grind up the plastic and metal parts of the syringes, after which, the pieces are heated and cooled.  the end result is metal particles encapsulated in a piece of plastic.however, electricity in developing countries is not a dependable source, so hand-powered needle-cutters would be preferred.  some designs use the squeezing force from a hand to force one or two blades to shear across each other and hence cut the needle between the blades.  there are other designs in which a twisting motion brings a shearing blade in contact with the needle and thus cuts it.  another design has a stationary outer surface that the syringe body rests against and a cylindrical inner cutting body with a bore for the needle to pass through.  a lever rotates the inner body, which shears the needle from the syringe and dumps the needle into a container.  a crank system can be used to power a similar design, which also uses a cylindrical inner body.  however instead of cutting the needle, the device pulls the needle completely out of the syringe, which deforms the needle, and dumps it into a container.  a more complicated design actually pulls the needle and collar from the barrel of the syringe without a rotational motion: the downward motion of putting the syringe into the device powers two arms to pull the needle off the syringe.  the interesting aspect of this device is that it appears to be one-handed.  another one-handed device uses a downward motion to cause rotating gears to unscrew the needle and collar from the syringe.  this design is very complex to implement, so an improvement of this design involves pegs that grip and rotate the needle collar instead of gears.  the downward force is transferred into moving the pegs in helical slots, which causes the collar to rotate and the needle to be removed from the syringe.in 2006 a cheap and simple solution utilizing old cola or beer cans to dispose needles and specially developed lid to safely seal them was designed by yellowone and given the name antivirus. the lid snaps onto the top of the can permanently sealing it without using any glue or tools. the ’collar’ of the cap is protecting the user during the needle separation process. the insertion hole is designed to separate needle and syringe at the point of use. no finger can pass through the opening. each can securely contains 150-200 used needles (yellowone).   == commercial models == there are several electrically powered needle-removers on the market now.  the disintegrator needle destruction device, offered by american scientific resources (asfx), uses plasma arch technology to destroy the needle, kill pathogens and blunt the syringe. designed to be used with only one hand, this device completely eliminates the sharp. one model from techno fab uses a regular electrical short-circuit to melt the needle, while another needle-remover, seen at carepathways.com, uses a plasma arc to melt the needle.  a unique needle-remover design is the needle remover device, designed by the program for appropriate technology in health (path).  it uses two handles that are squeezed together to slide two circular blades across each other, which cuts the hub from the syringe.  it is also reusable, and its target cost is about $15.  another needle-removers currently on the market is advanced care products's clip&stor, which uses a hand-powered clipper action to remove the needle.  the cost of the clip&stor is about seven dollars.  there is also the bd hub cutter, which uses a squeezing hand motion to cut the syringe.  the edges of the squeezable parts have blades that do the actual cutting.  however, unlike a regular needle-remover, the bd hub cutter cuts the syringe at the hub so the needle is completely separated from the syringe.  as a result, the risk of a contaminated puncture is completely eliminated because no needle shards remain on the syringe.  the hub cutter is not reusable though, and disposal of the whole unit must occur.  the cost of the hub cutter is about four dollars.   == limitations == most of these current needle-removers require the use of two hands; one to hold the needle in place and the other to activate the mechanism.  this form of operation can cause problems because if hospital personnel are busy, especially in a developing world country, they may not have the time or hands needed to operate the device.  as a result, the needle will remain exposed on the syringe, posing a risk to both health care workers and patients. furthermore, many of these existing needle-removers do not make use of cheap and readily available materials, like used motor oil jugs, for containers, which raises the price of the device and requires that the hospital continuously buys more containers from the company.  a typical 3-gallon bemis sharps container with a rotating lid costs about $8 without including shipping costs.  if these containers must be shipped overseas, the price of the device can far exceed the available resources of many hospitals in developing countries, which causes them not to buy needle-remover   == see also == occupational safety and health administration hypodermic needle injection (medicine) international council of nurses biomedical technology biomedical engineering needle-exchange programme   == references ==   == external links == becton dickinson corporate website international health care worker safety center"")"
99,"Metabolic network reconstruction and simulation allows for an in-depth insight into the molecular mechanisms of a particular organism. In particular, these models correlate the genome with molecular physiology. A reconstruction breaks down metabolic pathways (such as glycolysis and the citric acid cycle) into their respective reactions and enzymes, and analyzes them within the perspective of the entire network. In simplified terms, a reconstruction collects all of the relevant metabolic information of an organism and compiles it in a mathematical model.  Validation and analysis of reconstructions can allow identification of key features of metabolism such as growth yield, resource distribution, network robustness, and gene essentiality. This knowledge can then be applied to create novel biotechnology.
In general, the process to build a reconstruction is as follows:

Draft a reconstruction
Refine the model
Convert model into a mathematical/computational representation
Evaluate and debug model through experimentation


== Genome-scale metabolic reconstruction ==
A metabolic reconstruction provides a highly mathematical, structured platform on which to understand the systems biology of metabolic pathways within an organism.  The integration of biochemical metabolic pathways with rapidly available, annotated genome sequences has developed what are called genome-scale metabolic models. Simply put, these models correlate metabolic genes with metabolic pathways. In general, the more information about physiology, biochemistry and genetics is available for the target organism, the better the predictive capacity of the reconstructed models. Mechanically speaking, the process of reconstructing prokaryotic and eukaryotic metabolic networks is essentially the same. Having said this, eukaryote reconstructions are typically more challenging because of the size of genomes, coverage of knowledge, and the multitude of cellular compartments. The first genome-scale metabolic model was generated in 1995 for Haemophilus influenzae.  The first multicellular organism, C. elegans, was reconstructed in 1998.  Since then, many reconstructions have been formed. For a list of reconstructions that have been converted into a model and experimentally validated, see http://sbrg.ucsd.edu/InSilicoOrganisms/OtherOrganisms.


== Drafting a reconstruction ==


=== Resources ===
Because the timescale for the development of reconstructions is so recent, most reconstructions have been built manually. However, now, there are quite a few resources that allow for the semi-automatic assembly of these reconstructions that are utilized due to the time and effort necessary for a reconstruction. An initial fast reconstruction can be developed automatically using resources like PathoLogic or ERGO in combination with encyclopedias like MetaCyc, and then manually updated by using resources like PathwayTools. These semi-automatic methods allow for a fast draft to be created while allowing the fine tune adjustments required once new experimental data is found. It is only in this manner that the field of metabolic reconstructions will keep up with the ever-increasing numbers of annotated genomes.


==== Databases ====
Kyoto Encyclopedia of Genes and Genomes (KEGG): a bioinformatics database containing information on genes, proteins, reactions, and pathways. The ‘KEGG Organisms’ section, which is divided into eukaryotes and prokaryotes, encompasses many organisms for which gene and DNA information can be searched by typing in the enzyme of choice.
BioCyc, EcoCyc, and MetaCyc: BioCyc Is a collection of 3,000 pathway/genome databases (as of Oct 2013), with each database dedicated to one organism. For example, EcoCyc is a highly detailed bioinformatics database on the genome and metabolic reconstruction of Escherichia coli, including thorough descriptions of E. coli signaling pathways and regulatory network. The EcoCyc database can serve as a paradigm and model for any reconstruction. Additionally, MetaCyc, an encyclopedia of experimentally defined metabolic pathways and enzymes, contains 2,100 metabolic pathways and 11,400 metabolic reactions (Oct 2013).
ENZYME: An enzyme nomenclature database (part of the ExPASy proteonomics server of the Swiss Institute of Bioinformatics). After searching for a particular enzyme on the database, this resource gives you the reaction that is catalyzed. ENZYME has direct links to other gene/enzyme/literature databases such as KEGG, BRENDA, and PUBMED.
BRENDA: A comprehensive enzyme database that allows for an enzyme to be searched by name, EC number, or organism.
BiGG: A knowledge base of biochemically, genetically, and genomically structured genome-scale metabolic network reconstructions.
metaTIGER: Is a collection of metabolic profiles and phylogenomic information on a taxonomically diverse range of eukaryotes which provides novel facilities for viewing and comparing the metabolic profiles between organisms.


==== Tools for metabolic modeling ====
Pathway Tools: A bioinformatics software package that assists in the construction of pathway/genome databases such as EcoCyc. Developed by Peter Karp and associates at the SRI International Bioinformatics Research Group, Pathway Tools has several components. Its PathoLogic module takes an annotated genome for an organism and infers probable metabolic reactions and pathways to produce a new pathway/genome database.  Its MetaFlux component can generate a quantitative metabolic model from that pathway/genome database using flux-balance analysis.  Its Navigator component provides extensive query and visualization tools, such as visualization of metabolites, pathways, and the complete metabolic network.
ERGO: A subscription-based service developed by Integrated Genomics. It integrates data from every level including genomic, biochemical data, literature, and high-throughput analysis into a comprehensive user friendly network of metabolic and nonmetabolic pathways.
KEGGtranslator: an easy-to-use stand-alone application that can visualize and convert KEGG files (KGML formatted XML-files) into multiple output formats. Unlike other translators, KEGGtranslator supports a plethora of output formats, is able to augment the information in translated documents (e.g., MIRIAM annotations) beyond the scope of the KGML document, and amends missing components to fragmentary reactions within the pathway to allow simulations on those. KEGGtranslator converts these files to SBML, BioPAX, SIF, SBGN, SBML with qualitative modeling extension, GML, GraphML, JPG, GIF, LaTeX, etc.
ModelSEED: An online resource for the analysis, comparison, reconstruction, and curation of genome-scale metabolic models. Users can submit genome sequences to the RAST annotation system, and the resulting annotation can be automatically piped into the ModelSEED to produce a draft metabolic model. The ModelSEED automatically constructs a network of metabolic reactions, gene-protein-reaction associations for each reaction, and a biomass composition reaction for each genome to produce a model of microbial metabolism that can be simulated using Flux Balance Analysis.
MetaMerge: algorithm for semi-automatically reconciling a pair of existing metabolic network reconstructions into a single metabolic network model.
CoReCo:  algorithm for automatic reconstruction of metabolic models of related species. The first version of the software used KEGG as reaction database to link with the EC number predictions from CoReCo. Its automatic gap filling using atom map of all the reactions produce functional models ready for simulation.


==== Tools for literature ====
PUBMED: This is an online library developed by the National Center for Biotechnology Information, which contains a massive collection of medical journals. Using the link provided by ENZYME, the search can be directed towards the organism of interest, thus recovering literature on the enzyme and its use inside of the organism.


=== Methodology to draft a reconstruction ===

A reconstruction is built by compiling data from the resources above.  Database tools such as KEGG and BioCyc can be used in conjunction with each other to find all the metabolic genes in the organism of interest.  These genes will be compared to closely related organisms that have already developed reconstructions to find homologous genes and reactions. These homologous genes and reactions are carried over from the known reconstructions to form the draft reconstruction of the organism of interest. Tools such as ERGO, Pathway Tools and Model SEED can compile data into pathways to form a network of metabolic and non-metabolic pathways. These networks are then verified and refined before being made into a mathematical simulation.The predictive aspect of a metabolic reconstruction hinges on the ability to predict the biochemical reaction catalyzed by a protein using that protein's amino acid sequence as an input, and to infer the structure of a metabolic network based on the predicted set of reactions. A network of enzymes and metabolites is drafted to relate sequences and function.  When an uncharacterized protein is found in the genome, its amino acid sequence is first compared to those of previously characterized proteins to search for homology.  When a homologous protein is found, the proteins are considered to have a common ancestor and their functions are inferred as being similar.  However, the quality of a reconstruction model is dependent on its ability to accurately infer phenotype directly from sequence, so this rough estimation of protein function will not be sufficient.  A number of algorithms and bioinformatics resources have been developed for refinement of sequence homology-based assignments of protein functions:

InParanoid:  Identifies eukaryotic orthologs by looking only at in-paralogs.
CDD:  Resource for the annotation of functional units in proteins. Its collection of domain models utilizes 3D structure to provide insights into sequence/structure/function relationships.
InterPro:  Provides functional analysis of proteins by classifying them into families and predicting domains and important sites.
STRING:  Database of known and predicted protein interactions.Once proteins have been established, more information about the enzyme structure, reactions catalyzed, substrates and products, mechanisms, and more can be acquired from databases such as KEGG, MetaCyc and NC-IUBMB.  Accurate metabolic reconstructions require additional information about the reversibility and preferred physiological direction of an enzyme-catalyzed reaction which can come from databases such as BRENDA or MetaCyc database.


== Model refinement ==
An initial metabolic reconstruction of a genome is typically far from perfect due to the high variability and diversity of microorganisms.  Often, metabolic pathway databases such as KEGG and MetaCyc will have ""holes"", meaning that there is a conversion from a substrate to a product (i.e., an enzymatic activity) for which there is no known protein in the genome that encodes the enzyme that facilitates the catalysis.  What can also happen in semi-automatically drafted reconstructions is that some pathways are falsely predicted and don't actually occur in the predicted manner.  Because of this, a systematic verification is made in order to make sure no inconsistencies are present and that all the entries listed are correct and accurate.  Furthermore, previous literature can be researched in order to support any information obtained from one of the many metabolic reaction and genome databases. This provides an added level of assurance for the reconstruction that the enzyme and the reaction it catalyzes do actually occur in the organism.
Enzyme promiscuity and spontaneous chemical reactions can damage metabolites. This metabolite damage, and its repair or pre-emption, create energy costs that need to be incorporated into models. It is likely that many genes of unknown function encode proteins that repair or pre-empt metabolite damage, but most genome-scale metabolic reconstructions only include a fraction of all genes.Any new reaction not present in the databases needs to be added to the reconstruction. This is an iterative process that cycles between the experimental phase and the coding phase. As new information is found about the target organism, the model will be adjusted to predict the metabolic and phenotypical output of the cell. The presence or absence of certain reactions of the metabolism will affect the amount of reactants/products that are present for other reactions within the particular pathway. This is because products in one reaction go on to become the reactants for another reaction, i.e. products of one reaction can combine with other proteins or compounds to form new proteins/compounds in the presence of different enzymes or catalysts.Francke et al.  provide an excellent example as to why the verification step of the project needs to be performed in significant detail. During a metabolic network reconstruction of Lactobacillus plantarum, the model showed that succinyl-CoA was one of the reactants for a reaction that was a part of the biosynthesis of methionine. However, an understanding of the physiology of the organism would have revealed that due to an incomplete tricarboxylic acid pathway, Lactobacillus plantarum does not actually produce succinyl-CoA, and the correct reactant for that part of the reaction was acetyl-CoA.
Therefore, systematic verification of the initial reconstruction will bring to light several inconsistencies that can adversely affect the final interpretation of the reconstruction, which is to accurately comprehend the molecular mechanisms of the organism. Furthermore, the simulation step also ensures that all the reactions present in the reconstruction are properly balanced. To sum up, a reconstruction that is fully accurate can lead to greater insight about understanding the functioning of the organism of interest.


== Metabolic network simulation ==
A metabolic network can be broken down into a stoichiometric matrix where the rows represent the compounds of the reactions, while the columns of the matrix correspond to the reactions themselves. Stoichiometry is a quantitative relationship between substrates of a chemical reaction. In order to deduce what the metabolic network suggests, recent research has centered on a few approaches, such as extreme pathways, elementary mode analysis, flux balance analysis, and a number of other constraint-based modeling methods.


=== Extreme pathways ===
Price, Reed, and Papin, from the Palsson lab, use a method of singular value decomposition (SVD) of extreme pathways in order to understand regulation of a human red blood cell metabolism. Extreme pathways are convex basis vectors that consist of steady state functions of a metabolic network. For any particular metabolic network, there is always a unique set of extreme pathways available. Furthermore, Price, Reed, and Papin, define a constraint-based approach, where through the help of constraints like mass balance and maximum reaction rates, it is possible to develop a ‘solution space’ where all the feasible options fall within. Then, using a kinetic model approach, a single solution that falls within the extreme pathway solution space can be determined. Therefore, in their study, Price, Reed, and Papin, use both constraint and kinetic approaches to understand the human red blood cell metabolism. In conclusion, using extreme pathways, the regulatory mechanisms of a metabolic network can be studied in further detail.


=== Elementary mode analysis ===
Elementary mode analysis closely matches the approach used by extreme pathways. Similar to extreme pathways, there is always a unique set of elementary modes available for a particular metabolic network. These are the smallest sub-networks that allow a metabolic reconstruction network to function in steady state. According to Stelling (2002), elementary modes can be used to understand cellular objectives for the overall metabolic network. Furthermore, elementary mode analysis takes into account stoichiometrics and thermodynamics when evaluating whether a particular metabolic route or network is feasible and likely for a set of proteins/enzymes.


=== Minimal metabolic behaviors (MMBs) ===
In 2009, Larhlimi and Bockmayr presented a new approach called ""minimal metabolic behaviors"" for the analysis of metabolic networks. Like elementary modes or extreme pathways, these are uniquely determined by the network, and yield a complete description of the flux cone. However, the new description is much more compact. In contrast with elementary modes and extreme pathways, which use an inner description based on generating vectors of the flux cone, MMBs are using an outer description of the flux cone. This approach is based on sets of non-negativity constraints. These can be identified with irreversible reactions, and thus have a direct biochemical interpretation. One can characterize a metabolic network by MMBs and the reversible metabolic space.


=== Flux balance analysis ===

A different technique to simulate the metabolic network is to perform flux balance analysis. This method uses linear programming, but in contrast to elementary mode analysis and extreme pathways, only a single solution results in the end. Linear programming is usually used to obtain the maximum potential of the objective function that you are looking at, and therefore, when using flux balance analysis, a single solution is found to the optimization problem. In a flux balance analysis approach, exchange fluxes are assigned to those metabolites that enter or leave the particular network only. Those metabolites that are consumed within the network are not assigned any exchange flux value. Also, the exchange fluxes along with the enzymes can have constraints ranging from a negative to positive value (ex: -10 to 10).
Furthermore, this particular approach can accurately define if the reaction stoichiometry is in line with predictions by providing fluxes for the balanced reactions. Also, flux balance analysis can highlight the most effective and efficient pathway through the network in order to achieve a particular objective function. In addition, gene knockout studies can be performed using flux balance analysis. The enzyme that correlates to the gene that needs to be removed is given a constraint value of 0. Then, the reaction that the particular enzyme catalyzes is completely removed from the analysis.


=== Dynamic simulation and parameter estimation ===
In order to perform a dynamic simulation with such a network it is necessary to construct an ordinary differential equation
system that describes the rates of change in each metabolite's concentration or amount. To this end, a rate law, i.e., a kinetic equation that determines the rate of reaction based on the concentrations of all reactants is required for each reaction. Software packages that include numerical integrators, such as COPASI or SBMLsimulator, are then able to simulate the system dynamics given an initial condition. Often these rate laws contain kinetic parameters with uncertain values. In many cases it is desired to estimate these parameter values with respect to given time-series data of metabolite concentrations. The system is then supposed to reproduce the given data. For this purpose the distance between the given data set and the result of the simulation, i.e., the numerically or in few cases analytically obtained solution of the differential equation system is computed. The values of the parameters are then estimated to minimize this distance. One step further, it may be desired to estimate the mathematical structure of the differential equation system because the real rate laws are not known for the reactions within the system under study. To this end, the program SBMLsqueezer allows automatic creation of appropriate rate laws for all reactions with the network.


=== Synthetic accessibility ===
Synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. The synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. To simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. An increase in the total number of steps is predicted to cause lethality. Wunderlich and Mirny showed this simple, parameter-free approach predicted knockout lethality in E. coli and S. cerevisiae as well as elementary mode analysis and flux balance analysis in a variety of media.


== Applications of a reconstruction ==
Several inconsistencies exist between gene, enzyme, reaction databases, and published literature sources regarding the metabolic information of an organism. A reconstruction is a systematic verification and compilation of data from various sources that takes into account all of the discrepancies.
The combination of relevant metabolic and genomic information of an organism.
Metabolic comparisons can be performed between various organisms of the same species as well as between different organisms.
Analysis of synthetic lethality
Predict adaptive evolution outcomes
Use in metabolic engineering for high value outputsReconstructions and their corresponding models allow the formulation of hypotheses about the presence of certain enzymatic activities and the production of metabolites that can be experimentally tested, complementing the primarily discovery-based approach of traditional microbial biochemistry with hypothesis-driven research. The results these experiments can uncover novel pathways and metabolic activities and decipher between discrepancies in previous experimental data.  Information about the chemical reactions of metabolism and the genetic background of various metabolic properties (sequence to structure to function) can be utilized by genetic engineers to modify organisms to produce high value outputs whether those products be medically relevant like pharmaceuticals; high value chemical intermediates such as terpenoids and isoprenoids; or biotechnological outputs like biofuels.Metabolic network reconstructions and models are used to understand how an organism or parasite functions inside of the host cell. For example, if the parasite serves to compromise the immune system by lysing macrophages, then the goal of metabolic reconstruction/simulation would be to determine the metabolites that are essential to the organism's proliferation inside of macrophages. If the proliferation cycle is inhibited, then the parasite would not continue to evade the host's immune system. A reconstruction model serves as a first step to deciphering the complicated mechanisms surrounding disease. These models can also look at the minimal genes necessary for a cell to maintain virulence. The next step would be to use the predictions and postulates generated from a reconstruction model and apply it to discover novel biological functions such as drug-engineering and drug delivery techniques.


== See also ==
Computational systems biology
Computer simulation
Metabolic control analysis
Metabolic network
Metabolic pathway
Metagenomics


== References ==


== Further reading ==
Overbeek R, Larsen N, Walunas T, D'Souza M, Pusch G, Selkov Jr, Liolios K, Joukov V, Kaznadzey D, Anderson I, Bhattacharyya A, Burd H, Gardner W, Hanke P, Kapatral V, Mikhailova N, Vasieva O, Osterman A, Vonstein V, Fonstein M, Ivanova N, Kyrpides N. (2003) The ERGO genome analysis and discovery system.   Nucleic Acids Res. 31(1):164-71
Whitaker, J.W., Letunic, I., McConkey, G.A. and Westhead, D.R. metaTIGER: a metabolic evolution resource. Nucleic Acids Res. 2009 37: D531-8.


== External links ==
ERGO
GeneDB
KEGG
PathCase Case Western Reserve University
BRENDA
BioCyc and Cyclone - provides an open source Java API to the pathway tool BioCyc to extract Metabolic graphs.
EcoCyc
MetaCyc
SEED
ModelSEED
ENZYME
SBRI Bioinformatics Tools and Software
TIGR
Pathway Tools
metaTIGER
Stanford Genomic Resources
Pathway Hunter Tool
IMG The Integrated Microbial Genomes system, for genome analysis by the DOE-JGI.
Systems Analysis, Modelling and Prediction Group at the University of Oxford, Biochemical reaction pathway inference techniques.
efmtool provided by Marco Terzer
SBMLsqueezer
Cellnet analyzer from Klamt and von Kamp
Copasi
gEFM A graph-based tool for EFM computation","pandas(index=99, _1=99, text='metabolic network reconstruction and simulation allows for an in-depth insight into the molecular mechanisms of a particular organism. in particular, these models correlate the genome with molecular physiology. a reconstruction breaks down metabolic pathways (such as glycolysis and the citric acid cycle) into their respective reactions and enzymes, and analyzes them within the perspective of the entire network. in simplified terms, a reconstruction collects all of the relevant metabolic information of an organism and compiles it in a mathematical model.  validation and analysis of reconstructions can allow identification of key features of metabolism such as growth yield, resource distribution, network robustness, and gene essentiality. this knowledge can then be applied to create novel biotechnology. in general, the process to build a reconstruction is as follows:  draft a reconstruction refine the model convert model into a mathematical/computational representation evaluate and debug model through experimentation   == genome-scale metabolic reconstruction == a metabolic reconstruction provides a highly mathematical, structured platform on which to understand the systems biology of metabolic pathways within an organism.  the integration of biochemical metabolic pathways with rapidly available, annotated genome sequences has developed what are called genome-scale metabolic models. simply put, these models correlate metabolic genes with metabolic pathways. in general, the more information about physiology, biochemistry and genetics is available for the target organism, the better the predictive capacity of the reconstructed models. mechanically speaking, the process of reconstructing prokaryotic and eukaryotic metabolic networks is essentially the same. having said this, eukaryote reconstructions are typically more challenging because of the size of genomes, coverage of knowledge, and the multitude of cellular compartments. the first genome-scale metabolic model was generated in 1995 for haemophilus influenzae.  the first multicellular organism, c. elegans, was reconstructed in 1998.  since then, many reconstructions have been formed. for a list of reconstructions that have been converted into a model and experimentally validated, see http://sbrg.ucsd.edu/insilicoorganisms/otherorganisms.   == drafting a reconstruction == synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. the synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. to simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. an increase in the total number of steps is predicted to cause lethality. wunderlich and mirny showed this simple, parameter-free approach predicted knockout lethality in e. coli and s. cerevisiae as well as elementary mode analysis and flux balance analysis in a variety of media.   == applications of a reconstruction == several inconsistencies exist between gene, enzyme, reaction databases, and published literature sources regarding the metabolic information of an organism. a reconstruction is a systematic verification and compilation of data from various sources that takes into account all of the discrepancies. the combination of relevant metabolic and genomic information of an organism. metabolic comparisons can be performed between various organisms of the same species as well as between different organisms. analysis of synthetic lethality predict adaptive evolution outcomes use in metabolic engineering for high value outputsreconstructions and their corresponding models allow the formulation of hypotheses about the presence of certain enzymatic activities and the production of metabolites that can be experimentally tested, complementing the primarily discovery-based approach of traditional microbial biochemistry with hypothesis-driven research. the results these experiments can uncover novel pathways and metabolic activities and decipher between discrepancies in previous experimental data.  information about the chemical reactions of metabolism and the genetic background of various metabolic properties (sequence to structure to function) can be utilized by genetic engineers to modify organisms to produce high value outputs whether those products be medically relevant like pharmaceuticals; high value chemical intermediates such as terpenoids and isoprenoids; or biotechnological outputs like biofuels.metabolic network reconstructions and models are used to understand how an organism or parasite functions inside of the host cell. for example, if the parasite serves to compromise the immune system by lysing macrophages, then the goal of metabolic reconstruction/simulation would be to determine the metabolites that are essential to the organism\'s proliferation inside of macrophages. if the proliferation cycle is inhibited, then the parasite would not continue to evade the host\'s immune system. a reconstruction model serves as a first step to deciphering the complicated mechanisms surrounding disease. these models can also look at the minimal genes necessary for a cell to maintain virulence. the next step would be to use the predictions and postulates generated from a reconstruction model and apply it to discover novel biological functions such as drug-engineering and drug delivery techniques.   == see also == computational systems biology computer simulation metabolic control analysis metabolic network metabolic pathway metagenomics   == references ==   == further reading == overbeek r, larsen n, walunas t, d\'souza m, pusch g, selkov jr, liolios k, joukov v, kaznadzey d, anderson i, bhattacharyya a, burd h, gardner w, hanke p, kapatral v, mikhailova n, vasieva o, osterman a, vonstein v, fonstein m, ivanova n, kyrpides n. (2003) the ergo genome analysis and discovery system.   nucleic acids res. 31(1):164-71 whitaker, j.w., letunic, i., mcconkey, g.a. and westhead, d.r. metatiger: a metabolic evolution resource. nucleic acids res. 2009 37: d531-8.   == external links == ergo genedb kegg pathcase case western reserve university brenda biocyc and cyclone - provides an open source java api to the pathway tool biocyc to extract metabolic graphs. ecocyc metacyc seed modelseed enzyme sbri bioinformatics tools and software tigr pathway tools metatiger stanford genomic resources pathway hunter tool img the integrated microbial genomes system, for genome analysis by the doe-jgi. systems analysis, modelling and prediction group at the university of oxford, biochemical reaction pathway inference techniques. efmtool provided by marco terzer sbmlsqueezer cellnet analyzer from klamt and von kamp copasi gefm a graph-based tool for efm computation')"
100,"Articles related specifically to biomedical engineering include:


== A ==
Artificial heart —
Artificial heart valve —
Artificial intelligence —
Artificial limb —
Artificial pacemaker —
Automated external defibrillator —


== B ==
Bachelor of Science in Biomedical Engineering—
Bedsores—
Biochemistry —
Biochemistry topics list —
Bioelectrochemistry—
Bioelectronics—
Bioimpedance —
Bio-implants —
Bioinformatics —
Biology —
Biology topics list —
Biomechanics —
Biomedical engineering —
Biomedical imaging —
Biomedical Imaging Resource —
Bionics —
Biotechnology —
Biotelemetry —
Biothermia —
BMES —
Brain-computer interface —
Brain implant


== C ==
Cell engineering —
Chemistry —
Chemistry topics list —
Clinical engineering —
Cochlear implant —
Corrective lens —
Crutch —


== D ==
Dental implant —
Dialysis machines —
Diaphragmatic pacemaker —


== E ==
Engineering —


== F ==
Functional electrical stimulation


== G ==
Genetic engineering —
Genetic engineering topics —
Genetics —


== H ==
Health care —
Heart-lung machine —
Heart rate monitor —


== I ==
Implant —
Implantable cardioverter-defibrillator —
Infusion pump —
Instrumentation for medical devices —


== J ==


== K ==


== L ==
Laser applications in medicine —


== M ==
Magnetic resonance imaging —
Maxillo-facial prosthetics —
Medical equipment —
Medical imaging —
Medical research —
Medication —
Medicine —
Microfluidics —
Molecular biology —
Molecular biology topics —


== N ==
Nanoengineering —
Nano-scaffold —
Nanotechnology —
Neural engineering —
Neurally controlled animat —
Neuroengineering —
Neuroprosthetics —
Neurostimulator —
Neurotechnology —


== O ==
Ocular prosthetics —
Optical imaging —
Optical spectroscopy —
Orthosis —


== P ==
Pharmacology —
Physiological system modelling —
Positron emission tomography —
Prosthesis —
Polysomnograph —


== Q ==


== R ==
Radiological imaging —
Radiation therapy —
Reliability engineering —
Remote physiological monitoring —
Replacement joint —
Retinal implant —


== S ==
Safety engineering —
Stem cell —


== T ==
Tissue engineering —
Tissue viability —


== U ==


== V ==


== W ==


== X ==
X-ray —


== Z ==","pandas(index=100, _1=100, text='articles related specifically to biomedical engineering include:   == a == artificial heart — artificial heart valve — artificial intelligence — artificial limb — artificial pacemaker — automated external defibrillator —   == b == bachelor of science in biomedical engineering— bedsores— biochemistry — biochemistry topics list — bioelectrochemistry— bioelectronics— bioimpedance — bio-implants — bioinformatics — biology — biology topics list — biomechanics — biomedical engineering — biomedical imaging — biomedical imaging resource — bionics — biotechnology — biotelemetry — biothermia — bmes — brain-computer interface — brain implant   == c == cell engineering — chemistry — chemistry topics list — clinical engineering — cochlear implant — corrective lens — crutch —   == d == dental implant — dialysis machines — diaphragmatic pacemaker —   == e == engineering —   == f == functional electrical stimulation   == g == genetic engineering — genetic engineering topics — genetics —   == h == health care — heart-lung machine — heart rate monitor —   == i == implant — implantable cardioverter-defibrillator — infusion pump — instrumentation for medical devices —   == j ==   == k ==   == l == laser applications in medicine —   == m == magnetic resonance imaging — maxillo-facial prosthetics — medical equipment — medical imaging — medical research — medication — medicine — microfluidics — molecular biology — molecular biology topics —   == n == nanoengineering — nano-scaffold — nanotechnology — neural engineering — neurally controlled animat — neuroengineering — neuroprosthetics — neurostimulator — neurotechnology —   == o == ocular prosthetics — optical imaging — optical spectroscopy — orthosis —   == p == pharmacology — physiological system modelling — positron emission tomography — prosthesis — polysomnograph —   == q ==   == r == radiological imaging — radiation therapy — reliability engineering — remote physiological monitoring — replacement joint — retinal implant —   == s == safety engineering — stem cell —   == t == tissue engineering — tissue viability —   == u ==   == v ==   == w ==   == x == x-ray —   == z ==')"
101,"Microshock refers to the risk that patients undergoing medical procedures involving externally protruding intracardiac electrical conductors, such as external pacemaker  electrodes, or saline filled catheters, could suffer an electric shock causing ventricular fibrillation (VF) due to currents entering the body via these parts.


== Some definitions related to micro-shock ==
It is important to note that microshock (or micro-shock) are not IEV defined terms and are not used in any international standard.
""Micro-shock"" is an otherwise imperceptible electric current applied directly, or in very close proximity, to the heart muscle of sufficient strength, frequency, and duration to cause disruption of normal cardiac function.
Note: It can be safely assumed (and it usually is) that micro-shock is only possible during certain medical procedures as the electric current needs to be focused directly into the heart by some conductor inserted by invasive means for some desired medical outcome (for example Cardiac Catheterisation).
Micro-shock, if it occurs, is not always lethal. “Micro-electrocution” is the term that should be used whenever a micro-shock causes death.
“Macro-shock” is when a much larger current is passed through the body, usually via a skin to skin pathway, but more generally the current is not applied directly through the heart muscle. The current in macro-shock events can vary widely from being imperceptible to being extremely destructive of tissue. (see Macroshock)
“Electric Shock” is usually referring to macro-shock. (see Electric Shock)
“Electrocution” is usually referring to a macro-shock that has caused prolonged or severe disruption of normal cardiac function - ultimately leading to death. (see Electrocution)


== Theory ==
Microschock requires direct electrical connection to the heart muscle and is normally illustrated using a diagram such as Figure 1 (from TGE).
(an image is to be uploaded here shortly)
In this scenario the patient has inadvertently contacted both a source of current (it does not have to be AC, as shown) as well as a common return pathway during an invasive cardiac medical procedure. If the current flowing is below the threshold of perception, or the patient is sedated, or anaesthetized, there may be no pain or reflex response of either arm. If the current flow continues for sufficient time, at sufficient strength, the patient may die. Because of the low current and lack of patient response, this death may be unexpected, and without any obvious cause. In practice, however, this has never been proven to have happened.To a novice, however, this scenario looks incredibly dangerous, and it is therefore worth examining in some detail.
Firstly, let’s follow the current path. There is a generic source of current. This source can be either large or small, as only a small voltage is required to drive the low current for micro-shock. Such sources might be, a wall socket, a faulty item of equipment, an inappropriate item of equipment, a poorly designed item of equipment, or an item of equipment designed to deliver current into the body. Our patient has unfortunately contacted one such source and current is dispersing through their right arm and upper torso, to eventually converge on a catheter (as labelled – but it could be a lead or wire) that is placed into their heart. This concentration of current flow at the heart muscle is the danger from micro-shock.  If the catheter is conductive and insulated, the current may follow the catheter, emerging through the skin into some other item of equipment. For the circuit as shown to be complete, the conductive part of the catheter needs to also be connected to ground within this equipment. Finally, the hazardous circuit is complete – current can flow and if it continues the patient is in mortal danger. Again, while this is theoretically possible this has never been proven to have actually happened.So, why is this situation not arisen? The details are explored below but the basic presumptions that a patient will simultaneously contact the mains, with a catheter inserted, using equipment that grounds the catheter, is very, very unlikely. Also most electrical connections made in or around a patients heart will be those of medical electrical equipment. Medical electrical equipment, which have such applied parts, are constructed to strict standards that limit the allowable currents flowing via such connections (applied parts). This ensures the safety of the patient.


== History ==
There has never been a documented case of microshock. A U.S. Senate inquiry in the early 1970s, sparked by exaggerated reports of thousands of U.S hospital patients dying of microshock, heard expert testimony about the effect. A review of the evidence in the early 2000s found that not a single case had been reported in the 30 years since the Senate inquiry. Regular checks of the FDA's MAUDE database also show no evidence of this risk being manifest, before or since the review.
Based on studies with dogs by Prof Leslie Geddes in the middle of last century, it is theorised that a current as low as 10 μA (microampere) directly through the heart, may send a human patient directly into ventricular fibrillation. Of course, the exact outcome is dependent on the duration of the current, the exact position of contact, the frequency of current oscillation, and the timing of the shock with the hearts rhythm e.g R on T phenomenon. It is feared that such a small current may be introduced unwittingly, and unobserved, creating a very perilous situation for the patient. To guard against this slim theoretical possibility then, modern medical devices include a range of protective measures to limit current in cardiac-connected circuits to the assumed safe levels of below 10 μA (microampere) . These measures include isolated patient connections, high impedance connections and current limiting circuits. Despite the in-built protections, and lack of observed incidents, microshock continues to be a concern to many practitioners of the fields of Biomedical and Clinical Engineering.
Despite the evidence of decades of absence of reports, in any condition where electrical conductors are run into the body in proximity of the heart (i.e. cardiac catheterizations) precautions are still taken to ensure hazardous current is not introduced through these conductors and it is still regarded as a high risk activity.


== See also ==
Macroshock
Electric Shock


== Notes ==


== References ==
Gross J (2005) Less Jolts from Your Volts: Electrical Safety in the Operating Room. ASA Refresher Courses in Anesthesiology. 33(1):101-114
Ridgway M The Great Debate on Electrical Safety  - in Retrospect in Clinical Engineering Handbook, 1st Edition, Chapter 65, J Dyro (ed) Academic Press 2004
O'Meley P L Who's Afraid of Microshock - presentation to SMBE NSW Conference, Albury NSW, 2011
Hsu J The Hypertextbook http://hypertextbook.com/facts/2000/JackHsu.shtml accessed 23 July 2013
FDA MAUDE Database http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/TextSearch.cfm accessed 25 July 2013
Road Safety Report http://roadsafety.transport.nsw.gov.au/downloads/fatality_rate_1908_to_2009.pdf accessed 25 July 2013
IEV Electropedia produced by the International Electrotechnical Commission (IEC) http://www.electropedia.org
IEC/TS 60479-1 - Effects of current on human beings and livestock https://webstore.iec.ch/publication/25402","pandas(index=101, _1=101, text='microshock refers to the risk that patients undergoing medical procedures involving externally protruding intracardiac electrical conductors, such as external pacemaker  electrodes, or saline filled catheters, could suffer an electric shock causing ventricular fibrillation (vf) due to currents entering the body via these parts.   == some definitions related to micro-shock == it is important to note that microshock (or micro-shock) are not iev defined terms and are not used in any international standard. ""micro-shock"" is an otherwise imperceptible electric current applied directly, or in very close proximity, to the heart muscle of sufficient strength, frequency, and duration to cause disruption of normal cardiac function. note: it can be safely assumed (and it usually is) that micro-shock is only possible during certain medical procedures as the electric current needs to be focused directly into the heart by some conductor inserted by invasive means for some desired medical outcome (for example cardiac catheterisation). micro-shock, if it occurs, is not always lethal. “micro-electrocution” is the term that should be used whenever a micro-shock causes death. “macro-shock” is when a much larger current is passed through the body, usually via a skin to skin pathway, but more generally the current is not applied directly through the heart muscle. the current in macro-shock events can vary widely from being imperceptible to being extremely destructive of tissue. (see macroshock) “electric shock” is usually referring to macro-shock. (see electric shock) “electrocution” is usually referring to a macro-shock that has caused prolonged or severe disruption of normal cardiac function - ultimately leading to death. (see electrocution)   == theory == microschock requires direct electrical connection to the heart muscle and is normally illustrated using a diagram such as figure 1 (from tge). (an image is to be uploaded here shortly) in this scenario the patient has inadvertently contacted both a source of current (it does not have to be ac, as shown) as well as a common return pathway during an invasive cardiac medical procedure. if the current flowing is below the threshold of perception, or the patient is sedated, or anaesthetized, there may be no pain or reflex response of either arm. if the current flow continues for sufficient time, at sufficient strength, the patient may die. because of the low current and lack of patient response, this death may be unexpected, and without any obvious cause. in practice, however, this has never been proven to have happened.to a novice, however, this scenario looks incredibly dangerous, and it is therefore worth examining in some detail. firstly, let’s follow the current path. there is a generic source of current. this source can be either large or small, as only a small voltage is required to drive the low current for micro-shock. such sources might be, a wall socket, a faulty item of equipment, an inappropriate item of equipment, a poorly designed item of equipment, or an item of equipment designed to deliver current into the body. our patient has unfortunately contacted one such source and current is dispersing through their right arm and upper torso, to eventually converge on a catheter (as labelled – but it could be a lead or wire) that is placed into their heart. this concentration of current flow at the heart muscle is the danger from micro-shock.  if the catheter is conductive and insulated, the current may follow the catheter, emerging through the skin into some other item of equipment. for the circuit as shown to be complete, the conductive part of the catheter needs to also be connected to ground within this equipment. finally, the hazardous circuit is complete – current can flow and if it continues the patient is in mortal danger. again, while this is theoretically possible this has never been proven to have actually happened.so, why is this situation not arisen? the details are explored below but the basic presumptions that a patient will simultaneously contact the mains, with a catheter inserted, using equipment that grounds the catheter, is very, very unlikely. also most electrical connections made in or around a patients heart will be those of medical electrical equipment. medical electrical equipment, which have such applied parts, are constructed to strict standards that limit the allowable currents flowing via such connections (applied parts). this ensures the safety of the patient.   == history == there has never been a documented case of microshock. a u.s. senate inquiry in the early 1970s, sparked by exaggerated reports of thousands of u.s hospital patients dying of microshock, heard expert testimony about the effect. a review of the evidence in the early 2000s found that not a single case had been reported in the 30 years since the senate inquiry. regular checks of the fda\'s maude database also show no evidence of this risk being manifest, before or since the review. based on studies with dogs by prof leslie geddes in the middle of last century, it is theorised that a current as low as 10 μa (microampere) directly through the heart, may send a human patient directly into ventricular fibrillation. of course, the exact outcome is dependent on the duration of the current, the exact position of contact, the frequency of current oscillation, and the timing of the shock with the hearts rhythm e.g r on t phenomenon. it is feared that such a small current may be introduced unwittingly, and unobserved, creating a very perilous situation for the patient. to guard against this slim theoretical possibility then, modern medical devices include a range of protective measures to limit current in cardiac-connected circuits to the assumed safe levels of below 10 μa (microampere) . these measures include isolated patient connections, high impedance connections and current limiting circuits. despite the in-built protections, and lack of observed incidents, microshock continues to be a concern to many practitioners of the fields of biomedical and clinical engineering. despite the evidence of decades of absence of reports, in any condition where electrical conductors are run into the body in proximity of the heart (i.e. cardiac catheterizations) precautions are still taken to ensure hazardous current is not introduced through these conductors and it is still regarded as a high risk activity.   == see also == macroshock electric shock   == notes ==   == references == gross j (2005) less jolts from your volts: electrical safety in the operating room. asa refresher courses in anesthesiology. 33(1):101-114 ridgway m the great debate on electrical safety  - in retrospect in clinical engineering handbook, 1st edition, chapter 65, j dyro (ed) academic press 2004 o\'meley p l who\'s afraid of microshock - presentation to smbe nsw conference, albury nsw, 2011 hsu j the hypertextbook http://hypertextbook.com/facts/2000/jackhsu.shtml accessed 23 july 2013 fda maude database http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/textsearch.cfm accessed 25 july 2013 road safety report http://roadsafety.transport.nsw.gov.au/downloads/fatality_rate_1908_to_2009.pdf accessed 25 july 2013 iev electropedia produced by the international electrotechnical commission (iec) http://www.electropedia.org iec/ts 60479-1 - effects of current on human beings and livestock https://webstore.iec.ch/publication/25402')"
102,"The Whitaker Foundation was based in Arlington, Virginia and was an organization that primarily supported biomedical engineering education and research, but also supported other forms of medical research. It was founded and funded by U. A. Whitaker in 1975 upon his death with additional support coming from his wife Helen Whitaker upon her death in 1982. The foundation contributed more than $700 million to various universities and medical schools. The foundation decided to spend its financial resources over a finite period, rather than creating an organization that would be around forever, in order to have the maximum impact. The Whitaker Foundation closed on June 30, 2006.  The foundation helped create 30 biomedical engineering programs at various universities in the United States and helped finance the construction of 13 buildings, many of them subsequently bearing the name ""Whitaker"" in some form. 


== Whitaker International Fellows and Scholars Program ==
The Whitaker International Fellows and Scholars Program funded more than 400 pre-doctoral research fellows and post-doctoral scholars between 2011 and 2018 to perform biomedical research outside of the United States. The program was managed by the Institute for International Education, who also manages the Fulbright Program. In addition to traditional laboratory research, the Whitaker International Program also funded internships in scientific policy and classroom-based educational programs. The last grants were awarded in 2018, however, the program continues to pursue Concluding Initiatives that develop and promote leadership in biomedical engineering, with an international focus.


== References ==


== External links ==
The Whitaker Foundation
Whitaker International Fellows and Scholars Program","pandas(index=102, _1=102, text='the whitaker foundation was based in arlington, virginia and was an organization that primarily supported biomedical engineering education and research, but also supported other forms of medical research. it was founded and funded by u. a. whitaker in 1975 upon his death with additional support coming from his wife helen whitaker upon her death in 1982. the foundation contributed more than $700 million to various universities and medical schools. the foundation decided to spend its financial resources over a finite period, rather than creating an organization that would be around forever, in order to have the maximum impact. the whitaker foundation closed on june 30, 2006.  the foundation helped create 30 biomedical engineering programs at various universities in the united states and helped finance the construction of 13 buildings, many of them subsequently bearing the name ""whitaker"" in some form.   == whitaker international fellows and scholars program == the whitaker international fellows and scholars program funded more than 400 pre-doctoral research fellows and post-doctoral scholars between 2011 and 2018 to perform biomedical research outside of the united states. the program was managed by the institute for international education, who also manages the fulbright program. in addition to traditional laboratory research, the whitaker international program also funded internships in scientific policy and classroom-based educational programs. the last grants were awarded in 2018, however, the program continues to pursue concluding initiatives that develop and promote leadership in biomedical engineering, with an international focus.   == references ==   == external links == the whitaker foundation whitaker international fellows and scholars program')"
103,"Biotelemetry (or medical telemetry) involves the application of telemetry in biology, medicine, and other health care to remotely monitor various vital signs of ambulatory patients.


== Application ==
The most common usage for biotelemetry is in dedicated cardiac care telemetry units or step-down units in hospitals. Although virtually any physiological signal could be transmitted, application is typically limited to cardiac monitoring and SpO2.
Biotelemetry is increasingly being used to understand animals and wildlife by remotely measuring physiology, behaviour and energetic status. It can be used to understand the way that animals migrate, and also the environment that they are experiencing by measuring the abiotic variables, and how it is affecting their physiological status by measuring biotic variables such as heart rate and temperature. Telemetry systems can either be attached externally to animals, or placed internally, with the types of transmission for the devices dependent on the environment that the animal moves in. For example, to study the movement of swimming animals signals using radio transmission or ultrasonic transmission are often used but land based or flying animals can be tracked with GPS and satellite transmissions.


== Components of a biotelemetry system ==
A typical biotelemetry system comprises:

Sensors appropriate for the particular signals to be monitored
Battery-powered, Patient worn transmitters
A Radio Antenna and Receiver
A display unit capable of concurrently presenting information from multiple patients


== History ==
Some of the first uses of biotelemetry systems date to the early space race, where physiological signals obtained from animals or human passengers were transmitted back to Earth for analysis (the name of the medical device manufacturer Spacelabs Healthcare is a reflection of their start in 1958 developing biotelemetry systems for the early U.S. space program).
Animal biotelemetry has been used since at least the 1980s. Animal biotelemetry has now advanced to not only understand the physiology and movement of free ranging animals, but also how different animals interact, for example, between predators and prey.


== Current trends ==
Because of the crowding of the radio spectrum due to the recent introduction of digital television in the United States and many other countries, the Federal Communications Commission (FCC) as well as similar agencies elsewhere have recently begun to allocate dedicated frequency bands for exclusive biotelemetry usage, for example, the Wireless Medical Telemetry Service (WMTS).   The FCC has designated the American Society for Healthcare Engineering of the American Hospital Association (ASHE/AHA) as the frequency coordinator for the WMTS.
In addition, there are many products that utilize commonly available standard radio devices such as Bluetooth and IEEE 802.11.


== See also ==
Battlefield medicine
Heart rate monitor
Remote physiological monitoring
Remote patient monitoring


== References ==


== External links ==
International Society on Biotelemetry
[1]","pandas(index=103, _1=103, text='biotelemetry (or medical telemetry) involves the application of telemetry in biology, medicine, and other health care to remotely monitor various vital signs of ambulatory patients.   == application == the most common usage for biotelemetry is in dedicated cardiac care telemetry units or step-down units in hospitals. although virtually any physiological signal could be transmitted, application is typically limited to cardiac monitoring and spo2. biotelemetry is increasingly being used to understand animals and wildlife by remotely measuring physiology, behaviour and energetic status. it can be used to understand the way that animals migrate, and also the environment that they are experiencing by measuring the abiotic variables, and how it is affecting their physiological status by measuring biotic variables such as heart rate and temperature. telemetry systems can either be attached externally to animals, or placed internally, with the types of transmission for the devices dependent on the environment that the animal moves in. for example, to study the movement of swimming animals signals using radio transmission or ultrasonic transmission are often used but land based or flying animals can be tracked with gps and satellite transmissions.   == components of a biotelemetry system == a typical biotelemetry system comprises:  sensors appropriate for the particular signals to be monitored battery-powered, patient worn transmitters a radio antenna and receiver a display unit capable of concurrently presenting information from multiple patients   == history == some of the first uses of biotelemetry systems date to the early space race, where physiological signals obtained from animals or human passengers were transmitted back to earth for analysis (the name of the medical device manufacturer spacelabs healthcare is a reflection of their start in 1958 developing biotelemetry systems for the early u.s. space program). animal biotelemetry has been used since at least the 1980s. animal biotelemetry has now advanced to not only understand the physiology and movement of free ranging animals, but also how different animals interact, for example, between predators and prey.   == current trends == because of the crowding of the radio spectrum due to the recent introduction of digital television in the united states and many other countries, the federal communications commission (fcc) as well as similar agencies elsewhere have recently begun to allocate dedicated frequency bands for exclusive biotelemetry usage, for example, the wireless medical telemetry service (wmts).   the fcc has designated the american society for healthcare engineering of the american hospital association (ashe/aha) as the frequency coordinator for the wmts. in addition, there are many products that utilize commonly available standard radio devices such as bluetooth and ieee 802.11.   == see also == battlefield medicine heart rate monitor remote physiological monitoring remote patient monitoring   == references ==   == external links == international society on biotelemetry [1]')"
104,"Six degrees of freedom (6DoF) refers to the freedom of movement of a rigid body in three-dimensional space.   Specifically, the body is free to change position as forward/backward (surge), up/down (heave), left/right (sway) translation in three perpendicular axes, combined with changes in orientation through rotation about three perpendicular axes, often termed yaw (normal axis), pitch (transverse axis), and roll (longitudinal axis). Three degrees of freedom (3DOF), a term often used in the context of virtual reality, refers to tracking of rotational motion only: pitch, yaw, and roll.


== Robotics ==
Serial and parallel manipulator systems are generally designed to position an end-effector with six degrees of freedom, consisting of three in translation and three in orientation.  This provides a direct relationship between actuator positions and the configuration of the manipulator defined by its forward and inverse kinematics.
Robot arms are described by their degrees of freedom. This is a practical metric, in contrast to the abstract definition of degrees of freedom which measures the aggregate positioning capability of a system.In 2007, Dean Kamen, inventor of the Segway, unveiled a prototype robotic arm with 14 degrees of freedom for DARPA. Humanoid robots typically have 30 or more degrees of freedom, with six degrees of freedom per arm, five or six in each leg, and several more in torso and neck.


== Engineering ==
The term is important in mechanical systems, especially biomechanical systems for analyzing and measuring properties of these types of systems that need to account for all six degrees of freedom. Measurement of the six degrees of freedom is accomplished today through both AC and DC magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit. The data is made relevant through software that integrates the data based on the needs and programming of the users.

The six degrees of freedom of a mobile unit are divided in two motional classes well as described below ;
Translational envelopes :

Moving forward and backward on the X-axis. (Surge)
Moving left and right on the Y-axis. (Sway)
Moving up and down on the Z-axis. (Heave)Rotational envelopes :

Tilting side to side on the X-axis. (Roll)
Tilting forward and backward on the Y-axis. (Pitch)
Turning left and right on the Z-axis. (Yaw)


== Operational envelope types ==
There are three types of operational envelope in the Six degrees of freedom. These types are Direct, Semi-direct (conditional) and Non-direct, all regardless of the time remaining for the execution of the maneuver, the energy remaining to execute the maneuver and finally, if the motion is commanded via a biological entity (e.g. human), a robotical entity (e.g. computer) or both.

Direct type: Involved a degree can be commanded directly without particularly conditions and described as a normal operation. (An aileron on a basic airplane)
Semi-direct type: Involved a degree can be commanded when some specific conditions are met. (Reverse thrust on an aircraft)
Non-direct type: Involved a degree when is achieved via the interaction with its environment and cannot be commanded. (Pitching motion of a vessel at sea)Transitional type also exists in some vehicles. For example, when the Space Shuttle operates in space, the craft is described as fully-direct-six because its six degrees can be commanded. However, when the Space Shuttle is in the earth's atmosphere for its return, the fully-direct-six degrees are no longer applicable for many technical reasons.


== Game controllers ==
Six degrees of freedom also refers to movement in video game-play.
First-person shooter (FPS) games generally provide five degrees of freedom: forwards/backwards, slide left/right, up/down (jump/crouch/lie), yaw (turn left/right), and pitch (look up/down).  If the game allows leaning control, then some consider it a sixth DoF; however, this may not be completely accurate, as a lean is a limited partial rotation.
The term 6DoF has sometimes been used to describe games which allow freedom of movement, but do not necessarily meet the full 6DoF criteria.  For example, Dead Space 2, and to a lesser extent, Homeworld and Zone Of The Enders allow freedom of movement.
Some examples of true 6DoF games, which allow independent control of all three movement axes and all three rotational axes, include Elite Dangerous, Shattered Horizon, the Descent franchise, Retrovirus, Miner Wars, Space Engineers, Forsaken and Overload (from the same creators of Descent).  The space MMO Vendetta Online also features 6 degrees of freedom.
Motion tracking devices such as TrackIR are used for 6DoF head tracking. This device often finds its places in flight simulators and other vehicle simulators that require looking around the cockpit to locate enemies or simply avoiding accidents in-game.
The acronym 3DoF, meaning movement in the three dimensions but not rotation, is sometimes encountered.
The Razer Hydra, a motion controller for PC, tracks position and rotation of two wired nunchucks, providing six degrees of freedom on each hand.
The SpaceOrb 360 is a 6DOF computer input device released in 1996 originally manufactured and sold by the SpaceTec IMC company (first bought by Labtec, which itself was later bought by Logitech).
The controllers sold with HTC VIVE provide 6DOF information by the lighthouse, which adopts Time of Flight (TOF) technology to determine the position of controllers.


== See also ==
Degrees of freedom (mechanics) – Number of independent parameters that define the configuration or state of a mechanical system.
Degrees of freedom problem – The multiple ways for multi-joint objects to realize a movement
Geometric terms of location – Directions or positions relative to the shape and position of an object
Ship motions – Terms connected to the 6 degrees of freedom of motion
Aircraft principal axes


== References ==","pandas(index=104, _1=104, text=""six degrees of freedom (6dof) refers to the freedom of movement of a rigid body in three-dimensional space.   specifically, the body is free to change position as forward/backward (surge), up/down (heave), left/right (sway) translation in three perpendicular axes, combined with changes in orientation through rotation about three perpendicular axes, often termed yaw (normal axis), pitch (transverse axis), and roll (longitudinal axis). three degrees of freedom (3dof), a term often used in the context of virtual reality, refers to tracking of rotational motion only: pitch, yaw, and roll.   == robotics == serial and parallel manipulator systems are generally designed to position an end-effector with six degrees of freedom, consisting of three in translation and three in orientation.  this provides a direct relationship between actuator positions and the configuration of the manipulator defined by its forward and inverse kinematics. robot arms are described by their degrees of freedom. this is a practical metric, in contrast to the abstract definition of degrees of freedom which measures the aggregate positioning capability of a system.in 2007, dean kamen, inventor of the segway, unveiled a prototype robotic arm with 14 degrees of freedom for darpa. humanoid robots typically have 30 or more degrees of freedom, with six degrees of freedom per arm, five or six in each leg, and several more in torso and neck.   == engineering == the term is important in mechanical systems, especially biomechanical systems for analyzing and measuring properties of these types of systems that need to account for all six degrees of freedom. measurement of the six degrees of freedom is accomplished today through both ac and dc magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit. the data is made relevant through software that integrates the data based on the needs and programming of the users.  the six degrees of freedom of a mobile unit are divided in two motional classes well as described below ; translational envelopes :  moving forward and backward on the x-axis. (surge) moving left and right on the y-axis. (sway) moving up and down on the z-axis. (heave)rotational envelopes :  tilting side to side on the x-axis. (roll) tilting forward and backward on the y-axis. (pitch) turning left and right on the z-axis. (yaw)   == operational envelope types == there are three types of operational envelope in the six degrees of freedom. these types are direct, semi-direct (conditional) and non-direct, all regardless of the time remaining for the execution of the maneuver, the energy remaining to execute the maneuver and finally, if the motion is commanded via a biological entity (e.g. human), a robotical entity (e.g. computer) or both.  direct type: involved a degree can be commanded directly without particularly conditions and described as a normal operation. (an aileron on a basic airplane) semi-direct type: involved a degree can be commanded when some specific conditions are met. (reverse thrust on an aircraft) non-direct type: involved a degree when is achieved via the interaction with its environment and cannot be commanded. (pitching motion of a vessel at sea)transitional type also exists in some vehicles. for example, when the space shuttle operates in space, the craft is described as fully-direct-six because its six degrees can be commanded. however, when the space shuttle is in the earth's atmosphere for its return, the fully-direct-six degrees are no longer applicable for many technical reasons.   == game controllers == six degrees of freedom also refers to movement in video game-play. first-person shooter (fps) games generally provide five degrees of freedom: forwards/backwards, slide left/right, up/down (jump/crouch/lie), yaw (turn left/right), and pitch (look up/down).  if the game allows leaning control, then some consider it a sixth dof; however, this may not be completely accurate, as a lean is a limited partial rotation. the term 6dof has sometimes been used to describe games which allow freedom of movement, but do not necessarily meet the full 6dof criteria.  for example, dead space 2, and to a lesser extent, homeworld and zone of the enders allow freedom of movement. some examples of true 6dof games, which allow independent control of all three movement axes and all three rotational axes, include elite dangerous, shattered horizon, the descent franchise, retrovirus, miner wars, space engineers, forsaken and overload (from the same creators of descent).  the space mmo vendetta online also features 6 degrees of freedom. motion tracking devices such as trackir are used for 6dof head tracking. this device often finds its places in flight simulators and other vehicle simulators that require looking around the cockpit to locate enemies or simply avoiding accidents in-game. the acronym 3dof, meaning movement in the three dimensions but not rotation, is sometimes encountered. the razer hydra, a motion controller for pc, tracks position and rotation of two wired nunchucks, providing six degrees of freedom on each hand. the spaceorb 360 is a 6dof computer input device released in 1996 originally manufactured and sold by the spacetec imc company (first bought by labtec, which itself was later bought by logitech). the controllers sold with htc vive provide 6dof information by the lighthouse, which adopts time of flight (tof) technology to determine the position of controllers.   == see also == degrees of freedom (mechanics) – number of independent parameters that define the configuration or state of a mechanical system. degrees of freedom problem – the multiple ways for multi-joint objects to realize a movement geometric terms of location – directions or positions relative to the shape and position of an object ship motions – terms connected to the 6 degrees of freedom of motion aircraft principal axes   == references =="")"
105,"A Bachelor of Science in Biomedical Engineering is a kind of bachelor's degree typically conferred after a four-year undergraduate course of study in biomedical engineering (BME). The degree itself is largely equivalent to a Bachelor of Science and many institutions conferring degrees in the fields of biomedical engineering and bioengineering do not append the field to the degree itself. Courses of study in BME are also extremely diverse as the field itself is relatively new and developing. In general, an undergraduate course of study in BME is likened to a cross between engineering and biological science with varying degrees of proportionality between the two.


== Professional status ==
Engineers typically require a type of professional certification, such as satisfying certain education requirements and passing an examination to become a professional engineer. These certifications are usually nationally regulated and registered, but there are also cases where a self-governing body, such as the Canadian Association of Professional Engineers. In many cases, carrying the title of ""Professional Engineer"" is legally protected.
As BME is an emerging field, professional certifications are not as standard and uniform as they are for other engineering fields. For example, the Fundamentals of Engineering exam in the U.S. does not include a biomedical engineering section, though it does cover biology. Biomedical engineers often simply possess a university degree as their qualification. However, some countries do regulate biomedical engineers, such as Australia, however registration is typically recommended, but not always a requirement.As with many engineering fields, a bachelor's degree is usually the minimum and often most common degree for a profession in BME, though it is not uncommon for the bachelor's degree to serve as a launching pad into graduate studies. ABET does accredit undergraduate programs in the field. However, even this is not a strict requirement since it is an emerging field and due to the young age of many programs.


== Curriculum ==
The curriculum for BME programs varies significantly from institution to institution and often within a single program. In general, a basic engineering curriculum, including mathematics through differential equations, statistics, and a basic understanding of biology and other basic sciences are hallmarks of a BME program.
Many BME programs have a series of tracks that focus on a particular area of study within BME. Often, the tracks also coincide with a particular engineering or science field. Examples of tracks include:

Biomechanics: Focus includes medical devices, modeling of biological systems and mechanics of organisms. This track interfaces with mechanical engineering and often physiology.
Bioinstrumentation/Bioelectrical Systems: Focus includes medical devices, modeling of biological systems, in particular circuit analogies to the nervous system, bioelectric phenomena and signal processing. This track interfaces with electrical engineering.
Cell, Tissue and Biomolecular Engineering: This track is often quite diverse, with focus ranging from artificial tissues, modeling of biological systems, drug delivery, genetic engineering, biochemical engineering and protein production. This track can interface with chemical engineering, mechanical engineering, molecular biology, physiology, genetics, materials science and other fields.
Medical Optics: Focus on medical diagnostics and medical optical technology. This track interfaces with optics, physics and electrical engineering.Many other tracks may exist within specific programs as well as combinations of multiple tracks.
Another common feature of many BME programs is a capstone design project where students become involved in researching and developing technology in the field. At some schools, this culminates in the creation of medical devices and prototypes. Capstone design projects also often include exposure to issues like funding, regulatory issues and other topics that are related to careers in the field.


== Research and Industry Experience ==
An important feature of many programs is the inclusion of research and/or industry experience into either the curricular or extracurricular work done by students. Since BME careers often focuses on research or industrial applications of the field, many programs have seen fit to either encourage or sometimes require experience outside of the standard curricular requirements. Many research universities offer chances for students to participate in faculty research at the undergraduate level. Other schools have an industry practicum or co-ops to give students relevant work experience before graduation.
Students that participate in either research or industry during the course of study often see advantages when they enter the job market, as many employers prefer experienced candidates or offer higher pay to those with prior experience. Also, research or industry experience is often a factor in graduate school admission.


== Value of the degree ==
Recently, many universities, such as Case Western Reserve University have been implementing new initiatives to either create or expand upon undergraduate programs in BME. This is in part due to rising demand in the biotechnology sector and the increasing interest in biological research. A degree in BME instantly identifies a candidate as having training in both traditional engineering as well as biological science, which has become an increasingly desirable qualification as aspects of biology are permeating into other industries.
Since BME is a diverse field, many programs have a broad curriculum with students usually choosing to specialize in a particular aspect of BME. However, due to the diversity, some degree holders may find their education lacking in deep emphasis, which may prompt continuing studies in graduate school or by learning through experience. 
Numerous rankings of undergraduate BME programs exist with highly varying basis for each ranking. As with many degrees, the reputation of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have more tangible factors for rating, such as research funding and volume, publications and citations.


== References ==","pandas(index=105, _1=105, text='a bachelor of science in biomedical engineering is a kind of bachelor\'s degree typically conferred after a four-year undergraduate course of study in biomedical engineering (bme). the degree itself is largely equivalent to a bachelor of science and many institutions conferring degrees in the fields of biomedical engineering and bioengineering do not append the field to the degree itself. courses of study in bme are also extremely diverse as the field itself is relatively new and developing. in general, an undergraduate course of study in bme is likened to a cross between engineering and biological science with varying degrees of proportionality between the two.   == professional status == engineers typically require a type of professional certification, such as satisfying certain education requirements and passing an examination to become a professional engineer. these certifications are usually nationally regulated and registered, but there are also cases where a self-governing body, such as the canadian association of professional engineers. in many cases, carrying the title of ""professional engineer"" is legally protected. as bme is an emerging field, professional certifications are not as standard and uniform as they are for other engineering fields. for example, the fundamentals of engineering exam in the u.s. does not include a biomedical engineering section, though it does cover biology. biomedical engineers often simply possess a university degree as their qualification. however, some countries do regulate biomedical engineers, such as australia, however registration is typically recommended, but not always a requirement.as with many engineering fields, a bachelor\'s degree is usually the minimum and often most common degree for a profession in bme, though it is not uncommon for the bachelor\'s degree to serve as a launching pad into graduate studies. abet does accredit undergraduate programs in the field. however, even this is not a strict requirement since it is an emerging field and due to the young age of many programs.   == curriculum == the curriculum for bme programs varies significantly from institution to institution and often within a single program. in general, a basic engineering curriculum, including mathematics through differential equations, statistics, and a basic understanding of biology and other basic sciences are hallmarks of a bme program. many bme programs have a series of tracks that focus on a particular area of study within bme. often, the tracks also coincide with a particular engineering or science field. examples of tracks include:  biomechanics: focus includes medical devices, modeling of biological systems and mechanics of organisms. this track interfaces with mechanical engineering and often physiology. bioinstrumentation/bioelectrical systems: focus includes medical devices, modeling of biological systems, in particular circuit analogies to the nervous system, bioelectric phenomena and signal processing. this track interfaces with electrical engineering. cell, tissue and biomolecular engineering: this track is often quite diverse, with focus ranging from artificial tissues, modeling of biological systems, drug delivery, genetic engineering, biochemical engineering and protein production. this track can interface with chemical engineering, mechanical engineering, molecular biology, physiology, genetics, materials science and other fields. medical optics: focus on medical diagnostics and medical optical technology. this track interfaces with optics, physics and electrical engineering.many other tracks may exist within specific programs as well as combinations of multiple tracks. another common feature of many bme programs is a capstone design project where students become involved in researching and developing technology in the field. at some schools, this culminates in the creation of medical devices and prototypes. capstone design projects also often include exposure to issues like funding, regulatory issues and other topics that are related to careers in the field.   == research and industry experience == an important feature of many programs is the inclusion of research and/or industry experience into either the curricular or extracurricular work done by students. since bme careers often focuses on research or industrial applications of the field, many programs have seen fit to either encourage or sometimes require experience outside of the standard curricular requirements. many research universities offer chances for students to participate in faculty research at the undergraduate level. other schools have an industry practicum or co-ops to give students relevant work experience before graduation. students that participate in either research or industry during the course of study often see advantages when they enter the job market, as many employers prefer experienced candidates or offer higher pay to those with prior experience. also, research or industry experience is often a factor in graduate school admission.   == value of the degree == recently, many universities, such as case western reserve university have been implementing new initiatives to either create or expand upon undergraduate programs in bme. this is in part due to rising demand in the biotechnology sector and the increasing interest in biological research. a degree in bme instantly identifies a candidate as having training in both traditional engineering as well as biological science, which has become an increasingly desirable qualification as aspects of biology are permeating into other industries. since bme is a diverse field, many programs have a broad curriculum with students usually choosing to specialize in a particular aspect of bme. however, due to the diversity, some degree holders may find their education lacking in deep emphasis, which may prompt continuing studies in graduate school or by learning through experience. numerous rankings of undergraduate bme programs exist with highly varying basis for each ranking. as with many degrees, the reputation of a program may factor into the desirability of a degree holder for either employment or graduate admission. the reputation of many undergraduate degrees are also linked to the institution\'s graduate or research programs, which have more tangible factors for rating, such as research funding and volume, publications and citations.   == references ==')"
106,"Materialise Mimics is an image processing software for 3D design and modeling, developed by Materialise NV, a Belgian company specialized in additive manufacturing software and technology for medical, dental and additive manufacturing industries. Materialise Mimics is used to create 3D surface models from stacks of 2D image data. These 3D models can then be used for a variety of engineering applications. Mimics is an acronym for Materialise Interactive Medical Image Control System. It is developed in an ISO environment with CE and FDA 510k premarket clearance. Materialise Mimics is commercially available as part of the Materialise Mimics Innovation Suite, which also contains Materialise 3-matic, a design and meshing software for anatomical data. The current version is 20.0, it supports Windows 10, Windows 7, Vista and XP in x64.


== Process ==
Materialise Mimics calculates surface 3D models from stacked image data such as Computed Tomography (CT), Micro CT, Magnetic Resonance Imaging (MRI), Confocal Microscopy, X-ray and Ultrasound, through image segmentation. The ROI, selected in the segmentation process is converted to a 3D surface model using an adapted marching cubes algorithm that takes the partial volume effect into account, leading to very accurate 3D models. The 3D files are represented in the STL format.


== Uploading Data ==
DICOM data from CT or MRI images can be uploaded into Materialise Mimics in order to begin the segmentation process. From this data, 3 different views are present: the coronal, axial, and sagittal views. Another window is present to display 3D objects. 


== Mask Creation ==
The ""New Mask"" tool can be used to highlight specific anatomy from the DICOM data.


== Printing Models ==
Models can be sent to 3D printers in the form of STLs. 


== Gallery ==

		
		


== See also ==
3D modeling
3D Slicer
Computer representation of surfaces
Computed tomography
Medical imaging


== References ==


== External links ==
Official website
User community","pandas(index=106, _1=106, text='materialise mimics is an image processing software for 3d design and modeling, developed by materialise nv, a belgian company specialized in additive manufacturing software and technology for medical, dental and additive manufacturing industries. materialise mimics is used to create 3d surface models from stacks of 2d image data. these 3d models can then be used for a variety of engineering applications. mimics is an acronym for materialise interactive medical image control system. it is developed in an iso environment with ce and fda 510k premarket clearance. materialise mimics is commercially available as part of the materialise mimics innovation suite, which also contains materialise 3-matic, a design and meshing software for anatomical data. the current version is 20.0, it supports windows 10, windows 7, vista and xp in x64.   == process == materialise mimics calculates surface 3d models from stacked image data such as computed tomography (ct), micro ct, magnetic resonance imaging (mri), confocal microscopy, x-ray and ultrasound, through image segmentation. the roi, selected in the segmentation process is converted to a 3d surface model using an adapted marching cubes algorithm that takes the partial volume effect into account, leading to very accurate 3d models. the 3d files are represented in the stl format.   == uploading data == dicom data from ct or mri images can be uploaded into materialise mimics in order to begin the segmentation process. from this data, 3 different views are present: the coronal, axial, and sagittal views. another window is present to display 3d objects.   == mask creation == the ""new mask"" tool can be used to highlight specific anatomy from the dicom data.   == printing models == models can be sent to 3d printers in the form of stls.   == gallery ==          == see also == 3d modeling 3d slicer computer representation of surfaces computed tomography medical imaging   == references ==   == external links == official website user community')"
107,"Automated insulin delivery systems are automated (or semi-automated) systems designed to assist people with diabetes, primarily type 1, by automatically adjusting insulin delivery to help them control their blood glucose levels. Currently available systems (as of October, 2020) can only deliver (and regulate delivery of) a single hormone- insulin. Other systems currently in development aim to improve on current systems by adding one or more additional hormones that can be delivered as needed, providing something closer to the endocrine functionality of a healthy pancreas.
The endocrine functionality of the pancreas is provided by islet cells which produce the hormones insulin and glucagon. Artificial pancreatic technology mimics the secretion of these hormones into the bloodstream in response to the body's changing blood glucose levels. Maintaining balanced blood sugar levels is crucial to the function of the brain, liver, and kidneys. Therefore, for type 1 patients, it is necessary that the levels be kept balanced when the body cannot produce insulin itself.Automated insulin delivery systems are often referred to using the term artificial pancreas, but the term has no precise, universally accepted definition. For uses other than automated insulin delivery, see Artificial pancreas (disambiguation). 


== General Overview ==


=== History ===
The first automated insulin delivery system was known as the Biostator.


=== Classes of AID systems ===
Currently available AID systems fall into four broad classes based on their capabilities. The first systems released- suspend systems- can only halt insulin delivery. Loop systems can modulate delivery both up and down.


==== Threshold suspend ====
Threshold suspend systems are the simplest form of insulin delivery automation. They halt the constant flow of insulin from a pump (known as basal insulin) when a connected CGM reports a glucose level below a pre-set threshold. Halting basal delivery stops the normal preprogrammed rate of delivery, but it cannot remove insulin that has already been infused, so the overall efficacy of threshold suspend systems is limited due to the relatively slow pharmacokinetics of insulin delivered subcutaneously.


==== Predictive Low Glucose Suspend ====
A step forward from threshold suspend systems, Predictive Low Glucose Suspend (PLGS) systems use a mathematical model to extrapolate predicted future blood sugar levels based on recent past readings from a CGM. This allows the system to halt insulin delivery as much as 30 minutes prior to a predicted hypoglycemic event, allowing addition time for the slow pharmacokinetics of insulin to reflect that delivery has been halted.


==== Hybrid Closed Loop ====
Hybrid Closed Loop (HCL) systems further expand on the capabilities of PGLS systems by adjusting basal insulin delivery rates both up and down in response to values from a continuous glucose monitor. Through this modulation of basal insulin, the system is able to reduce the magnitude and duration both hyperglycemic and hypoglycemic events.


==== Advanced Hybrid Closed Loop ====
In addition to modulating basal insulin, Advanced Hybrid Closed Loop systems have the ability to deliver boluses of insulin to correct for elevated blood sugar.


=== Required Components ===
An automated insulin delivery system consists of three distinct components: a continuous glucose monitor to determine blood sugar levels, a pump to deliver insulin, and an algorithm that uses the data from the CGM to send commands to the pump. In the United States, the Food and Drug Administration (FDA) allows each component to be approved independently, allowing for more rapid approvals and incremental innovation. Each component is discussed in greater detail below.


==== Continuous Glucose Monitor (CGM) ====

Continuous glucose monitors (CGMs) are medical devices which extrapolate an estimate of the glucose concentration in a patient's blood based on the level of glucose present in the subcutaneous interstitial fluid. A thin, biocompatible sensor wire coated with a glucose-reactive enzyme is inserted into the skin, allowing the system to read the voltage generated, and based on it, estimate blood glucose. The biggest advantage of a CGM over a traditional fingerstick blood glucose meter is that the CGM can take a new reading as often as every 60 seconds (although most only take a reading every 5 minutes), allowing for a sampling frequency that is able to provide not just a current blood sugar level, but a record of past measurements; allowing computer systems to project past short-term trends into the future, showing patients where their blood sugar levels are likely headed.
Early CGMs were not particularly accurate, but were still useful for observing and recording overall trends and provide warnings in the event of rapid changes in blood glucose readings. 
Continuous blood glucose monitors are one of the set of devices that make up an artificial pancreas device system, the other being an insulin pump, and a glucose meter to calibrate the device. Continuous glucose monitors are a more recent breakthrough and have begun to hit the markets for patient use after approval from the FDA. Both the traditional and the continuous monitor require manual insulin delivery or carbohydrate intake depending on the readings from the devices. While the traditional blood glucose meters require the user to prick their finger every few hours to obtain data, continuous monitors use sensors placed just under the skin on the arm or abdomen to deliver blood sugar level data to receivers or smartphone apps as often as every few minutes. The sensors can be used for up to fourteen days.  A number of different continuous monitors are currently approved by the FDA.The first continuous glucose monitor (CGM) was approved in December 2016. Developed by Dexcom, the G5 Mobile Continuous Monitoring System requires users to prick their fingers twice a day (as opposed to the typical average 8 times daily with the traditional meters) in order to calibrate the sensors. The sensors last up to seven days. The device uses Bluetooth technology to warn the user either through a handheld receiver or app on a smartphone if blood glucose levels reach below a certain point. The cost for this device excluding any co-insurance is an estimated $4,800 a year.

Abbott Laboratories' FreeStyle Libre CGM was approved in September 2017. Recently, the technology was modified to support smartphone use through the LibreLink app. This device does not require finger pricks at all and the sensor, placed on the upper arm, lasts 14 days. The estimated cost for this monitor is $1,300 a year.Dexcom's next G6 model CGM was approved in March 2018, which can last up to ten days and does not need finger prick calibration. Like Medtronic's monitor, it can predict glucose level trends. It is compatible for integration into insulin pumps.


==== Control Algorithm ====


==== Insulin Pump ====


== Currently Available Systems ==


=== Do-It-Yourself ===


=== Commercial ===


==== MiniMed 670G ====
In September 2016, the FDA approved the Medtronic MiniMed 670G, which was the first approved hybrid closed loop system. The device senses a diabetic person's basal insulin requirement and automatically adjusts its delivery to the body. It is made up of a continuous glucose monitor, an insulin pump, and a glucose meter for calibration. It automatically functions to modify the level of insulin delivery based on the detection of blood glucose levels by continuous monitor. It does this by sending the blood glucose data through an algorithm that analyzes and makes the subsequent adjustments. The system has two modes. Manual mode lets the user choose the rate at which basal insulin is delivered. Auto mode regulates basal insulin levels from the continuous monitor's readings every five minutes.The device was originally available only to those aged 14 or older, and in June 2018 was approved by the FDA for use in children aged 7–14. Families have reported better sleep quality from use of the new system, as they do not have to worry about manually checking blood glucose levels during the night. The full cost of the system is $3700, but patients have the opportunity to get it for less.


== Systems in Development ==


=== Ilet Bionic Pancreas ===
A team at Boston University working in collaboration with Massachusetts General Hospital on a dual hormone artificial pancreas system  began clinical trials on their device called the Bionic Pancreas in 2008. In 2016, the Public Benefit Corporation Beta Bionics was formed. In conjunction with the formation of the company, Beta Bionics changed the preliminary name for their device from the Bionic Pancreas to the iLet. The device uses a closed-loop system to deliver both insulin and glucagon in response to sensed blood glucose levels. While not yet approved for public use, the 4th generation iLet prototype, presented in 2017, is around the size of an iPhone, with a touchscreen interface. It contains two chambers for both insulin and glucagon, and the device is configurable for use with only one hormone, or both. While trials continue to be run, the iLet has a projected final approval for the insulin-only system in 2020.


=== Inreda Diabetic ===
In collaboration with the Academic Medical Centre (AMC) in Amsterdam, Inreda is developing a closed loop system with insulin and glucagon. The initiator, Robin Koops, started to develop the device in 2004 and ran the first tests on himself. After several highly successful trials it received the European EC license in 2016. The product is expected to market in the second half of 2020. A smaller improved version is scheduled for 2023.


== Approaches ==


=== Medical equipment ===
The medical equipment approach involves combining a continuous glucose monitor and an implanted insulin pump that can function together with a computer-controlled algorithm to replace the normal function of the pancreas. The development of continuous glucose monitors has led to the progress in artificial pancreas technology using this integrated system.


==== Closed-loop systems ====
Unlike the continuous sensor alone, the closed-loop system requires no user input in response to reading from the monitor; the monitor and insulin pump system automatically delivers the correct amount of hormone calculated from the readings transmitted. The system is what makes up the artificial pancreas device.


===== Current studies =====
Four studies on different artificial pancreas systems are being conducted starting in 2017 and going into the near future. The projects are funded by the National Institute of Diabetes and Digestive and Kidney Diseases, and are the final part of testing the devices before applying for approval for use. Participants in the studies are able to live their lives at home while using the devices and being monitored remotely for safety, efficacy, and a number of other factors.The International Diabetes Closed-Loop trial, led by researchers from the University of Virginia, is testing a closed-loop system called inControl, which has a smartphone user interface. 240 people of ages 14 and up are participating for 6 months.A full-year trial led by researchers from the University of Cambridge started in May 2017 and has enrolled an estimated 150 participants of ages 6 to 18 years. The artificial pancreas system being studied uses a smartphone and has a low glucose feature to improve glucose level control.The International Diabetes Center in Minneapolis, Minnesota, in collaboration with Schneider Children's Medical Center of Israel, are planning a 6-month study that will begin in early 2019 and will involve 112 adolescents and young adults, ages 14 to 30. The main object of the study is to compare the current Medtronic 670G system to a new Medtronic-developed system. The new system has programming that aims to improve glucose control around mealtime, which is still a big challenge in the field.The current 6-month study lead by the Bionic Pancreas team started in mid-2018 and enrolled 312 participants of ages 18 and above.


=== Physiological ===

The biotechnical company Defymed, based in France, is developing an implantable bio-artificial device called MailPan which features a bio-compatible membrane with selective permeability to encapsulate different cell types, including pancreatic beta cells. The implantation of the device does not require conjunctive immuno-suppressive therapy because the membrane prevents antibodies of the patient from entering the device and damaging the encapsulated cells. After being surgically implanted, the membrane sheet will be viable for years. The cells that the device holds can be produced from stem cells rather than human donors, and may also be replaced over time using input and output connections without surgery. Defymed is partially funded by JDRF, formerly known as the Juvenile Diabetes Research Foundation, but is now defined as an organization for all ages and all stages of type 1 diabetes.In November 2018, it was announced that Defymed would partner with the Israel-based Kadimastem, a bio-pharmaceutical company developing stem-cell based regenerative therapies, to receive a two-year grant worth approximately $1.47 million for the development of a bio-artificial pancreas that would treat type 1 diabetes. Kadimastem's stem cell technology uses differentiation of human embryonic stem cells to obtain pancreatic endocrine cells. These include insulin-producing beta cells, as well as alpha cells, which produce glucagon. Both cells arrange in islet-like clusters, mimicking the structure of the pancreas. The aim of the partnership is to combine both technologies in a bio-artificial pancreas device, which releases insulin in response to blood glucose levels, to bring to clinical trial stages.The San Diego, California based biotech company ViaCyte has also developed a product aiming to provide a solution for type 1 diabetes which uses an encapsulation device made of a semi-permeable immune reaction-protective membrane. The device contains pancreatic progenitor cells that have been differentiated from embryonic stem cells. After surgical implantation in an outpatient procedure, the cells mature into endocrine cells which arrange in islet-like clusters and mimic the function of the pancreas, producing insulin and glucagon. The technology advanced from pre-clinical studies to FDA approval for phase 1 clinical trials in 2014, and presented two-year data from the trial in June 2018. They reported that their product, called PEC-Encap, has so far been safe and well tolerated in patients at a dose below therapeutic levels. The encapsulated cells were able to survive and mature after implantation, and immune system rejection was decreased due to the protective membrane. The second phase of the trial will evaluate the efficacy of the product. ViaCyte has also been receiving financial support from JDRF on this project.


== Initiatives around the globe ==
In the United States in 2006, JDRF (formerly the Juvenile Diabetes Research Foundation) launched a multi-year initiative to help accelerate the development, regulatory approval, and acceptance of continuous glucose monitoring and artificial pancreas technology.Grassroots efforts to create and commercialize a fully automated artificial pancreas system have also arisen directly from patient advocates and the diabetes community. Bigfoot Biomedical, a company founded by parents of children with T1D have created algorithms and are developing a closed loop device that monitor blood sugar and appropriately provide insulin.


== References ==","pandas(index=107, _1=107, text=""automated insulin delivery systems are automated (or semi-automated) systems designed to assist people with diabetes, primarily type 1, by automatically adjusting insulin delivery to help them control their blood glucose levels. currently available systems (as of october, 2020) can only deliver (and regulate delivery of) a single hormone- insulin. other systems currently in development aim to improve on current systems by adding one or more additional hormones that can be delivered as needed, providing something closer to the endocrine functionality of a healthy pancreas. the endocrine functionality of the pancreas is provided by islet cells which produce the hormones insulin and glucagon. artificial pancreatic technology mimics the secretion of these hormones into the bloodstream in response to the body's changing blood glucose levels. maintaining balanced blood sugar levels is crucial to the function of the brain, liver, and kidneys. therefore, for type 1 patients, it is necessary that the levels be kept balanced when the body cannot produce insulin itself.automated insulin delivery systems are often referred to using the term artificial pancreas, but the term has no precise, universally accepted definition. for uses other than automated insulin delivery, see artificial pancreas (disambiguation).   == general overview == the biotechnical company defymed, based in france, is developing an implantable bio-artificial device called mailpan which features a bio-compatible membrane with selective permeability to encapsulate different cell types, including pancreatic beta cells. the implantation of the device does not require conjunctive immuno-suppressive therapy because the membrane prevents antibodies of the patient from entering the device and damaging the encapsulated cells. after being surgically implanted, the membrane sheet will be viable for years. the cells that the device holds can be produced from stem cells rather than human donors, and may also be replaced over time using input and output connections without surgery. defymed is partially funded by jdrf, formerly known as the juvenile diabetes research foundation, but is now defined as an organization for all ages and all stages of type 1 diabetes.in november 2018, it was announced that defymed would partner with the israel-based kadimastem, a bio-pharmaceutical company developing stem-cell based regenerative therapies, to receive a two-year grant worth approximately $1.47 million for the development of a bio-artificial pancreas that would treat type 1 diabetes. kadimastem's stem cell technology uses differentiation of human embryonic stem cells to obtain pancreatic endocrine cells. these include insulin-producing beta cells, as well as alpha cells, which produce glucagon. both cells arrange in islet-like clusters, mimicking the structure of the pancreas. the aim of the partnership is to combine both technologies in a bio-artificial pancreas device, which releases insulin in response to blood glucose levels, to bring to clinical trial stages.the san diego, california based biotech company viacyte has also developed a product aiming to provide a solution for type 1 diabetes which uses an encapsulation device made of a semi-permeable immune reaction-protective membrane. the device contains pancreatic progenitor cells that have been differentiated from embryonic stem cells. after surgical implantation in an outpatient procedure, the cells mature into endocrine cells which arrange in islet-like clusters and mimic the function of the pancreas, producing insulin and glucagon. the technology advanced from pre-clinical studies to fda approval for phase 1 clinical trials in 2014, and presented two-year data from the trial in june 2018. they reported that their product, called pec-encap, has so far been safe and well tolerated in patients at a dose below therapeutic levels. the encapsulated cells were able to survive and mature after implantation, and immune system rejection was decreased due to the protective membrane. the second phase of the trial will evaluate the efficacy of the product. viacyte has also been receiving financial support from jdrf on this project.   == initiatives around the globe == in the united states in 2006, jdrf (formerly the juvenile diabetes research foundation) launched a multi-year initiative to help accelerate the development, regulatory approval, and acceptance of continuous glucose monitoring and artificial pancreas technology.grassroots efforts to create and commercialize a fully automated artificial pancreas system have also arisen directly from patient advocates and the diabetes community. bigfoot biomedical, a company founded by parents of children with t1d have created algorithms and are developing a closed loop device that monitor blood sugar and appropriately provide insulin.   == references =="")"
108,"The Institute of Biomedical Engineering (BME) is an academic unit at the University of Toronto. The main goal of the institute is provide graduate education and advance research in the field of biomedical engineering. Established in 1962 by Dr. Norman Moody under the moniker 'Institute of Biomedical Electronics', the Institute has taken on several other names in the past.
BME is home to research and teaching interests of the Faculties of Applied Science and Engineering, Dentistry, and Medicine at the University of Toronto. Located in the heart of the Toronto Discovery District, BME offers research ties with sister departments at University of Toronto, as well as researchers in the Hospital for Sick Children, University Health Network, Mount Sinai Hospital, Sunnybrook Health Sciences Centre, the new Women's College Hospital, St. Michael's Hospital (Toronto), Toronto Rehabilitation Institute, Bloorview Kids Rehab and MaRS Discovery District.


== History ==
Founded in 1962 by Dr. Norman Moody, the Institute of Biomedical Engineering (BME) is one of the oldest biomedical engineering academic units in Canada. With a heavy emphasis on applying electrical engineering concept to solve biological problems, the Institute formed between a collaboration between the Faculty of Engineering and the Faculty of Medicine at the University of Toronto.
In 1999, the Institute of Biomedical Engineering (IBME) merged with the Centre for Biomaterials and the tissue engineering group in Chemical Engineering, to create the Institute of Biomaterials and Biomedical Engineering (IBBME, 1999 - 2000). In addition to Medicine and Engineering, IBBME is also a part of the Faculty of Dentistry.
In the fall of 2001, with the help of funding from the Whitaker Foundation, the Institute launched the new Graduate Program in Biomedical Engineering. 
In summer of 2020, the Institute of Biomaterials and Biomedical Engineering changed its name to the Institute of Biomedical Engineering (BME).


== Academics ==
The Institute of Biomedical Engineering hosts faculty members and students who conduct research in the topics of clinical engineering, cell & tissue engineering, and molecular engineering. BME offers Doctors of Philosophy (PhD), Masters of Science (MASc), Masters of Health Science (MHSc), and Masters of Engineering (MEng). Students enrolled in the PhD and the MASc programs are required to perform research in one of the three disciplines listed above. As of 2019, there are a total of 332 graduate students, half of whom were PhD students.


== References ==


== External links ==
Official website","pandas(index=108, _1=108, text=""the institute of biomedical engineering (bme) is an academic unit at the university of toronto. the main goal of the institute is provide graduate education and advance research in the field of biomedical engineering. established in 1962 by dr. norman moody under the moniker 'institute of biomedical electronics', the institute has taken on several other names in the past. bme is home to research and teaching interests of the faculties of applied science and engineering, dentistry, and medicine at the university of toronto. located in the heart of the toronto discovery district, bme offers research ties with sister departments at university of toronto, as well as researchers in the hospital for sick children, university health network, mount sinai hospital, sunnybrook health sciences centre, the new women's college hospital, st. michael's hospital (toronto), toronto rehabilitation institute, bloorview kids rehab and mars discovery district.   == history == founded in 1962 by dr. norman moody, the institute of biomedical engineering (bme) is one of the oldest biomedical engineering academic units in canada. with a heavy emphasis on applying electrical engineering concept to solve biological problems, the institute formed between a collaboration between the faculty of engineering and the faculty of medicine at the university of toronto. in 1999, the institute of biomedical engineering (ibme) merged with the centre for biomaterials and the tissue engineering group in chemical engineering, to create the institute of biomaterials and biomedical engineering (ibbme, 1999 - 2000). in addition to medicine and engineering, ibbme is also a part of the faculty of dentistry. in the fall of 2001, with the help of funding from the whitaker foundation, the institute launched the new graduate program in biomedical engineering. in summer of 2020, the institute of biomaterials and biomedical engineering changed its name to the institute of biomedical engineering (bme).   == academics == the institute of biomedical engineering hosts faculty members and students who conduct research in the topics of clinical engineering, cell & tissue engineering, and molecular engineering. bme offers doctors of philosophy (phd), masters of science (masc), masters of health science (mhsc), and masters of engineering (meng). students enrolled in the phd and the masc programs are required to perform research in one of the three disciplines listed above. as of 2019, there are a total of 332 graduate students, half of whom were phd students.   == references ==   == external links == official website"")"
109,"BMES (the Biomedical Engineering Society) is the professional society for students, faculty, researcher and industry working in the broad area of biomedical engineering. BMES is the leading biomedical engineering society in the United States and was founded on February 1, 1968 ""to promote the increase of biomedical engineering knowledge and its utilization.""  There are 7,000 members in 2018.Since 1972, the society has published an academic journal, the Annals of Biomedical Engineering (online archive).


== References ==


== External links ==
Engineering Society","pandas(index=109, _1=109, text='bmes (the biomedical engineering society) is the professional society for students, faculty, researcher and industry working in the broad area of biomedical engineering. bmes is the leading biomedical engineering society in the united states and was founded on february 1, 1968 ""to promote the increase of biomedical engineering knowledge and its utilization.""  there are 7,000 members in 2018.since 1972, the society has published an academic journal, the annals of biomedical engineering (online archive).   == references ==   == external links == engineering society')"
110,"Annual Review of Biomedical Engineering is an academic journal published by Annual Reviews. In publication since 1999, this journal covers the significant developments in the broad field of biomedical engineering with an annual volume of review articles. It is edited by Martin L. Yarmush and Mehmet Toner. According to the Journal Citation Reports, the journal has a 2019 impact factor of 15.541, ranking it second out of 87 journals in the category ""Biomedical Engineering"".


== History ==
The Annual Review of Biomedical Engineering was first published in 1999 by the nonprofit publisher Annual Reviews. The inaugural editor was Martin L. Yarmush; Yarmush remained editor until 2021, at which point he was co-editor along with Mehmet Toner. Though it began with a physical editiion, it is now only published electronically.


== Scope and indexing ==
The Annual Review of Biomedical Engineering defines its scope as covering significant developments relevant to biomedical engineering. Included subfields are biomechanics; biomaterials; computational genomics; proteomics; healthcare, biochemical, and tissue engineering; biomonitoring; and medical imaging. As of 2019, Journal Citation Reports lists the journal's impact factor as 15.541, ranking it second of 87 journal titles in the category ""Biomedical Engineering"". It is abstracted and indexed in Scopus, Science Citation Index Expanded, MEDLINE, EMBASE, Inspec and Academic Search, among others.


== Editorial processes ==
The Annual Review of Biomedical Engineering is helmed by the editor or the co-editors. The editor is assisted by the editorial committee, which includes associate editors, regular members, and occasionally guest editors. Guest members participate at the invitation of the editor, and serve terms of one year. All other members of the editorial committee are appointed by the Annual Reviews board of directors and serve five-year terms. The editorial committee determines which topics should be included in each volume and solicits reviews from qualified authors. Unsolicited manuscripts are not accepted. Peer review of accepted manuscripts is undertaken by the editorial committee.


=== Current editorial board ===
As of 2021, the editorial committee consists of the co-editors and the following members:


== See also ==
List of engineering journals and magazines


== References ==","pandas(index=110, _1=110, text='annual review of biomedical engineering is an academic journal published by annual reviews. in publication since 1999, this journal covers the significant developments in the broad field of biomedical engineering with an annual volume of review articles. it is edited by martin l. yarmush and mehmet toner. according to the journal citation reports, the journal has a 2019 impact factor of 15.541, ranking it second out of 87 journals in the category ""biomedical engineering"".   == history == the annual review of biomedical engineering was first published in 1999 by the nonprofit publisher annual reviews. the inaugural editor was martin l. yarmush; yarmush remained editor until 2021, at which point he was co-editor along with mehmet toner. though it began with a physical editiion, it is now only published electronically.   == scope and indexing == the annual review of biomedical engineering defines its scope as covering significant developments relevant to biomedical engineering. included subfields are biomechanics; biomaterials; computational genomics; proteomics; healthcare, biochemical, and tissue engineering; biomonitoring; and medical imaging. as of 2019, journal citation reports lists the journal\'s impact factor as 15.541, ranking it second of 87 journal titles in the category ""biomedical engineering"". it is abstracted and indexed in scopus, science citation index expanded, medline, embase, inspec and academic search, among others.   == editorial processes == the annual review of biomedical engineering is helmed by the editor or the co-editors. the editor is assisted by the editorial committee, which includes associate editors, regular members, and occasionally guest editors. guest members participate at the invitation of the editor, and serve terms of one year. all other members of the editorial committee are appointed by the annual reviews board of directors and serve five-year terms. the editorial committee determines which topics should be included in each volume and solicits reviews from qualified authors. unsolicited manuscripts are not accepted. peer review of accepted manuscripts is undertaken by the editorial committee. as of 2021, the editorial committee consists of the co-editors and the following members:   == see also == list of engineering journals and magazines   == references ==')"
111,"NeuroArm is an engineering research surgical robot specifically designed for neurosurgery. It is the first image-guided, MR-compatible surgical robot that has the capability to perform both microsurgery and stereotaxy.IMRIS, Inc. acquired NeuroArm assets in 2010, and the company is working to develop a next generation of the technology for worldwide commercialization.  It will be integrated with the VISIUS(TM) Surgical Theatre under the name SYMBIS(TM) Surgical System.


== Design ==
NeuroArm was designed to be image-guided and can perform procedures inside an MRI. NeuroArm includes two remote detachable manipulators on a mobile base, a workstation and a system control cabinet. For biopsy-stereotaxy, either the left or right arm is transferred to a stereotactic platform that attaches to the MR bore. The procedure is performed with image-guidance, as MR images are acquired in near real-time. The end-effectors interface with surgical tools which are based on standard neurosurgical instruments.
End-effectors are equipped with three-dimensional force-sensors, providing the sense of touch. The surgeon seated at the workstation controls the robot using force feedback hand controllers. The workstation recreates the sight and sensation of microsurgery by displaying the surgical site and 3D MRI displays, with superimposed tools. NeuroArm enables remote manipulation of the surgical tools from a control room adjacent to the surgical suite. It was designed to function within the environment of 1.5 and 3.0 tesla intraoperative MRI systems. As neuroArm is MR-compatible, stereotaxy can be performed inside the bore of the magnet with near real-time image guidance. NeuroArm possesses the dexterity to perform microsurgery, outside of the MRI system.
Telerobotic operations both inside and outside the magnet are performed using specialized tool sets based on standard neurosurgical instruments, adapted to the end effectors. Using these, NeuroArm is able to cut and manipulate soft tissue, dissect tissue planes, suture, biopsy, electrocauterize, aspirate and irrigate.


== History ==
The project began in 2002 when Daryl, B.J., and Don Seaman provided $2 million to fund the design efforts.  Dr. Sutherland and his group established a collaboration with the Canadian space engineering company  MacDonald Dettwiler and Associates (MDA).  Close collaboration between MDA's robotic engineers and University of Calgary physicians, nurses, and scientists contributed to the design and development of NeuroArm. Official launch of the project was on April 17, 2007.NeuroArm was designed to take full advantage of the imaging environment provided by intraoperative MRI. The ability to couple near real-time, high resolution images to robotic technologies provides the surgeon with image guidance, precision, accuracy, and dexterity. MDA's engineers were immersed in the operating room to study typical tool and surgeon motions in order to use biomimicry for effective design of the computer-assisted surgical device. The OR environment, personnel, surgical rhythm and instrumentation remain unchanged. The surgeon, sitting at the workstation, is provided a virtual environment that recreates the sight, sound, and touch of surgery. Functions like tremor filtering and motion scaling were applied to increase precision and accuracy while functions like no-go zones and linear lock were applied to enhance safety. Surgical tools near the patient's head are incapable of fully independent movement and are slaved to the surgeon’s movement at all times. Pre-planned automatic motions are used to move the robot arms away from the patient's head for manual tool exchange, and then return them to the original position and orientation.
On May 12, 2008, the first image-guided MR-compatible robotic neurosurgical procedure was performed at University of Calgary by Dr. Garnette Sutherland using the NeuroArm.


== References ==


== External links ==
Project neuroArm
Seaman Family MR Research Centre
SYMBIS Homepage on IMRIS Website


=== Videos ===
Video in press release for NeuroArm unveiling, University of Calgary, April 17, 2007


=== Related patents ===
Canadian Patent 2246369 Surgical procedure with magnetic resonance imaging
US Patent 5,735,278 (at USPTO) Surgical procedure with magnetic resonance imaging
US Patent 5,735,278 (at Google) Surgical procedure with magnetic resonance imaging","pandas(index=111, _1=111, text=""neuroarm is an engineering research surgical robot specifically designed for neurosurgery. it is the first image-guided, mr-compatible surgical robot that has the capability to perform both microsurgery and stereotaxy.imris, inc. acquired neuroarm assets in 2010, and the company is working to develop a next generation of the technology for worldwide commercialization.  it will be integrated with the visius(tm) surgical theatre under the name symbis(tm) surgical system.   == design == neuroarm was designed to be image-guided and can perform procedures inside an mri. neuroarm includes two remote detachable manipulators on a mobile base, a workstation and a system control cabinet. for biopsy-stereotaxy, either the left or right arm is transferred to a stereotactic platform that attaches to the mr bore. the procedure is performed with image-guidance, as mr images are acquired in near real-time. the end-effectors interface with surgical tools which are based on standard neurosurgical instruments. end-effectors are equipped with three-dimensional force-sensors, providing the sense of touch. the surgeon seated at the workstation controls the robot using force feedback hand controllers. the workstation recreates the sight and sensation of microsurgery by displaying the surgical site and 3d mri displays, with superimposed tools. neuroarm enables remote manipulation of the surgical tools from a control room adjacent to the surgical suite. it was designed to function within the environment of 1.5 and 3.0 tesla intraoperative mri systems. as neuroarm is mr-compatible, stereotaxy can be performed inside the bore of the magnet with near real-time image guidance. neuroarm possesses the dexterity to perform microsurgery, outside of the mri system. telerobotic operations both inside and outside the magnet are performed using specialized tool sets based on standard neurosurgical instruments, adapted to the end effectors. using these, neuroarm is able to cut and manipulate soft tissue, dissect tissue planes, suture, biopsy, electrocauterize, aspirate and irrigate.   == history == the project began in 2002 when daryl, b.j., and don seaman provided $2 million to fund the design efforts.  dr. sutherland and his group established a collaboration with the canadian space engineering company  macdonald dettwiler and associates (mda).  close collaboration between mda's robotic engineers and university of calgary physicians, nurses, and scientists contributed to the design and development of neuroarm. official launch of the project was on april 17, 2007.neuroarm was designed to take full advantage of the imaging environment provided by intraoperative mri. the ability to couple near real-time, high resolution images to robotic technologies provides the surgeon with image guidance, precision, accuracy, and dexterity. mda's engineers were immersed in the operating room to study typical tool and surgeon motions in order to use biomimicry for effective design of the computer-assisted surgical device. the or environment, personnel, surgical rhythm and instrumentation remain unchanged. the surgeon, sitting at the workstation, is provided a virtual environment that recreates the sight, sound, and touch of surgery. functions like tremor filtering and motion scaling were applied to increase precision and accuracy while functions like no-go zones and linear lock were applied to enhance safety. surgical tools near the patient's head are incapable of fully independent movement and are slaved to the surgeon’s movement at all times. pre-planned automatic motions are used to move the robot arms away from the patient's head for manual tool exchange, and then return them to the original position and orientation. on may 12, 2008, the first image-guided mr-compatible robotic neurosurgical procedure was performed at university of calgary by dr. garnette sutherland using the neuroarm.   == references ==   == external links == project neuroarm seaman family mr research centre symbis homepage on imris website canadian patent 2246369 surgical procedure with magnetic resonance imaging us patent 5,735,278 (at uspto) surgical procedure with magnetic resonance imaging us patent 5,735,278 (at google) surgical procedure with magnetic resonance imaging"")"
112,"Biomimetic materials are materials developed using inspiration from nature. This may be useful in the design of composite materials. Natural structures have inspired and innovated human creations. Notable examples of these natural structures include: honeycomb structure of the beehive, strength of spider silks, bird flight mechanics, and shark skin water repellency. The etymological roots of the neologism (new term) biomimetic derive from Greek, since bios means ""life"" and mimetikos means ""imitative"",


== Tissue Engineering ==
Biomimetic materials in tissue engineering are materials that have been designed such that they elicit specified cellular responses mediated by interactions with scaffold-tethered peptides from extracellular matrix (ECM) proteins; essentially, the incorporation of cell-binding peptides into biomaterials via chemical or physical modification. Amino acids located within the peptides are used as building blocks by other biological structures. These peptides are often referred to as ""self-assembling peptides"", since they can be modified to contain biologically active motifs. This allows them to replicate information derived from tissue and to reproduce the same information independently. Thus, these peptides act as building blocks capable of conducting multiple biochemical activities, including tissue engineering. Tissue engineering research currently being performed on both short chain and long chain peptides is still in early stages.
Such peptides include both native long chains of ECM proteins as well as short peptide sequences derived from intact ECM proteins. The idea is that the biomimetic material will mimic some of the roles that an ECM plays in neural tissue. In addition to promoting cellular growth and mobilization, the incorporated peptides could also mediate by specific protease enzymes or initiate cellular responses not present in a local native tissue.In the beginning, long chains of ECM proteins including fibronectin (FN), vitronectin (VN), and laminin (LN) were used, but more recently the advantages of using short peptides have been discovered. Short peptides are more advantageous because, unlike the long chains that fold randomly upon adsorption causing the active protein domains to be sterically unavailable, short peptides remain stable and do not hide the receptor binding domains when adsorbed. Another advantage to short peptides is that they can be replicated more economically due to the smaller size. A bi-functional cross-linker with a long spacer arm is used to tether peptides to the substrate surface. If a functional group is not available for attaching the cross-linker, photochemical immobilization may be used.In addition to modifying the surface, biomaterials can be modified in bulk, meaning that the cell signaling peptides and recognition sites are present not just on the surface but also throughout the bulk of the material. The strength of cell attachment, cell migration rate, and extent of cytoskeletal organization formation is determined by the receptor binding to the ligand bound to the material; thus, receptor-ligand affinity, the density of the ligand, and the spatial distribution of the ligand must be carefully considered when designing a biomimetic material.


== Biomimetic mineralization ==
Proteins of the developing enamel extracellular matrix (such as Amelogenin) control initial mineral deposition (nucleation) and subsequent crystal growth, ultimately determining the physico-mechanical properties of the mature mineralized tissue. Nucleators bring together mineral ions from the surrounding fluids (such as saliva) into the form of a crystal lattice structure, by stabilizing small nuclei to permit crystal growth, forming mineral tissue. Mutations in enamel ECM proteins result in enamel defects such as amelogenesis imperfecta. Type-I collagen is thought to have a similar role for the formation of dentin and bone.Dental enamel mineral (as well as dentin and bone) is made of hydroxylapatite with foreign ions incorporated in the structure. Carbonate, fluoride, and magnesium are the most common heteroionic substituents.In a biomimetic mineralization strategy based on normal enamel histogenesis, a three-dimensional scaffold is formed to attract and arrange calcium and/or phosphate ions to induce de novo precipitation of hydroxylapatite.Two general strategies have been applied. One is using fragments known to support natural mineralization proteins, such as Amelogenin, Collagen, or Dentin Phosphophoryn as the basis. Alternatively, de novo macromolecular structures have been designed to support mineralization, not based on natural molecules, but on rational design. One example is oligopeptide P11-4.In dental orthopedics and implants, a more traditional strategy to improve the density of the underlying jaw bone is via the in situ application of calcium phosphate materials. Commonly used materials include hydroxylapatite, tricalcium phosphate, and calcium phosphate cement. Newer bioactive glasses follow this line of strategy, where the added silicone provides an important bonus to the local absorption of calcium.


== Extracellular matrix proteins ==
Many studies utilize laminin-1 when designing a biomimetic material. Laminin is a component of the extracellular matrix that is able to promote neuron attachment and differentiation, in addition to axon growth guidance. Its primary functional site for bioactivity is its core protein domain isoleucine-lysine-valine-alanine-valine (IKVAV), which is located in the α-1 chain of laminin.A recent study by Wu, Zheng et al., synthesized a self-assembled IKVAV peptide nanofiber and tested its effect on the adhesion of neuron-like pc12 cells. Early cell adhesion is very important for preventing cell degeneration; the longer cells are suspended in culture, the more likely they are to degenerate. The purpose was to develop a biomaterial with good cell adherence and bioactivity with IKVAV, which is able to inhibit differentiation and adhesion of glial cells in addition to promoting neuronal cell adhesion and differentiation. The IKVAV peptide domain is on the surface of the nanofibers so that it is exposed and accessible for promoting cell contact interactions. The IKVAV nanofibers promoted stronger cell adherence than the electrostatic attraction induced by poly-L-lysine, and cell adherence increased with increasing density of IKVAV until the saturation point was reached. IKVAV does not exhibit time dependent effects because the adherence was shown to be the same at 1 hour and at 3 hours.Laminin is known to stimulate neurite outgrowth and it plays a role in the developing nervous system. It is known that gradients are critical for the guidance of growth cones to their target tissues in the developing nervous system. There has been much research done on soluble gradients; however, little emphasis has been placed on gradients of substratum bound substances of the extracellular matrix such as laminin. Dodla and Bellamkonda, fabricated an anisotropic 3D agarose gel with gradients of coupled laminin-1 (LN-1). Concentration gradients of LN-1 were shown to promote faster neurite extension than the highest neurite growth rate observed with isotropic LN-1 concentrations. Neurites grew both up and down the gradients, but growth was faster at less steep gradients and was faster up the gradients than down the gradients.


== Biomimetic artificial muscles ==
Electroactive polymers (EAPs) are also known as artificial muscles. EAPs are polymeric materials and they are able to produce large deformation when applied in an electric field. This provides large potential in applications in biotechnology and robotics, sensors, and actuators.


== Biomimetic photonic structures ==

The production of structural colours concerns a large array of organisms. From bacteria (Flavobacterium strain IR1) to multicellular organisms, (Hibiscus trionum, Doryteuthis pealeii (squid), or Chrysochroa fulgidissima (beetle)), manipulation of light is not limited to rare and exotic life forms. Different organisms evolved different mechanisms to produce structural colours: multilayered cuticle in some insects and plants, grating like surface in plants, geometrically organised cells in bacteria... all of theme stand for a source of inspiration towards the development of structurally coloured materials.
Study of the firefly abdomen revealed the presence of a 3-layer system comprising the cuticle, the Photogenic layer and then a reflector layer. Microscopy of the reflector layer revealed a granulate structure. Directly inspired from the fire fly Reflector layer, an artificial granulate film composed of hollow silica beads of about 1.05 μm was correlated with a high reflection index and could be used to improve light emission in chemiluminescent systems.


== Artificial enzyme ==
Artificial enzymes are synthetic materials that can mimic (partial) function of a natural enzyme without necessarily being a protein. Among them, some nanomaterials have been used to mimic natural enzymes. These nanomaterials are termed nanozymes. Nanozymes as well as other artificial enzymes have found wide applications, from biosensing and immunoassays, to stem cell growth and pollutant removal.


== References ==","pandas(index=112, _1=112, text='biomimetic materials are materials developed using inspiration from nature. this may be useful in the design of composite materials. natural structures have inspired and innovated human creations. notable examples of these natural structures include: honeycomb structure of the beehive, strength of spider silks, bird flight mechanics, and shark skin water repellency. the etymological roots of the neologism (new term) biomimetic derive from greek, since bios means ""life"" and mimetikos means ""imitative"",   == tissue engineering == biomimetic materials in tissue engineering are materials that have been designed such that they elicit specified cellular responses mediated by interactions with scaffold-tethered peptides from extracellular matrix (ecm) proteins; essentially, the incorporation of cell-binding peptides into biomaterials via chemical or physical modification. amino acids located within the peptides are used as building blocks by other biological structures. these peptides are often referred to as ""self-assembling peptides"", since they can be modified to contain biologically active motifs. this allows them to replicate information derived from tissue and to reproduce the same information independently. thus, these peptides act as building blocks capable of conducting multiple biochemical activities, including tissue engineering. tissue engineering research currently being performed on both short chain and long chain peptides is still in early stages. such peptides include both native long chains of ecm proteins as well as short peptide sequences derived from intact ecm proteins. the idea is that the biomimetic material will mimic some of the roles that an ecm plays in neural tissue. in addition to promoting cellular growth and mobilization, the incorporated peptides could also mediate by specific protease enzymes or initiate cellular responses not present in a local native tissue.in the beginning, long chains of ecm proteins including fibronectin (fn), vitronectin (vn), and laminin (ln) were used, but more recently the advantages of using short peptides have been discovered. short peptides are more advantageous because, unlike the long chains that fold randomly upon adsorption causing the active protein domains to be sterically unavailable, short peptides remain stable and do not hide the receptor binding domains when adsorbed. another advantage to short peptides is that they can be replicated more economically due to the smaller size. a bi-functional cross-linker with a long spacer arm is used to tether peptides to the substrate surface. if a functional group is not available for attaching the cross-linker, photochemical immobilization may be used.in addition to modifying the surface, biomaterials can be modified in bulk, meaning that the cell signaling peptides and recognition sites are present not just on the surface but also throughout the bulk of the material. the strength of cell attachment, cell migration rate, and extent of cytoskeletal organization formation is determined by the receptor binding to the ligand bound to the material; thus, receptor-ligand affinity, the density of the ligand, and the spatial distribution of the ligand must be carefully considered when designing a biomimetic material.   == biomimetic mineralization == proteins of the developing enamel extracellular matrix (such as amelogenin) control initial mineral deposition (nucleation) and subsequent crystal growth, ultimately determining the physico-mechanical properties of the mature mineralized tissue. nucleators bring together mineral ions from the surrounding fluids (such as saliva) into the form of a crystal lattice structure, by stabilizing small nuclei to permit crystal growth, forming mineral tissue. mutations in enamel ecm proteins result in enamel defects such as amelogenesis imperfecta. type-i collagen is thought to have a similar role for the formation of dentin and bone.dental enamel mineral (as well as dentin and bone) is made of hydroxylapatite with foreign ions incorporated in the structure. carbonate, fluoride, and magnesium are the most common heteroionic substituents.in a biomimetic mineralization strategy based on normal enamel histogenesis, a three-dimensional scaffold is formed to attract and arrange calcium and/or phosphate ions to induce de novo precipitation of hydroxylapatite.two general strategies have been applied. one is using fragments known to support natural mineralization proteins, such as amelogenin, collagen, or dentin phosphophoryn as the basis. alternatively, de novo macromolecular structures have been designed to support mineralization, not based on natural molecules, but on rational design. one example is oligopeptide p11-4.in dental orthopedics and implants, a more traditional strategy to improve the density of the underlying jaw bone is via the in situ application of calcium phosphate materials. commonly used materials include hydroxylapatite, tricalcium phosphate, and calcium phosphate cement. newer bioactive glasses follow this line of strategy, where the added silicone provides an important bonus to the local absorption of calcium.   == extracellular matrix proteins == many studies utilize laminin-1 when designing a biomimetic material. laminin is a component of the extracellular matrix that is able to promote neuron attachment and differentiation, in addition to axon growth guidance. its primary functional site for bioactivity is its core protein domain isoleucine-lysine-valine-alanine-valine (ikvav), which is located in the α-1 chain of laminin.a recent study by wu, zheng et al., synthesized a self-assembled ikvav peptide nanofiber and tested its effect on the adhesion of neuron-like pc12 cells. early cell adhesion is very important for preventing cell degeneration; the longer cells are suspended in culture, the more likely they are to degenerate. the purpose was to develop a biomaterial with good cell adherence and bioactivity with ikvav, which is able to inhibit differentiation and adhesion of glial cells in addition to promoting neuronal cell adhesion and differentiation. the ikvav peptide domain is on the surface of the nanofibers so that it is exposed and accessible for promoting cell contact interactions. the ikvav nanofibers promoted stronger cell adherence than the electrostatic attraction induced by poly-l-lysine, and cell adherence increased with increasing density of ikvav until the saturation point was reached. ikvav does not exhibit time dependent effects because the adherence was shown to be the same at 1 hour and at 3 hours.laminin is known to stimulate neurite outgrowth and it plays a role in the developing nervous system. it is known that gradients are critical for the guidance of growth cones to their target tissues in the developing nervous system. there has been much research done on soluble gradients; however, little emphasis has been placed on gradients of substratum bound substances of the extracellular matrix such as laminin. dodla and bellamkonda, fabricated an anisotropic 3d agarose gel with gradients of coupled laminin-1 (ln-1). concentration gradients of ln-1 were shown to promote faster neurite extension than the highest neurite growth rate observed with isotropic ln-1 concentrations. neurites grew both up and down the gradients, but growth was faster at less steep gradients and was faster up the gradients than down the gradients.   == biomimetic artificial muscles == electroactive polymers (eaps) are also known as artificial muscles. eaps are polymeric materials and they are able to produce large deformation when applied in an electric field. this provides large potential in applications in biotechnology and robotics, sensors, and actuators.   == biomimetic photonic structures ==  the production of structural colours concerns a large array of organisms. from bacteria (flavobacterium strain ir1) to multicellular organisms, (hibiscus trionum, doryteuthis pealeii (squid), or chrysochroa fulgidissima (beetle)), manipulation of light is not limited to rare and exotic life forms. different organisms evolved different mechanisms to produce structural colours: multilayered cuticle in some insects and plants, grating like surface in plants, geometrically organised cells in bacteria... all of theme stand for a source of inspiration towards the development of structurally coloured materials. study of the firefly abdomen revealed the presence of a 3-layer system comprising the cuticle, the photogenic layer and then a reflector layer. microscopy of the reflector layer revealed a granulate structure. directly inspired from the fire fly reflector layer, an artificial granulate film composed of hollow silica beads of about 1.05 μm was correlated with a high reflection index and could be used to improve light emission in chemiluminescent systems.   == artificial enzyme == artificial enzymes are synthetic materials that can mimic (partial) function of a natural enzyme without necessarily being a protein. among them, some nanomaterials have been used to mimic natural enzymes. these nanomaterials are termed nanozymes. nanozymes as well as other artificial enzymes have found wide applications, from biosensing and immunoassays, to stem cell growth and pollutant removal.   == references ==')"
113,"The Luer taper is a standardized system of small-scale fluid fittings used for making leak-free connections between a male-taper fitting and its mating female part on medical and laboratory instruments, including hypodermic syringe tips and needles or stopcocks and needles. Currently ISO 80369 governs the Luer standards and testing methods.Invented by Karl Schneider and named after the 19th-century German medical instrument maker Hermann Wülfing Lüer, it originated as a 6% taper fitting for glass bottle stoppers (so one side is at 1.72 degrees to the centerline). Key features of Luer taper connectors are defined in the ISO 594 standards. It is also defined in the DIN and EN standard 1707:1996 and 20594-1:1993.


== Variants ==
There are two varieties of Luer taper connections: locking and slipping. Their trade names are confusingly similar to the nonproprietary names. ""Luer-Lock"" and ""Luer-slip"" are registered trademarks of Becton Dickinson. ""Luer-Lock"" style connectors are often generically referred to as ""Luer lock"", and ""Luer-slip"" style connectors may be generically referred to as ""slip tip"". Luer lock fittings are securely joined by means of a tabbed hub on the female fitting which screws into threads in a sleeve on the male fitting.  The Luer lock fitting was developed in the United States by Fairleigh S. Dickinson. 'Luer lock' style connectors are divided into two types ""one piece luer lock"" and ""two piece luer lock"" or ""rotating collar luer lock"". One piece Luer lock comes as a single mold, and locking is achieved by rotating the entire luer connector or system. In two piece luer lock, a free rotating collar with threads is assembled to the luer and the locking is achieved by rotating the collar.
Slip tip (Luer-slip) fittings simply conform to Luer taper dimensions and are pressed together and held by friction (they have no threads). Luer components are manufactured from either metal or plastic and are available from many companies worldwide.


== References ==","pandas(index=113, _1=113, text='the luer taper is a standardized system of small-scale fluid fittings used for making leak-free connections between a male-taper fitting and its mating female part on medical and laboratory instruments, including hypodermic syringe tips and needles or stopcocks and needles. currently iso 80369 governs the luer standards and testing methods.invented by karl schneider and named after the 19th-century german medical instrument maker hermann wülfing lüer, it originated as a 6% taper fitting for glass bottle stoppers (so one side is at 1.72 degrees to the centerline). key features of luer taper connectors are defined in the iso 594 standards. it is also defined in the din and en standard 1707:1996 and 20594-1:1993.   == variants == there are two varieties of luer taper connections: locking and slipping. their trade names are confusingly similar to the nonproprietary names. ""luer-lock"" and ""luer-slip"" are registered trademarks of becton dickinson. ""luer-lock"" style connectors are often generically referred to as ""luer lock"", and ""luer-slip"" style connectors may be generically referred to as ""slip tip"". luer lock fittings are securely joined by means of a tabbed hub on the female fitting which screws into threads in a sleeve on the male fitting.  the luer lock fitting was developed in the united states by fairleigh s. dickinson. \'luer lock\' style connectors are divided into two types ""one piece luer lock"" and ""two piece luer lock"" or ""rotating collar luer lock"". one piece luer lock comes as a single mold, and locking is achieved by rotating the entire luer connector or system. in two piece luer lock, a free rotating collar with threads is assembled to the luer and the locking is achieved by rotating the collar. slip tip (luer-slip) fittings simply conform to luer taper dimensions and are pressed together and held by friction (they have no threads). luer components are manufactured from either metal or plastic and are available from many companies worldwide.   == references ==')"
114,"Critical Reviews in Biomedical Engineering is a bimonthly peer-reviewed scientific journal published by Begell House covering biomedical engineering, bioengineering, clinical engineering, and related subjects. The editor-in-chief is Chenzhong Li.


== External links ==
Official website","pandas(index=114, _1=114, text='critical reviews in biomedical engineering is a bimonthly peer-reviewed scientific journal published by begell house covering biomedical engineering, bioengineering, clinical engineering, and related subjects. the editor-in-chief is chenzhong li.   == external links == official website')"
115,"The IEEE Engineering in Medicine and Biology Society (EMBS) is an IEEE group dedicated to the study of Biomedical Engineering.


== History ==
The IRE Professional Group on Medical Electronics was formed in 1952 to consider ""problems in biology and medicine which might be aided in solution by use of electronic engineering principles and devices."" After IRE's merge with AIEE in 1963, the Professional Group on Medical Electronics merged with AIEE's Committee on Electrical Techniques in Medicine and Biology to form the IEEE Engineering in Medicine and Biology Society.The group also has student chapters.


== Publications ==


=== Magazines ===
IEEE Pulse
IEEE Engineering in Medicine and Biology Magazine (ceased publication in 2010)


=== Journals ===
IEEE Transactions on Biomedical Engineering
IEEE Transactions on Neural Systems and Rehabilitation Engineering
IEEE Journal of Biomedical and Health Informatics
IEEE Journal of Translational Engineering in Health and Medicine
IEEE Open Journal of Engineering in Medicine and Biology
IEEE Reviews in Biomedical Engineering
IEEE Transactions on Medical Imaging
IEEE Transactions on NanoBioscience
IEEE/ACM Transactions on Computational Biology and Bioinformatics
IEEE Transactions on Biomedical Circuits and Systems


== Conferences ==
The Annual International Conference of the IEEE Engineering in Medicine and Biology Society is held in various locations in the United States and around the world. 
The 42th Annual International Conference was to be held in Montreal, Canada on July 20–24, 2020, but due to the global pandemic was held virtually.


== References ==


== External links ==
EMBS Official Website
EMBC 2020","pandas(index=115, _1=115, text='the ieee engineering in medicine and biology society (embs) is an ieee group dedicated to the study of biomedical engineering.   == history == the ire professional group on medical electronics was formed in 1952 to consider ""problems in biology and medicine which might be aided in solution by use of electronic engineering principles and devices."" after ire\'s merge with aiee in 1963, the professional group on medical electronics merged with aiee\'s committee on electrical techniques in medicine and biology to form the ieee engineering in medicine and biology society.the group also has student chapters.   == publications == ieee transactions on biomedical engineering ieee transactions on neural systems and rehabilitation engineering ieee journal of biomedical and health informatics ieee journal of translational engineering in health and medicine ieee open journal of engineering in medicine and biology ieee reviews in biomedical engineering ieee transactions on medical imaging ieee transactions on nanobioscience ieee/acm transactions on computational biology and bioinformatics ieee transactions on biomedical circuits and systems   == conferences == the annual international conference of the ieee engineering in medicine and biology society is held in various locations in the united states and around the world. the 42th annual international conference was to be held in montreal, canada on july 20–24, 2020, but due to the global pandemic was held virtually.   == references ==   == external links == embs official website embc 2020')"
116,"An autonomous building is a building designed to be operated independently from infrastructural support services such as the electric power grid, gas grid, municipal water systems, sewage treatment systems, storm drains, communication services, and in some cases, public roads.
Advocates of autonomous building describe advantages that include reduced environmental impacts, increased security, and lower costs of ownership. Some cited advantages satisfy tenets of green building, not independence per se (see below). Off-grid buildings often rely very little on civil services and are therefore safer and more comfortable during civil disaster or military attacks. For example, Off-grid buildings would not lose power or water if public supplies were compromised.
As of 2018, most research and published articles concerning autonomous building focus on residential homes.
In 2002, British architects Brenda and Robert Vale said that

It is quite possible in all parts of Australia to construct a 'house with no bills', which would be comfortable without heating and cooling, which would make its own electricity, collect its own water and deal with its own waste...These houses can be built now, using off-the-shelf techniques. It is possible to build a ""house with no bills"" for the same price as a conventional house, but it would be (25%) smaller.


== History ==
In the 1970s, groups of activists and engineers were inspired by the warnings of imminent resource depletion and starvation. In the US a group calling themselves the New Alchemists were famous for the depth of research effort placed in their projects. Using conventional construction techniques, they designed a series of ""bioshelter"" projects, the most famous of which was the Ark Bioshelter community for Prince Edward Island. They published the plans for all of these, with detailed design calculations and blueprints. The Ark used wind based water pumping and electricity, and was self-contained in food production. It had living quarters for people, fish tanks raising tilapia for protein, a greenhouse watered with fish water and a closed loop sewage reclamation system that recycled human waste into sanitized fertilizer for the fish tanks. As of January 2010, the successor organization to the New Alchemists has a web page up as the ""New Alchemy Institute"". The PEI Ark has been abandoned and partially renovated several times.

The 1990s saw the development of Earthships, similar in intent to the Ark project, but organized as a for-profit venture, with construction details published in a series of 3 books by Mike Reynolds. The building material is tires filled with earth. This makes a wall that has large amounts of thermal mass (see earth sheltering). Berms are placed on exposed surfaces to further increase the house's temperature stability. The water system starts with rain water, processed for drinking, then washing, then plant watering, then toilet flushing, and finally black water is recycled again for more plant watering. The cisterns are placed and used as thermal masses. Power, including electricity, heat and water heating, is from solar power.
1990s architects such as William McDonough and Ken Yeang applied environmentally responsible building design to large commercial buildings, such as office buildings, making them largely self-sufficient in energy production. One major bank building (ING's Amsterdam headquarters) in the Netherlands was constructed to be autonomous and artistic as well.


== Advantages ==
As an architect or engineer becomes more concerned with the disadvantages of transportation networks, and dependence on distant resources, their designs tend to include more autonomous elements. The historic path to autonomy was a concern for secure sources of heat, power, water and food. A nearly parallel path toward autonomy has been to start with a concern for environmental impacts, which cause disadvantages.
Autonomous buildings can increase security and reduce environmental impacts by using on-site resources (such as sunlight and rain) that would otherwise be wasted. Autonomy often dramatically reduces the costs and impacts of networks that serve the building, because autonomy short-circuits the multiplying inefficiencies of collecting and transporting resources. Other impacted resources, such as oil reserves and the retention of the local watershed, can often be cheaply conserved by thoughtful designs.
Autonomous buildings are usually energy-efficient in operation, and therefore cost-efficient, for the obvious reason that smaller energy needs are easier to satisfy off-grid. But they may substitute energy production or other techniques to avoid diminishing returns in extreme conservation.
An autonomous structure is not always environmentally friendly. The goal of independence from support systems is associated with, but not identical to, other goals of environmentally responsible green building. However, autonomous buildings also usually include some degree of sustainability through the use of renewable energy and other renewable resources, producing no more greenhouse gases than they consume, and other measures.


== Disadvantages ==
First and fundamentally, independence is a matter of degree, with many choices. For example, eliminating dependence on the electrical grid is relatively easy. In contrast, running an efficient, reliable food source can be a chore.
Living within an autonomous shelter may also require sacrifices in lifestyle or social opportunities. Even the most comfortable and technologically advanced autonomous homes could require alterations of residents' behavior. Some may not welcome the extra chores. The Vails described some clients' experiences as inconvenient, irritating, isolating, or even as an unwanted full-time job. A well-designed building can reduce this issue, but usually at the expense of reduced autonomy.
An autonomous house must be custom-built (or extensively retrofitted) to suit the climate and location. Passive solar techniques, alternative toilet and sewage systems, thermal massing designs, basement battery systems, efficient windowing, and the array of other design tactics require some degree of non-standard construction, added expense, ongoing experimentation and maintenance, and also have an effect on the psychology of the space.


== Systems ==
This section includes some minimal descriptions of methods, to give some feel for such a building's practicality, provide indexes to further information, and give a sense of modern trends.


=== Water ===

There are many methods of collecting and conserving water. Use reduction is cost-effective.
Greywater systems reuse drained wash water to flush toilets or to water lawns and gardens. Greywater systems can halve the water use of most residential buildings; however, they require the purchase of a sump, greywater pressurization pump, and secondary plumbing. Some builders are installing waterless urinals and even composting toilets that completely eliminate water usage in sewage disposal.
The classic solution with minimal life-style changes is using a well. Once drilled, a well-foot requires substantial power. However, advanced well-foots can reduce power usage by twofold or more from older models. Well water can be contaminated in some areas. The Sono arsenic filter eliminates unhealthy arsenic in well water.
However drilling a well is an uncertain activity, with aquifers depleted in some areas.  It can also be expensive.
In regions with sufficient rainfall, it is often more economical to design a building to use rainwater harvesting, with supplementary water deliveries in a drought. Rain water makes excellent soft washwater, but needs antibacterial treatment.  If used for drinking, mineral supplements or mineralization is necessary.Most desert and temperate climates get at least 250 millimetres (9.8 in) of rain per year. This means that a typical one-story house with a greywater system can supply its year-round water needs from its roof alone. In the driest areas, it might require a cistern of 30 cubic metres (7,900 US gal). Many areas average 13 millimetres (0.51 in) of rain per week, and these can use a cistern as small as 10 cubic metres (2,600 US gal).
In many areas, it is difficult to keep a roof clean enough for drinking. To reduce dirt and bad tastes, systems use a metal collecting-roof and a ""roof cleaner"" tank that diverts the first 40 liters.  Cistern water is usually chlorinated, though reverse osmosis systems provide even better quality drinking water.
In the classic Roman house (""Domus""), household water was provided from a cistern (the ""impluvium""), which was a decorative feature of the atrium, the house's main public space. It was fed by downspout tiles from the inward-facing roof-opening (the ""compluvium""). Often water lilies were grown in it to purify the water. Wealthy households often supplemented the rain with a small fountain fed from a city's cistern. The impluvium always had an overflow drain so it could not flood the house.Modern cisterns are usually large plastic tanks.  Gravity tanks on short towers are reliable, so pump repairs are less urgent.  The least expensive bulk cistern is a fenced pond or pool at ground level.
Reducing autonomy reduces the size and expense of cisterns. Many autonomous homes can reduce water use below 10 US gallons (38 L) per person per day, so that in a drought a month of water can be delivered inexpensively via truck. Self-delivery is often possible by installing fabric water tanks that fit the bed of a pick-up truck.
It can be convenient to use the cistern as a heat sink or trap for a heat pump or air conditioning system; however this can make cold drinking water warm, and in drier years may decrease the efficiency of the HVAC system.
Solar stills can efficiently produce drinking water from ditch water or cistern water, especially high-efficiency multiple effect humidification designs, which separate the evaporator(s) and condenser(s).
New technologies, like reverse osmosis can create unlimited amounts of pure water from polluted water, ocean water, and even from humid air. Watermakers are available for yachts that convert seawater and electricity into potable water and brine. Atmospheric water generators extract moisture from dry desert air and filter it to pure water.


=== Sewage ===


==== Resource ====

Composting toilets use bacteria to decompose human feces into useful, odourless, sanitary compost. The process is sanitary because soil bacteria eat the human pathogens as well as most of the mass of the waste.  Nevertheless, most health authorities forbid direct use of ""humanure"" for growing food. The risk is microbial and viral contamination.  In a dry composting toilet, the waste is evaporated or digested to gas (mostly carbon dioxide) and vented, so a toilet produces only a few pounds of compost every six months.  To control the odor, modern toilets use a small fan to keep the toilet under negative pressure, and exhaust the gasses to a vent pipe.Some home sewage treatment systems use biological treatment, usually beds of plants and aquaria, that absorb nutrients and bacteria and convert greywater and sewage to clear water. This odor- and color-free reclaimed water can be used to flush toilets and water outside plants. When tested, it approaches standards for potable water. In climates that freeze, the plants and aquaria need to be kept in a small greenhouse space. Good systems need about as much care as a large aquarium.
Electric incinerating toilets turn excrement into a small amount of ash. They are cool to the touch, have no water and no pipes, and require an air vent in a wall. They are used in remote areas where use of septic tanks is limited, usually to reduce nutrient loads in lakes.
NASA's bioreactor is an extremely advanced biological sewage system. It can turn sewage into air and water through microbial action. NASA plans to use it in the manned Mars mission. Another method is NASA's urine-to-water distillation system.
A big disadvantage of complex biological sewage treatment systems is that if the house is empty, the sewage system biota may starve to death.


==== Waste ====
Sewage handling is essential for public health. Many diseases are transmitted by poorly functioning sewage systems.
The standard system is a tiled leach field combined with a septic tank. The basic idea is to provide a small system with primary sewage treatment. Sludge settles to the bottom of the septic tank, is partially reduced by anaerobic digestion, and fluid is dispersed in the leach field. The leach field is usually under a yard growing grass. Septic tanks can operate entirely by gravity, and if well managed, are reasonably safe.
Septic tanks have to be pumped periodically by a vacuum truck to eliminate non reducing solids. Failure to pump a septic tank can cause overflow that damages the leach field, and contaminates ground water. Septic tanks may also require some lifestyle changes, such as not using garbage disposals, minimizing fluids flushed into the tank, and minimizing nondigestible solids flushed into the tank. For example, septic safe toilet paper is recommended.
However, septic tanks remain popular because they permit standard plumbing fixtures, and require few or no lifestyle sacrifices.
Composting or packaging toilets make it economical and sanitary to throw away sewage as part of the normal garbage collection service. They also reduce water use by half, and eliminate the difficulty and expense of septic tanks. However, they require the local landfill to use sanitary practices.
Incinerator systems are quite practical. The ashes are biologically safe, and less than 1/10 the volume of the original waste, but like all incinerator waste, are usually classified as hazardous waste.
Traditional methods of sewage handling include pit toilets, latrines, and outhouses. These can be safe, inexpensive and practical. They are still used in many regions.


=== Storm drains ===
Drainage systems are a crucial compromise between human habitability and a secure, sustainable watershed. Paved areas and lawns or turf do not allow much precipitation to filter through the ground to recharge aquifers. They can cause flooding and damage in neighbourhoods, as the water flows over the surface towards a low point.
Typically, elaborate, capital-intensive storm sewer networks are engineered to deal with stormwater. In some cities, such as the Victorian era London sewers or much of the old City of Toronto, the storm water system is combined with the sanitary sewer system. In the event of heavy precipitation, the load on the sewage treatment plant at the end of the pipe becomes too great to handle and raw sewage is dumped into holding tanks, and sometimes into surface water.
Autonomous buildings can address precipitation in a number of ways:
If a water-absorbing swale for each yard is combined with permeable concrete streets, storm drains can be omitted from the neighbourhood. This can save more than $800 per house (1970s) by eliminating storm drains. One way to use the savings is to purchase larger lots, which permits more amenities at the same cost. Permeable concrete is an established product in warm climates, and in development for freezing climates. In freezing climates, the elimination of storm drains can often still pay for enough land to construct swales (shallow water collecting ditches) or water impeding berms instead. This plan provides more land for homeowners and can offer more interesting topography for landscaping.
A green roof captures precipitation and uses the water to grow plants. It can be built into a new building or used to replace an existing roof.


=== Electricity ===

Since electricity is an expensive utility, the first step towards autonomy is to design a house and lifestyle to reduce demand. LED lights, laptop computers and gas-powered refrigerators save electricity, although gas-powered refrigerators are not very efficient. There are also superefficient electric refrigerators, such as those produced by the Sun Frost company, some of which use only about half as much electricity as a mass-market energy star-rated refrigerator.
Using a solar roof, solar cells can provide electric power. Solar roofs can be more cost-effective than retrofitted solar power, because buildings need roofs anyway. Modern solar cells last about 40 years, which makes them a reasonable investment in some areas. At a sufficient angle, solar cells are cleaned by run-off rain water and therefore have almost no life-style impact.
However, many areas have long winter nights or dark cloudy days. In these climates, a solar installation might not pay for itself or large battery storage systems are necessary to achieve electric self-sufficiency. In stormy or windy climates, wind generators can replace or significantly supplement solar power. The average autonomous house needs only one small wind turbine, 5 metres or less in diameter. On a 30-metre (100-foot) tower, this turbine can provide enough power to supplement solar power on cloudy days. Commercially available wind turbines use sealed, one-moving-part AC generators and passive, self-feathering blades for years of operation without service.
The main advantage of wind power is that larger wind turbines have a lower per-watt cost than solar cells, provided there is wind. However, location is critical. Just as some locations lack sun for solar cells, many areas lack enough wind to make a turbine pay for itself. In the Great Plains of the United States, a 10-metre (33-foot) turbine can supply enough energy to heat and cool a well-built all-electric house. Economic use in other areas requires research, and possibly a site-survey.Some sites have access to a stream with a change in elevation. These sites can use small hydropower systems to generate electricity. If the difference in elevation is above 30 metres (100 feet), and the stream runs in all seasons, this can provide continuous power with a small, inexpensive installation. Lower changes of elevation require larger installations or dams, and can be less efficient. Clogging at the turbine intake can be a practical problem. The usual solution is a small pool and waterfall (a penstock) to carry away floating debris. Another solution is to utilize a turbine that resists debris, such as a Gorlov helical turbine or Ossberger turbine.
During times of low demand, excess power can be stored in batteries for future use. However, batteries need to be replaced every few years. In many areas, battery expenses can be eliminated by attaching the building to the electric power grid and operating the power system with net metering. Utility permission is required, but such cooperative generation is legally mandated in some areas (for example, California).A grid-based building is less autonomous, but more economical and sustainable with fewer lifestyle sacrifices. In rural areas the grid's cost and impacts can be reduced by using single-wire earth return systems (for example, the MALT-system).
In areas that lack access to the grid, battery size can be reduced with a generator to recharge the batteries during energy droughts such as extended fogs. Auxiliary generators are usually run from propane, natural gas, or sometimes diesel. An hour of charging usually provides a day of operation. Modern residential chargers permit the user to set the charging times, so the generator is quiet at night. Some generators automatically test themselves once per week.Recent advances in passively stable magnetic bearings may someday permit inexpensive storage of power in a flywheel in a vacuum. Research groups like Canada's Ballard Power Systems are also working to develop a ""regenerative fuel cell"", a device that can generate hydrogen and oxygen when power is available, and combine these efficiently when power is needed.
Earth batteries tap electric currents in the earth called telluric current. They can be installed anywhere in the ground. They provide only low voltages and current. They were used to power telegraphs in the 19th century. As appliance efficiencies increase, they may become practical.
Microbial fuel cells and thermoelectric generators allow electricity to be generated from biomass. The plant can be dried, chopped and converted or burned as a whole, or it can be left alive so that waste saps from the plant can be converted by bacteria.


=== Heating ===

Most autonomous buildings are designed to use insulation, thermal mass and passive solar heating and cooling. Examples of these are trombe walls and other technologies as skylights.
Passive solar heating can heat most buildings in even the mild and chilly climates. In colder climates, extra construction costs can be as little as 15% more than new, conventional buildings. In warm climates, those having less than two weeks of frosty nights per year, there is no cost impact.
The basic requirement for passive solar heating is that the solar collectors must face the prevailing sunlight (south in the Northern Hemisphere, north in the Southern Hemisphere), and the building must incorporate thermal mass to keep it warm in the night.
A recent, somewhat experimental solar heating system ""Annualized geo solar heating"" is practical even in regions that get little or no sunlight in winter. It uses the ground beneath a building for thermal mass. Precipitation can carry away the heat, so the ground is shielded with 6 m skirts of plastic insulation. The thermal mass of this system is sufficiently inexpensive and large that it can store enough summer heat to warm a building for the whole winter, and enough winter cold to cool the building in summer.
In annualized geo solar systems, the solar collector is often separate from (and hotter or colder than) the living space. The building may actually be constructed from insulation, for example, straw-bale construction. Some buildings have been aerodynamically designed so that convection via ducts and interior spaces eliminates any need for electric fans.
A more modest ""daily solar"" design is very practical.  For example, for about a 15% premium in building costs, the Passivhaus building codes in Europe use high performance insulating windows, R-30 insulation, HRV ventilation, and a small thermal mass. With modest changes in the building's position, modern krypton- or argon-insulated windows permit normal-looking windows to provide passive solar heat without compromising insulation or structural strength. If a small heater is available for the coldest nights, a slab or basement cistern can inexpensively provide the required thermal mass. Passivhaus building codes, in particular, bring unusually good interior air quality, because the buildings change the air several times per hour, passing it through a heat exchanger to keep heat inside.
In all systems, a small supplementary heater increases personal security and reduces lifestyle impacts for a small reduction of autonomy. The two most popular heaters for ultra-high-efficiency houses are a small heat pump, which also provides air conditioning, or a central hydronic (radiator) air heater with water recirculating from the water heater.  Passivhaus designs usually integrate the heater with the ventilation system.
Earth sheltering and windbreaks can also reduce the absolute amount of heat needed by a building. Several feet below the earth, temperature ranges from 4 °C (39 °F) in North Dakota to 26 °C (79 °F), in Southern Florida. Wind breaks reduce the amount of heat carried away from a building.
Rounded, aerodynamic buildings also lose less heat.
An increasing number of commercial buildings use a combined cycle with cogeneration to provide heating, often water heating, from the output of a natural gas reciprocating engine, gas turbine or stirling electric generator.Houses designed to cope with interruptions in civil services generally incorporate a wood stove, or heat and power from diesel fuel or bottled gas, regardless of their other heating mechanisms.
Electric heaters and electric stoves may provide pollution-free heat (depending on the power source), but use large amounts of electricity. If enough electricity is provided by solar panels, wind turbines, or other means, then electric heaters and stoves become a practical autonomous design.


=== Water heating ===

Hot water heat recycling units recover heat from water drain lines. They increase a building's autonomy by decreasing the heat or fuel used to heat water.  They are attractive because they have no lifestyle changes.
Current practical, comfortable domestic water-heating systems combine a solar preheating system with a thermostatic gas-powered flow-through heater, so that the temperature of the water is consistent, and the amount is unlimited. This reduces life-style impacts at some cost in autonomy.
Solar water heaters can save large amounts of fuel. Also, small changes in lifestyle, such as doing laundry, dishes and bathing on sunny days, can greatly increase their efficiency. Pure solar heaters are especially useful for laundries, swimming pools and external baths, because these can be scheduled for use on sunny days.
The basic trick in a solar water heating system is to use a well-insulated holding tank. Some systems are vacuum- insulated, acting something like large thermos bottles. The tank is filled with hot water on sunny days, and made available at all times. Unlike a conventional tank water heater, the tank is filled only when there is sunlight. Good storage makes a smaller, higher-technology collector feasible. Such collectors can use relatively exotic technologies, such as vacuum insulation, and reflective concentration of sunlight.
Cogeneration systems produce hot water from waste heat.  They usually get the heat from the exhaust of a generator or fuel cell.
Heat recycling, cogeneration and solar pre-heating can save 50–75% of the gas otherwise used.  Also, some combinations provide redundant reliability by having several sources of heat.
Some authorities advocate replacing bottled gas or natural gas with biogas. However, this is usually impractical unless live-stock are on-site.  The wastes of a single family are usually insufficient to produce enough methane for anything more than small amounts of cooking.


=== Cooling ===
Annualized geo solar buildings often have buried, sloped water-tight skirts of insulation that extend 6 metres (20 ft) from the foundations, to prevent heat leakage between the earth used as thermal mass, and the surface.
Less dramatic improvements are possible. Windows can be shaded in summer. Eaves can be overhung to provide the necessary shade. These also shade the walls of the house, reducing cooling costs.
Another trick is to cool the building's thermal mass at night, perhaps with a whole-house fan and then cool the building from the thermal mass during the day. It helps to be able to route cold air from a sky-facing radiator (perhaps an air heating solar collector with an alternate purpose) or evaporative cooler directly through the thermal mass. On clear nights, even in tropical areas, sky-facing radiators can cool below freezing.
If a circular building is aerodynamically smooth, and cooler than the ground, it can be passively cooled by the ""dome effect.""  Many installations have reported that a reflective or light-colored dome induces a local vertical heat-driven vortex that sucks cooler overhead air downward into a dome if the dome is vented properly (a single overhead vent, and peripheral vents). Some people have reported a temperature differential as high as 8 °C (15 °F) between the inside of the dome and the outside. Buckminster Fuller discovered this effect with a simple house design adapted from a grain silo, and adapted his Dymaxion house and geodesic domes to use it.
Refrigerators and air conditioners operating from the waste heat of a diesel engine exhaust, heater flue or solar collector are entering use. These use the same principles as a gas refrigerator. Normally, the heat from a flue powers an ""absorptive chiller"". The cold water or brine from the chiller is used to cool air or a refrigerated space.
Cogeneration is popular in new commercial buildings. In current cogeneration systems small gas turbines or stirling engines powered from natural gas produce electricity and their exhaust drives an absorptive chiller.
A truck trailer refrigerator operating from the waste heat of a tractor's diesel exhaust was demonstrated by NRG Solutions, Inc. NRG developed a hydronic ammonia gas heat exchanger and vaporizer, the two essential new, not commercially available components of a waste heat driven refrigerator.
A similar scheme (multiphase cooling) can be by a multistage evaporative cooler. The air is passed through a spray of salt solution to dehumidify it, then through a spray of water solution to cool it, then another salt solution to dehumidify it again. The brine has to be regenerated, and that can be done economically with a low-temperature solar still. Multiphase evaporative coolers can lower the air's temperature by 50 °F (28 °C), and still control humidity. If the brine regenerator uses high heat, they also partially sterilise the air.
If enough electric power is available, cooling can be provided by conventional air conditioning using a heat pump.


=== Food production ===
Food production has often been included in historic autonomous projects to provide security.
Skilled, intensive gardening can support an adult from as little as 100 square meters of land per person,
possibly requiring the use of organic farming and aeroponics. Some proven intensive, low-effort food-production systems include urban gardening (indoors and outdoors). Indoor cultivation may be set up using hydroponics, while outdoor cultivation may be done using permaculture, forest gardening, no-till farming, and do nothing farming.
Greenhouses are also sometimes included. Sometimes they are also outfitted with irrigation systems or heat sink-systems which can respectively irrigate the plants or help to store energy from the sun and redistribute it at night (when the greenhouses starts to cool down).


== See also ==


== Notes ==


== External links ==
The Buckminster Fuller Institute is still in existence. B. Fuller left thousands of pages of notes to the university where he last taught.
There is a section on Autonomous Houses in the Reality Sculptors wiki, including links to a mailing list which frequently discusses autonomous design considerations.
Designs for a geodesic dome version of an Autonomous House can be found at reality.sculptors.com.
""Wind Power for Home and Business"" by Paul Gipe
An opinion piece by Brenda and Robert Vale
The Cropthorne House - notes on design and comparison with the Vales' Southwell House
Bad End 2 - 21st Century Hobbit Hole - precast concrete in home construction
Off-grid.net
Self Sufficiency Guide
Self Sufficient Living
GreenSpec","pandas(index=116, _1=116, text='an autonomous building is a building designed to be operated independently from infrastructural support services such as the electric power grid, gas grid, municipal water systems, sewage treatment systems, storm drains, communication services, and in some cases, public roads. advocates of autonomous building describe advantages that include reduced environmental impacts, increased security, and lower costs of ownership. some cited advantages satisfy tenets of green building, not independence per se (see below). off-grid buildings often rely very little on civil services and are therefore safer and more comfortable during civil disaster or military attacks. for example, off-grid buildings would not lose power or water if public supplies were compromised. as of 2018, most research and published articles concerning autonomous building focus on residential homes. in 2002, british architects brenda and robert vale said that  it is quite possible in all parts of australia to construct a \'house with no bills\', which would be comfortable without heating and cooling, which would make its own electricity, collect its own water and deal with its own waste...these houses can be built now, using off-the-shelf techniques. it is possible to build a ""house with no bills"" for the same price as a conventional house, but it would be (25%) smaller.   == history == in the 1970s, groups of activists and engineers were inspired by the warnings of imminent resource depletion and starvation. in the us a group calling themselves the new alchemists were famous for the depth of research effort placed in their projects. using conventional construction techniques, they designed a series of ""bioshelter"" projects, the most famous of which was the ark bioshelter community for prince edward island. they published the plans for all of these, with detailed design calculations and blueprints. the ark used wind based water pumping and electricity, and was self-contained in food production. it had living quarters for people, fish tanks raising tilapia for protein, a greenhouse watered with fish water and a closed loop sewage reclamation system that recycled human waste into sanitized fertilizer for the fish tanks. as of january 2010, the successor organization to the new alchemists has a web page up as the ""new alchemy institute"". the pei ark has been abandoned and partially renovated several times.  the 1990s saw the development of earthships, similar in intent to the ark project, but organized as a for-profit venture, with construction details published in a series of 3 books by mike reynolds. the building material is tires filled with earth. this makes a wall that has large amounts of thermal mass (see earth sheltering). berms are placed on exposed surfaces to further increase the house\'s temperature stability. the water system starts with rain water, processed for drinking, then washing, then plant watering, then toilet flushing, and finally black water is recycled again for more plant watering. the cisterns are placed and used as thermal masses. power, including electricity, heat and water heating, is from solar power. 1990s architects such as william mcdonough and ken yeang applied environmentally responsible building design to large commercial buildings, such as office buildings, making them largely self-sufficient in energy production. one major bank building (ing\'s amsterdam headquarters) in the netherlands was constructed to be autonomous and artistic as well.   == advantages == as an architect or engineer becomes more concerned with the disadvantages of transportation networks, and dependence on distant resources, their designs tend to include more autonomous elements. the historic path to autonomy was a concern for secure sources of heat, power, water and food. a nearly parallel path toward autonomy has been to start with a concern for environmental impacts, which cause disadvantages. autonomous buildings can increase security and reduce environmental impacts by using on-site resources (such as sunlight and rain) that would otherwise be wasted. autonomy often dramatically reduces the costs and impacts of networks that serve the building, because autonomy short-circuits the multiplying inefficiencies of collecting and transporting resources. other impacted resources, such as oil reserves and the retention of the local watershed, can often be cheaply conserved by thoughtful designs. autonomous buildings are usually energy-efficient in operation, and therefore cost-efficient, for the obvious reason that smaller energy needs are easier to satisfy off-grid. but they may substitute energy production or other techniques to avoid diminishing returns in extreme conservation. an autonomous structure is not always environmentally friendly. the goal of independence from support systems is associated with, but not identical to, other goals of environmentally responsible green building. however, autonomous buildings also usually include some degree of sustainability through the use of renewable energy and other renewable resources, producing no more greenhouse gases than they consume, and other measures.   == disadvantages == first and fundamentally, independence is a matter of degree, with many choices. for example, eliminating dependence on the electrical grid is relatively easy. in contrast, running an efficient, reliable food source can be a chore. living within an autonomous shelter may also require sacrifices in lifestyle or social opportunities. even the most comfortable and technologically advanced autonomous homes could require alterations of residents\' behavior. some may not welcome the extra chores. the vails described some clients\' experiences as inconvenient, irritating, isolating, or even as an unwanted full-time job. a well-designed building can reduce this issue, but usually at the expense of reduced autonomy. an autonomous house must be custom-built (or extensively retrofitted) to suit the climate and location. passive solar techniques, alternative toilet and sewage systems, thermal massing designs, basement battery systems, efficient windowing, and the array of other design tactics require some degree of non-standard construction, added expense, ongoing experimentation and maintenance, and also have an effect on the psychology of the space.   == systems == this section includes some minimal descriptions of methods, to give some feel for such a building\'s practicality, provide indexes to further information, and give a sense of modern trends. food production has often been included in historic autonomous projects to provide security. skilled, intensive gardening can support an adult from as little as 100 square meters of land per person, possibly requiring the use of organic farming and aeroponics. some proven intensive, low-effort food-production systems include urban gardening (indoors and outdoors). indoor cultivation may be set up using hydroponics, while outdoor cultivation may be done using permaculture, forest gardening, no-till farming, and do nothing farming. greenhouses are also sometimes included. sometimes they are also outfitted with irrigation systems or heat sink-systems which can respectively irrigate the plants or help to store energy from the sun and redistribute it at night (when the greenhouses starts to cool down).   == see also ==   == notes ==   == external links == the buckminster fuller institute is still in existence. b. fuller left thousands of pages of notes to the university where he last taught. there is a section on autonomous houses in the reality sculptors wiki, including links to a mailing list which frequently discusses autonomous design considerations. designs for a geodesic dome version of an autonomous house can be found at reality.sculptors.com. ""wind power for home and business"" by paul gipe an opinion piece by brenda and robert vale the cropthorne house - notes on design and comparison with the vales\' southwell house bad end 2 - 21st century hobbit hole - precast concrete in home construction off-grid.net self sufficiency guide self sufficient living greenspec')"
117,"The Eden Project (Cornish: Edenva) is a visitor attraction in Cornwall, England, UK. The project is located in a reclaimed china clay pit, located 2 km (1.2 mi) from the town of St Blazey and 5 km (3 mi) from the larger town of St Austell.The complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. The biomes consist of hundreds of hexagonal and pentagonal ethylene tetrafluoroethylene (ETFE) inflated cells supported by Geodesic tubular steel domes. The largest of the two biomes simulates a rainforest environment (and is the largest indoor rainforest in the world) and the second, a Mediterranean environment. The attraction also has an outside botanical garden which is home to many plants and wildlife native to Cornwall and the UK in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage.
There are plans to build an Eden Project North in the seaside town of Morecambe, Lancashire, with a focus on the marine environment.


== History ==

The clay pit in which the project is sited was in use for over 160 years. In 1981, the pit was used by the BBC as the planet surface of Magrathea in the TV series the Hitchhiker's Guide to the Galaxy. By the mid-1990s the pit was all but exhausted.The initial idea for the project dates back to 1996, with construction beginning in 1998. The work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits 15 m (49 ft) below the water table.The first part of the Eden Project, the visitor centre, opened to the public in May 2000. The first plants began arriving in September of that year, and the full site opened on 17 March 2001.
To counter criticism from environmental groups, the Eden Project committed to investigate a rail link to the site. The rail link was never built, and car parking on the site is still funded from revenue generated from general admission ticket sales, meaning that those who do not drive to the site also pay for the car park.
The Eden Project was used as a filming location for the 2002 James Bond film, Die Another Day. On 2 July 2005 The Eden Project hosted the ""Africa Calling"" concert of the Live 8 concert series.  It has also provided some plants for the British Museum's Africa garden.
In 2005, the Project launched ""A Time of Gifts"" for the winter months, November to February. This features an ice rink covering the lake, with a small café/bar attached, as well as a Christmas market. Cornish choirs regularly perform in the biomes.
In 2007, the Eden Project campaigned unsuccessfully for £50 million in Big Lottery Fund money for a proposed desert biome. It received just 12.07% of the votes, the lowest for the four projects being considered. As part of the campaign, the Eden Project invited people all over Cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to Cornwall.In December 2009, much of the project, including both greenhouses, became available to navigate through Google Street View.
The Eden Trust revealed a trading loss of £1.3 million for 2012–13, on a turnover of £25.4 million. The Eden Project had posted a surplus of £136,000 for the previous year. In 2014 Eden accounts showed a surplus of £2 million.The World Pasty Championships have been held at the Eden Project since 2012, an international competition to find the best Cornish pasties and other pasty-type savoury snacks.The Eden Project is said to have contributed over £1 billion to the Cornish economy. In 2016, Eden became home to Europe's second largest Redwood forest (after the Giants Grove at Birr Castle, Birr Castle Ireland giantsgrove.ie)when forty saplings of coast redwoods, Sequoia sempervirens, which could live for 4,000 years and reach 115 metres in height, were planted there.The Eden Project received 1,010,095 visitors in 2019.In December 2020 the project was closed after heavy rain caused several landslips at the site. Managers at the site are assessing the damage and will announce when the project will reopen on the company's website. Reopening became irrelevant as Covid lockdown measures in the UK indefinitely closed the venue from early 2021.


== Design and construction ==
The project was conceived by Tim Smit and designed by architecture firm Grimshaw Architects and engineering firm Anthony Hunt and Associates (now part of Sinclair Knight Merz). Davis Langdon carried out the project management, Sir Robert McAlpine and Alfred McAlpine did the construction, MERO designed and built the biomes, and Arup was the services engineer, economic consultant, environmental engineer and transportation engineer. Land Use Consultants led the masterplan and landscape design. The project took 2½ years to construct and opened to the public on 17 March 2001.


== Site ==


=== Layout ===

Once into the attraction, there is a meandering path with views of the two biomes, planted landscapes, including vegetable gardens, and sculptures that include a giant bee and previously The WEEE Man (removed in 2016), a towering figure made from old electrical appliances and was meant to represent the average electrical waste used by one person in a lifetime.


=== Biomes ===
At the bottom of the pit are two covered biomes:
The Tropical Biome, covers 1.56 ha (3.9 acres) and measures 55 m (180 ft) high, 100 m (328 ft) wide, and 200 m (656 ft) long. It is used for tropical plants, such as fruiting banana plants, coffee, rubber and giant bamboo, and is kept at a tropical temperature and moisture level.

The Mediterranean Biome covers 0.654 ha (1.6 acres) and measures 35 m (115 ft) high, 65 m (213 ft) wide, and 135 m (443 ft) long. It houses familiar warm temperate and arid plants such as olives and grape vines and various sculptures.
The Outdoor Gardens represent the temperate regions of the world with plants such as tea, lavender, hops, hemp, and sunflowers, as well as local plant species.
The covered biomes are constructed from a tubular steel (hex-tri-hex) with mostly hexagonal external cladding panels made from the thermoplastic ETFE. Glass was avoided due to its weight and potential dangers. The cladding panels themselves are created from several layers of thin UV-transparent ETFE film, which are sealed around their perimeter and inflated to create a large cushion. The resulting cushion acts as a thermal blanket to the structure. The ETFE material is resistant to most stains, which simply wash off in the rain. If required, cleaning can be performed by abseilers. Although the ETFE is susceptible to punctures, these can be easily fixed with ETFE tape. The structure is completely self-supporting, with no internal supports, and takes the form of a geodesic structure. The panels vary in size up to 9 m (29.5 ft) across, with the largest at the top of the structure.
The ETFE technology was supplied and installed by the firm Vector Foiltec, which is also responsible for ongoing maintenance of the cladding. The steel spaceframe and cladding package (with Vector Foiltec as ETFE subcontractor) was designed, supplied and installed by MERO (UK) PLC, who also jointly developed the overall scheme geometry with the architect, Nicholas Grimshaw & Partners.
The entire build project was managed by McAlpine Joint Venture.

		
		
		


=== The Core ===

The Core is the latest addition to the site and opened in September 2005. It provides the Eden Project with an education facility, incorporating classrooms and exhibition spaces designed to help communicate Eden's central message about the relationship between people and plants. Accordingly, the building has taken its inspiration from plants, most noticeable in the form of the soaring timber roof, which gives the building its distinctive shape.
Grimshaw developed the geometry of the copper-clad roof in collaboration with a sculptor, Peter Randall-Page, and Mike Purvis of structural engineers SKM Anthony Hunts. It is derived from phyllotaxis, which is the mathematical basis for nearly all plant growth; the ""opposing spirals"" found in many plants such as the seeds in a sunflower's head, pine cones and pineapples. The copper was obtained from traceable sources, and the Eden Project is working with Rio Tinto Group to explore the possibility of encouraging further traceable supply routes for metals, which would enable users to avoid metals mined unethically. The services and acoustic, mechanical, and electrical engineering design was carried out by Buro Happold.


==== Art at The Core ====

The Core is also home to art exhibitions throughout the year. A permanent installation entitled Seed, by Peter Randall-Page, occupies the anteroom. Seed is a large, 70 tonne egg-shaped stone installation standing some 13 feet (4.0 m) tall and displaying a complex pattern of protrusions that are based upon the geometric and mathematical principles that underlie plant growth.


== Environmental aspects ==
The biomes provide diverse growing conditions, and many plants are on display.
The Eden Project includes environmental education focusing on the interdependence of plants and people; plants are labelled with their medicinal uses. The massive amounts of water required to create the humid conditions of the Tropical Biome, and to serve the toilet facilities, are all sanitised rain water that would otherwise collect at the bottom of the quarry. The only mains water used is for hand washing and for cooking. The complex also uses Green Tariff Electricity – the energy comes from one of the many wind turbines in Cornwall, which were among the first in Europe.
In December 2010 the Eden Project received permission to build a geothermal electricity plant which will generate approx 4MWe, enough to supply Eden and about 5000 households. The project will involve geothermal heating as well as geothermal electricity. Cornwall Council and the European Union came up with the greater part of £16.8m required to start the project.  First a well will be sunk nearly 3 miles (4.5 km) into the granite crust underneath Eden.  Funding has been secured and drilling is set to begin in summer 2020.  Eden co-founder, Sir Tim Smit said, ""Since we began, Eden has had a dream that the world should be powered by renewable energy. The sun can provide massive solar power and the wind has been harnessed by humankind for thousands of years, but because both are intermittent and battery technology cannot yet store all we need there is a gap.  We believe the answer lies beneath our feet in the heat underground that can be accessed by drilling technology that pumps water towards the centre of the Earth and brings it back up superheated to provide us with heat and electricity"".


== Other projects ==


=== Eden Project North ===
In 2018, the Eden Project revealed its design for a new version of the project, located on the seafront in Morecambe, Lancashire. There will be biomes shaped like mussels and a focus on the marine environment. There will also be reimagined lidos, gardens, performance spaces, immersive experiences and observatories.Grimshaw are the architects for the project, which is expected to cost £80 million. The project is a partnership with the Lancashire Enterprise Partnership, Lancaster University, Lancashire County Council and Lancaster City Council. In December 2018, the four local partners agreed to provide £1 million to develop the idea, which will allow the development of an outline planning application for the project. It is expected that there will be 500 jobs created and 8,000 visitors a day to the site.


=== South Downs ===
In 2020 Eastbourne Borough Council and the Eden Project announced a joint project to explore the viability of a new Eden site in the South Downs National Park.


== Eden Sessions ==
Since 2002, the Project has hosted a series of musical performances, called the Eden Sessions. Artists have included Amy Winehouse, James Morrison, Muse, Lily Allen, Snow Patrol, Pulp, Brian Wilson, and The Magic Numbers. Oasis were also set to play in the summer of 2008, but the concert was postponed because Noel Gallagher was unable to perform after breaking three ribs in a stage invasion incident several weeks before. The concert was instead played in the summer of 2009.


=== Performance headliners ===
2008: The Verve, Kaiser Chiefs, and KT Tunstall.
2010: Mika, Jack Johnson, Mojave 3, Doves, Paolo Nutini, Mumford & Sons, and Martha Wainwright.
2011: The Flaming Lips, Primal Scream, Pendulum, Fleet Foxes, and Brandon Flowers with support from The Horrors, The Go! Team, OK Go, Villagers, and The Bees.
2012: Tim Minchin, Example, Frank Turner, Chase & Status, Plan B, Blink-182, Noah and the Whale, and The Vaccines.
2013: Kaiser Chiefs, Jessie J, Eddie Izzard, Sigur Rós, and The xx.
2014: Dizzee Rascal, Skrillex, Pixar in Concert, Ellie Goulding, and Elbow.
2015: Paolo Nutini, Elton John, Paloma Faith, Motörhead, The Stranglers, Spandau Ballet, and Ben Howard.
2016: Lionel Richie, Tom Jones, PJ Harvey, Manic Street Preachers, and Jess Glynne.
2017: Bastille, Madness, Royal Blood, Blondie, Van Morrison, Bryan Adams, and Foals.
2018: Gary Barlow, Massive Attack, A Beautiful Day Out, Ben Howard, Queens of the Stone Age, Jack Johnson, Björk.
2019: Stereophonics, Nile Rogers & Chic, The 100th session, Liam Gallagher, The Chemical Brothers, Snow Patrol, Kylie Minogue.My Chemical Romance were to play the 2020 Eden Sessions in their first UK shows after a six-year hiatus, as part of their reunion tour. The 2020 Sessions were postponed until 2021 due to the COVID-19 pandemic and the closing of the Eden Project.The 2021 sessions will be headlined by Diana Ross, Lionel Richie, The Script and My Chemical Romance.


== In the media ==
Robin Kewell (Ed.): Eden: The Inside Story. St Austell n.d.: The Eden Project. DVD.
The Eden Radio Project. Every Thursday between 5:30 and 7 p.m. on Radio St Austell Bay.
Die Another Day: Graves' diamond mine
Trees A Crowd Podcast: An Interview between the Project's head of interpretation, Dr Jo Elworthy, and David Oakes was released on 18 November.
Andy Day's CBeebies TV show Andy's Aquatic Adventures has had scenes filmed in the Eden Project.


== See also ==


== References ==


== Further reading ==
Philip McMillan Browse, Louise Frost, Alistair Griffiths: Plants of Eden (Eden Project). Penzance 2001: Alison Hodge.
Richard Mabey: Fencing Paradise: Exploring the Gardens of Eden London 2005: Eden Project Books. ISBN 1-903919-31-2
Hugh Pearman, Andrew Whalley: The Architecture of Eden. With a foreword by Sir Nicholas Grimshaw. London 2003: Eden Project Books. ISBN 1-903919-15-0
Eden Team (Ed.): Eden Project: The Guide 2008/9. London 2008: Eden Project Books.
Tim Smit: Eden. London 2001: Bantam Press.
Paul Spooner: The Revenge of the Green Planet: The Eden Project Book of Amazing Facts About Plants. London 2003: Eden Project Books.
Alan Titchmarsh: The Eden Project. United Kingdom: Acorn Media, 2006. OCLC 225403941.


== External links ==
Official website
Eden Sessions Website—Official site for live gigs","pandas(index=117, _1=117, text='the eden project (cornish: edenva) is a visitor attraction in cornwall, england, uk. the project is located in a reclaimed china clay pit, located 2 km (1.2 mi) from the town of st blazey and 5 km (3 mi) from the larger town of st austell.the complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. the biomes consist of hundreds of hexagonal and pentagonal ethylene tetrafluoroethylene (etfe) inflated cells supported by geodesic tubular steel domes. the largest of the two biomes simulates a rainforest environment (and is the largest indoor rainforest in the world) and the second, a mediterranean environment. the attraction also has an outside botanical garden which is home to many plants and wildlife native to cornwall and the uk in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage. there are plans to build an eden project north in the seaside town of morecambe, lancashire, with a focus on the marine environment.   == history ==  the clay pit in which the project is sited was in use for over 160 years. in 1981, the pit was used by the bbc as the planet surface of magrathea in the tv series the hitchhiker\'s guide to the galaxy. by the mid-1990s the pit was all but exhausted.the initial idea for the project dates back to 1996, with construction beginning in 1998. the work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits 15 m (49 ft) below the water table.the first part of the eden project, the visitor centre, opened to the public in may 2000. the first plants began arriving in september of that year, and the full site opened on 17 march 2001. to counter criticism from environmental groups, the eden project committed to investigate a rail link to the site. the rail link was never built, and car parking on the site is still funded from revenue generated from general admission ticket sales, meaning that those who do not drive to the site also pay for the car park. the eden project was used as a filming location for the 2002 james bond film, die another day. on 2 july 2005 the eden project hosted the ""africa calling"" concert of the live 8 concert series.  it has also provided some plants for the british museum\'s africa garden. in 2005, the project launched ""a time of gifts"" for the winter months, november to february. this features an ice rink covering the lake, with a small café/bar attached, as well as a christmas market. cornish choirs regularly perform in the biomes. in 2007, the eden project campaigned unsuccessfully for £50 million in big lottery fund money for a proposed desert biome. it received just 12.07% of the votes, the lowest for the four projects being considered. as part of the campaign, the eden project invited people all over cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to cornwall.in december 2009, much of the project, including both greenhouses, became available to navigate through google street view. the eden trust revealed a trading loss of £1.3 million for 2012–13, on a turnover of £25.4 million. the eden project had posted a surplus of £136,000 for the previous year. in 2014 eden accounts showed a surplus of £2 million.the world pasty championships have been held at the eden project since 2012, an international competition to find the best cornish pasties and other pasty-type savoury snacks.the eden project is said to have contributed over £1 billion to the cornish economy. in 2016, eden became home to europe\'s second largest redwood forest (after the giants grove at birr castle, birr castle ireland giantsgrove.ie)when forty saplings of coast redwoods, sequoia sempervirens, which could live for 4,000 years and reach 115 metres in height, were planted there.the eden project received 1,010,095 visitors in 2019.in december 2020 the project was closed after heavy rain caused several landslips at the site. managers at the site are assessing the damage and will announce when the project will reopen on the company\'s website. reopening became irrelevant as covid lockdown measures in the uk indefinitely closed the venue from early 2021.   == design and construction == the project was conceived by tim smit and designed by architecture firm grimshaw architects and engineering firm anthony hunt and associates (now part of sinclair knight merz). davis langdon carried out the project management, sir robert mcalpine and alfred mcalpine did the construction, mero designed and built the biomes, and arup was the services engineer, economic consultant, environmental engineer and transportation engineer. land use consultants led the masterplan and landscape design. the project took 2½ years to construct and opened to the public on 17 march 2001.   == site == 2008: the verve, kaiser chiefs, and kt tunstall. 2010: mika, jack johnson, mojave 3, doves, paolo nutini, mumford & sons, and martha wainwright. 2011: the flaming lips, primal scream, pendulum, fleet foxes, and brandon flowers with support from the horrors, the go! team, ok go, villagers, and the bees. 2012: tim minchin, example, frank turner, chase & status, plan b, blink-182, noah and the whale, and the vaccines. 2013: kaiser chiefs, jessie j, eddie izzard, sigur rós, and the xx. 2014: dizzee rascal, skrillex, pixar in concert, ellie goulding, and elbow. 2015: paolo nutini, elton john, paloma faith, motörhead, the stranglers, spandau ballet, and ben howard. 2016: lionel richie, tom jones, pj harvey, manic street preachers, and jess glynne. 2017: bastille, madness, royal blood, blondie, van morrison, bryan adams, and foals. 2018: gary barlow, massive attack, a beautiful day out, ben howard, queens of the stone age, jack johnson, björk. 2019: stereophonics, nile rogers & chic, the 100th session, liam gallagher, the chemical brothers, snow patrol, kylie minogue.my chemical romance were to play the 2020 eden sessions in their first uk shows after a six-year hiatus, as part of their reunion tour. the 2020 sessions were postponed until 2021 due to the covid-19 pandemic and the closing of the eden project.the 2021 sessions will be headlined by diana ross, lionel richie, the script and my chemical romance.   == in the media == robin kewell (ed.): eden: the inside story. st austell n.d.: the eden project. dvd. the eden radio project. every thursday between 5:30 and 7 p.m. on radio st austell bay. die another day: graves\' diamond mine trees a crowd podcast: an interview between the project\'s head of interpretation, dr jo elworthy, and david oakes was released on 18 november. andy day\'s cbeebies tv show andy\'s aquatic adventures has had scenes filmed in the eden project.   == see also ==   == references ==   == further reading == philip mcmillan browse, louise frost, alistair griffiths: plants of eden (eden project). penzance 2001: alison hodge. richard mabey: fencing paradise: exploring the gardens of eden london 2005: eden project books. isbn 1-903919-31-2 hugh pearman, andrew whalley: the architecture of eden. with a foreword by sir nicholas grimshaw. london 2003: eden project books. isbn 1-903919-15-0 eden team (ed.): eden project: the guide 2008/9. london 2008: eden project books. tim smit: eden. london 2001: bantam press. paul spooner: the revenge of the green planet: the eden project book of amazing facts about plants. london 2003: eden project books. alan titchmarsh: the eden project. united kingdom: acorn media, 2006. oclc 225403941.   == external links == official website eden sessions website—official site for live gigs')"
118,"The interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. 
Many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.Materials scientists emphasize understanding, how the history of a material (processing) influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the materials paradigm. This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis –  investigating materials, products, structures or components, which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.


== History ==

The material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are historic, if arbitrary examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science were products of the Space Race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.
Before the 1960s (and in some cases decades after), many eventual materials science departments were metallurgy or ceramics engineering departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s, ""to expand the national program of basic research and training in the materials sciences.""  The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. The prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties and understand phenomena.


== Fundamentals ==

A material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications.   There are a myriad of materials around us, they can be found in anything from buildings,cars to spacecraft. Materials can generally be further divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, semiconductors, ceramics and polymers. New and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few.
The basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.


=== Structure ===
As mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves methods such as diffraction with X-rays, electronsor neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy, chromatography, thermal analysis, electron microscope analysis, etc.
Structure is studied at various levels, as detailed below.


==== Atomic structure ====
This deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms (Å). The chemical bonding and atomic arrangement (crystallography) are fundamental to studying the properties and behavior of any material.


===== Bonding =====

To obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid-state chemistry and physical chemistry are also involved in the study of bonding and structure.


===== Crystallography =====

Crystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, as an aggregate of small crystals with different orientations. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination. Most materials have a crystalline structure, but some important materials do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely non-crystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical descriptions of physical properties.


==== Nanostructure ====

Materials, which atoms and molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.
Nanostructure deals with objects and structures that are in the 1 - 100 nm range.  In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This causes many interesting electrical, magnetic, optical, and mechanical properties.
In describing nanostructures, it is necessary to differentiate between the number of dimensions on the nanoscale. 
Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. 
Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. 
Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used, when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.


==== Microstructure ====

Microstructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects from 100 nm to a few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.
The manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), vacancies, interstitial atoms or substitutional atoms. The microstructure of materials reveals these larger defects, so that they can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.


==== Macrostructure ====
Macrostructure is the appearance of a material in the scale millimeters to meters, it is the structure of the material as seen with the naked eye.


=== Properties ===

Materials exhibit myriad properties, including the following.

Mechanical properties, see Strength of materials
Chemical properties, see Chemistry
Electrical properties, see Electricity
Thermal properties, see Thermodynamics
Optical properties, see Optics and Photonics
Magnetic properties, see MagnetismThe properties of a material determine its usability and hence its engineering application.


=== Processing ===
Synthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry, if no economical production method for it has been developed. Thus, the processing of materials is vital to the field of materials science. Different materials require different processing or synthesis methods. For example, the processing of metals has historically been very important and is studied under the branch of materials science named physical metallurgy. Also, chemical and physical methods are also used to synthesize other materials such as polymers, ceramics, thin films, etc. As of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene.


=== Thermodynamics ===

Thermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints common to all materials. These general constraints are expressed in the four laws of thermodynamics.  Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics. 
The study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.


=== Kinetics ===

Chemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change. Kinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.


== Research ==
Materials science is a highly active area of research. Together with materials science departments, physics, chemistry, and many engineering departments are involved in materials research. Materials research covers a broad range of topics, following non-exhaustive list highlights a few important research areas.


=== Nanomaterials ===

Nanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10−9 meter), but is usually 1 nm - 100 nm. Nanomaterials research takes a materials science based approach to nanotechnology, using advances in materials metrology and synthesis, which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties. The field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials, such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.


=== Biomaterials ===

A biomaterial is any matter, surface, or construct that interacts with biological systems. The study of biomaterials is called bio materials science. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering, and materials science.
Biomaterials can be derived either from nature or synthesized in a laboratory using a variety of chemical approaches using metallic components, polymers, bioceramics, or composite materials. They are often intended or adapted for medical applications, such as biomedical devices which perform, augment, or replace a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxylapatite-coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as an organ transplant material.


=== Electronic, optical, and magnetic ===

Semiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.
Semiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to the concentration of impurities, which allows the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.
This field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid-state physics or condensed matter physics.


=== Computational materials science ===

With continuing increases in computing power, simulating the behavior of materials has become possible. This enables materials scientists to understand behavior and mechanisms, design new materials, and explain properties formerly poorly understood. Efforts surrounding Integrated computational materials engineering are now focusing on combining computational methods with experiments to drastically reduce the time and effort to optimize materials properties for a given application. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, Monte Carlo, dislocation dynamics, phase field, finite element, and many more.


== Industry ==
Radical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing methods (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytic methods (characterization methods such as electron microscopy, X-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).
Besides material characterization, the material scientist or engineer also deals with extracting materials and converting them into useful forms. Thus ingot casting, foundry methods, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence, or variation of minute quantities of secondary elements and compounds in a bulk material will greatly affect the final properties of the materials produced. For example, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extracting and purifying methods used to extract iron in a blast furnace can affect the quality of steel that is produced.


=== Ceramics and glasses ===

Another application of material science is the structures of ceramics and glass typically associated with the most brittle materials. Bonding in ceramics and glasses uses covalent and ionic-covalent types with SiO2 (silica or sand) as a fundamental building block. Ceramics are as soft as clay or as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.
Engineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.


=== Composites ===

Another application of materials science in industry is making composite materials. These are structured materials composed of two or more macroscopic phases. 
Applications range from structural elements such as steel-reinforced concrete, to the thermal insulating tiles, which play a key and integral role in NASA's Space Shuttle thermal protection system, which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material, which withstands re-entry temperatures up to 1,510 °C (2,750 °F) and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured-pyrolized to convert the furfural alcohol to carbon. To provide oxidation resistance for reuse ability, the outer layers of the RCC are converted to silicon carbide. 
Other examples can be seen in the ""plastic"" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile butadiene styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be termed reinforcing fibers, or dispersants, depending on their purpose.


=== Polymers ===

Polymers are chemical compounds made up of a large number of identical components linked together like chains. They are an important part of materials science. Polymers are the raw materials (the resins) used to make what are commonly called plastics and rubber. Plastics and rubber are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Plastics which have been around, and which are in current widespread use, include polyethylene, polypropylene, polyvinyl chloride (PVC), polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates and also rubbers, which have been around are natural rubber, styrene-butadiene rubber, chloroprene, and butadiene rubber. Plastics are generally classified as commodity, specialty and engineering plastics.
Polyvinyl chloride (PVC) is widely used, inexpensive, and annual production quantities are large. It lends itself to a vast array of applications, from artificial leather to electrical insulation and cabling, packaging, and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term ""additives"" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.
Polycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Such plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.
Specialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.
The dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For example, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable bags for shopping and trash, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called ultra-high-molecular-weight polyethylene (UHMWPE) is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.


=== Metal alloys ===

The study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. 
Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron-carbon alloy is only considered steel, if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties, however. Cast Iron is defined as an iron–carbon alloy with more than 2.00%, but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.
Other significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength to weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations, where high strength to weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.


=== Semiconductors ===
The study of semiconductors is a significant part of materials science. A semiconductor is a material that has a resistivity between a metal and insulator. Its electronic properties can be greatly altered through intentionally introducing impurities or doping. From these semiconductor materials, things such as diodes, transistors, light-emitting diodes (LEDs), and analog and digital electric circuits can be built, making them materials of interest in industry. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number—from a few to millions—of devices manufactured and interconnected on a single semiconductor substrate.Of all the semiconductors in use today, silicon makes up the largest portion both by quantity and commercial value. Monocrystalline silicon is used to produce wafers used in the semiconductor and electronics industry. Second to silicon, gallium arsenide (GaAs) is the second most popular semiconductor used. Due to its higher electron mobility and saturation velocity compared to silicon, it is a material of choice for high-speed electronics applications. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. Other semiconductor materials include germanium, silicon carbide, and gallium nitride and have various applications.


== Relation with other fields ==
Materials science evolved, starting from the 1950s, because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged in many ways: renaming and/or combining existing metallurgy and ceramics engineering departments; splitting from existing solid state physics research (itself growing into condensed matter physics); pulling in relatively new polymer engineering and polymer science; recombining from the previous, as well as chemistry, chemical engineering, mechanical engineering, and electrical engineering; and more.
The field of materials science and engineering is important both from a scientific perspective, as well as for applications field. Materials are of the utmost importance for engineers (or other applied fields), because usage of the appropriate materials is crucial when designing systems. As a result, materials science is an increasingly important part of an engineer's education.
The field is inherently interdisciplinary, and the materials scientists or engineers must be aware and make use of the methods of the physicist, chemist and engineer. Thus, there remain close relationships with these fields. Conversely, many physicists, chemists and engineers find themselves working in materials science due to the significant overlaps between the fields.


== Emerging technologies ==


== Subdisciplines ==
The main branches of materials science stem from the three main classes of materials: ceramics, metals, and polymers.

Ceramic engineering
Metallurgy
Polymer science and polymer engineeringThere are additionally broadly applicable, materials independent, endeavors.

Materials characterization
Computational materials science
Materials informaticsThere are also relatively broad focuses across materials on specific phenomena and techniques.

Crystallography
Nuclear spectroscopy
Surface science
Tribology


== Related fields ==
Condensed matter physics
Mineralogy
Solid-state chemistry
Solid-state physics
Supramolecular chemistry


== Professional societies ==
American Ceramic Society
ASM International
Association for Iron and Steel Technology
Materials Research Society
The Minerals, Metals & Materials Society


== See also ==


== References ==


=== Citations ===


=== Bibliography ===
Ashby, Michael; Hugh Shercliff; David Cebon (2007). Materials: engineering, science, processing and design (1st ed.). Butterworth-Heinemann. ISBN 978-0-7506-8391-3.
Askeland, Donald R.; Pradeep P. Phulé (2005). The Science & Engineering of Materials (5th ed.). Thomson-Engineering. ISBN 978-0-534-55396-8.
Callister, Jr., William D. (2000). Materials Science and Engineering – An Introduction (5th ed.). John Wiley and Sons. ISBN 978-0-471-32013-5.
Eberhart, Mark (2003). Why Things Break: Understanding the World by the Way It Comes Apart. Harmony. ISBN 978-1-4000-4760-4.
Gaskell, David R. (1995). Introduction to the Thermodynamics of Materials (4th ed.). Taylor and Francis Publishing. ISBN 978-1-56032-992-3.
González-Viñas, W. & Mancini, H.L. (2004). An Introduction to Materials Science. Princeton University Press. ISBN 978-0-691-07097-1.
Gordon, James Edward (1984). The New Science of Strong Materials or Why You Don't Fall Through the Floor (eissue ed.). Princeton University Press. ISBN 978-0-691-02380-9.
Mathews, F.L. & Rawlings, R.D. (1999). Composite Materials: Engineering and Science. Boca Raton: CRC Press. ISBN 978-0-8493-0621-1.
Lewis, P.R.; Reynolds, K. & Gagg, C. (2003). Forensic Materials Engineering: Case Studies. Boca Raton: CRC Press.
Wachtman, John B. (1996). Mechanical Properties of Ceramics. New York: Wiley-Interscience, John Wiley & Son's. ISBN 978-0-471-13316-2.
Walker, P., ed. (1993). Chambers Dictionary of Materials Science and Technology. Chambers Publishing. ISBN 978-0-550-13249-9.


== Further reading ==
Timeline of Materials Science at The Minerals, Metals & Materials Society (TMS) –  accessed March 2007
Burns, G.; Glazer, A.M. (1990). Space Groups for Scientists and Engineers (2nd ed.). Boston: Academic Press, Inc. ISBN 978-0-12-145761-7.
Cullity, B.D. (1978). Elements of X-Ray Diffraction (2nd ed.). Reading, Massachusetts: Addison-Wesley Publishing Company. ISBN 978-0-534-55396-8.
Giacovazzo, C; Monaco HL; Viterbo D; Scordari F; Gilli G; Zanotti G; Catti M (1992). Fundamentals of Crystallography. Oxford: Oxford University Press. ISBN 978-0-19-855578-0.
Green, D.J.; Hannink, R.; Swain, M.V. (1989). Transformation Toughening of Ceramics. Boca Raton: CRC Press. ISBN 978-0-8493-6594-2.
Lovesey, S. W. (1984). Theory of Neutron Scattering from Condensed Matter; Volume 1: Neutron Scattering. Oxford: Clarendon Press. ISBN 978-0-19-852015-3.
Lovesey, S. W. (1984). Theory of Neutron Scattering from Condensed Matter; Volume 2: Condensed Matter. Oxford: Clarendon Press. ISBN 978-0-19-852017-7.
O'Keeffe, M.; Hyde, B.G. (1996). Crystal Structures; I. Patterns and Symmetry. Zeitschrift für Kristallographie. 212. Washington, DC: Mineralogical Society of America, Monograph Series. p. 899. Bibcode:1997ZK....212..899K. doi:10.1524/zkri.1997.212.12.899. ISBN 978-0-939950-40-9.
Squires, G.L. (1996). Introduction to the Theory of Thermal Neutron Scattering (2nd ed.). Mineola, New York: Dover Publications Inc. ISBN 978-0-486-69447-4.
Young, R.A., ed. (1993). The Rietveld Method. Oxford: Oxford University Press & International Union of Crystallography. ISBN 978-0-19-855577-3.


== External links ==
MS&T conference organized by the main materials societies
MIT OpenCourseWare for MSE
Materials science at Curlie","pandas(index=118, _1=118, text='the interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. the intellectual origins of materials science stem from the enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. materials science still incorporates elements of physics, chemistry, and engineering. as such, the field was long considered by academic institutions as a sub-field of these related fields. beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. thus, breakthroughs in materials science are likely to affect the future of technology significantly.materials scientists emphasize understanding, how the history of a material (processing) influences its structure, and thus the material\'s properties and performance. the understanding of processing-structure-properties relationships is called the materials paradigm. this paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. materials science is also an important part of forensic engineering and failure analysis –  investigating materials, products, structures or components, which fail or do not function as intended, causing personal injury or damage to property. such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.   == history ==  the material of choice of a given era is often a defining point. phrases such as stone age, bronze age, iron age, and steel age are historic, if arbitrary examples. originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. a major breakthrough in the understanding of materials occurred in the late 19th century, when the american scientist josiah willard gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. important elements of modern materials science were products of the space race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials. before the 1960s (and in some cases decades after), many eventual materials science departments were metallurgy or ceramics engineering departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. the growth of materials science in the united states was catalyzed in part by the advanced research projects agency, which funded a series of university-hosted laboratories in the early 1960s, ""to expand the national program of basic research and training in the materials sciences.""  the field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. the prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties and understand phenomena.   == fundamentals ==  a material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications.   there are a myriad of materials around us, they can be found in anything from buildings,cars to spacecraft. materials can generally be further divided into two classes: crystalline and non-crystalline. the traditional examples of materials are metals, semiconductors, ceramics and polymers. new and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few. the basis of materials science involves studying the structure of materials, and relating them to their properties. once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. the major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. these characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material\'s microstructure, and thus its properties. ashby, michael; hugh shercliff; david cebon (2007). materials: engineering, science, processing and design (1st ed.). butterworth-heinemann. isbn 978-0-7506-8391-3. askeland, donald r.; pradeep p. phulé (2005). the science & engineering of materials (5th ed.). thomson-engineering. isbn 978-0-534-55396-8. callister, jr., william d. (2000). materials science and engineering – an introduction (5th ed.). john wiley and sons. isbn 978-0-471-32013-5. eberhart, mark (2003). why things break: understanding the world by the way it comes apart. harmony. isbn 978-1-4000-4760-4. gaskell, david r. (1995). introduction to the thermodynamics of materials (4th ed.). taylor and francis publishing. isbn 978-1-56032-992-3. gonzález-viñas, w. & mancini, h.l. (2004). an introduction to materials science. princeton university press. isbn 978-0-691-07097-1. gordon, james edward (1984). the new science of strong materials or why you don\'t fall through the floor (eissue ed.). princeton university press. isbn 978-0-691-02380-9. mathews, f.l. & rawlings, r.d. (1999). composite materials: engineering and science. boca raton: crc press. isbn 978-0-8493-0621-1. lewis, p.r.; reynolds, k. & gagg, c. (2003). forensic materials engineering: case studies. boca raton: crc press. wachtman, john b. (1996). mechanical properties of ceramics. new york: wiley-interscience, john wiley & son\'s. isbn 978-0-471-13316-2. walker, p., ed. (1993). chambers dictionary of materials science and technology. chambers publishing. isbn 978-0-550-13249-9.   == further reading == timeline of materials science at the minerals, metals & materials society (tms) –  accessed march 2007 burns, g.; glazer, a.m. (1990). space groups for scientists and engineers (2nd ed.). boston: academic press, inc. isbn 978-0-12-145761-7. cullity, b.d. (1978). elements of x-ray diffraction (2nd ed.). reading, massachusetts: addison-wesley publishing company. isbn 978-0-534-55396-8. giacovazzo, c; monaco hl; viterbo d; scordari f; gilli g; zanotti g; catti m (1992). fundamentals of crystallography. oxford: oxford university press. isbn 978-0-19-855578-0. green, d.j.; hannink, r.; swain, m.v. (1989). transformation toughening of ceramics. boca raton: crc press. isbn 978-0-8493-6594-2. lovesey, s. w. (1984). theory of neutron scattering from condensed matter; volume 1: neutron scattering. oxford: clarendon press. isbn 978-0-19-852015-3. lovesey, s. w. (1984). theory of neutron scattering from condensed matter; volume 2: condensed matter. oxford: clarendon press. isbn 978-0-19-852017-7. o\'keeffe, m.; hyde, b.g. (1996). crystal structures; i. patterns and symmetry. zeitschrift für kristallographie. 212. washington, dc: mineralogical society of america, monograph series. p. 899. bibcode:1997zk....212..899k. doi:10.1524/zkri.1997.212.12.899. isbn 978-0-939950-40-9. squires, g.l. (1996). introduction to the theory of thermal neutron scattering (2nd ed.). mineola, new york: dover publications inc. isbn 978-0-486-69447-4. young, r.a., ed. (1993). the rietveld method. oxford: oxford university press & international union of crystallography. isbn 978-0-19-855577-3.   == external links == ms&t conference organized by the main materials societies mit opencourseware for mse materials science at curlie')"
119,"In architecture, post and lintel (also called prop and lintel or a trabeated system)  is a building system where strong horizontal elements are held up by strong vertical elements with large spaces between them. This is usually used to hold up a roof, creating a largely open space beneath, for whatever use the building is designed.  The horizontal elements are called by a variety of names including lintel, header, architrave or beam, and the supporting vertical elements may be called columns, pillars, or posts.  The use of wider elements at the top of the post, called capitals, to help spread the load, is common to many traditions. 
The trabeated system is a fundamental principle of Neolithic architecture, ancient Indian architecture, ancient Greek architecture and ancient Egyptian architecture. Other trabeated styles are the Persian, Lycian, Japanese, traditional Chinese, and ancient Chinese architecture, especially in northern China, and nearly all the Indian styles. The traditions are represented in North and Central America by Mayan architecture, and in South America by Inca architecture.  In all or most of these traditions, certainly in Greece and India, the earliest versions developed using wood, which were later translated into stone for larger and grander buildings. Timber framing, also using trusses, remains common for smaller buildings such as houses to the modern day.
The biggest disadvantage to a post and lintel construction is the limited weight that can be held up, and the small distances required between the posts. Ancient Roman architecture's development of the arch allowed for much larger structures to be constructed. The arcuated system spreads larger loads more effectively, and replaced the post and lintel trabeated system in most larger buildings and structures, until the introduction of steel girder beams in the industrial era.  As with the Roman temple portico front and its descendants in later classical architecture, trabeated features were often retained in parts of buildings as an aesthetic choice.  The classical orders of Greek origin were in particular retained in buildings designed to impress, even though they usually had little or no structural role.


== Lintel beams ==

In architecture, a post-and-lintel or trabeated system refers to the use of horizontal beams or lintels which are borne up by columns or posts.  The name is from the Latin trabs, beam; influenced by trabeatus, clothed in the trabea, a ritual garment.
A noteworthy example of a trabeated system is in Volubilis, from the Roman era, where one side of the Decumanus Maximus is lined with trabeated elements, while the opposite side of the roadway is designed in arched style.In India the style was used originally for wooden construction, but later the technique was adopted for stone structures for decorated load-bearing and purely ornamented non-structural purposes.


== Engineering ==
Post and lintel construction is one of four ancient structural methods of building, the others being the corbel, arch-and-vault, and truss.There are two main force vectors acting upon the post and lintel system: weight carrying compression at the joint between lintel and post, and tension induced by deformation of self-weight and the load above between the posts. The two posts are under compression from the weight of the lintel (or beam) above. The lintel will deform by sagging in the middle because the underside is under tension and the topside is under compression.


== See also ==
Architrave – structural lintel or beam resting on columns-pillars
Atalburu – Basque decorative lintel
Dolmen – Neolithic megalithic tombs with structural stone lintels
Dougong – traditional Chinese structural element
I-beam – steel lintels and beams
Marriage stone – decorative lintel
Opus caementicium
Structural design
Timber framing – post and beam systems
Stonehenge


== Notes ==


== References ==
Summerson, John, The Classical Language of Architecture, 1980 edition, Thames and Hudson World of Art series, ISBN 0500201773","pandas(index=119, _1=119, text=""in architecture, post and lintel (also called prop and lintel or a trabeated system)  is a building system where strong horizontal elements are held up by strong vertical elements with large spaces between them. this is usually used to hold up a roof, creating a largely open space beneath, for whatever use the building is designed.  the horizontal elements are called by a variety of names including lintel, header, architrave or beam, and the supporting vertical elements may be called columns, pillars, or posts.  the use of wider elements at the top of the post, called capitals, to help spread the load, is common to many traditions. the trabeated system is a fundamental principle of neolithic architecture, ancient indian architecture, ancient greek architecture and ancient egyptian architecture. other trabeated styles are the persian, lycian, japanese, traditional chinese, and ancient chinese architecture, especially in northern china, and nearly all the indian styles. the traditions are represented in north and central america by mayan architecture, and in south america by inca architecture.  in all or most of these traditions, certainly in greece and india, the earliest versions developed using wood, which were later translated into stone for larger and grander buildings. timber framing, also using trusses, remains common for smaller buildings such as houses to the modern day. the biggest disadvantage to a post and lintel construction is the limited weight that can be held up, and the small distances required between the posts. ancient roman architecture's development of the arch allowed for much larger structures to be constructed. the arcuated system spreads larger loads more effectively, and replaced the post and lintel trabeated system in most larger buildings and structures, until the introduction of steel girder beams in the industrial era.  as with the roman temple portico front and its descendants in later classical architecture, trabeated features were often retained in parts of buildings as an aesthetic choice.  the classical orders of greek origin were in particular retained in buildings designed to impress, even though they usually had little or no structural role.   == lintel beams ==  in architecture, a post-and-lintel or trabeated system refers to the use of horizontal beams or lintels which are borne up by columns or posts.  the name is from the latin trabs, beam; influenced by trabeatus, clothed in the trabea, a ritual garment. a noteworthy example of a trabeated system is in volubilis, from the roman era, where one side of the decumanus maximus is lined with trabeated elements, while the opposite side of the roadway is designed in arched style.in india the style was used originally for wooden construction, but later the technique was adopted for stone structures for decorated load-bearing and purely ornamented non-structural purposes.   == engineering == post and lintel construction is one of four ancient structural methods of building, the others being the corbel, arch-and-vault, and truss.there are two main force vectors acting upon the post and lintel system: weight carrying compression at the joint between lintel and post, and tension induced by deformation of self-weight and the load above between the posts. the two posts are under compression from the weight of the lintel (or beam) above. the lintel will deform by sagging in the middle because the underside is under tension and the topside is under compression.   == see also == architrave – structural lintel or beam resting on columns-pillars atalburu – basque decorative lintel dolmen – neolithic megalithic tombs with structural stone lintels dougong – traditional chinese structural element i-beam – steel lintels and beams marriage stone – decorative lintel opus caementicium structural design timber framing – post and beam systems stonehenge   == notes ==   == references == summerson, john, the classical language of architecture, 1980 edition, thames and hudson world of art series, isbn 0500201773"")"
120,"The skull crucible process was developed at the Lebedev Physical Institute in Moscow to manufacture cubic zirconia. It was invented to solve the problem of cubic zirconia's melting-point being too high for even platinum crucibles. 
In essence, by heating only the center of a volume of cubic zirconia, the material forms its own ""crucible"" from its cooler outer layers. The term ""skull"" refers to these outer layers forming a shell enclosing the molten volume. Zirconium oxide powder is heated then gradually allowed to cool. Heating is accomplished by radio frequency induction using a coil wrapped around the apparatus.  The outside of the device is water-cooled in order to keep the radio frequency coil from melting and also to cool the outside of the zirconium oxide and thus maintain the shape of the zirconium powder.
Since zirconium oxide in its solid state does not conduct electricity, a piece of zirconium metal is placed inside the gob of zirconium oxide. As the zirconium melts it oxidizes and blends with the now molten zirconium oxide, a conductor, and is heated by radio frequency induction.
When the zirconium oxide is melted on the inside (but not completely, since the outside needs to remain solid) the amplitude of the RF induction coil is gradually reduced and crystals form as the material cools. Normally this would form a monoclinic crystal system of zirconium oxide. 
In order to maintain a cubic crystal system a stabilizer is added, magnesium oxide, calcium oxide or yttrium oxide as well as any material to color the crystal. After the mixture cools the outer shell is broken off and the interior of the gob is then used to manufacture gemstones.","pandas(index=120, _1=120, text='the skull crucible process was developed at the lebedev physical institute in moscow to manufacture cubic zirconia. it was invented to solve the problem of cubic zirconia\'s melting-point being too high for even platinum crucibles. in essence, by heating only the center of a volume of cubic zirconia, the material forms its own ""crucible"" from its cooler outer layers. the term ""skull"" refers to these outer layers forming a shell enclosing the molten volume. zirconium oxide powder is heated then gradually allowed to cool. heating is accomplished by radio frequency induction using a coil wrapped around the apparatus.  the outside of the device is water-cooled in order to keep the radio frequency coil from melting and also to cool the outside of the zirconium oxide and thus maintain the shape of the zirconium powder. since zirconium oxide in its solid state does not conduct electricity, a piece of zirconium metal is placed inside the gob of zirconium oxide. as the zirconium melts it oxidizes and blends with the now molten zirconium oxide, a conductor, and is heated by radio frequency induction. when the zirconium oxide is melted on the inside (but not completely, since the outside needs to remain solid) the amplitude of the rf induction coil is gradually reduced and crystals form as the material cools. normally this would form a monoclinic crystal system of zirconium oxide. in order to maintain a cubic crystal system a stabilizer is added, magnesium oxide, calcium oxide or yttrium oxide as well as any material to color the crystal. after the mixture cools the outer shell is broken off and the interior of the gob is then used to manufacture gemstones.')"
121,"Powder metallurgy (PM) is a term covering a wide range of ways in which materials or components are made from metal powders. PM processes can avoid, or greatly reduce, the need to use metal removal processes, thereby drastically reducing yield losses in manufacture and often resulting in lower costs.
Powder metallurgy is also used to make unique materials impossible to get from melting or forming in other ways. A very important product of this type is tungsten carbide (WC). WC is used to cut and form other metals and is made from WC particles bonded with cobalt. It is very widely used in industry for tools of many types and globally ~50,000 tonnes/year (t/y) is made by PM. Other products include sintered filters, porous oil-impregnated bearings, electrical contacts and diamond tools.
Since the advent of industrial production–scale metal powder–based additive manufacturing (AM) in the 2010s, selective laser sintering and other metal AM processes are a new category of commercially important powder metallurgy applications.


== Overview ==
The powder metallurgy press and sinter process generally consists of three basic steps: powder blending (pulverisation), die compaction, and sintering. Compaction is generally performed at room temperature, and the elevated-temperature process of sintering is usually conducted at atmospheric pressure and under carefully controlled atmosphere composition. Optional secondary processing such as coining or heat treatment often follows to obtain special properties or enhanced precision.One of the older such methods, and still one used to make around 1 Mt/y of structural components of iron-based alloys, is the process of blending fine (<180 microns) metal (normally iron) powders with additives such as a lubricant wax, carbon, copper, and/or nickel, pressing them into a die of the desired shape, and then heating the compressed material (""green part"") in a controlled atmosphere to bond the material by sintering. This produces precise parts, normally very close to the die dimensions, but with 5–15% porosity, and thus sub-wrought steel properties.  There are several other PM processes which have been developed over the last fifty years. These include:

Powder forging: A ""preform"" made by the conventional ""press and sinter"" method is heated and then hot forged to full density, resulting in practically as-wrought properties.
Hot isostatic pressing (HIP): Here the powder (normally gas atomized, spherical type) is filled into a mould, normally consisting of a metallic ""can"" of suitable shape. The can is vibrated, then evacuated and sealed.  It is then placed in a hot isostatic press, where it is heated to a homologous temperature of around 0.7, and subjected to an external gas pressure of ~100 MPa (1000 bar, 15,000 psi) for several hours. This results in a shaped part of full density with as-wrought or better, properties. HIP was invented in the 1950-60s and entered tonnage production in the 1970-80s. In 2015, it was used to produce ~25,000 t/y of stainless and tool steels, as well as important parts of superalloys for jet engines.
Metal injection moulding (MIM):  Here the powder, normally very fine (<25 microns) and spherical, is mixed with plastic or wax binder to near the maximum solid loading, typically around 65vol%, and injection moulded to form a ""green"" part of complex geometry. This part is then heated or otherwise treated to remove the binder (debinding) to give a ""brown"" part. This part is then sintered, and shrinks by ~18% to give a complex and 95–99% dense finished part (surface roughness ~3 microns). Invented in the 1970s, production has increased since 2000 with an estimated global volume in 2014 of 12,000 t worth €1265 millions.
Electric current assisted sintering (ECAS) technologies rely on electric currents to densify powders, with the advantage of reducing production time dramatically (from 15 minutes of the slowest ECAS to a few microseconds of the fastest), not requiring a long furnace heat and allowing near theoretical densities but with the drawback of simple shapes. Powders employed in ECAS can avoid binders thanks to the possibility of direct sintering, without the need of pre-pressing and a green compact. Molds are designed for the final part shape since the powders densify while filling the cavity under an applied pressure thus avoiding the problem of shape variations caused by non isotropic sintering and distortions caused by gravity at high temperatures. The most common of these technologies is hot pressing, which has been under use for the production of the diamond tools employed in the construction industry. Spark plasma sintering and electro sinter forging are two modern, industrial commercial ECAS technologies.
Additive manufacturing (AM) is a relatively novel family of techniques which use metal powders (among other materials, such as plastics) to make parts by laser sintering or melting. This is a process under rapid development as of 2015, and whether to classify it as a PM process is perhaps uncertain at this stage. Processes include 3D printing, selective laser sintering (SLS), selective laser melting (SLM), and electron beam melting (EBM).


== History and capabilities ==
The history of powder metallurgy and the art of metal and ceramic sintering are intimately related to each other. Sintering involves the production of a hard solid metal or ceramic piece from a starting powder. The ancient Incas made jewelry and other artifacts from precious metal powders, though mass manufacturing of PM products did not begin until the mid or late 19th century. In these early manufacturing operations, iron was extracted by hand from metal sponge following reduction and was then reintroduced as a powder for final melting or sintering.
A much wider range of products can be obtained from powder processes than from direct alloying of fused materials. In melting operations the ""phase rule"" applies to all pure and combined elements and strictly dictates the distribution of liquid and solid phases which can exist for specific compositions. In addition, whole body melting of starting materials is required for alloying, thus imposing unwelcome chemical, thermal, and containment constraints on manufacturing. Unfortunately, the handling of aluminium/iron powders poses major problems. Other substances that are especially reactive with atmospheric oxygen, such as titanium, are sinterable in special atmospheres or with temporary coatings.In powder metallurgy or ceramics it is possible to fabricate components which otherwise would decompose or disintegrate. All considerations of solid-liquid phase changes can be ignored, so powder processes are more flexible than casting, extrusion, or forging techniques. Controllable characteristics of products prepared using various powder technologies include mechanical, magnetic, and other unconventional properties of such materials as porous solids, aggregates, and intermetallic compounds. Competitive characteristics of manufacturing processing (e.g. tool wear, complexity, or vendor options) also may be closely controlled.


== Powder production techniques ==
Any fusible material can be atomized. Several techniques have been developed which permit large production rates of powdered particles, often with considerable control over the size ranges of the final grain population. Powders may be prepared by crushing, grinding, chemical reactions, or electrolytic deposition. The most commonly used powders are copper-base and iron-base materials.Powders of the elements titanium, vanadium, thorium, niobium, tantalum, calcium, and uranium have been produced by high-temperature reduction of the corresponding nitrides and carbides. Iron, nickel, uranium, and beryllium submicrometre powders are obtained by reducing metallic oxalates and formates. Exceedingly fine particles also have been prepared by directing a stream of molten metal through a high-temperature plasma jet or flame,  atomizing the material. Various chemical and flame associated powdering processes are adopted in part to prevent serious degradation of particle surfaces by atmospheric oxygen.
In tonnage terms, the production of iron powders for PM structural part production dwarfs the production of all of the non-ferrous metal powders combined. Virtually all iron powders are produced by one of two processes: the sponge iron process or water atomization.


=== Sponge iron process ===
The longest established of these processes is the sponge iron process, the leading example of a family of processes involving solid state reduction of an oxide. In the process, selected magnetite (Fe3O4) ore is mixed with coke and lime and placed in a silicon carbide retort. The filled retort is then heated in a kiln, where the reduction process leaves an iron “cake” and a slag. In subsequent steps, the retort is emptied, the reduced iron sponge is separated from the slag and is crushed and annealed.
The resultant powder is highly irregular in particle shape, therefore ensuring good “green strength” so that die-pressed compacts can be readily handled prior to sintering, and each particle contains internal pores (hence the term “sponge”) so that the good green strength is available at low compacted density levels.
Sponge iron provides the
feedstock for all iron-based self-lubricating bearings, and still accounts for around 30% of iron powder usage in PM structural parts.


=== Atomization ===
Atomization is accomplished by forcing a molten metal stream through an orifice at moderate pressures. A gas is introduced into the metal stream just before it leaves the nozzle, serving to create turbulence as the entrained gas expands (due to heating) and exits into a large collection volume exterior to the orifice. The collection volume is filled with gas to promote further turbulence of the molten metal jet. Air and powder streams are segregated using gravity or cyclonic separation. Most atomized powders are annealed, which helps reduce the oxide and carbon content. The water atomized particles are smaller, cleaner, and nonporous and have a greater breadth of size, which allows better compacting. The particles produced through this method are normally of spherical or pear shape. Usually, they also carry a layer of oxide over them.
There are three types of atomization:

Liquid atomization
Gas atomization
Centrifugal atomizationSimple atomization techniques are available in which liquid metal is forced through an orifice at a sufficiently high velocity to ensure turbulent flow. The usual performance index used is the Reynolds number R = fvd/n, where f = fluid density, v = velocity of the exit stream, d = diameter of the opening, and n = absolute viscosity. At low R the liquid jet oscillates, but at higher velocities the stream becomes turbulent and breaks into droplets. Pumping energy is applied to droplet formation with very low efficiency (on the order of 1%) and control over the size distribution of the metal particles produced is rather poor. Other techniques such as nozzle vibration, nozzle asymmetry, multiple impinging streams, or molten-metal injection into ambient gas are all available to increase atomization efficiency, produce finer grains, and to narrow the particle size distribution. Unfortunately, it is difficult to eject metals through orifices smaller than a few millimeters in diameter, which in practice limits the minimum size of powder grains to approximately 10 μm. Atomization also produces a wide spectrum of particle sizes, necessitating downstream classification by screening and remelting a significant fraction of the grain boundary.


=== Centrifugal disintegration ===
Centrifugal disintegration of molten particles offers one way around these problems. Extensive experience is available with iron, steel, and aluminium. Metal to be powdered is formed into a rod which is introduced into a chamber through a rapidly rotating spindle. Opposite the spindle tip is an electrode from which an arc is established which heats the metal rod. As the tip material fuses, the rapid rod rotation throws off tiny melt droplets which solidify before hitting the chamber walls. A circulating gas sweeps particles from the chamber.  Similar techniques could be employed in space or on the Moon. The chamber wall could be rotated to force new powders into remote collection vessels, and the electrode could be replaced by a solar mirror focused at the end of the rod.
An alternative approach capable of producing a very narrow distribution of grain sizes but with low throughput consists of a rapidly spinning bowl heated to well above the melting point of the material to be powdered. Liquid metal, introduced onto the surface of the basin near the center at flow rates adjusted to permit a thin metal film to skim evenly up the walls and over the edge, breaks into droplets, each approximately the thickness of the film.


=== Other techniques ===
Another powder-production technique involves a thin jet of liquid metal intersected by high-speed streams of atomized water which break the jet into drops and cool the powder before it reaches the bottom of the bin. In subsequent operations the powder is dried. This is called water atomization. The advantage of water atomization is that metal solidifies faster than by gas atomization since the heat capacity of water is some magnitudes higher than gases. Since the solidification rate is inversely proportional to the particle size, smaller particles can be made using water atomization. The smaller the particles, the more homogeneous the micro structure will be. Notice that particles will have a more irregular shape and the particle size distribution will be wider. In addition, some surface contamination can occur by oxidation skin formation. Powder can be reduced by some kind of pre-consolidation treatment, such as annealing used for the manufacture of ceramic tools.


== Powder compaction ==

Powder compaction is the process of compacting metal powder in a die through the application of high pressures. Typically the tools are held in the vertical orientation with the punch tool forming the bottom of the cavity. The powder is then compacted into a shape and then ejected from the die cavity. In a number of these applications the parts may require very little additional work for their intended use; making for very cost efficient manufacturing.
The density of the compacted powder increases with the amount of pressure applied. Typical pressures range from 80 psi to 1000 psi (0.5 MPa to 7 MPa), pressures from 1000 psi to 1,000,000 psi have been obtained. Pressure of 10 t/in² to 50 t/in² (150 MPa to 700 MPa) are commonly used for metal powder compaction. To attain the same compression ratio across a component with more than one level or height, it is necessary to work with multiple lower punches. A cylindrical workpiece is made by single-level tooling. A more complex shape can be made by the common multiple-level tooling.
Production rates of 15 to 30 parts per minute are common.
There are four major classes of tool styles: single-action compaction, used for thin, flat components; opposed double-action with two punch motions, which accommodates thicker components; double-action with floating die; and double action withdrawal die. Double action classes give much better density distribution than single action. Tooling must be designed so that it will withstand the extreme pressure without deforming or bending. Tools must be made from materials that are polished and wear-resistant.
Better workpiece materials can be obtained by repressing and re-sintering.


=== Die pressing ===
The dominant technology for the forming of products from powder materials, in terms of both tonnage quantities and numbers of parts produced, is die pressing. There are mechanical, servo-electrical and hydraulic presses available in the market, whereby the biggest powder throughput is processed by hydraulic presses.
This forming technology involves a production cycle comprising:

Filling a die cavity with a known volume of the powder feedstock, delivered from a fill shoe.
Compaction of the powder within the die with punches to form the compact. Generally, compaction pressure is applied through punches from both ends of the toolset in order to reduce the level of density gradient within the compact.
Ejection of the compact from the die, using the lower punch(es) withdrawal from the die.
Removal of the compact from the upper face of the die using the fill shoe in the fill stage of the next cycle, or an automation system or robot.This cycle offers a readily automated and high production rate process.


=== Design considerations ===
Probably the most basic consideration is being able to remove the part from the die after it is pressed, along with avoiding sharp corners in the design. Keeping the maximum surface area below 20 square inches (0.013 m2) and the height-to-diameter ratio below 7-to-1 is recommended. Along with having walls thicker than 0.08 inches (2.0 mm) and keeping the adjacent wall thickness ratios below 2.5-to-1.
One of the major advantages of this process is its ability to produce complex geometries. Parts with undercuts and threads require a secondary machining operation. Typical part sizes range from 0.1 square inches (0.65 cm2) to 20 square inches (130 cm2). in area and from 0.1 to 4 inches (0.25 to 10.16 cm) in length. However, it is possible to produce parts that are less than 0.1 square inches (0.65 cm2) and larger than 25 square inches (160 cm2). in area and from a fraction of an inch (2.54 cm) to approximately 8 inches (20 cm) in length.


=== Isostatic pressing ===
In some pressing operations, such as hot isostatic pressing (HIP) compact formation and sintering occur simultaneously. This procedure, together with explosion-driven compressive techniques is used extensively in the production of high-temperature and high-strength parts such as turbine disks for jet engines. In most applications of powder metallurgy the compact is hot-pressed, heated to a temperature above which the materials cannot remain work-hardened. Hot pressing lowers the pressures required to reduce porosity and speeds welding and grain deformation processes. It also permits better dimensional control of the product, lessens sensitivity to physical characteristics of starting materials, and allows powder to be compressed to higher densities than with cold pressing, resulting in higher strength. Negative aspects of hot pressing include shorter die life, slower throughput because of powder heating, and the frequent necessity for protective atmospheres during forming and cooling stages.


== Isostatic powder compacting ==
Isostatic powder compacting is a mass-conserving shaping process. Fine metal particles are placed into a flexible mould and then high fluid pressure is applied to the mold, in contrast to the direct pressure applied by the die faces of a die pressing process.  The resulting article is then sintered in a furnace which increases the strength of the part by bonding the metal particles. This manufacturing process produces very little scrap metal and can be used to make many different shapes. The tolerances that this process can achieve are very precise, ranging from +/- 0.008 inches (0.2 mm) for axial dimensions and +/- 0.020 inches (0.5 mm) for radial dimensions.  This is the most efficient type of powder compacting (the following subcategories are also from this reference).  This operation is generally only applicable on small production quantities, although the cost of a mold much lower than that of pressing dies it is generally not reusable and the production time is much longer.Compacting pressures range from 15,000 psi (100,000 kPa) to 40,000 psi (280,000 kPa) for most metals and approximately 2,000 psi (14,000 kPa) to 10,000 psi (69,000 kPa) for non-metals.  The density of isostatic compacted parts is 5% to 10% higher than with other powder metallurgy processes.


=== Equipment ===
There are many types of equipment used in isostatic powder compacting.  There is the mold containing the part, which is flexible, a flexible outer pressure mold that contains and seals the mold, and the machine delivering the pressure.  There are also devices to control the amount of pressure and how long the pressure is held. The machines need to apply pressures from 15,000 to 40,000 pounds per square inch (100 to 280 MPa) for metals.


=== Geometrical possibilities ===
Typical workpiece sizes range from 0.25 in (6.35 mm) to 0.75 in (19.05 mm) thick and 0.5 in (12.70 mm) to 10 in (254 mm) long. It is possible to compact workpieces that are between 0.0625 in (1.59 mm) and 5 in (127 mm) thick and 0.0625 in (1.59 mm) to 40 in (1,016 mm) long.


=== Tool style ===
Isostatic tools are available in three styles, free mold (wet-bag), coarse mold (damp-bag) and fixed mold (dry-bag).  The free mold style is the traditional style of isostatic compaction and is not generally used for high production work. In free mold tooling the mold is removed and filled outside the canister. Damp bag is where the mold is located in the canister, yet filled outside. In fixed mold tooling, the mold is contained within the canister, which facilitates automation of the process.


=== Hot isostatic pressing ===

Hot isostatic pressing (HIP) compresses and sinters the part simultaneously by applying heat ranging from 900 °F (480 °C) to 2250 °F (1230 °C). Argon gas is the most common gas used in HIP because it is an inert gas, thus prevents chemical reactions during the operation.


=== Cold isostatic pressing ===
Cold isostatic pressing (CIP) uses fluid as a means of applying pressure to the mold at room temperature. After removal the part still needs to be sintered.
It is helpful in distributing pressure uniformly over the compaction material contained in a rubber bag.


=== Design considerations ===
Advantages over standard powder compaction are the possibility of thinner walls and larger workpieces. Height to diameter ratio has no limitation. No specific limitations exist in wall thickness variations, undercuts, reliefs, threads, and cross holes. No lubricants are need for isostatic powder compaction. The minimum wall thickness is 0.05 inches (1.27 mm) and the product can have a weight between 40 and 300 pounds (18 and 136 kg). There is 25 to 45% shrinkage of the powder after compacting.


== Sintering ==
After compaction, powdered materials are heated in a controlled atmosphere in a process known as sintering. During this process, the surfaces of the particles are bonded and desirable properties are achieved.Sintering of powder metals is a process in which particles under pressure chemically bond to themselves in order to form a coherent shape when exposed to a high temperature. The temperature in which the particles are sintered is most commonly below the melting point of the main component in the powder. If the temperature is above the melting point of a component in the powder metal part, the liquid of the melted particles fills the pores. This type of sintering is known as liquid-state sintering. A major challenge with sintering in general is knowing the effect of the process on the dimensions of the compact particles. This is especially difficult for tooling purposes in which specific dimensions may be needed. It is most common for the sintered part to shrink and become denser, but it can also expand or experience no net change.The main driving force for solid state sintering is an excess of surface free energy. The process of solid-state sintering is complex and dependent on the material and furnace (temperature and gas) conditions. There are six main stages that sintering processes can be grouped in which may overlap with one another: 1) initial bonding among particles, 2) neck growth, 3) pore channel closure, 4) pore rounding, 5) densification or pore shrinkage, and 6) pore coarsening. The main mechanisms present in these stages are evaporation, condensation, grain boundaries, volume diffusion, and plastic deformation.Most sintering furnaces contain three zones with three different properties that help to carry out the six steps above. The first zone, commonly coined the burn-off or purge stage, is designed to combust air, burn any contaminants such as lubricant or binders, and slowly raise the temperature of the compact materials. If the temperature of the compact parts is raised too quickly, the air in the pores will be at a very high internal pressure which could lead to expansion or fracture of the part. The second zone, known as the high-temperature stage, is used to produce solid-state diffusion and particle bonding. The material is seeking to lower its surface energy and does so by moving toward the points of contact between particles. The contact points become larger and eventually a solid mass with small pores is created. The third zone, also called the cooling period, is used to cool down the parts while still in a controlled atmosphere. This is an important zone as it prevents oxidation from immediate contact with the air or a phenomenon known as rapid cooling. All of the three stages must be carried out in a controlled atmosphere containing no oxygen. Hydrogen, nitrogen, dissociated ammonia, and cracked hydrocarbons are common gases pumped into the furnace zones providing a reducing atmosphere, preventing oxide formation.During this process, a number of characteristics are increased including the strength, ductility, toughness, and electrical and thermal conductivity of the material. If different elemental powders are compact and sintered, the material would form into alloys and intermetallic phases.As the pore sizes decrease, the density of the material will increase. As stated above, this shrinkage is a huge problem in making parts or tooling in which particular dimensions are required. The shrinkage of test materials is monitored and used to manipulate the furnace conditions or to oversize the compact materials in order to achieve the desired dimensions. Although, sintering does not deplete the compact part of porosity. In general, powder metal parts contain five to twenty-five percent porosity after sintering.To allow efficient stacking of product in the furnace during sintering and prevent parts sticking together, many manufacturers separate ware using ceramic powder separator sheets. These sheets are available in various materials such as alumina, zirconia, and magnesia. They are also available in fine, medium, and coarse particle sizes. By matching the material and particle size to the wares being sintered, surface damage and contamination can be reduced, while maximizing furnace loading per batch.
One recently developed technique for high-speed sintering involves passing high electric current through a powder to preferentially heat the asperities. Most of the energy serves to melt that portion of the compact where migration is desirable for densification; comparatively little energy is absorbed by the bulk materials and forming machinery. Naturally, this technique is not applicable to electrically insulating powders.


== Continuous powder processing ==
The phrase ""continuous process"" should be used only to describe modes of manufacturing which could be extended indefinitely in time. Normally, however, the term refers to processes whose products are much longer in one physical dimension than in the other two. Compression, rolling, and extrusion are the most common examples.
In a simple compression process, powder flows from a bin onto a two-walled channel and is repeatedly compressed vertically by a horizontally stationary punch. After stripping the compress from the conveyor, the compacted mass is introduced into a sintering furnace. An even easier approach is to spray powder onto a moving belt and sinter it without compression. However, good methods for stripping cold-pressed materials from moving belts are hard to find. One alternative that avoids the belt-stripping difficulty altogether is the manufacture of metal sheets using opposed hydraulic rams, although weakness lines across the sheet may arise during successive press operations.Powders can also be rolled to produce sheets.  The powdered metal is fed into a two-high rolling mill, and is compacted into strip form at up to 100 feet per minute (0.5 m/s).  The strip is then sintered and subjected to another rolling and further sintering.  Rolling is commonly used to produce sheet metal for electrical and electronic components, as well as coins. Considerable work also has been done on rolling multiple layers of different materials simultaneously into sheets.Extrusion processes are of two general types. In one type, the powder is mixed with a binder or plasticizer at room temperature; in the other, the powder is extruded at elevated temperatures without fortification. Extrusions with binders are used extensively in the preparation of tungsten-carbide composites. Tubes, complex sections, and spiral drill shapes are manufactured in extended lengths and diameters varying in the range 0.5–300 mm (0.020–11.811 in). Hard metal wires of 0.1 mm (0.0039 in) diameter have been drawn from powder stock. At the opposite extreme, large extrusions on a tonnage basis may be feasible.
For softer, easier to form metals such as aluminium and copper alloys continuous extrusion may also be performed using processes such as conform or continuous rotary extrusion. These processes use a rotating wheel with a groove around its circumference to drive the loose powder through a forming die. Through a combination of high pressure and a complex strain path the powder particles deform, generate a large amount of frictional heat and bond together to form a bulk solid. Theoretically fully continuous operation is possible as long as the powder can be fed into the process.There appears to be no limitation to the variety of metals and alloys that can be extruded, provided the temperatures and pressures involved are within the capabilities of die materials. Extrusion lengths may range from 3 to 30 m and diameters from 0.2 to 1 m. Modern presses are largely automatic and operate at high speeds (on the order of m/s).


== Shock (dynamic) consolidation ==
Shock consolidation, or dynamic consolidation, is an experimental technique of consolidating powders using high pressure shock waves.  These are commonly produced by impacting the workpiece with an explosively accelerated plate.  Despite being researched for a long time, the technique still has some problems in controlability and uniformity. However, it offers some valuable potential advantages.  As an example, consolidation occurs so rapidly that metastable microstructures may be retained.


== Electric current assisted sintering ==
These techniques employ electric currents to drive or enhance sintering. Through a combination of electric currents and mechanical pressure powders sinter more rapidly thereby reducing the sintering time compared to conventional thermal solutions. The techniques can be divided into two main categories: resistance sintering, which incorporates spark plasma sintering and hot pressing; and electric discharge sintering, such as capacitor discharge sintering or its derivative, electro sinter forging. Resistance sintering techniques are consolidation methods based on temperature, where heating of the mold and of the powders is accomplished through electric currents, usually with a characteristic processing time of 15 to 30 minutes.  On the other hand, electric discharge sintering methods rely on high-density currents (from 0.1 to 1 kA/mm^2) to directly sinter electrically conductive powders, with a characteristic time between tens of microseconds to hundreds of milliseconds.


== Special products ==
Many special products are possible with powder metallurgy technology. A nonexhaustive list includes Al2O3 whiskers coated with very thin oxide layers for improved refraction; iron compacts with Al2O3 coatings for improved high-temperature creep strength; light bulb filaments made with powder technology; linings for friction brakes; metal glasses for high-strength films and ribbons; heat shields for spacecraft reentry into Earth's atmosphere; electrical contacts for handling large current flows; magnets; microwave ferrites; filters for gases; and bearings which can be infiltrated with lubricants.
Extremely thin films and tiny spheres exhibit high strength. One application of this observation is to coat brittle materials in whisker form with a submicrometre film of much softer metal (e.g. cobalt-coated tungsten). The surface strain of the thin layer places the harder metal under compression, so that when the entire composite is sintered the rupture strength increases markedly. With this method, strengths on the order of 2.8 GPa versus 550 MPa have been observed for, respectively, coated (25% cobalt) and uncoated tungsten carbides.


== Hazards ==
The special materials and processes used in powder metallurgy can pose hazards to life and property. The high surface-area-to-volume ratio of the powders can increase their chemical reactivity in biological exposures (for example, inhalation or ingestion), and increases the risk of dust explosions. Materials considered relatively benign in bulk can pose special toxicological risks when in a finely divided form.


== See also ==
Electro sinter forging
Mechanical powder press
Selective laser melting
Selective laser sintering
Sintering
Spark plasma sintering
Spray forming


== References ==


== Cited sources ==
DeGarmo, E. P. (2008). Materials and Processes in Manufacturing (PDF) (10th ed.). Wiley. ISBN 9780470055120.


== Further reading ==
An earlier version of this article was copied from Appendix 4C of Advanced Automation for Space Missions, a NASA report in the public domain.
R. M. German, ""Powder Metallurgy and Particulate Materials Processing,"" Metal Powder Industries Federation, Princeton, New Jersey, 2005.
F. Thummler and R.Oberacker ""An Introduction to Powder Metallurgy"" The institute of Materials, London 1993
G. S. Upadhyaya, ""Sintered Metallic and Ceramic Materials"" John Wiley and Sons, West Sussex, England, 2000


== External links ==
Rapid manufacturing technique developed at the KU Leuven, Belgium
Slow motion video images of metal atomization at the Ames Laboratory
APMI International ""The Global Professional Society for Powder Metallurgy""[1], a non-profit organization","pandas(index=121, _1=121, text='powder metallurgy (pm) is a term covering a wide range of ways in which materials or components are made from metal powders. pm processes can avoid, or greatly reduce, the need to use metal removal processes, thereby drastically reducing yield losses in manufacture and often resulting in lower costs. powder metallurgy is also used to make unique materials impossible to get from melting or forming in other ways. a very important product of this type is tungsten carbide (wc). wc is used to cut and form other metals and is made from wc particles bonded with cobalt. it is very widely used in industry for tools of many types and globally ~50,000 tonnes/year (t/y) is made by pm. other products include sintered filters, porous oil-impregnated bearings, electrical contacts and diamond tools. since the advent of industrial production–scale metal powder–based additive manufacturing (am) in the 2010s, selective laser sintering and other metal am processes are a new category of commercially important powder metallurgy applications.   == overview == the powder metallurgy press and sinter process generally consists of three basic steps: powder blending (pulverisation), die compaction, and sintering. compaction is generally performed at room temperature, and the elevated-temperature process of sintering is usually conducted at atmospheric pressure and under carefully controlled atmosphere composition. optional secondary processing such as coining or heat treatment often follows to obtain special properties or enhanced precision.one of the older such methods, and still one used to make around 1 mt/y of structural components of iron-based alloys, is the process of blending fine (<180 microns) metal (normally iron) powders with additives such as a lubricant wax, carbon, copper, and/or nickel, pressing them into a die of the desired shape, and then heating the compressed material (""green part"") in a controlled atmosphere to bond the material by sintering. this produces precise parts, normally very close to the die dimensions, but with 5–15% porosity, and thus sub-wrought steel properties.  there are several other pm processes which have been developed over the last fifty years. these include:  powder forging: a ""preform"" made by the conventional ""press and sinter"" method is heated and then hot forged to full density, resulting in practically as-wrought properties. hot isostatic pressing (hip): here the powder (normally gas atomized, spherical type) is filled into a mould, normally consisting of a metallic ""can"" of suitable shape. the can is vibrated, then evacuated and sealed.  it is then placed in a hot isostatic press, where it is heated to a homologous temperature of around 0.7, and subjected to an external gas pressure of ~100 mpa (1000 bar, 15,000 psi) for several hours. this results in a shaped part of full density with as-wrought or better, properties. hip was invented in the 1950-60s and entered tonnage production in the 1970-80s. in 2015, it was used to produce ~25,000 t/y of stainless and tool steels, as well as important parts of superalloys for jet engines. metal injection moulding (mim):  here the powder, normally very fine (<25 microns) and spherical, is mixed with plastic or wax binder to near the maximum solid loading, typically around 65vol%, and injection moulded to form a ""green"" part of complex geometry. this part is then heated or otherwise treated to remove the binder (debinding) to give a ""brown"" part. this part is then sintered, and shrinks by ~18% to give a complex and 95–99% dense finished part (surface roughness ~3 microns). invented in the 1970s, production has increased since 2000 with an estimated global volume in 2014 of 12,000 t worth €1265 millions. electric current assisted sintering (ecas) technologies rely on electric currents to densify powders, with the advantage of reducing production time dramatically (from 15 minutes of the slowest ecas to a few microseconds of the fastest), not requiring a long furnace heat and allowing near theoretical densities but with the drawback of simple shapes. powders employed in ecas can avoid binders thanks to the possibility of direct sintering, without the need of pre-pressing and a green compact. molds are designed for the final part shape since the powders densify while filling the cavity under an applied pressure thus avoiding the problem of shape variations caused by non isotropic sintering and distortions caused by gravity at high temperatures. the most common of these technologies is hot pressing, which has been under use for the production of the diamond tools employed in the construction industry. spark plasma sintering and electro sinter forging are two modern, industrial commercial ecas technologies. additive manufacturing (am) is a relatively novel family of techniques which use metal powders (among other materials, such as plastics) to make parts by laser sintering or melting. this is a process under rapid development as of 2015, and whether to classify it as a pm process is perhaps uncertain at this stage. processes include 3d printing, selective laser sintering (sls), selective laser melting (slm), and electron beam melting (ebm).   == history and capabilities == the history of powder metallurgy and the art of metal and ceramic sintering are intimately related to each other. sintering involves the production of a hard solid metal or ceramic piece from a starting powder. the ancient incas made jewelry and other artifacts from precious metal powders, though mass manufacturing of pm products did not begin until the mid or late 19th century. in these early manufacturing operations, iron was extracted by hand from metal sponge following reduction and was then reintroduced as a powder for final melting or sintering. a much wider range of products can be obtained from powder processes than from direct alloying of fused materials. in melting operations the ""phase rule"" applies to all pure and combined elements and strictly dictates the distribution of liquid and solid phases which can exist for specific compositions. in addition, whole body melting of starting materials is required for alloying, thus imposing unwelcome chemical, thermal, and containment constraints on manufacturing. unfortunately, the handling of aluminium/iron powders poses major problems. other substances that are especially reactive with atmospheric oxygen, such as titanium, are sinterable in special atmospheres or with temporary coatings.in powder metallurgy or ceramics it is possible to fabricate components which otherwise would decompose or disintegrate. all considerations of solid-liquid phase changes can be ignored, so powder processes are more flexible than casting, extrusion, or forging techniques. controllable characteristics of products prepared using various powder technologies include mechanical, magnetic, and other unconventional properties of such materials as porous solids, aggregates, and intermetallic compounds. competitive characteristics of manufacturing processing (e.g. tool wear, complexity, or vendor options) also may be closely controlled.   == powder production techniques == any fusible material can be atomized. several techniques have been developed which permit large production rates of powdered particles, often with considerable control over the size ranges of the final grain population. powders may be prepared by crushing, grinding, chemical reactions, or electrolytic deposition. the most commonly used powders are copper-base and iron-base materials.powders of the elements titanium, vanadium, thorium, niobium, tantalum, calcium, and uranium have been produced by high-temperature reduction of the corresponding nitrides and carbides. iron, nickel, uranium, and beryllium submicrometre powders are obtained by reducing metallic oxalates and formates. exceedingly fine particles also have been prepared by directing a stream of molten metal through a high-temperature plasma jet or flame,  atomizing the material. various chemical and flame associated powdering processes are adopted in part to prevent serious degradation of particle surfaces by atmospheric oxygen. in tonnage terms, the production of iron powders for pm structural part production dwarfs the production of all of the non-ferrous metal powders combined. virtually all iron powders are produced by one of two processes: the sponge iron process or water atomization. advantages over standard powder compaction are the possibility of thinner walls and larger workpieces. height to diameter ratio has no limitation. no specific limitations exist in wall thickness variations, undercuts, reliefs, threads, and cross holes. no lubricants are need for isostatic powder compaction. the minimum wall thickness is 0.05 inches (1.27 mm) and the product can have a weight between 40 and 300 pounds (18 and 136 kg). there is 25 to 45% shrinkage of the powder after compacting.   == sintering == after compaction, powdered materials are heated in a controlled atmosphere in a process known as sintering. during this process, the surfaces of the particles are bonded and desirable properties are achieved.sintering of powder metals is a process in which particles under pressure chemically bond to themselves in order to form a coherent shape when exposed to a high temperature. the temperature in which the particles are sintered is most commonly below the melting point of the main component in the powder. if the temperature is above the melting point of a component in the powder metal part, the liquid of the melted particles fills the pores. this type of sintering is known as liquid-state sintering. a major challenge with sintering in general is knowing the effect of the process on the dimensions of the compact particles. this is especially difficult for tooling purposes in which specific dimensions may be needed. it is most common for the sintered part to shrink and become denser, but it can also expand or experience no net change.the main driving force for solid state sintering is an excess of surface free energy. the process of solid-state sintering is complex and dependent on the material and furnace (temperature and gas) conditions. there are six main stages that sintering processes can be grouped in which may overlap with one another: 1) initial bonding among particles, 2) neck growth, 3) pore channel closure, 4) pore rounding, 5) densification or pore shrinkage, and 6) pore coarsening. the main mechanisms present in these stages are evaporation, condensation, grain boundaries, volume diffusion, and plastic deformation.most sintering furnaces contain three zones with three different properties that help to carry out the six steps above. the first zone, commonly coined the burn-off or purge stage, is designed to combust air, burn any contaminants such as lubricant or binders, and slowly raise the temperature of the compact materials. if the temperature of the compact parts is raised too quickly, the air in the pores will be at a very high internal pressure which could lead to expansion or fracture of the part. the second zone, known as the high-temperature stage, is used to produce solid-state diffusion and particle bonding. the material is seeking to lower its surface energy and does so by moving toward the points of contact between particles. the contact points become larger and eventually a solid mass with small pores is created. the third zone, also called the cooling period, is used to cool down the parts while still in a controlled atmosphere. this is an important zone as it prevents oxidation from immediate contact with the air or a phenomenon known as rapid cooling. all of the three stages must be carried out in a controlled atmosphere containing no oxygen. hydrogen, nitrogen, dissociated ammonia, and cracked hydrocarbons are common gases pumped into the furnace zones providing a reducing atmosphere, preventing oxide formation.during this process, a number of characteristics are increased including the strength, ductility, toughness, and electrical and thermal conductivity of the material. if different elemental powders are compact and sintered, the material would form into alloys and intermetallic phases.as the pore sizes decrease, the density of the material will increase. as stated above, this shrinkage is a huge problem in making parts or tooling in which particular dimensions are required. the shrinkage of test materials is monitored and used to manipulate the furnace conditions or to oversize the compact materials in order to achieve the desired dimensions. although, sintering does not deplete the compact part of porosity. in general, powder metal parts contain five to twenty-five percent porosity after sintering.to allow efficient stacking of product in the furnace during sintering and prevent parts sticking together, many manufacturers separate ware using ceramic powder separator sheets. these sheets are available in various materials such as alumina, zirconia, and magnesia. they are also available in fine, medium, and coarse particle sizes. by matching the material and particle size to the wares being sintered, surface damage and contamination can be reduced, while maximizing furnace loading per batch. one recently developed technique for high-speed sintering involves passing high electric current through a powder to preferentially heat the asperities. most of the energy serves to melt that portion of the compact where migration is desirable for densification; comparatively little energy is absorbed by the bulk materials and forming machinery. naturally, this technique is not applicable to electrically insulating powders.   == continuous powder processing == the phrase ""continuous process"" should be used only to describe modes of manufacturing which could be extended indefinitely in time. normally, however, the term refers to processes whose products are much longer in one physical dimension than in the other two. compression, rolling, and extrusion are the most common examples. in a simple compression process, powder flows from a bin onto a two-walled channel and is repeatedly compressed vertically by a horizontally stationary punch. after stripping the compress from the conveyor, the compacted mass is introduced into a sintering furnace. an even easier approach is to spray powder onto a moving belt and sinter it without compression. however, good methods for stripping cold-pressed materials from moving belts are hard to find. one alternative that avoids the belt-stripping difficulty altogether is the manufacture of metal sheets using opposed hydraulic rams, although weakness lines across the sheet may arise during successive press operations.powders can also be rolled to produce sheets.  the powdered metal is fed into a two-high rolling mill, and is compacted into strip form at up to 100 feet per minute (0.5 m/s).  the strip is then sintered and subjected to another rolling and further sintering.  rolling is commonly used to produce sheet metal for electrical and electronic components, as well as coins. considerable work also has been done on rolling multiple layers of different materials simultaneously into sheets.extrusion processes are of two general types. in one type, the powder is mixed with a binder or plasticizer at room temperature; in the other, the powder is extruded at elevated temperatures without fortification. extrusions with binders are used extensively in the preparation of tungsten-carbide composites. tubes, complex sections, and spiral drill shapes are manufactured in extended lengths and diameters varying in the range 0.5–300 mm (0.020–11.811 in). hard metal wires of 0.1 mm (0.0039 in) diameter have been drawn from powder stock. at the opposite extreme, large extrusions on a tonnage basis may be feasible. for softer, easier to form metals such as aluminium and copper alloys continuous extrusion may also be performed using processes such as conform or continuous rotary extrusion. these processes use a rotating wheel with a groove around its circumference to drive the loose powder through a forming die. through a combination of high pressure and a complex strain path the powder particles deform, generate a large amount of frictional heat and bond together to form a bulk solid. theoretically fully continuous operation is possible as long as the powder can be fed into the process.there appears to be no limitation to the variety of metals and alloys that can be extruded, provided the temperatures and pressures involved are within the capabilities of die materials. extrusion lengths may range from 3 to 30 m and diameters from 0.2 to 1 m. modern presses are largely automatic and operate at high speeds (on the order of m/s).   == shock (dynamic) consolidation == shock consolidation, or dynamic consolidation, is an experimental technique of consolidating powders using high pressure shock waves.  these are commonly produced by impacting the workpiece with an explosively accelerated plate.  despite being researched for a long time, the technique still has some problems in controlability and uniformity. however, it offers some valuable potential advantages.  as an example, consolidation occurs so rapidly that metastable microstructures may be retained.   == electric current assisted sintering == these techniques employ electric currents to drive or enhance sintering. through a combination of electric currents and mechanical pressure powders sinter more rapidly thereby reducing the sintering time compared to conventional thermal solutions. the techniques can be divided into two main categories: resistance sintering, which incorporates spark plasma sintering and hot pressing; and electric discharge sintering, such as capacitor discharge sintering or its derivative, electro sinter forging. resistance sintering techniques are consolidation methods based on temperature, where heating of the mold and of the powders is accomplished through electric currents, usually with a characteristic processing time of 15 to 30 minutes.  on the other hand, electric discharge sintering methods rely on high-density currents (from 0.1 to 1 ka/mm^2) to directly sinter electrically conductive powders, with a characteristic time between tens of microseconds to hundreds of milliseconds.   == special products == many special products are possible with powder metallurgy technology. a nonexhaustive list includes al2o3 whiskers coated with very thin oxide layers for improved refraction; iron compacts with al2o3 coatings for improved high-temperature creep strength; light bulb filaments made with powder technology; linings for friction brakes; metal glasses for high-strength films and ribbons; heat shields for spacecraft reentry into earth\'s atmosphere; electrical contacts for handling large current flows; magnets; microwave ferrites; filters for gases; and bearings which can be infiltrated with lubricants. extremely thin films and tiny spheres exhibit high strength. one application of this observation is to coat brittle materials in whisker form with a submicrometre film of much softer metal (e.g. cobalt-coated tungsten). the surface strain of the thin layer places the harder metal under compression, so that when the entire composite is sintered the rupture strength increases markedly. with this method, strengths on the order of 2.8 gpa versus 550 mpa have been observed for, respectively, coated (25% cobalt) and uncoated tungsten carbides.   == hazards == the special materials and processes used in powder metallurgy can pose hazards to life and property. the high surface-area-to-volume ratio of the powders can increase their chemical reactivity in biological exposures (for example, inhalation or ingestion), and increases the risk of dust explosions. materials considered relatively benign in bulk can pose special toxicological risks when in a finely divided form.   == see also == electro sinter forging mechanical powder press selective laser melting selective laser sintering sintering spark plasma sintering spray forming   == references ==   == cited sources == degarmo, e. p. (2008). materials and processes in manufacturing (pdf) (10th ed.). wiley. isbn 9780470055120.   == further reading == an earlier version of this article was copied from appendix 4c of advanced automation for space missions, a nasa report in the public domain. r. m. german, ""powder metallurgy and particulate materials processing,"" metal powder industries federation, princeton, new jersey, 2005. f. thummler and r.oberacker ""an introduction to powder metallurgy"" the institute of materials, london 1993 g. s. upadhyaya, ""sintered metallic and ceramic materials"" john wiley and sons, west sussex, england, 2000   == external links == rapid manufacturing technique developed at the ku leuven, belgium slow motion video images of metal atomization at the ames laboratory apmi international ""the global professional society for powder metallurgy""[1], a non-profit organization')"
122,"In materials science, the sol–gel process is a method for producing solid materials from small molecules. The method is used for the fabrication of metal oxides, especially the oxides of silicon (Si) and titanium (Ti). The process involves conversion of monomers into a colloidal solution (sol) that acts as the precursor for an integrated network (or gel) of either discrete particles or network polymers. Typical precursors are metal alkoxides.


== Stages in the process ==

In this chemical procedure, a ""sol"" (a colloidal solution) is formed that then gradually evolves towards the formation of a gel-like diphasic system containing both a liquid phase and solid phase whose morphologies range from discrete particles to continuous polymer networks. In the case of the colloid, the volume fraction of particles (or particle density) may be so low that a significant amount of fluid may need to be removed initially for the gel-like properties to be recognized. This can be accomplished in any number of ways. The simplest method is to allow time for sedimentation to occur, and then pour off the remaining liquid. Centrifugation can also be used to accelerate the process of phase separation.
Removal of the remaining liquid (solvent) phase requires a drying process, which is typically accompanied by a significant amount of shrinkage and densification. The rate at which the solvent can be removed is ultimately determined by the distribution of porosity in the gel. The ultimate microstructure of the final component will clearly be strongly influenced by changes imposed upon the structural template during this phase of processing.
Afterwards, a thermal treatment, or firing process, is often necessary in order to favor further polycondensation and enhance mechanical properties and structural stability via final sintering, densification, and grain growth. One of the distinct advantages of using this methodology as opposed to the more traditional processing techniques is that densification is often achieved at a much lower temperature.
The precursor sol can be either deposited on a substrate to form a film (e.g., by dip-coating or spin coating), cast into a suitable container with the desired shape (e.g., to obtain monolithic ceramics, glasses, fibers, membranes, aerogels), or used to synthesize powders (e.g., microspheres, nanospheres). The sol–gel approach is a cheap and low-temperature technique that allows the fine control of the product's chemical composition. Even small quantities of dopants, such as organic dyes and rare-earth elements, can be introduced in the sol and end up uniformly dispersed in the final product. It can be used in ceramics processing and manufacturing as an investment casting material, or as a means of producing very thin films of metal oxides for various purposes. Sol–gel derived materials have diverse applications in optics, electronics, energy, space, (bio)sensors, medicine (e.g., controlled drug release), reactive material, and separation (e.g., chromatography) technology.
The interest in sol–gel processing can be traced back in the mid-1800s with the observation that the hydrolysis of tetraethyl orthosilicate (TEOS) under acidic conditions led to the formation of SiO2 in the form of fibers and monoliths. Sol–gel research grew to be so important that in the 1990s more than 35,000 papers were published worldwide on the process.


== Particles and polymers ==
The sol–gel process is a wet-chemical technique used for the fabrication of both glassy and ceramic materials. In this process, the sol (or solution) evolves gradually towards the formation of a gel-like network containing both a liquid phase and a solid phase. Typical precursors are metal alkoxides and metal chlorides, which undergo hydrolysis and polycondensation reactions to form a colloid. The basic structure or morphology of the solid phase can range anywhere from discrete colloidal particles to continuous chain-like polymer networks.The term colloid is used primarily to describe a broad range of solid-liquid (and/or liquid-liquid) mixtures, all of which contain distinct solid (and/or liquid) particles which are dispersed to various degrees in a liquid medium. The term is specific to the size of the individual particles, which are larger than atomic dimensions but small enough to exhibit Brownian motion. If the particles are large enough, then their dynamic behavior in any given period of time in suspension would be governed by forces of gravity and sedimentation. But if they are small enough to be colloids, then their irregular motion in suspension can be attributed to the collective bombardment of a myriad of thermally agitated molecules in the liquid suspending medium, as described originally by Albert Einstein in his dissertation. Einstein concluded that this erratic behavior could adequately be described using the theory of Brownian motion, with sedimentation being a possible long-term result. This critical size range (or particle diameter) typically ranges from tens of angstroms (10−10 m) to a few micrometres (10−6 m).
Under certain chemical conditions (typically in base-catalyzed sols), the particles may grow to sufficient size to become colloids, which are affected both by sedimentation and forces of gravity. Stabilized suspensions of such sub-micrometre spherical particles may eventually result in their self-assembly—yielding highly ordered microstructures reminiscent of the prototype colloidal crystal: precious opal.
Under certain chemical conditions (typically in acid-catalyzed sols), the interparticle forces have sufficient strength to cause considerable aggregation and/or flocculation prior to their growth. The formation of a more open continuous network of low density polymers exhibits certain advantages with regard to physical properties in the formation of high performance glass and glass/ceramic components in 2 and 3 dimensions.In either case (discrete particles or continuous polymer network) the sol evolves then towards the formation of an inorganic network containing a liquid phase (gel). Formation of a metal oxide involves connecting the metal centers with oxo (M-O-M) or hydroxo (M-OH-M) bridges, therefore generating metal-oxo or metal-hydroxo polymers in solution.
In both cases (discrete particles or continuous polymer network), the drying process serves to remove the liquid phase from the gel, yielding a micro-porous amorphous glass or micro-crystalline ceramic. Subsequent thermal treatment (firing) may be performed in order to favor further polycondensation and enhance mechanical properties.
With the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. In addition, uniform ceramic powders of a wide range of chemical composition can be formed by precipitation.


== Polymerization ==

The Stöber process is a well-studied example of polymerization of an alkoxide, specifically TEOS. The chemical formula for TEOS is given by Si(OC2H5)4, or Si(OR)4, where the alkyl group R = C2H5. Alkoxides are ideal chemical precursors for sol–gel synthesis because they react readily with water. The reaction is called hydrolysis, because a hydroxyl ion becomes attached to the silicon atom as follows:

Si(OR)4 + H2O → HO−Si(OR)3 + R−OHDepending on the amount of water and catalyst present, hydrolysis may proceed to completion to silica:

Si(OR)4 + 2 H2O → SiO2 + 4 R−OHComplete hydrolysis often requires an excess of water and/or the use of a hydrolysis catalyst such as acetic acid or hydrochloric acid. Intermediate species including [(OR)2−Si−(OH)2] or [(OR)3−Si−(OH)] may result as products of partial hydrolysis reactions. Early intermediates result from two partially hydrolyzed monomers linked with a siloxane [Si−O−Si] bond:

(OR)3−Si−OH + HO−Si−(OR)3 → [(OR)3Si−O−Si(OR)3] + H−O−Hor

(OR)3−Si−OR + HO−Si−(OR)3 → [(OR)3Si−O−Si(OR)3] + R−OHThus, polymerization is associated with the formation of a 1-, 2-, or 3-dimensional network of siloxane [Si−O−Si] bonds accompanied by the production of H−O−H and R−O−H species.
By definition, condensation liberates a small molecule, such as water or alcohol. This type of reaction can continue to build larger and larger silicon-containing molecules by the process of polymerization. Thus, a polymer is a huge molecule (or macromolecule) formed from hundreds or thousands of units called monomers. The number of bonds that a monomer can form is called its functionality. Polymerization of silicon alkoxide, for instance, can lead to complex branching of the polymer, because a fully hydrolyzed monomer Si(OH)4 is tetrafunctional (can branch or bond in 4 different directions). Alternatively, under certain conditions (e.g., low water concentration) fewer than 4 of the OR or OH groups (ligands) will be capable of condensation, so relatively little branching will occur. The mechanisms of hydrolysis and condensation, and the factors that bias the structure toward linear or branched structures are the most critical issues of sol–gel science and technology. This reaction is favored in both basic and acidic conditions.


== Sono-Ormosil ==
Sonication is an efficient tool for the synthesis of polymers. The cavitational shear forces, which stretch out and break the chain in a non-random process, result in a lowering of the molecular weight and poly-dispersity. Furthermore, multi-phase systems are very efficient dispersed and emulsified, so that very fine mixtures are provided. This means that ultrasound increases the rate of polymerisation over conventional stirring and results in higher molecular weights with lower polydispersities. Ormosils (organically modified silicate) are obtained when silane is added to gel-derived silica during sol–gel process. The product is a molecular-scale composite with improved mechanical properties. Sono-Ormosils are characterized by a higher density than classic gels as well as an improved thermal stability. An explanation therefore might be the increased degree of polymerization.


== Pechini process ==
For single cation systems like SiO2 and TiO2, hydrolysis and condensation processes naturally give rise to homogenous compositions. For systems involving multiple cations, such as strontium titanate, SrTiO3 and other perovskite systems, the concept of steric immobilisation becomes relevant. To avoid the formation of multiple phases of binary oxides as the result of differing hydrolysis and condensation rates, the entrapment of cations in a polymer network is an effective approach, generally termed the Pechini Process. In this process, a chelating agent is used, most often citric acid, to surround aqueous cations and sterically entrap them. Subsequently, a polymer network is formed to immobilize the chelated cations in a gel or resin. This is most often achieved by poly-esterification using ethylene glycol. The resulting polymer is then combusted under oxidising conditions to remove organic content and yield a product oxide with homogeneously dispersed cations.


== Nanomaterials ==

In the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. Uncontrolled flocculation of powders due to attractive van der Waals forces can also give rise to microstructural heterogeneities.Differential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. Such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved.
In addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding heterogeneous densification.
Some pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. Differential stresses arising from heterogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.It would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. The containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. Monodisperse colloids provide this potential.Monodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. The degree of order appears to be limited by the time and space allowed for longer-range correlations to be established. Such defective polycrystalline structures would appear to be the basic elements of nanoscale materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as sintered ceramic nanomaterials.


== Applications ==
The applications for sol gel-derived products are numerous. For example, scientists have used it to produce the world's lightest materials and also some of its toughest ceramics.


=== Protective coatings ===
One of the largest application areas is thin films, which can be produced on a piece of substrate by spin coating or dip-coating. Protective and decorative coatings, and electro-optic components can be applied to glass, metal and other types of substrates with these methods. Cast into a mold, and with further drying and heat-treatment, dense ceramic or glass articles with novel properties can be formed that cannot be created by any other method. Other coating methods include spraying, electrophoresis, inkjet printing, or roll coating.


=== Thin films and fibers ===
With the viscosity of a sol adjusted into a proper range, both optical and refractory ceramic fibers can be drawn which are used for fiber optic sensors and thermal insulation, respectively. Thus, many ceramic materials, both glassy and crystalline, have found use in various forms from bulk solid-state components to high surface area forms such as thin films, coatings and fibers.


=== Nanoscale powders ===
Ultra-fine and uniform ceramic powders can be formed by precipitation. These powders of single and multiple component compositions can be produced on a nanoscale particle size for dental and biomedical applications. Composite powders have been patented for use as agrochemicals and herbicides. Powder abrasives, used in a variety of finishing operations, are made using a sol–gel type process. One of the more important applications of sol–gel processing is to carry out zeolite synthesis. Other elements (metals, metal oxides) can be easily incorporated into the final product and the silicate sol formed by this method is very stable.
Another application in research is to entrap biomolecules for sensory (biosensors) or catalytic purposes, by physically or chemically preventing them from leaching out and, in the case of protein or chemically-linked small molecules, by shielding them from the external environment yet allowing small molecules to be monitored. The major disadvantages are that the change in local environment may alter the functionality of the protein or small molecule entrapped and that the synthesis step may damage the protein. To circumvent this, various strategies have been explored, such as monomers with protein friendly leaving groups (e.g. glycerol) and the inclusion of polymers which stabilize protein (e.g. PEG).Other products fabricated with this process include various ceramic membranes for microfiltration, ultrafiltration, nanofiltration, pervaporation, and reverse osmosis. If the liquid in a wet gel is removed under a supercritical condition, a highly porous and extremely low density material called aerogel is obtained. Drying the gel by means of low temperature treatments (25-100 °C), it is possible to obtain porous solid matrices called xerogels. In addition, a sol–gel process was developed in the 1950s for the production of radioactive powders of UO2 and ThO2 for nuclear fuels, without generation of large quantities of dust.


=== Opto-mechanical ===
Macroscopic optical elements and active optical components as well as large area hot mirrors, cold mirrors, lenses, and beam splitters all with optimal geometry can be made quickly and at low cost via the sol–gel route. In the processing of high performance ceramic nanomaterials with superior opto-mechanical properties under adverse conditions, the size of the crystalline grains is determined largely by the size of the crystalline particles present in the raw material during the synthesis or formation of the object. Thus a reduction of the original particle size well below the wavelength of visible light (~500 nm) eliminates much of the light scattering, resulting in a translucent or even transparent material.
Furthermore, results indicate that microscopic pores in sintered ceramic nanomaterials, mainly trapped at the junctions of microcrystalline grains, cause light to scatter and prevented true transparency. it has been observed that the total volume fraction of these nanoscale pores (both intergranular and intragranular porosity) must be less than 1% for high-quality optical transmission. I.E. The density has to be 99.99% of the theoretical crystalline density.


=== Medicine ===
Unique properties of the sol–gel provide the possibility of their use for a variety of medical applications. A sol–gel processed alumina can be used as a carrier for the sustained delivery of drugs and as an established wound healer. A marked decrease in scar size was observed because of the wound healing composite including sol–gel processed alumina. A novel approach to thrombolysis treatment is possible by developing a new family of injectable composites: plasminogen activator entrapped within alumina.


== See also ==
Coacervate, small spheroidal droplet of colloidal particles in suspension
Freeze-casting
Freeze gelation
Mechanics of gelation


== References ==


== Further reading ==
Colloidal Dispersions, Russel, W. B., et al., Eds., Cambridge University Press (1989)
Glasses and the Vitreous State, Zarzycki. J., Cambridge University Press, 1991
The Sol to Gel Transition. Plinio Innocenzi. Springer Briefs in Materials. Springer. 2016.


== External links ==
International Sol–Gel Society
The Sol–Gel Gateway","pandas(index=122, _1=122, text='in materials science, the sol–gel process is a method for producing solid materials from small molecules. the method is used for the fabrication of metal oxides, especially the oxides of silicon (si) and titanium (ti). the process involves conversion of monomers into a colloidal solution (sol) that acts as the precursor for an integrated network (or gel) of either discrete particles or network polymers. typical precursors are metal alkoxides.   == stages in the process ==  in this chemical procedure, a ""sol"" (a colloidal solution) is formed that then gradually evolves towards the formation of a gel-like diphasic system containing both a liquid phase and solid phase whose morphologies range from discrete particles to continuous polymer networks. in the case of the colloid, the volume fraction of particles (or particle density) may be so low that a significant amount of fluid may need to be removed initially for the gel-like properties to be recognized. this can be accomplished in any number of ways. the simplest method is to allow time for sedimentation to occur, and then pour off the remaining liquid. centrifugation can also be used to accelerate the process of phase separation. removal of the remaining liquid (solvent) phase requires a drying process, which is typically accompanied by a significant amount of shrinkage and densification. the rate at which the solvent can be removed is ultimately determined by the distribution of porosity in the gel. the ultimate microstructure of the final component will clearly be strongly influenced by changes imposed upon the structural template during this phase of processing. afterwards, a thermal treatment, or firing process, is often necessary in order to favor further polycondensation and enhance mechanical properties and structural stability via final sintering, densification, and grain growth. one of the distinct advantages of using this methodology as opposed to the more traditional processing techniques is that densification is often achieved at a much lower temperature. the precursor sol can be either deposited on a substrate to form a film (e.g., by dip-coating or spin coating), cast into a suitable container with the desired shape (e.g., to obtain monolithic ceramics, glasses, fibers, membranes, aerogels), or used to synthesize powders (e.g., microspheres, nanospheres). the sol–gel approach is a cheap and low-temperature technique that allows the fine control of the product\'s chemical composition. even small quantities of dopants, such as organic dyes and rare-earth elements, can be introduced in the sol and end up uniformly dispersed in the final product. it can be used in ceramics processing and manufacturing as an investment casting material, or as a means of producing very thin films of metal oxides for various purposes. sol–gel derived materials have diverse applications in optics, electronics, energy, space, (bio)sensors, medicine (e.g., controlled drug release), reactive material, and separation (e.g., chromatography) technology. the interest in sol–gel processing can be traced back in the mid-1800s with the observation that the hydrolysis of tetraethyl orthosilicate (teos) under acidic conditions led to the formation of sio2 in the form of fibers and monoliths. sol–gel research grew to be so important that in the 1990s more than 35,000 papers were published worldwide on the process.   == particles and polymers == the sol–gel process is a wet-chemical technique used for the fabrication of both glassy and ceramic materials. in this process, the sol (or solution) evolves gradually towards the formation of a gel-like network containing both a liquid phase and a solid phase. typical precursors are metal alkoxides and metal chlorides, which undergo hydrolysis and polycondensation reactions to form a colloid. the basic structure or morphology of the solid phase can range anywhere from discrete colloidal particles to continuous chain-like polymer networks.the term colloid is used primarily to describe a broad range of solid-liquid (and/or liquid-liquid) mixtures, all of which contain distinct solid (and/or liquid) particles which are dispersed to various degrees in a liquid medium. the term is specific to the size of the individual particles, which are larger than atomic dimensions but small enough to exhibit brownian motion. if the particles are large enough, then their dynamic behavior in any given period of time in suspension would be governed by forces of gravity and sedimentation. but if they are small enough to be colloids, then their irregular motion in suspension can be attributed to the collective bombardment of a myriad of thermally agitated molecules in the liquid suspending medium, as described originally by albert einstein in his dissertation. einstein concluded that this erratic behavior could adequately be described using the theory of brownian motion, with sedimentation being a possible long-term result. this critical size range (or particle diameter) typically ranges from tens of angstroms (10−10 m) to a few micrometres (10−6 m). under certain chemical conditions (typically in base-catalyzed sols), the particles may grow to sufficient size to become colloids, which are affected both by sedimentation and forces of gravity. stabilized suspensions of such sub-micrometre spherical particles may eventually result in their self-assembly—yielding highly ordered microstructures reminiscent of the prototype colloidal crystal: precious opal. under certain chemical conditions (typically in acid-catalyzed sols), the interparticle forces have sufficient strength to cause considerable aggregation and/or flocculation prior to their growth. the formation of a more open continuous network of low density polymers exhibits certain advantages with regard to physical properties in the formation of high performance glass and glass/ceramic components in 2 and 3 dimensions.in either case (discrete particles or continuous polymer network) the sol evolves then towards the formation of an inorganic network containing a liquid phase (gel). formation of a metal oxide involves connecting the metal centers with oxo (m-o-m) or hydroxo (m-oh-m) bridges, therefore generating metal-oxo or metal-hydroxo polymers in solution. in both cases (discrete particles or continuous polymer network), the drying process serves to remove the liquid phase from the gel, yielding a micro-porous amorphous glass or micro-crystalline ceramic. subsequent thermal treatment (firing) may be performed in order to favor further polycondensation and enhance mechanical properties. with the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. in addition, uniform ceramic powders of a wide range of chemical composition can be formed by precipitation.   == polymerization ==  the stöber process is a well-studied example of polymerization of an alkoxide, specifically teos. the chemical formula for teos is given by si(oc2h5)4, or si(or)4, where the alkyl group r = c2h5. alkoxides are ideal chemical precursors for sol–gel synthesis because they react readily with water. the reaction is called hydrolysis, because a hydroxyl ion becomes attached to the silicon atom as follows:  si(or)4h2o → ho−si(or)3r−ohdepending on the amount of water and catalyst present, hydrolysis may proceed to completion to silica:  si(or)42 h2o → sio24 r−ohcomplete hydrolysis often requires an excess of water and/or the use of a hydrolysis catalyst such as acetic acid or hydrochloric acid. intermediate species including [(or)2−si−(oh)2] or [(or)3−si−(oh)] may result as products of partial hydrolysis reactions. early intermediates result from two partially hydrolyzed monomers linked with a siloxane [si−o−si] bond:  (or)3−si−ohho−si−(or)3 → [(or)3si−o−si(or)3]h−o−hor  (or)3−si−orho−si−(or)3 → [(or)3si−o−si(or)3]r−ohthus, polymerization is associated with the formation of a 1-, 2-, or 3-dimensional network of siloxane [si−o−si] bonds accompanied by the production of h−o−h and r−o−h species. by definition, condensation liberates a small molecule, such as water or alcohol. this type of reaction can continue to build larger and larger silicon-containing molecules by the process of polymerization. thus, a polymer is a huge molecule (or macromolecule) formed from hundreds or thousands of units called monomers. the number of bonds that a monomer can form is called its functionality. polymerization of silicon alkoxide, for instance, can lead to complex branching of the polymer, because a fully hydrolyzed monomer si(oh)4 is tetrafunctional (can branch or bond in 4 different directions). alternatively, under certain conditions (e.g., low water concentration) fewer than 4 of the or or oh groups (ligands) will be capable of condensation, so relatively little branching will occur. the mechanisms of hydrolysis and condensation, and the factors that bias the structure toward linear or branched structures are the most critical issues of sol–gel science and technology. this reaction is favored in both basic and acidic conditions.   == sono-ormosil == sonication is an efficient tool for the synthesis of polymers. the cavitational shear forces, which stretch out and break the chain in a non-random process, result in a lowering of the molecular weight and poly-dispersity. furthermore, multi-phase systems are very efficient dispersed and emulsified, so that very fine mixtures are provided. this means that ultrasound increases the rate of polymerisation over conventional stirring and results in higher molecular weights with lower polydispersities. ormosils (organically modified silicate) are obtained when silane is added to gel-derived silica during sol–gel process. the product is a molecular-scale composite with improved mechanical properties. sono-ormosils are characterized by a higher density than classic gels as well as an improved thermal stability. an explanation therefore might be the increased degree of polymerization.   == pechini process == for single cation systems like sio2 and tio2, hydrolysis and condensation processes naturally give rise to homogenous compositions. for systems involving multiple cations, such as strontium titanate, srtio3 and other perovskite systems, the concept of steric immobilisation becomes relevant. to avoid the formation of multiple phases of binary oxides as the result of differing hydrolysis and condensation rates, the entrapment of cations in a polymer network is an effective approach, generally termed the pechini process. in this process, a chelating agent is used, most often citric acid, to surround aqueous cations and sterically entrap them. subsequently, a polymer network is formed to immobilize the chelated cations in a gel or resin. this is most often achieved by poly-esterification using ethylene glycol. the resulting polymer is then combusted under oxidising conditions to remove organic content and yield a product oxide with homogeneously dispersed cations.   == nanomaterials ==  in the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. uncontrolled flocculation of powders due to attractive van der waals forces can also give rise to microstructural heterogeneities.differential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved. in addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding heterogeneous densification. some pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. differential stresses arising from heterogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.it would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. the containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. monodisperse colloids provide this potential.monodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. the degree of order appears to be limited by the time and space allowed for longer-range correlations to be established. such defective polycrystalline structures would appear to be the basic elements of nanoscale materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as sintered ceramic nanomaterials.   == applications == the applications for sol gel-derived products are numerous. for example, scientists have used it to produce the world\'s lightest materials and also some of its toughest ceramics. unique properties of the sol–gel provide the possibility of their use for a variety of medical applications. a sol–gel processed alumina can be used as a carrier for the sustained delivery of drugs and as an established wound healer. a marked decrease in scar size was observed because of the wound healing composite including sol–gel processed alumina. a novel approach to thrombolysis treatment is possible by developing a new family of injectable composites: plasminogen activator entrapped within alumina.   == see also == coacervate, small spheroidal droplet of colloidal particles in suspension freeze-casting freeze gelation mechanics of gelation   == references ==   == further reading == colloidal dispersions, russel, w. b., et al., eds., cambridge university press (1989) glasses and the vitreous state, zarzycki. j., cambridge university press, 1991 the sol to gel transition. plinio innocenzi. springer briefs in materials. springer. 2016.   == external links == international sol–gel society the sol–gel gateway')"
123,"Chemical engineering is a certain type of engineering which deals with the study of operation and design of chemical plants as well as methods of improving production. Chemical engineers develop economical commercial processes to convert raw material into useful products. Chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. The work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering,  construction specification, and operating instructions.
Chemical engineers typically hold a degree in Chemical Engineering or Process Engineering. Practicing engineers may have professional certification and be accredited members of a professional body. Such bodies include the Institution of Chemical Engineers (IChemE) or the American Institute of Chemical Engineers (AIChE).
A degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.


== Etymology ==

A 1996 British Journal for the History of Science article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. In the same paper, however, George E. Davis, an English consultant, was credited with having coined the term. Davis also tried to found a Society of Chemical Engineering, but instead it was named the Society of Chemical Industry (1881), with Davis as its first secretary. The History of Science in United States: An Encyclopedia puts the use of the term around 1890. ""Chemical engineering"", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850. By 1910, the profession, ""chemical engineer,"" was already in common use in Britain and the United States.


== History ==
 
Chemical engineering emerged upon the development make unit operations considered essential to the discipline.


=== New concepts and innovations ===

In 1940s, it became clear that unit operations alone were insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to experience greater focus. Along with other novel concepts, such as process systems engineering (PSE), a ""second paradigm"" was defined. Transport phenomena gave an analytical approach to chemical engineering while PSE focused on its synthetic elements, such as control system and process design. Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry; however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin. Meanwhile, progress in polymer science in the 1950s paved way for the ""age of plastics"".


=== Safety and hazard developments ===
Concerns regarding the safety and environmental impact of large-scale chemical manufacturing facilities were also raised during this period. Silent Spring, published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide. The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages. The 1984 Bhopal disaster in India resulted in almost 4,000 deaths. These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus. In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States.


=== Recent progress ===
Advancements in computer science found applications designing and managing plants, simplifying calculations and drawings that previously had to be done manually. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well. Chemical engineering principles were used to produce DNA sequences in large quantities.


== Concepts ==
Chemical engineering involves the application of several principles. Key concepts are presented below.


=== Plant design and construction ===
Chemical engineering design concerns the creation of plans, specifications, and economic analyses for pilot plants, new plants, or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by several factors, including funding, government regulations, and safety standards. These constraints dictate a plant's choice of process, materials, and equipment.Plant construction is coordinated by project engineers and project managers, depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills or act as a consultant to the project group. In the USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.


=== Process design and analysis ===

A unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors. On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration and oxidation) involve the conversion of material by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.Process design requires the definition of equipment types and sizes as well as how they are connected and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or existing chemical factory.
Education for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.


=== Transport phenomena ===

Modeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species, respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena, therefore, requires an understanding of applied mathematics.


== Applications and practice ==

Chemical engineers ""develop economic ways of using materials and energy"". Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals, and plastics on a large-scale, industrial setting. They are also involved in waste management and research. Both applied and research facets could make extensive use of computers.Chemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, troubleshooting, and daily operations in either full-time or consulting roles. 


== See also ==


=== Related topics ===


=== Related fields and concepts ===


=== Associations ===


== References ==


== Bibliography ==","pandas(index=123, _1=123, text='chemical engineering is a certain type of engineering which deals with the study of operation and design of chemical plants as well as methods of improving production. chemical engineers develop economical commercial processes to convert raw material into useful products. chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. the work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering,  construction specification, and operating instructions. chemical engineers typically hold a degree in chemical engineering or process engineering. practicing engineers may have professional certification and be accredited members of a professional body. such bodies include the institution of chemical engineers (icheme) or the american institute of chemical engineers (aiche). a degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.   == etymology ==  a 1996 british journal for the history of science article cites james f. donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. in the same paper, however, george e. davis, an english consultant, was credited with having coined the term. davis also tried to found a society of chemical engineering, but instead it was named the society of chemical industry (1881), with davis as its first secretary. the history of science in united states: an encyclopedia puts the use of the term around 1890. ""chemical engineering"", describing the use of mechanical equipment in the chemical industry, became common vocabulary in england after 1850. by 1910, the profession, ""chemical engineer,"" was already in common use in britain and the united states.   == history ==  chemical engineering emerged upon the development make unit operations considered essential to the discipline. == references ==   == bibliography ==')"
124,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.Commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. Incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. For example, asking whether a kilogram is larger than an hour is meaningless.
Any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
The concept of physical dimension, and of dimensional analysis, was introduced by Joseph Fourier in 1822.


== Concrete numbers and base units ==
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof.
A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units.
Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). The newton is defined as 1 N = 1 kg⋅m⋅s−2.


=== Percentages and derivatives ===
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as ""hundredths"", since 1% = 1/100.
Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:

position (x) has the dimension L (length);
derivative of position with respect to time (dx/dt, velocity) has dimension LT−1—length from position, time due to the gradient;
the second derivative (d2x/dt2 = d(dx/dt) / dt, acceleration) has dimension LT−2.In economics, one distinguishes between stocks and flows: a stock has units of ""units"" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of ""units/time"" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.


== Conversion factor ==

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and 100 kPa = 1 bar. The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to 100 kPa / 1 bar = 1.  Since any quantity can be multiplied by 1 without changing it, the expression ""100 kPa / 1 bar"" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, 5 bar × 100 kPa / 1 bar = 500 kPa because 5 × 100 / 1 = 500, and bar/bar cancels out, so 5 bar = 500 kPa.


== Dimensional homogeneity ==

The most basic rule of dimensional analysis is that of dimensional homogeneity.
Only commensurable quantities (physical quantities having the same dimension) may be compared, equated, added, or subtracted.However, the dimensions form an abelian group under multiplication, so:

One may take ratios of incommensurable quantities (quantities with different dimensions), and multiply or divide them.For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometre being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful expression only quantities of the same dimension can be added, subtracted, or compared. For example, if mman, mrat and Lman denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression mman + mrat is meaningful, but the heterogeneous expression mman + Lman is meaningless. However, mman/L2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
This has the implication that most mathematical functions, particularly the transcendental functions, must have a dimensionless quantity, a pure number, as the argument and must return a dimensionless number as a result. This is clear because many transcendental functions can be expressed as an infinite power series with dimensionless coefficients.

  
    
      
        f
        (
        x
        )
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        =
        
          a
          
            0
          
        
        +
        
          a
          
            1
          
        
        x
        +
        
          a
          
            2
          
        
        
          x
          
            2
          
        
        +
        
          a
          
            3
          
        
        
          x
          
            3
          
        
        +
        ⋯
      
    
    {\displaystyle f(x)=\sum _{n=0}^{\infty }a_{n}x^{n}=a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}+\cdots }
  All powers of x must have the same dimension for the terms to be commensurable. But if x is not dimensionless, then the different powers of x will have different, incommensurable dimensions. However, power functions including root functions may have a dimensional argument and will return a result having dimension that is the same power applied to the argument dimension. This is because power functions and root functions are, loosely, just an expression of multiplication of quantities.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension L2MT−2, they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.


== The factor-label method for converting units ==
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:

  
    
      
        
          
            
              10
               
              
                
                  mile
                
              
            
            
              1
               
              
                
                  hour
                
              
            
          
        
        ×
        
          
            
              1609.344
              
                 meter
              
            
            
              1
               
              
                
                  mile
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  hour
                
              
            
            
              3600
              
                 second
              
            
          
        
        =
        4.4704
         
        
          
            meter
            second
          
        
        .
      
    
    {\displaystyle {\frac {10\ {\cancel {\text{mile}}}}{1\ {\cancel {\text{hour}}}}}\times {\frac {1609.344{\text{ meter}}}{1\ {\cancel {\text{mile}}}}}\times {\frac {1\ {\cancel {\text{hour}}}}{3600{\text{ second}}}}=4.4704\ {\frac {\text{meter}}{\text{second}}}.}
  Each conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as ""mile"" is the numerator in the original fraction and 
  
    
      
        1
         
        
          mile
        
        =
        1609.344
         
        
          meter
        
      
    
    {\displaystyle 1\ {\text{mile}}=1609.344\ {\text{meter}}}
  , ""mile"" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields 
  
    
      
        
          
            
              1
               
              
                mile
              
            
            
              1
               
              
                mile
              
            
          
        
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle {\frac {1\ {\text{mile}}}{1\ {\text{mile}}}}={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  , which when simplified results in the dimensionless 
  
    
      
        1
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle 1={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  . Multiplying any quantity (physical quantity or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units mile and hour, 10 miles per hour converts to 4.4704 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., 
  
    
      
        
          
            
              NO
            
            
              x
            
          
        
      
    
    {\displaystyle \color {Blue}{\ce {NO}}_{x}}
  ) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of 
  
    
      
        
          
            NO
          
          
            x
          
        
      
    
    {\displaystyle {\ce {NO}}_{x}}
   by using the following information as shown below:

NOx concentration
= 10 parts per million by volume = 10 ppmv = 10 volumes/106 volumes
NOx molar mass
= 46 kg/kmol = 46 g/mol
Flow rate of flue gas
= 20 cubic meters per minute = 20 m3/min
The flue gas exits the furnace at 0 °C temperature and 101.325 kPa absolute pressure.
The molar volume of a gas at 0 °C temperature and 101.325 kPa is 22.414 m3/kmol.
  
    
      
        
          
            
              1000
               
              
                
                  g
                   
                  NO
                
                
                  x
                
              
            
            
              1
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              46
               
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              22.414
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              10
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              
                10
                
                  6
                
              
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
          
        
        ×
        
          
            
              20
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
            
              1
               
              
                
                  minute
                
              
            
          
        
        ×
        
          
            
              60
               
              
                
                  minute
                
              
            
            
              1
               
              
                hour
              
            
          
        
        =
        24.63
         
        
          
            
              
                g
                 
                NO
              
              
                x
              
            
            hour
          
        
      
    
    {\displaystyle {\frac {1000\ {\ce {g\ NO}}_{x}}{1{\cancel {{\ce {kg\ NO}}_{x}}}}}\times {\frac {46\ {\cancel {{\ce {kg\ NO}}_{x}}}}{1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}}\times {\frac {1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}{22.414\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}}\times {\frac {10\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}{10^{6}\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}}\times {\frac {20\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}{1\ {\cancel {\ce {minute}}}}}\times {\frac {60\ {\cancel {\ce {minute}}}}{1\ {\ce {hour}}}}=24.63\ {\frac {{\ce {g\ NO}}_{x}}{\ce {hour}}}}
  After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.


=== Checking equations that involve dimensions ===
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.
For example, check the Universal Gas Law equation of PV = nRT, when:

the pressure P is in pascals (Pa)
the volume V is in cubic meters (m3)
the amount of substance n is in moles (mol)
the universal gas law constant R is 8.3145 Pa⋅m3/(mol⋅K)
the temperature T is in kelvins (K)
  
    
      
        
          Pa
          ⋅
          
            m
            
              3
            
          
        
        =
        
          
            
              
                mol
              
            
            1
          
        
        ×
        
          
            
              Pa
              ⋅
              
                m
                
                  3
                
              
            
            
              
                
                  
                    mol
                  
                
              
               
              
                
                  
                    K
                  
                
              
            
          
        
        ×
        
          
            
              
                K
              
            
            1
          
        
      
    
    {\displaystyle {\ce {Pa.m^3}}={\frac {\cancel {{\ce {mol}}}}{1}}\times {\frac {{\ce {Pa.m^3}}}{{\cancel {{\ce {mol}}}}\ {\cancel {{\ce {K}}}}}}\times {\frac {\cancel {{\ce {K}}}}{1}}}
  As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions — dimensional  adjusters — that can then be assigned physical significance. It is important to point out that such ‘mathematical manipulation’ is   neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck's constant, a fundamental constant of the universe, was ‘discovered’ as a purely mathematical abstraction or representation that built on the Rayleigh-Jeans Equation for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment – not earlier.


=== Limitations ===
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (
  
    
      
        x
        ↦
        a
        x
        +
        b
      
    
    {\displaystyle x\mapsto ax+b}
  , rather than a linear transform 
  
    
      
        x
        ↦
        a
        x
      
    
    {\displaystyle x\mapsto ax}
  ) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature T[F] in degrees Fahrenheit to a numerical quantity value T[C] in degrees Celsius, this formula may be used:

T[C] = (T[F] − 32) × 5/9.To convert T[C] in degrees Celsius to T[F] in degrees Fahrenheit, this formula may be used:

T[F] = (T[C] × 9/5) + 32.


== Applications ==
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.


=== Mathematics ===
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an n-ball (the solid ball in n dimensions), or the area of its surface, the n-sphere: being an n-dimensional figure, the volume scales as 
  
    
      
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle x^{n},}
   while the surface area, being 
  
    
      
        (
        n
        −
        1
        )
      
    
    {\displaystyle (n-1)}
  -dimensional, scales as 
  
    
      
        
          x
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle x^{n-1}.}
   Thus the volume of the n-ball in terms of the radius is 
  
    
      
        
          C
          
            n
          
        
        
          r
          
            n
          
        
        ,
      
    
    {\displaystyle C_{n}r^{n},}
   for some constant 
  
    
      
        
          C
          
            n
          
        
        .
      
    
    {\displaystyle C_{n}.}
   Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.


=== Finance, economics, and accounting ===
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

For example, the P/E ratio has dimensions of time (units of years), and can be interpreted as ""years of earnings to earn the price paid"".
In economics, debt-to-GDP ratio also has units of years (debt has units of currency, GDP has units of currency/year).
In financial analysis, some bond duration types also have dimension of time (unit of years) and can be interpreted as ”years to balance point between interest payments and nominal repayment”.
Velocity of money has units of 1/years (GDP/money supply has units of currency/year over currency): how often a unit of currency circulates per year.
Interest rates are often expressed as a percentage, but more properly percent per annum, which has dimensions of 1/years.


=== Fluid mechanics ===
In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:

Reynolds number (Re), generally important in all types of fluid problems:

  
    
      
        
          R
          e
        
        =
        
          
            
              ρ
              
              u
              d
            
            μ
          
        
      
    
    {\displaystyle \mathrm {Re} ={\frac {\rho \,ud}{\mu }}}
  .
Froude number (Fr), modeling flow with a free surface:

  
    
      
        
          F
          r
        
        =
        
          
            u
            
              g
              
              L
            
          
        
        .
      
    
    {\displaystyle \mathrm {Fr} ={\frac {u}{\sqrt {g\,L}}}.}
  
Euler number (Eu), used in problems in which pressure is of interest:

  
    
      
        
          E
          u
        
        =
        
          
            
              Δ
              p
            
            
              ρ
              
                u
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle \mathrm {Eu} ={\frac {\Delta p}{\rho u^{2}}}.}
  
Mach number (Ma), important in high speed flows where the velocity approaches or exceeds the local speed of sound:

  
    
      
        
          M
          a
        
        =
        
          
            u
            c
          
        
        ,
      
    
    {\displaystyle \mathrm {Ma} ={\frac {u}{c}},}
   where: c is the local speed of sound.


== History ==
The origins of dimensional analysis have been disputed by historians.The first written application of dimensional analysis has been credited to an article of François Daviet at the Turin Academy of Science. Daviet had the master Lagrange as teacher. 
His fundamental works are contained in acta of the Academy dated 1799.This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p.39). In the second edition of 1833, Poisson explicitly introduces the term dimension instead of the Daviet homogeneity.
In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like F = ma should be independent of the units employed to measure the physical variables.
Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be ""the three fundamental units"", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant G is taken as unity, thereby defining M = L3T−2. By assuming a form of Coulomb's law in which Coulomb's constant ke is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were Q = L3/2M1/2T−1, which, after substituting his M = L3T−2 equation for mass, results in charge having the same dimensions as mass, viz. Q = L3T−2.
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize.  It was used for the first time (Pesic 2005) in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue.  Rayleigh first published the technique in his 1877 book The Theory of Sound.The original meaning of the word dimension, in Fourier's Theorie de la Chaleur, was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT−2, instead of just the exponents.


== Mathematical formulation ==
The Buckingham π theorem describes how every physically meaningful equation involving n variables can be equivalently rewritten as an equation of n − m dimensionless parameters, where m is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.


=== Definition ===
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The dimension of a physical quantity is more fundamental than some scale unit used to express the amount of that physical quantity.  For example, mass is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.
There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity Q is given by 

  
    
      
        
          dim
        
         
        
          Q
        
        =
        
          
            
              L
            
          
          
            a
          
        
        
          
            
              M
            
          
          
            b
          
        
        
          
            
              T
            
          
          
            c
          
        
        
          
            
              I
            
          
          
            d
          
        
        
          
            
              Θ
            
          
          
            e
          
        
        
          
            
              N
            
          
          
            f
          
        
        
          
            
              J
            
          
          
            g
          
        
      
    
    {\displaystyle {\text{dim}}~{Q}={\mathsf {L}}^{a}{\mathsf {M}}^{b}{\mathsf {T}}^{c}{\mathsf {I}}^{d}{\mathsf {\Theta }}^{e}{\mathsf {N}}^{f}{\mathsf {J}}^{g}}
  where a, b, c, d, e, f, g are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electric current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.
As examples, the dimension of the physical quantity speed v is

  
    
      
        
          dim
        
         
        v
        =
        
          
            length
            time
          
        
        =
        
          
            
              L
            
            
              T
            
          
        
        =
        
          
            
              L
              T
            
          
          
            −
            1
          
        
      
    
    {\displaystyle {\text{dim}}~v={\frac {\text{length}}{\text{time}}}={\frac {\mathsf {L}}{\mathsf {T}}}={\mathsf {LT}}^{-1}}
  and the dimension of the physical quantity force F is

  
    
      
        
          dim
        
         
        F
        =
        
          mass
        
        ×
        
          acceleration
        
        =
        
          mass
        
        ×
        
          
            length
            
              
                time
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
            
            
              
                
                  T
                
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
              T
            
          
          
            −
            2
          
        
      
    
    {\displaystyle {\text{dim}}~F={\text{mass}}\times {\text{acceleration}}={\text{mass}}\times {\frac {\text{length}}{{\text{time}}^{2}}}={\frac {\mathsf {ML}}{{\mathsf {T}}^{2}}}={\mathsf {MLT}}^{-2}}
  The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.
There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.


=== Mathematical properties ===

The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; L0 = 1, and the inverse to L is 1/L or L−1. L raised to any rational power p is a member of the group, having an inverse of L−p or 1/Lp.  The operation of the group is multiplication, having the usual rules for handling exponents (Ln × Lm = Ln+m).
This group can be described as a vector space over the rational numbers, with for example dimensional symbol MiLjTk corresponding to the vector (i, j, k). When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., m) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., πm}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity X can be expressed in the general form

  
    
      
        X
        =
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        (
        
          π
          
            i
          
        
        
          )
          
            
              k
              
                i
              
            
          
        
        
        .
      
    
    {\displaystyle X=\prod _{i=1}^{m}(\pi _{i})^{k_{i}}\,.}
  Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

  
    
      
        f
        (
        
          π
          
            1
          
        
        ,
        
          π
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          π
          
            m
          
        
        )
        =
        0
        
        .
      
    
    {\displaystyle f(\pi _{1},\pi _{2},...,\pi _{m})=0\,.}
  Knowing this restriction can be a powerful tool for obtaining new insight into the system.


=== Mechanics ===
The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T2], L, M, while the latter can be expressed as M, L, [T = (ML/F)1/2].
On the other hand, length, velocity and time (L, V, T) do not form a set of base dimensions for mechanics, for two reasons:

There is no way to obtain mass – or anything derived from it, such as force – without introducing another base dimension (thus, they do not span the space).
Velocity, being expressible in terms of length and time (V = L/T), is redundant (the set is not linearly independent).


=== Other fields of physics and chemistry ===
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge.  In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ.  In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ 6.02×1023 mol−1) is defined as a base dimension, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.


=== Polynomials and transcendental functions ===
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities.  (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does not hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials (xn) of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for x2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for x2 + x, the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless.  For example,

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          (
          
            −
            9.8
             
            
              
                meter
                
                  
                    second
                  
                  
                    2
                  
                
              
            
          
          )
        
        ⋅
        
          t
          
            2
          
        
        +
        
          (
          
            500
             
            
              
                meter
                second
              
            
          
          )
        
        ⋅
        t
        .
      
    
    {\displaystyle {\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot t^{2}+\left(500\ {\frac {\text{meter}}{\text{second}}}\right)\cdot t.}
  This is the height to which an object rises in time t if the acceleration of gravity is 9.8 meter per second per second and the initial upward speed is 500 meter per second.  It is not necessary for t to be in seconds.  For example, suppose t = 0.01 minutes.  Then the first term would be

  
    
      
        
          
            
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                
                  (
                  
                    −
                    9.8
                     
                    
                      
                        meter
                        
                          
                            second
                          
                          
                            2
                          
                        
                      
                    
                  
                  )
                
                ⋅
                (
                0.01
                
                   minute
                
                
                  )
                  
                    2
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                
                  
                    (
                    
                      
                        minute
                        second
                      
                    
                    )
                  
                  
                    2
                  
                
                ⋅
                
                  meter
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                ⋅
                
                  60
                  
                    2
                  
                
                ⋅
                
                  meter
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot (0.01{\text{ minute}})^{2}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\left({\frac {\text{minute}}{\text{second}}}\right)^{2}\cdot {\text{meter}}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\cdot 60^{2}\cdot {\text{meter}}.\end{aligned}}}
  


=== Incorporating units ===
The value of a dimensional physical quantity Z is written as the product of a unit [Z] within the dimension and a dimensionless numerical factor, n.

  
    
      
        Z
        =
        n
        ×
        [
        Z
        ]
        =
        n
        [
        Z
        ]
      
    
    {\displaystyle Z=n\times [Z]=n[Z]}
  When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

  
    
      
        1
         
        
          
            ft
          
        
        =
        0.3048
         
        
          
            m
          
        
         
      
    
    {\displaystyle 1\ {\mbox{ft}}=0.3048\ {\mbox{m}}\ }
    is identical to 
  
    
      
        1
        =
        
          
            
              0.3048
               
              
                
                  m
                
              
            
            
              1
               
              
                
                  ft
                
              
            
          
        
        .
         
      
    
    {\displaystyle 1={\frac {0.3048\ {\mbox{m}}}{1\ {\mbox{ft}}}}.\ }
  The factor 
  
    
      
        0.3048
         
        
          
            
              m
            
            
              ft
            
          
        
      
    
    {\displaystyle 0.3048\ {\frac {\mbox{m}}{\mbox{ft}}}}
   is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.


=== Position vs displacement ===

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:

adding two displacements should yield a new displacement (walking ten paces then twenty paces gets you thirty paces forward),
adding a displacement to a position should yield a new position (walking one block down the street from an intersection gets you to the next intersection),
subtracting two positions should yield a displacement,
but one may not add two positions.This illustrates the subtle distinction between affine quantities (ones modeled by an affine space, such as position) and vector quantities (ones modeled by a vector space, such as displacement).

Vector quantities may be added to each other, yielding a new vector quantity, and a vector quantity may be added to a suitable affine quantity (a vector space acts on an affine space), yielding a new affine quantity.
Affine quantities cannot be added, but may be subtracted, yielding relative quantities which are vectors, and these relative differences may then be added to each other or to an affine quantity.Properly then, positions have dimension of affine length, while displacements have dimension of vector length. To assign a number to an affine unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a vector unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,

−273.15 °C ≘ 0 K = 0 °R ≘ −459.67 °F,where the symbol ≘ means corresponds to, since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.
For temperature differences,

1 K = 1 °C ≠ 1 °F = 1 °R.(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.


=== Orientation and frame of reference ===
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a direction. (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.


== Examples ==


=== A simple example: period of a harmonic oscillator ===
What is the period of oscillation T of a mass m attached to an ideal linear spring with spring constant k suspended in gravity of strength g? That period is the solution for T of some dimensionless equation in the variables T, m, k, and g.
The four quantities have the following dimensions:  T  [T];  m  [M]; k [M/T2]; and  g [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, 
  
    
      
        
          G
          
            1
          
        
      
    
    {\displaystyle G_{1}}
   = 
  
    
      
        
          T
          
            2
          
        
        k
        
          /
        
        m
      
    
    {\displaystyle T^{2}k/m}
   [T2 · M/T2 / M = 1], and putting 
  
    
      
        
          G
          
            1
          
        
        =
        C
      
    
    {\displaystyle G_{1}=C}
   for some dimensionless constant C gives the dimensionless equation sought.  The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term ""group"" means ""collection"" rather than mathematical group.  They are often called dimensionless numbers as well.
Note that the variable g does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines g with k, m, and T, because g is the only quantity that involves the dimension L. This implies that in this problem the g is irrelevant. Dimensional analysis can sometimes yield strong statements about the irrelevance of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of g: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way:  
  
    
      
        T
        =
        κ
        
          
            
              
                m
                k
              
            
          
        
      
    
    {\displaystyle T=\kappa {\sqrt {\tfrac {m}{k}}}}
  , for some dimensionless constant κ (equal to 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\sqrt {C}}}
   from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (g, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be ""complete"" – although it still may involve unknown dimensionless constants, such as κ.


=== A more complex example: energy of a vibrating wire ===
Consider the case of a vibrating wire of length ℓ (L) vibrating with an amplitude A (L).  The wire has a linear density ρ (M/L) and is under tension s (ML/T2), and we want to know the energy E (ML2/T2) in the wire.  Let π1 and π2 be two dimensionless products of powers of the variables chosen, given by

  
    
      
        
          
            
              
                
                  π
                  
                    1
                  
                
              
              
                
                =
                
                  
                    E
                    
                      A
                      s
                    
                  
                
              
            
            
              
                
                  π
                  
                    2
                  
                
              
              
                
                =
                
                  
                    ℓ
                    A
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\pi _{1}&={\frac {E}{As}}\\\pi _{2}&={\frac {\ell }{A}}.\end{aligned}}}
  The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation

  
    
      
        F
        
          (
          
            
              
                E
                
                  A
                  s
                
              
            
            ,
            
              
                ℓ
                A
              
            
          
          )
        
        =
        0
        ,
      
    
    {\displaystyle F\left({\frac {E}{As}},{\frac {\ell }{A}}\right)=0,}
  where F is some unknown function, or, equivalently as

  
    
      
        E
        =
        A
        s
        f
        
          (
          
            
              ℓ
              A
            
          
          )
        
        ,
      
    
    {\displaystyle E=Asf\left({\frac {\ell }{A}}\right),}
  where f is some other unknown function.  Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension.  Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function f.  But our experiments are simpler than in the absence of dimensional analysis.  We'd perform none to verify that the energy is proportional to the tension.  Or perhaps we might guess that the energy is proportional to ℓ, and so infer that E = ℓs.  The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex.  Consider, for example, a small pebble sitting on the bed of a river.  If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water.  At what critical velocity will this occur?  Sorting out the guessed variables is not so easy as before.  But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.


=== A third example: demand versus capacity for a rotating disc ===

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness t (L) and radius R (L).  The disc has a density ρ (M/L3), rotates at an angular velocity ω (T−1) and this leads to a stress S (ML−1T−2) in the material.  There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid.  As the disc becomes thicker relative to the radius then the plane stress solution breaks down.  If the disc is restrained axially on its free faces then a state of plane strain will occur.  However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case.  An engineer might, therefore, be interested in establishing a relationship between the five variables.  Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:

demand/capacity = ρR2ω2/S
thickness/radius or aspect ratio = t/RThrough the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure.  As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs


== Extensions ==


=== Huntley's extension: directed dimensions and quantity of matter ===
Huntley (Huntley 1967) has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank 
  
    
      
        m
      
    
    {\displaystyle m}
   of the dimensional matrix. He introduced two approaches to doing so:

The magnitudes of the components of a vector are to be considered dimensionally independent. For example, rather than an undifferentiated length dimension L, we may have Lx represent dimension in the x-direction, and so forth. This requirement stems ultimately from the requirement that each component of a physically meaningful equation (scalar, vector, or tensor) must be dimensionally consistent.
Mass as a measure of the quantity of matter is to be considered dimensionally independent from mass as a measure of inertia.As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   and a horizontal velocity component 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
  , both dimensioned as LT−1, R, the distance travelled, having dimension L, and g the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range R may be written:

  
    
      
        R
        ∝
        
          V
          
            x
          
          
            a
          
        
        
        
          V
          
            y
          
          
            b
          
        
        
        
          g
          
            c
          
        
        .
        
      
    
    {\displaystyle R\propto V_{\text{x}}^{a}\,V_{\text{y}}^{b}\,g^{c}.\,}
  Or dimensionally

  
    
      
        
          
            L
          
        
        =
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            a
            +
            b
          
        
        
          
            (
            
              
                
                  L
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
        
      
    
    {\displaystyle {\mathsf {L}}=\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{a+b}\left({\frac {\mathsf {L}}{{\mathsf {T}}^{2}}}\right)^{c}\,}
  from which we may deduce that 
  
    
      
        a
        +
        b
        +
        c
        =
        1
      
    
    {\displaystyle a+b+c=1}
   and 
  
    
      
        a
        +
        b
        +
        2
        c
        =
        0
      
    
    {\displaystyle a+b+2c=0}
  , which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
   will be dimensioned as LxT−1, 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   as LyT−1, R as Lx and g as LyT−2. The dimensional equation becomes:

  
    
      
        
          
            
              L
            
          
          
            
              x
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      x
                    
                  
                
                
                  T
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
      
    
    {\displaystyle {\mathsf {L}}_{\mathrm {x} }=\left({\frac {{\mathsf {L}}_{\mathrm {x} }}{\mathsf {T}}}\right)^{a}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{\mathsf {T}}}\right)^{b}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{{\mathsf {T}}^{2}}}\right)^{c}}
  and we may solve completely as 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
  , 
  
    
      
        b
        =
        1
      
    
    {\displaystyle b=1}
   and 
  
    
      
        c
        =
        −
        1
      
    
    {\displaystyle c=-1}
  . The increase in deductive power gained by the use of directed length dimensions is apparent.
In his second approach, Huntley holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity (a) proportional to inertial mass, but (b) not implicating inertial properties. No further restrictions are added to its definition.
For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables

  
    
      
        
          
            
              m
              ˙
            
          
        
      
    
    {\displaystyle {\dot {m}}}
   the mass flow rate with dimension MT−1

  
    
      
        
          p
          
            x
          
        
      
    
    {\displaystyle p_{\text{x}}}
   the pressure gradient along the pipe with dimension ML−2T−2
ρ the density with dimension ML−3
η the dynamic fluid viscosity with dimension ML−1T−1
r the radius of the pipe with dimension LThere are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be 
  
    
      
        
          π
          
            1
          
        
        =
        
          
            
              m
              ˙
            
          
        
        
          /
        
        η
        r
      
    
    {\displaystyle \pi _{1}={\dot {m}}/\eta r}
   and 
  
    
      
        
          π
          
            2
          
        
        =
        
          p
          
            
              x
            
          
        
        ρ
        
          r
          
            5
          
        
        
          /
        
        
          
            
              
                m
                ˙
              
            
          
          
            2
          
        
      
    
    {\displaystyle \pi _{2}=p_{\mathrm {x} }\rho r^{5}/{\dot {m}}^{2}}
   and we may express the dimensional equation as

  
    
      
        C
        =
        
          π
          
            1
          
        
        
          π
          
            2
          
          
            a
          
        
        =
        
          (
          
            
              
                
                  m
                  ˙
                
              
              
                η
                r
              
            
          
          )
        
        
          
            (
            
              
                
                  
                    p
                    
                      
                        x
                      
                    
                  
                  ρ
                  
                    r
                    
                      5
                    
                  
                
                
                  
                    
                      
                        m
                        ˙
                      
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
      
    
    {\displaystyle C=\pi _{1}\pi _{2}^{a}=\left({\frac {\dot {m}}{\eta r}}\right)\left({\frac {p_{\mathrm {x} }\rho r^{5}}{{\dot {m}}^{2}}}\right)^{a}}
  where C and a are undetermined constants. If we draw a distinction between inertial mass with dimension 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{\text{i}}}
   and quantity of matter with dimension 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{\text{m}}}
  , then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

  
    
      
        C
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              ρ
              
                r
                
                  4
                
              
            
            
              η
              
                
                  
                    m
                    ˙
                  
                
              
            
          
        
      
    
    {\displaystyle C={\frac {p_{\mathrm {x} }\rho r^{4}}{\eta {\dot {m}}}}}
  where now only C is an undetermined constant (found to be equal to 
  
    
      
        π
        
          /
        
        8
      
    
    {\displaystyle \pi /8}
   by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements (a) and (b) he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.
Huntley's concept of directed length dimensions however has some serious limitations:

It does not deal well with vector equations involving the cross product,
nor does it handle well the use of angles as physical variables.It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the ""symmetry"" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of ""symmetry"" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?
Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.


=== Siano's extension: orientational analysis ===
Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin (x, y) = (0, 0) at a speed v and angle θ above the x-axis, with the force of gravity directed along the negative y-axis. It is desired to find the range R, at which point the mass returns to the x-axis. Conventional analysis will yield the dimensionless variable π = R g/v2, but offers no insight into the relationship between R and θ.
Siano (1985-I, 1985-II) has suggested that the directed dimensions of Huntley be replaced by using orientational symbols 1x 1y 1z to denote vector directions, and an orientationless symbol 10. Thus, Huntley's Lx becomes L 1x with L specifying the dimension of length, and 1x specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own.  Along with the requirement that 1i−1 = 1i, the following multiplication table for the orientation symbols results:

  
    
      
        
          
            
              
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  
                    1
                    
                      z
                    
                  
                
              
            
            
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
            
            
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
            
            
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
            
            
              
                
                  
                    1
                    
                      z
                    
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{c|cccc}&\mathbf {1_{0}} &\mathbf {1_{\text{x}}} &\mathbf {1_{\text{y}}} &\mathbf {1_{\text{z}}} \\\hline \mathbf {1_{0}} &1_{0}&1_{\text{x}}&1_{\text{y}}&1_{\text{z}}\\\mathbf {1_{\text{x}}} &1_{\text{x}}&1_{0}&1_{\text{z}}&1_{\text{y}}\\\mathbf {1_{\text{y}}} &1_{\text{y}}&1_{\text{z}}&1_{0}&1_{\text{x}}\\\mathbf {1_{\text{z}}} &1_{\text{z}}&1_{\text{y}}&1_{\text{x}}&1_{0}\end{array}}}
  Note that the orientational symbols form a group (the Klein four-group or ""Viergruppe""). In this system, scalars always have the same orientation as the identity element, independent of the ""symmetry of the problem"".  Physical quantities that are vectors have the orientation expected:  a force or a velocity in the z-direction has the orientation of 1z.  For angles, consider an angle θ that lies in the z-plane.  Form a right triangle in the z-plane with θ being one of the acute angles.  The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y.  Since (using ~ to indicate orientational equivalence) tan(θ) = θ + ... ~ 1y/1x we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable.  Analogous reasoning forces the conclusion that sin(θ) has orientation 1z while cos(θ) has orientation 10.  These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form a cos(θ) + b sin(θ), where a and b are real scalars. Note that an expression such as 
  
    
      
        sin
        ⁡
        (
        θ
        +
        π
        
          /
        
        2
        )
        =
        cos
        ⁡
        (
        θ
        )
      
    
    {\displaystyle \sin(\theta +\pi /2)=\cos(\theta )}
   is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:

  
    
      
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            +
            b
            
            
              1
              
                z
              
            
          
          )
        
        =
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            b
            
            
              1
              
                z
              
            
          
          )
        
        +
        sin
        ⁡
        
          (
          
            b
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            a
            
            
              1
              
                z
              
            
          
          )
        
        ,
      
    
    {\displaystyle \sin \left(a\,1_{\text{z}}+b\,1_{\text{z}}\right)=\sin \left(a\,1_{\text{z}})\cos(b\,1_{\text{z}}\right)+\sin \left(b\,1_{\text{z}})\cos(a\,1_{\text{z}}\right),}
  which for 
  
    
      
        a
        =
        θ
      
    
    {\displaystyle a=\theta }
   and 
  
    
      
        b
        =
        π
        
          /
        
        2
      
    
    {\displaystyle b=\pi /2}
   yields 
  
    
      
        sin
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        +
        [
        π
        
          /
        
        2
        ]
        
        
          1
          
            z
          
        
        )
        =
        
          1
          
            z
          
        
        cos
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        )
      
    
    {\displaystyle \sin(\theta \,1_{\text{z}}+[\pi /2]\,1_{\text{z}})=1_{\text{z}}\cos(\theta \,1_{\text{z}})}
  . Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is 
  
    
      
        
          1
          
            0
          
        
      
    
    {\displaystyle 1_{0}}
  .
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems.  In this approach one sets up the dimensional equation and solves it as far as one can.  If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral.  This puts it into ""normal form"".  The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension 1z and the range of the projectile R will be of the form:

  
    
      
        R
        =
        
          g
          
            a
          
        
        
        
          v
          
            b
          
        
        
        
          θ
          
            c
          
        
        
           which means 
        
        
          
            L
          
        
        
        
          1
          
            
              x
            
          
        
        ∼
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                  
                    1
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
        
          1
          
            
              z
            
          
          
            c
          
        
        .
        
      
    
    {\displaystyle R=g^{a}\,v^{b}\,\theta ^{c}{\text{ which means }}{\mathsf {L}}\,1_{\mathrm {x} }\sim \left({\frac {{\mathsf {L}}\,1_{\text{y}}}{{\mathsf {T}}^{2}}}\right)^{a}\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{b}\,1_{\mathsf {z}}^{c}.\,}
  Dimensional homogeneity will now correctly yield a = −1 and b = 2, and orientational homogeneity requires that 
  
    
      
        
          1
          
            x
          
        
        
          /
        
        (
        
          1
          
            y
          
          
            a
          
        
        
          1
          
            z
          
          
            c
          
        
        )
        =
        
          1
          
            z
          
          
            c
            +
            1
          
        
        =
        1
      
    
    {\displaystyle 1_{x}/(1_{y}^{a}1_{z}^{c})=1_{z}^{c+1}=1}
  . In other words, that c must be an odd integer. In fact the required function of theta will be sin(θ)cos(θ) which is a series consisting of odd powers of θ.
It is seen that the Taylor series of sin(θ) and cos(θ) are orientationally homogeneous using the above multiplication table, while expressions like cos(θ) + sin(θ) and exp(θ) are not, and are (correctly) deemed unphysical.
Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.


== Dimensionless concepts ==


=== Constants ===

The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
   in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation.  Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity.  This observation can allow one to sometimes make ""back of the envelope"" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.


=== Formalisms ===
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
   ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on ""dimensional grounds"" that the non-analytical part of the free energy per lattice site should be 
  
    
      
        ∼
        1
        
          /
        
        
          ξ
          
            d
          
        
      
    
    {\displaystyle \sim 1/\xi ^{d}}
   where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: c, ħ, and G, in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants ħ, c, and G (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit 
  
    
      
        c
        →
        ∞
      
    
    {\displaystyle c\rightarrow \infty }
  ,  
  
    
      
        ℏ
        →
        0
      
    
    {\displaystyle \hbar \rightarrow 0}
   and 
  
    
      
        G
        →
        0
      
    
    {\displaystyle G\rightarrow 0}
  . In problems involving a gravitational field the latter limit should be taken such that the field stays finite.


== Dimensional equivalences ==
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.


=== SI units ===


=== Natural units ===

If c = ħ = 1, where c is the speed of light and ħ is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length L, mass M and time T can be expressed (dimensionally) as a power of energy E, because length, mass and time can be expressed using speed v, action S, and energy E:

  
    
      
        M
        =
        
          
            E
            
              v
              
                2
              
            
          
        
        ,
        
        L
        =
        
          
            
              S
              v
            
            E
          
        
        ,
        
        t
        =
        
          
            S
            E
          
        
      
    
    {\displaystyle M={\frac {E}{v^{2}}},\quad L={\frac {Sv}{E}},\quad t={\frac {S}{E}}}
  though speed and action are dimensionless (v = c = 1 and S = ħ = 1) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:

  
    
      
        
          
            
              E
            
          
          
            n
          
        
        =
        
          
            
              M
            
          
          
            p
          
        
        
          
            
              L
            
          
          
            q
          
        
        
          
            
              T
            
          
          
            r
          
        
        =
        
          
            
              E
            
          
          
            p
            −
            q
            −
            r
          
        
      
    
    {\displaystyle {\mathsf {E}}^{n}={\mathsf {M}}^{p}{\mathsf {L}}^{q}{\mathsf {T}}^{r}={\mathsf {E}}^{p-q-r}}
  This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge e though other choices are possible.


== See also ==
Buckingham π theorem
Dimensionless numbers in fluid mechanics
Fermi estimate — used to teach dimensional analysis
Rayleigh's method of dimensional analysis
Similitude (model) — an application of dimensional analysis
System of measurement


=== Related areas of math ===
Covariance and contravariance of vectors
Exterior algebra
Geometric algebra
Quantity calculus


=== Programming languages ===
Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML,  and later in F#. There are implementations for Haskell, OCaml, and Rust, Python, and a code checker for Fortran.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.


== Notes ==


== References ==
Barenblatt, G. I. (1996), Scaling, Self-Similarity, and Intermediate Asymptotics, Cambridge, UK: Cambridge University Press, ISBN 978-0-521-43522-2
Bhaskar, R.; Nigam, Anil (1990), ""Qualitative Physics Using Dimensional Analysis"", Artificial Intelligence, 45 (1–2): 73–111, doi:10.1016/0004-3702(90)90038-2
Bhaskar, R.; Nigam, Anil (1991), ""Qualitative Explanations of Red Giant Formation"", The Astrophysical Journal, 372: 592–6, Bibcode:1991ApJ...372..592B, doi:10.1086/170003
Boucher; Alves (1960), ""Dimensionless Numbers"", Chemical Engineering Progress, 55: 55–64
Bridgman, P. W. (1922), Dimensional Analysis, Yale University Press, ISBN 978-0-548-91029-0
Buckingham, Edgar (1914), ""On Physically Similar Systems: Illustrations of the Use of Dimensional Analysis"", Physical Review, 4 (4): 345–376, Bibcode:1914PhRv....4..345B, doi:10.1103/PhysRev.4.345, hdl:10338.dmlcz/101743
Drobot, S. (1953–1954), ""On the foundations of dimensional analysis"" (PDF), Studia Mathematica, 14: 84–99, doi:10.4064/sm-14-1-84-99
Gibbings, J.C. (2011), Dimensional Analysis, Springer, ISBN 978-1-84996-316-9
Hart, George W. (1994), ""The theory of dimensioned matrices"",  in Lewis, John G. (ed.), Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, SIAM, pp. 186–190, ISBN 978-0-89871-336-7 As postscript
Hart, George W. (1995), Multidimensional Analysis: Algebras and Systems for Science and Engineering, Springer-Verlag, ISBN 978-0-387-94417-3
Huntley, H. E. (1967), Dimensional Analysis, Dover, LOC 67-17978
Klinkenberg, A. (1955), ""Dimensional systems and systems of units in physics with special reference to chemical engineering: Part I. The principles according to which dimensional systems and systems of units are constructed"", Chemical Engineering Science, 4 (3): 130–140, 167–177, doi:10.1016/0009-2509(55)80004-8
Langhaar, Henry L. (1951), Dimensional Analysis and Theory of Models, Wiley, ISBN 978-0-88275-682-0
Mendez, P.F.; Ordóñez, F. (September 2005), ""Scaling Laws From Statistical Data and Dimensional Analysis"", Journal of Applied Mechanics, 72 (5): 648–657, Bibcode:2005JAM....72..648M, CiteSeerX 10.1.1.422.610, doi:10.1115/1.1943434
Moody, L. F. (1944), ""Friction Factors for Pipe Flow"", Transactions of the American Society of Mechanical Engineers, 66 (671)
Murphy, N. F. (1949), ""Dimensional Analysis"", Bulletin of the Virginia Polytechnic Institute, 42 (6)
Perry, J. H.;  et al. (1944), ""Standard System of Nomenclature for Chemical Engineering Unit Operations"", Transactions of the American Institute of Chemical Engineers, 40 (251)
Pesic, Peter (2005), Sky in a Bottle, MIT Press, pp. 227–8, ISBN 978-0-262-16234-0
Petty, G. W. (2001), ""Automated computation and consistency checking of physical dimensions and units in scientific programs"", Software – Practice and Experience, 31 (11): 1067–76, doi:10.1002/spe.401, S2CID 206506776
Porter, Alfred W. (1933), The Method of Dimensions (3rd ed.), Methuen
J. W. Strutt (3rd Baron Rayleigh) (1915), ""The Principle of Similitude"", Nature, 95 (2368): 66–8, Bibcode:1915Natur..95...66R, doi:10.1038/095066c0
Siano, Donald (1985), ""Orientational Analysis – A Supplement to Dimensional Analysis – I"", Journal of the Franklin Institute, 320 (6): 267–283, doi:10.1016/0016-0032(85)90031-6
Siano, Donald (1985), ""Orientational Analysis, Tensor Analysis and The Group Properties of the SI Supplementary Units – II"", Journal of the Franklin Institute, 320 (6): 285–302, doi:10.1016/0016-0032(85)90032-8
Silberberg, I. H.; McKetta, J. J. Jr. (1953), ""Learning How to Use Dimensional Analysis"", Petroleum Refiner, 32 (4): 5, (5): 147, (6): 101, (7): 129
Van Driest, E. R. (March 1946), ""On Dimensional Analysis and the Presentation of Data in Fluid Flow Problems"", Journal of Applied Mechanics, 68 (A–34)
Whitney, H. (1968), ""The Mathematics of Physical Quantities, Parts I and II"", American Mathematical Monthly, 75 (2): 115–138, 227–256, doi:10.2307/2315883, JSTOR 2315883
Vignaux, GA (1992),  Erickson, Gary J.; Neudorfer, Paul O. (eds.), Dimensional Analysis in Data Modelling, Kluwer Academic, ISBN 978-0-7923-2031-9
Kasprzak, Wacław; Lysik, Bertold; Rybaczuk, Marek (1990), Dimensional Analysis in the Identification of Mathematical Models, World Scientific, ISBN 978-981-02-0304-7


== External links ==
List of dimensions for variety of physical quantities
Unicalc Live web calculator doing units conversion by dimensional analysis
A C++ implementation of compile-time dimensional analysis in the Boost open-source libraries
Buckingham’s pi-theorem
Quantity System calculator for units conversion based on dimensional approach
Units, quantities, and fundamental constants project dimensional analysis maps
Bowley, Roger (2009). ""[ ] Dimensional Analysis"". Sixty Symbols. Brady Haran for the University of Nottingham.
Dureisseix, David (2019). An introduction to dimensional analysis (lecture). INSA Lyon.


=== Converting units ===
Unicalc Live web calculator doing units conversion by dimensional analysis
Math Skills Review
U.S. EPA tutorial
A Discussion of Units
Short Guide to Unit Conversions
Canceling Units Lesson
Chapter 11: Behavior of Gases Chemistry: Concepts and Applications, Denton Independent School District
Air Dispersion Modeling Conversions and Formulas
www.gnu.org/software/units  free program, very practical","pandas(index=124, _1=124, text='in engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. the conversion of units from one dimensional unit to another is often easier within the metric or si system than in others, due to the regular 10-base in all units. dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. for example, asking whether a kilogram is larger than an hour is meaningless. any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. it also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation. the concept of physical dimension, and of dimensional analysis, was introduced by joseph fourier in 1822.   == concrete numbers and base units == many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof. a set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. for example, units for length and time are normally chosen as base units. units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units. sometimes the names of units obscure the fact that they are derived units. for example, a newton (n) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). the newton is defined as 1 n = 1 kg⋅m⋅s−2. unicalc live web calculator doing units conversion by dimensional analysis math skills review u.s. epa tutorial a discussion of units short guide to unit conversions canceling units lesson chapter 11: behavior of gases chemistry: concepts and applications, denton independent school district air dispersion modeling conversions and formulas www.gnu.org/software/units  free program, very practical')"
125,"Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. The behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology.
Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Nicolas Léonard Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars. Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, ""Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.""
The initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.


== Introduction ==
A description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. The first law specifies that energy can be exchanged between physical systems as heat and work. The second law defines the existence of a quantity called entropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.In thermodynamics, interactions between large ensembles of objects are studied and categorized. Central to this are the concepts of the thermodynamic system and its surroundings. A system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. Properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes.
With these tools, thermodynamics can be used to describe how systems respond to changes in their environment. This can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. The results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.This article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. Non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.


== History ==
The history of thermodynamics as a scientific discipline generally begins with Otto von Guericke who, in 1650, built and designed the world's first vacuum pump and demonstrated a vacuum using his Magdeburg hemispheres. Guericke was driven to make a vacuum in order to disprove Aristotle's long-held supposition that 'nature abhors a vacuum'. Shortly after Guericke, the Anglo-Irish physicist and chemist Robert Boyle had learned of Guericke's designs and, in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed a correlation between pressure, temperature, and volume. In time, Boyle's Law was formulated, which states that pressure and volume are inversely proportional. Then, in 1679, based on these concepts, an associate of Boyle's named Denis Papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated.
Later designs implemented a steam release valve that kept the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and a cylinder engine. He did not, however, follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine, followed by Thomas Newcomen in 1712. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time.
The fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by Professor Joseph Black at the University of Glasgow, where James Watt was employed as an instrument maker. Black and Watt performed experiments together, but it was Watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. Drawing on all the previous work led Sadi Carnot, the ""father of thermodynamics"", to publish Reflections on the Motive Power of Fire (1824), a discourse on heat, power, energy and engine efficiency. The book outlined the basic energetic relations between the Carnot engine, the Carnot cycle, and motive power. It marked the start of thermodynamics as a modern science.The first thermodynamic textbook was written in 1859 by William Rankine, originally trained as a physicist and a civil and mechanical engineering professor at the University of Glasgow. The first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of William Rankine, Rudolf Clausius, and William Thomson (Lord Kelvin).The foundations of statistical thermodynamics were set out by physicists such as James Clerk Maxwell, Ludwig Boltzmann, Max Planck, Rudolf Clausius and J. Willard Gibbs.
During the years 1873–76 the American mathematical physicist Josiah Willard Gibbs published a series of three papers, the most famous being On the Equilibrium of Heterogeneous Substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. Also Pierre Duhem in the 19th century wrote about chemical thermodynamics. During the early 20th century, chemists such as Gilbert N. Lewis, Merle Randall, and E. A. Guggenheim applied the mathematical methods of Gibbs to the analysis of chemical processes.


== Etymology ==
The etymology of thermodynamics has an intricate history. It was first spelled in a hyphenated form as an adjective (thermo-dynamic) and from 1854 to 1868 as the noun thermo-dynamics to represent the science of generalized heat engines.American biophysicist Donald Haynie claims that thermodynamics was coined in 1840 from the Greek root θέρμη therme, meaning “heat”, and δύναμις dynamis, meaning “power”.Pierre Perrot claims that the term thermodynamics was coined by James Joule in 1858 to designate the science of relations between heat and power, however, Joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to Thomson's 1849 phraseology.By 1858, thermo-dynamics, as a functional term, was used in William Thomson's paper ""An Account of Carnot's Theory of the Motive Power of Heat.""


== Branches of thermodynamics ==
The study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems.


=== Classical thermodynamics ===
Classical thermodynamics is the description of the states of thermodynamic systems at near-equilibrium, that uses macroscopic, measurable properties. It is used to model exchanges of energy, work and heat based on the laws of thermodynamics. The qualifier classical reflects the fact that it represents the first level of understanding of the subject as it developed in the 19th century and describes the changes of a system in terms of macroscopic empirical (large scale, and measurable) parameters. A microscopic interpretation of these concepts was later provided by the development of statistical mechanics.


=== Statistical mechanics ===
Statistical mechanics, also called statistical thermodynamics, emerged with the development of atomic and molecular theories in the late 19th century and early 20th century, and supplemented classical thermodynamics with an interpretation of the microscopic interactions between individual particles or quantum-mechanical states. This field relates the microscopic properties of individual atoms and molecules to the macroscopic, bulk properties of materials that can be observed on the human scale, thereby explaining classical thermodynamics as a natural result of statistics, classical mechanics, and quantum theory at the microscopic level.


=== Chemical thermodynamics ===
Chemical thermodynamics is the study of the interrelation of energy with chemical reactions or with a physical change of state within the confines of the laws of thermodynamics.


=== Equilibrium thermodynamics ===
Equilibrium thermodynamics is the study of transfers of matter and energy in systems or bodies that, by agencies in their surroundings, can be driven from one state of thermodynamic equilibrium to another. The term 'thermodynamic equilibrium' indicates a state of balance, in which all macroscopic flows are zero; in the case of the simplest systems or bodies, their intensive properties are homogeneous, and their pressures are perpendicular to their boundaries. In an equilibrium state there are no unbalanced potentials, or driving forces, between macroscopically distinct parts of the system. A central aim in equilibrium thermodynamics is: given a system in a well-defined initial equilibrium state, and given its surroundings, and given its constitutive walls, to calculate what will be the final equilibrium state of the system after a specified thermodynamic operation has changed its walls or surroundings.
Non-equilibrium thermodynamics is a branch of thermodynamics that deals with systems that are not in thermodynamic equilibrium. Most systems found in nature are not in thermodynamic equilibrium because they are not in stationary states, and are continuously and discontinuously subject to flux of matter and energy to and from other systems. The thermodynamic study of non-equilibrium systems requires more general concepts than are dealt with by equilibrium thermodynamics. Many natural systems still today remain beyond the scope of currently known macroscopic thermodynamic methods.


== Laws of thermodynamics ==

Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following.


=== Zeroth Law ===
The zeroth law of thermodynamics states: If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.
This statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration. Systems are said to be in equilibrium if the small, random exchanges between them (e.g. Brownian motion) do not lead to a net change in energy. This law is tacitly assumed in every measurement of temperature. Thus, if one seeks to decide whether two bodies are at the same temperature, it is not necessary to bring them into contact and measure any changes of their observable properties in time. The law provides an empirical definition of temperature, and justification for the construction of practical thermometers.
The zeroth law was not initially recognized as a separate law of thermodynamics, as its basis in thermodynamical equilibrium was implied in the other laws. The first, second, and third laws had been explicitly stated already, and found common acceptance in the physics community before the importance of the zeroth law for the definition of temperature was realized. As it was impractical to renumber the other laws, it was named the zeroth law.


=== First Law ===
The first law of thermodynamics states: In a process without transfer of matter, the change in internal energy, ΔU, of a thermodynamic system is equal to the energy gained as heat, Q, less the thermodynamic work, W, done by the system on its surroundings.

  
    
      
        Δ
        U
        =
        Q
        −
        W
      
    
    {\displaystyle \Delta U=Q-W}
  .For processes that include transfer of matter, a further statement is needed: With due account of the respective fiducial reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then

  
    
      
        
          U
          
            0
          
        
        =
        
          U
          
            1
          
        
        +
        
          U
          
            2
          
        
      
    
    {\displaystyle U_{0}=U_{1}+U_{2}}
  ,where U0 denotes the internal energy of the combined system, and U1 and U2 denote the internal energies of the respective separated systems.
Adapted for thermodynamics, this law is an expression of the principle of conservation of energy, which states that energy can be transformed (changed from one form to another), but cannot be created or destroyed.Internal energy is a principal property of the thermodynamic state, while heat and work are modes of energy transfer by which a process may change this state. A change of internal energy of a system may be achieved by any combination of heat added or removed and work performed on or by the system. As a function of state, the internal energy does not depend on the manner, or on the path through intermediate steps, by which the system arrived at its state.


=== Second Law ===
A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter.The second law is an observation of the fact that over time, inhomogeneities in temperature, pressure, and chemical potential tend to even out in a physical system that is isolated from the outside world. Entropy is a measure of how much this process has progressed. The entropy of an isolated system, that is internally constrained away from equilibrium, will increase when its internal constraints are removed, reaching a maximum value at thermodynamic equilibrium. Though several such have been proposed, there is known no general thermodynamic principle that guides the rates of changes in unconstrained systems that are far from thermodynamic equilibrium.
In classical thermodynamics, the second law is a basic postulate applicable to any actual thermodynamic process; in statistical thermodynamics, the second law is a consequence of molecular chaos. There are many versions of the second law, but they all have the same effect, which is to express the phenomenon of [irreversibility] in nature.


=== Third Law ===
The third law of thermodynamics states: As the temperature of a system approaches absolute zero, all processes cease and the entropy of the system approaches a minimum value.
This law of thermodynamics is a statistical law of nature regarding entropy and the impossibility of reaching absolute zero of temperature. This law provides an absolute reference point for the determination of entropy. The entropy determined relative to this point is the absolute entropy. Alternate definitions include ""the entropy of all systems and of all states of a system is smallest at absolute zero,"" or equivalently ""it is impossible to reach the absolute zero of temperature by any finite number of processes"".
Absolute zero, at which all activity would stop if it were possible to achieve, is −273.15 °C (degrees Celsius), or −459.67 °F (degrees Fahrenheit), or 0 K (kelvin), or 0° R (degrees Rankine).


== System models ==

An important concept in thermodynamics is the thermodynamic system, which is a precisely defined region of the universe under study. Everything in the universe except the system is called the surroundings. A system is separated from the remainder of the universe by a boundary which may be a physical or notional, but serve to confine the system to a finite volume. Segments of the boundary are often described as walls; they have respective defined 'permeabilities'. Transfers of energy as work, or as heat, or of matter, between the system and the surroundings, take place through the walls, according to their respective permeabilities.
Matter or energy that pass across the boundary so as to effect a change in the internal energy of the system need to be accounted for in the energy balance equation. The volume contained by the walls can be the region surrounding a single atom resonating energy, such as Max Planck defined in 1900; it can be a body of steam or air in a steam engine, such as Sadi Carnot defined in 1824. The system could also be just one nuclide (i.e. a system of quarks) as hypothesized in quantum thermodynamics. When a looser viewpoint is adopted, and the requirement of thermodynamic equilibrium is dropped, the system can be the body of a tropical cyclone, such as Kerry Emanuel theorized in 1986 in the field of atmospheric thermodynamics, or the event horizon of a black hole.
Boundaries are of four types: fixed, movable, real, and imaginary. For example, in an engine, a fixed boundary means the piston is locked at its position, within which a constant volume process might occur. If the piston is allowed to move that boundary is movable while the cylinder and cylinder head boundaries are fixed. For closed systems, boundaries are real while for open systems boundaries are often imaginary. In the case of a jet engine, a fixed imaginary boundary might be assumed at the intake of the engine, fixed boundaries along the surface of the case and a second fixed imaginary boundary across the exhaust nozzle.
Generally, thermodynamics distinguishes three classes of systems, defined in terms of what is allowed to cross their boundaries:

As time passes in an isolated system, internal differences of pressures, densities, and temperatures tend to even out. A system in which all equalizing processes have gone to completion is said to be in a state of thermodynamic equilibrium.
Once in thermodynamic equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to be reversible processes.


== States and processes ==
When a system is at equilibrium under a given set of conditions, it is said to be in a definite thermodynamic state. The state of the system can be described by a number of state quantities that do not depend on the process by which the system arrived at its state. They are called intensive variables or extensive variables according to how they change when the size of the system changes. The properties of the system can be described by an equation of state which specifies the relationship between these variables. State may be thought of as the instantaneous quantitative description of a system with a set number of variables held constant.
A thermodynamic process may be defined as the energetic evolution of a thermodynamic system proceeding from an initial state to a final state. It can be described by process quantities. Typically, each thermodynamic process is distinguished from other processes in energetic character according to what parameters, such as temperature, pressure, or volume, etc., are held fixed; Furthermore, it is useful to group these processes into pairs, in which each variable held constant is one member of a conjugate pair.
Several commonly studied thermodynamic processes are:

Adiabatic process: occurs without loss or gain of energy by heat
Isenthalpic process: occurs at a constant enthalpy
Isentropic process: a reversible adiabatic process, occurs at a constant entropy
Isobaric process: occurs at constant pressure
Isochoric process: occurs at constant volume (also called isometric/isovolumetric)
Isothermal process: occurs at a constant temperature
Steady state process: occurs without a change in the internal energy


== Instrumentation ==
There are two types of thermodynamic instruments, the meter and the reservoir. A thermodynamic meter is any device which measures any parameter of a thermodynamic system. In some cases, the thermodynamic parameter is actually defined in terms of an idealized measuring instrument. For example, the zeroth law states that if two bodies are in thermal equilibrium with a third body, they are also in thermal equilibrium with each other. This principle, as noted by James Maxwell in 1872, asserts that it is possible to measure temperature. An idealized thermometer is a sample of an ideal gas at constant pressure. From the ideal gas law pV=nRT, the volume of such a sample can be used as an indicator of temperature; in this manner it defines temperature. Although pressure is defined mechanically, a pressure-measuring device, called a barometer may also be constructed from a sample of an ideal gas held at a constant temperature. A calorimeter is a device which is used to measure and define the internal energy of a system.
A thermodynamic reservoir is a system which is so large that its state parameters are not appreciably altered when it is brought into contact with the system of interest. When the reservoir is brought into contact with the system, the system is brought into equilibrium with the reservoir. For example, a pressure reservoir is a system at a particular pressure, which imposes that pressure upon the system to which it is mechanically connected. The Earth's atmosphere is often used as a pressure reservoir. The ocean can act as temperature reservoir when used to cool power plants.


== Conjugate variables ==

The central concept of thermodynamics is that of energy, the ability to do work. By the First Law, the total energy of a system and its surroundings is conserved. Energy may be transferred into a system by heating, compression, or addition of matter, and extracted from a system by cooling, expansion, or extraction of matter. In mechanics, for example, energy transfer equals the product of the force applied to a body and the resulting displacement.
Conjugate variables are pairs of thermodynamic concepts, with the first being akin to a ""force"" applied to some thermodynamic system, the second being akin to the resulting ""displacement,"" and the product of the two equaling the amount of energy transferred. The common conjugate variables are:

Pressure-volume (the mechanical parameters);
Temperature-entropy (thermal parameters);
Chemical potential-particle number (material parameters).


== Potentials ==
Thermodynamic potentials are different quantitative measures of the stored energy in a system. Potentials are used to measure the energy changes in systems as they evolve from an initial state to a final state. The potential used depends on the constraints of the system, such as constant temperature or pressure. For example, the Helmholtz and Gibbs energies are the energies available in a system to do useful work when the temperature and volume or the pressure and temperature are fixed, respectively.
The five most well known potentials are:

where 
  
    
      
        T
      
    
    {\displaystyle T}
   is the temperature, 
  
    
      
        S
      
    
    {\displaystyle S}
   the entropy, 
  
    
      
        p
      
    
    {\displaystyle p}
   the pressure, 
  
    
      
        V
      
    
    {\displaystyle V}
   the volume, 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   the chemical potential, 
  
    
      
        N
      
    
    {\displaystyle N}
   the number of particles in the system, and 
  
    
      
        i
      
    
    {\displaystyle i}
   is the count of particles types in the system.
Thermodynamic potentials can be derived from the energy balance equation applied to a thermodynamic system. Other thermodynamic potentials can also be obtained through Legendre transformation.


== Applied fields ==


== See also ==

Thermodynamic process path


=== Lists and timelines ===
List of important publications in thermodynamics
List of textbooks on thermodynamics and statistical mechanics
List of thermal conductivities
List of thermodynamic properties
Table of thermodynamic equations
Timeline of thermodynamics


== Notes ==


== References ==


== Further reading ==
Goldstein, Martin & Inge F. (1993). The Refrigerator and the Universe. Harvard University Press. ISBN 978-0-674-75325-9. OCLC 32826343. A nontechnical introduction, good on historical and interpretive matters.
Kazakov, Andrei; Muzny, Chris D.; Chirico, Robert D.; Diky, Vladimir V.; Frenkel, Michael (2008). ""Web Thermo Tables – an On-Line Version of the TRC Thermodynamic Tables"". Journal of Research of the National Institute of Standards and Technology. 113 (4): 209–220. doi:10.6028/jres.113.016. ISSN 1044-677X. PMC 4651616. PMID 27096122.
Gibbs J.W. (1928). The Collected Works of J. Willard Gibbs Thermodynamics. New York: Longmans, Green and Co. Vol. 1, pp. 55–349.
Guggenheim E.A. (1933). Modern thermodynamics by the methods of Willard Gibbs. London: Methuen & co. ltd.
Denbigh K. (1981). The Principles of Chemical Equilibrium: With Applications in Chemistry and Chemical Engineering. London: Cambridge University Press.
Stull, D.R., Westrum Jr., E.F. and Sinke, G.C. (1969). The Chemical Thermodynamics of Organic Compounds. London: John Wiley and Sons, Inc.CS1 maint: multiple names: authors list (link)
Bazarov I.P. (2010). Thermodynamics: Textbook. St. Petersburg: Lan publishing house. p. 384. ISBN 978-5-8114-1003-3. 5th ed. (in Russian)
Bawendi Moungi G., Alberty Robert A. and Silbey Robert J. (2004). Physical Chemistry. J. Wiley & Sons, Incorporated.
Alberty Robert A. (2003). Thermodynamics of Biochemical Reactions. Wiley-Interscience.
Alberty Robert A. (2006). Biochemical Thermodynamics: Applications of Mathematica. John Wiley & Sons, Inc. ISBN 978-0-471-75798-6. PMID 16878778.
Dill Ken A., Bromberg Sarina (2011). Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience. Garland Science. ISBN 978-0-8153-4430-8.
M. Scott Shell (2015). Thermodynamics and Statistical Mechanics: An Integrated Approach. Cambridge University Press. ISBN 978-1107656789.
Douglas E. Barrick (2018). Biomolecular Thermodynamics: From Theory to Applications. CRC Press. ISBN 978-1-4398-0019-5.The following titles are more technical:

Bejan, Adrian (2016). Advanced Engineering Thermodynamics (4 ed.). Wiley. ISBN 978-1-119-05209-8.
Cengel, Yunus A., & Boles, Michael A. (2002). Thermodynamics – an Engineering Approach. McGraw Hill. ISBN 978-0-07-238332-4. OCLC 45791449.CS1 maint: multiple names: authors list (link)
Dunning-Davies, Jeremy (1997). Concise Thermodynamics: Principles and Applications. Horwood Publishing. ISBN 978-1-8985-6315-0. OCLC 36025958.
Kroemer, Herbert & Kittel, Charles (1980). Thermal Physics. W.H. Freeman Company. ISBN 978-0-7167-1088-2. OCLC 32932988.


== External links ==
 Media related to Thermodynamics at Wikimedia CommonsCallendar, Hugh Longbourne (1911). ""Thermodynamics"" . Encyclopædia Britannica. 26 (11th ed.). pp. 808–814.
Thermodynamics Data & Property Calculation Websites
Thermodynamics Educational Websites
Thermodynamics at ScienceWorld
Biochemistry Thermodynamics
Thermodynamics and Statistical Mechanics
Engineering Thermodynamics – A Graphical Approach
Thermodynamics and Statistical Mechanics by Richard Fitzpatrick","pandas(index=125, _1=125, text='thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. the behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology. historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of french physicist nicolas léonard sadi carnot (1824) who believed that engine efficiency was the key that could help france win the napoleonic wars. scots-irish physicist lord kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, ""thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency."" the initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. other formulations of thermodynamics emerged. statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. in 1909, constantin carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.   == introduction == a description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. the first law specifies that energy can be exchanged between physical systems as heat and work. the second law defines the existence of a quantity called entropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.in thermodynamics, interactions between large ensembles of objects are studied and categorized. central to this are the concepts of the thermodynamic system and its surroundings. a system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes. with these tools, thermodynamics can be used to describe how systems respond to changes in their environment. this can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. the results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.this article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.   == history == the history of thermodynamics as a scientific discipline generally begins with otto von guericke who, in 1650, built and designed the world\'s first vacuum pump and demonstrated a vacuum using his magdeburg hemispheres. guericke was driven to make a vacuum in order to disprove aristotle\'s long-held supposition that \'nature abhors a vacuum\'. shortly after guericke, the anglo-irish physicist and chemist robert boyle had learned of guericke\'s designs and, in 1656, in coordination with english scientist robert hooke, built an air pump. using this pump, boyle and hooke noticed a correlation between pressure, temperature, and volume. in time, boyle\'s law was formulated, which states that pressure and volume are inversely proportional. then, in 1679, based on these concepts, an associate of boyle\'s named denis papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated. later designs implemented a steam release valve that kept the machine from exploding. by watching the valve rhythmically move up and down, papin conceived of the idea of a piston and a cylinder engine. he did not, however, follow through with his design. nevertheless, in 1697, based on papin\'s designs, engineer thomas savery built the first engine, followed by thomas newcomen in 1712. although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. the fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by professor joseph black at the university of glasgow, where james watt was employed as an instrument maker. black and watt performed experiments together, but it was watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. drawing on all the previous work led sadi carnot, the ""father of thermodynamics"", to publish reflections on the motive power of fire (1824), a discourse on heat, power, energy and engine efficiency. the book outlined the basic energetic relations between the carnot engine, the carnot cycle, and motive power. it marked the start of thermodynamics as a modern science.the first thermodynamic textbook was written in 1859 by william rankine, originally trained as a physicist and a civil and mechanical engineering professor at the university of glasgow. the first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of william rankine, rudolf clausius, and william thomson (lord kelvin).the foundations of statistical thermodynamics were set out by physicists such as james clerk maxwell, ludwig boltzmann, max planck, rudolf clausius and j. willard gibbs. during the years 1873–76 the american mathematical physicist josiah willard gibbs published a series of three papers, the most famous being on the equilibrium of heterogeneous substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. also pierre duhem in the 19th century wrote about chemical thermodynamics. during the early 20th century, chemists such as gilbert n. lewis, merle randall, and e. a. guggenheim applied the mathematical methods of gibbs to the analysis of chemical processes.   == etymology == the etymology of thermodynamics has an intricate history. it was first spelled in a hyphenated form as an adjective (thermo-dynamic) and from 1854 to 1868 as the noun thermo-dynamics to represent the science of generalized heat engines.american biophysicist donald haynie claims that thermodynamics was coined in 1840 from the greek root θέρμη therme, meaning “heat”, and δύναμις dynamis, meaning “power”.pierre perrot claims that the term thermodynamics was coined by james joule in 1858 to designate the science of relations between heat and power, however, joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to thomson\'s 1849 phraseology.by 1858, thermo-dynamics, as a functional term, was used in william thomson\'s paper ""an account of carnot\'s theory of the motive power of heat.""   == branches of thermodynamics == the study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems. list of important publications in thermodynamics list of textbooks on thermodynamics and statistical mechanics list of thermal conductivities list of thermodynamic properties table of thermodynamic equations timeline of thermodynamics   == notes ==   == references ==   == further reading == goldstein, martin & inge f. (1993). the refrigerator and the universe. harvard university press. isbn 978-0-674-75325-9. oclc 32826343. a nontechnical introduction, good on historical and interpretive matters. kazakov, andrei; muzny, chris d.; chirico, robert d.; diky, vladimir v.; frenkel, michael (2008). ""web thermo tables – an on-line version of the trc thermodynamic tables"". journal of research of the national institute of standards and technology. 113 (4): 209–220. doi:10.6028/jres.113.016. issn 1044-677x. pmc 4651616. pmid 27096122. gibbs j.w. (1928). the collected works of j. willard gibbs thermodynamics. new york: longmans, green and co. vol. 1, pp. 55–349. guggenheim e.a. (1933). modern thermodynamics by the methods of willard gibbs. london: methuen & co. ltd. denbigh k. (1981). the principles of chemical equilibrium: with applications in chemistry and chemical engineering. london: cambridge university press. stull, d.r., westrum jr., e.f. and sinke, g.c. (1969). the chemical thermodynamics of organic compounds. london: john wiley and sons, inc.cs1 maint: multiple names: authors list (link) bazarov i.p. (2010). thermodynamics: textbook. st. petersburg: lan publishing house. p. 384. isbn 978-5-8114-1003-3. 5th ed. (in russian) bawendi moungi g., alberty robert a. and silbey robert j. (2004). physical chemistry. j. wiley & sons, incorporated. alberty robert a. (2003). thermodynamics of biochemical reactions. wiley-interscience. alberty robert a. (2006). biochemical thermodynamics: applications of mathematica. john wiley & sons, inc. isbn 978-0-471-75798-6. pmid 16878778. dill ken a., bromberg sarina (2011). molecular driving forces: statistical thermodynamics in biology, chemistry, physics, and nanoscience. garland science. isbn 978-0-8153-4430-8. m. scott shell (2015). thermodynamics and statistical mechanics: an integrated approach. cambridge university press. isbn 978-1107656789. douglas e. barrick (2018). biomolecular thermodynamics: from theory to applications. crc press. isbn 978-1-4398-0019-5.the following titles are more technical:  bejan, adrian (2016). advanced engineering thermodynamics (4 ed.). wiley. isbn 978-1-119-05209-8. cengel, yunus a., & boles, michael a. (2002). thermodynamics – an engineering approach. mcgraw hill. isbn 978-0-07-238332-4. oclc 45791449.cs1 maint: multiple names: authors list (link) dunning-davies, jeremy (1997). concise thermodynamics: principles and applications. horwood publishing. isbn 978-1-8985-6315-0. oclc 36025958. kroemer, herbert & kittel, charles (1980). thermal physics. w.h. freeman company. isbn 978-0-7167-1088-2. oclc 32932988.   == external links == media related to thermodynamics at wikimedia commonscallendar, hugh longbourne (1911). ""thermodynamics"" . encyclopædia britannica. 26 (11th ed.). pp. 808–814. thermodynamics data & property calculation websites thermodynamics educational websites thermodynamics at scienceworld biochemistry thermodynamics thermodynamics and statistical mechanics engineering thermodynamics – a graphical approach thermodynamics and statistical mechanics by richard fitzpatrick')"
126,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewerage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.


== History ==


=== Civil engineering as a discipline ===
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes Principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.


=== Civil engineering profession ===

Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley Civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.
Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The construction of pyramids in Egypt (circa 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system (the oldest is older than 3000 years and longer than 71 km,) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.

In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. In 1747, the first institution for the teaching of civil engineering, the École Nationale des Ponts et Chaussées was established in France; and more examples followed in other European countries, like Spain. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.

In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal Charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:the art of directing the great sources of power in nature for the use and convenience of man, as the means of production and of traffic in states, both for external and internal trade, as applied in the construction of roads, bridges, aqueducts, canals, river navigation and docks for internal intercourse and exchange, and in the construction of ports, harbours, moles, breakwaters and lighthouses, and in the art of navigation by artificial power for the purposes of commerce, and in the construction and application of machinery, and in the drainage of cities and towns.


=== Civil engineering education ===
The first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.In the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Class of Civil Engineering and Mining was founded at King's College London in 1838, mainly as a response to the growth of the railway system and the need for more qualified engineers, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.


== Education ==
Civil engineers typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of technology, or a bachelor of engineering. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move on to specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualification, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.


== Practicing engineers ==
In most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements including work experience and exam requirements before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.
The benefits of certification vary depending upon location. For example, in the United States and Canada, ""only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients."" This requirement is enforced under provincial law such as the Engineers Act in Quebec. No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.Engineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, they may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.


== Sub-disciplines ==

There are a number of sub-disciplines within the broad field of civil engineering. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.


=== Coastal engineering ===

Coastal engineering is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. The term coastal defense is the more traditional term, but coastal management has become more popular as the field has expanded to techniques that allow erosion to claim land.


=== Construction engineering ===

Construction engineering involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.


=== Earthquake engineering ===

Earthquake engineering involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.


=== Environmental engineering ===

Environmental engineering is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.
Environmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, recycling, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.


=== Forensic engineering ===

Forensic engineering is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.


=== Geotechnical engineering ===

Geotechnical engineering studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geo-environmental engineering.Identification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult. Geotechnical engineers frequently work with professional geologists and soil scientists.


=== Materials science and engineering ===

Materials science is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and thermosetting polymers including polymethylmethacrylate (PMMA) and carbon fibers.
Materials engineering involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.


=== Site development and planning ===

Site development, also known as site planning, is focused on the planning and development potential of a site as well as addressing possible impacts from permitting issues and environmental challenges.


=== Structural engineering ===

Structural engineering is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be serviceable). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.Design considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructability, safety, aesthetics and sustainability.


=== Surveying ===

Surveying is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment such as levels and theodolites are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures.
Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.

Land surveying
In the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying.
Construction surveyingConstruction surveying is generally performed by specialized technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:

Surveying existing conditions of the future work site, including topography, existing buildings and infrastructure, and underground infrastructure when possible;
""lay-out"" or ""setting-out"": placing reference points and markers that will guide the construction of new structures such as roads or buildings;
Verifying the location of structures during construction;
As-Built surveying: a survey conducted at the end of the construction project to verify that the work authorized was completed to the specifications set on plans.


=== Transportation engineering ===
Transportation engineering is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.


=== Municipal or urban engineering ===

Municipal engineering is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimizing of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority.  Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)


=== Water resources engineering ===

Water resources engineering is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines elements of hydrology, environmental science, meteorology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.

Hydraulic engineering is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.


=== Civil engineering systems ===
Civil engineering systems is a discipline that promotes the use of systems thinking to manage complexity and change in civil engineering within its wider public context. It posits that the proper development of civil engineering infrastructure requires a holistic, coherent understanding of the relationships between all of the important factors that contribute to successful projects while at the same time emphasizing the importance of attention to technical detail. Its purpose is to help integrate the entire civil engineering project life cycle from conception, through planning, designing, making, operating to decommissioning.


== See also ==


=== Associations ===


== References ==


== Further reading ==
W.F. Chen; J.Y. Richard Liew, eds. (2002). The Civil Engineering Handbook. CRC Press. ISBN 978-0-8493-0958-8.
Jonathan T. Ricketts; M. Kent Loftin; Frederick S. Merritt, eds. (2004). Standard handbook for civil engineers (5 ed.). McGraw Hill. ISBN 978-0-07-136473-7.
Muir Wood, David (2012). Civil Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-957863-4.
Blockley, David (2014). Structural Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-967193-9.


== External links ==
The Institution of Civil Engineers
Civil Engineering Software Database
The Institution of Civil Engineering Surveyors
Civil engineering classes, from MIT OpenCourseWare","pandas(index=126, _1=126, text='civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewerage systems, pipelines, structural components of buildings, and railways.civil engineering is traditionally broken into a number of sub-disciplines. it is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global fortune 500 companies.   == history == == references ==   == further reading == w.f. chen; j.y. richard liew, eds. (2002). the civil engineering handbook. crc press. isbn 978-0-8493-0958-8. jonathan t. ricketts; m. kent loftin; frederick s. merritt, eds. (2004). standard handbook for civil engineers (5 ed.). mcgraw hill. isbn 978-0-07-136473-7. muir wood, david (2012). civil engineering: a very short introduction. new york: oxford university press. isbn 978-0-19-957863-4. blockley, david (2014). structural engineering: a very short introduction. new york: oxford university press. isbn 978-0-19-967193-9.   == external links == the institution of civil engineers civil engineering software database the institution of civil engineering surveyors civil engineering classes, from mit opencourseware')"
127,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of man-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.


== History ==

Structural engineering dates back to 2700 B.C.E. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).The structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. The limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 MPa (MPa = Pa × 106). Therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.
Throughout ancient and medieval history most architectural design and construction were carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before'. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.No record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of a structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete. The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.


=== Timeline ===

1452–1519 Leonardo da Vinci made many contributions
1638: Galileo Galilei published the book Two New Sciences in which he examined the failure of simple
1660: Hooke's law by Robert Hooke
1687: Isaac Newton published Philosophiæ Naturalis Principia Mathematica which contains the Newton's laws of motion
1750: Euler–Bernoulli beam equation
1700–1782: Daniel Bernoulli introduced the principle of virtual work
1707–1783: Leonhard Euler developed the theory of buckling of columns
1826: Claude-Louis Navier published a treatise on the elastic behaviors of structures
1873: Carlo Alberto Castigliano presented his dissertation ""Intorno ai sistemi elastici"", which contains his theorem for computing displacement as the partial derivative of the strain energy. This theorem includes the method of ""least work"" as a special case
1874: Otto Mohr formalized the idea of a statically indeterminate structure.
1922: Timoshenko corrects the Euler-Bernoulli beam equation
1936: Hardy Cross' publication of the moment distribution method, an important innovation in the design of continuous frames.
1941: Alexander Hrennikoff solved the discretization of plane elasticity problems using a lattice framework
1942: R. Courant divided a domain into finite subregions
1956: J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp's paper on the ""Stiffness and Deflection of Complex Structures"" introduces the name ""finite-element method"" and is widely recognized as the first comprehensive treatment of the method as it is known today


=== Structural failure ===

The history of structural engineering contains many collapses and failures.  Sometimes this is due to obvious negligence, as in the case of the Pétion-Ville school collapse, in which Rev. Fortin Augustin "" constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction"" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.
In other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and a greater understanding of the science of structural engineering.  Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated.  A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.


== Theory ==

Structural engineering depends upon a detailed knowledge of applied mechanics, materials science, and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure, Inducta RCB, etc. Such software may also take into consideration environmental loads, such as earthquakes and winds.


== Profession ==

Structural engineers are responsible for engineering design and structural analysis. Entry-level structural engineers may design the individual structural elements of a structure, such as the beams and columns of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.
Structural engineers often specialize in particular types of structures, such as buildings, bridges, pipelines, industrial, tunnels, vehicles, ships, aircraft, and spacecraft.  Structural engineers who specialize in buildings often specialize in particular construction materials such as concrete, steel, wood, masonry, alloys, and composites, and may focus on particular types of buildings such as offices, schools, hospitals, residential, and so forth.
Structural engineering has existed since humans first started to construct their structures. It became a more defined and formalized profession with the emergence of architecture as a distinct profession from engineering during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same thing – the master builder. Only with the development of specialized knowledge of structural theories that emerged during the 19th and early 20th centuries, did the professional structural engineers come into existence.
The role of a structural engineer today involves a significant understanding of both static and dynamic loading and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five-year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.
Structural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.
Another international organisation is IABSE(International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.


== Specializations ==


=== Building structures ===

Structural building engineering includes all structural engineering related to the design of buildings. It is a branch of structural engineering closely affiliated with architecture.
Structural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end that fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture, and light to achieve an end which is aesthetic, functional, and often artistic.
The architect is usually the lead designer on buildings, with a structural engineer employed as a sub-consultant. The degree to which each discipline leads the design depends heavily on the type of structure. Many structures are structurally simple and led by architecture, such as multi-story office buildings and housing, while other structures, such as tensile structures, shells and gridshells are heavily dependent on their form for their strength, and the engineer may have a more significant influence on the form, and hence much of the aesthetic, than the architect.
The structural design for a building must ensure that the building can stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking, and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting, etc.). The structural design of a modern building can be extremely complex and often requires a large team to complete.
Structural engineering specialties for buildings include:

Earthquake engineering
Façade engineering
Fire engineering
Roof engineering
Tower engineering
Wind engineering


=== Earthquake engineering structures ===

Earthquake engineering structures are those engineered to withstand earthquakes.
The main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.
Earthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above.
One important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.


=== Civil engineering structures ===
Civil structural engineering includes all structural engineering related to the built environment. It includes:

The structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).
Civil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities, or below ground.


=== Mechanical structures ===
The principles of structural engineering apply to a variety of mechanical (moveable) structures. The design of static structures assumes they always have the same geometry (in fact, so-called static structures can move significantly, and structural engineering design must take this into account where necessary), but the design of moveable or moving structures must account for fatigue, variation in the method in which load is resisted and significant deflections of structures.
The forces which parts of a machine are subjected to can vary significantly and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures can endure such loading for their entire design life without failing.
These works can require mechanical structural engineering:

Boilers and pressure vessels
Coachworks and carriages
Cranes
Elevators
Escalators
Marine vessels and hulls


=== Aerospace structures ===

Aerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads, and frames to support the shape and fasteners such as welds, rivets, screws, and bolts to hold the components together.


=== Nanoscale structures ===
A nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometer range. The term 'nanostructure' is often used when referring to magnetic technology.


=== Structural engineering for medical science ===

Medical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: diagnostic equipment includes medical imaging machines, used to aid in diagnosis; equipment includes infusion pumps, medical lasers, and LASIK surgical machines; medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood; diagnostic medical equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.


== Structural elements ==

Any structure is essentially made up of only a small number of different types of elements:

Columns
Beams
Plates
Arches
Shells
CatenariesMany of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):


=== Columns ===

Columns are elements that carry only axial force (compression) or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element and the buckling capacity.
The buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is 
  
    
      
        K
        ∗
        l
      
    
    {\displaystyle K*l}
   where 
  
    
      
        l
      
    
    {\displaystyle l}
   is the real length of the column and K is the factor dependent on the restraint conditions.
The capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.


=== Beams ===

A beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.

cantilevered (supported at one end only with a fixed connection)
simply supported (fixed against vertical translation at each end and horizontal translation at one end only,  and able to rotate at the supports)
fixed (supported in all directions for translation and rotation at each end)
continuous (supported by three or more supports)
a combination of the above (ex. supported at one end and in the middle)Beams are elements that carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.


=== Trusses ===

A truss is a structure comprising members and connection points or nodes. When members are connected at nodes and forces are applied at nodes members can act in tension or compression. Members acting in compression are referred to as compression members or struts while members acting in tension are referred to as tension members or ties. Most trusses use gusset plates to connect intersecting elements.  Gusset plates are relatively flexible and unable to transfer bending moments. The connection is usually arranged so that the lines of force in the members are coincident at the joint thus allowing the truss members to act in pure tension or compression.
Trusses are usually used in large-span structures, where it would be uneconomical to use solid beams.


=== Plates ===

Plates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.
They can also be designed with yield line theory, where an assumed collapse mechanism is analyzed to give an upper bound on the collapse load. This technique is used in practice  but because the method provides an upper-bound, i.e. an unsafe prediction of the collapse load, for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic.


=== Shells ===

Shells derive their strength from their form and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension and inverting the form to achieve pure compression.


=== Arches ===

Arches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.


=== Catenaries ===

Catenaries derive their strength from their form and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.


== Materials ==

Structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads. It also involves a knowledge of Corrosion engineering to avoid for example galvanic coupling of dissimilar materials. 
Common structural materials are:

Iron: wrought iron, cast iron
Concrete: reinforced concrete, prestressed concrete
Alloy: steel, stainless steel
Masonry
Timber: hardwood, softwood
Aluminium
Composite materials: plywood
Other structural materials: adobe, bamboo, carbon fibre, fiber reinforced plastic, mudbrick, roofing materials


== See also ==


== Notes ==


== References ==
Hibbeler, R. C. (2010). Structural Analysis. Prentice-Hall.
Blank, Alan; McEvoy, Michael; Plank, Roger (1993). Architecture and Construction in Steel. Taylor & Francis. ISBN 0-419-17660-8.
Hewson, Nigel R. (2003). Prestressed Concrete Bridges: Design and Construction. Thomas Telford. ISBN 0-7277-2774-5.
Heyman, Jacques (1999). The Science of Structural Engineering. Imperial College Press. ISBN 1-86094-189-3.
Hosford, William F. (2005). Mechanical Behavior of Materials. Cambridge University Press. ISBN 0-521-84670-6.


== Further reading ==
Blockley, David (2014). A Very Short Introduction to Structural Engineering. Oxford University Press ISBN 978-0-19967193-9.
Bradley, Robert E.; Sandifer, Charles Edward (2007). Leonhard Euler: Life, Work, and Legacy. Elsevier. ISBN 0-444-52728-1.
Chapman, Allan. (2005). England's Leornardo: Robert Hooke and the Seventeenth Century's Scientific Revolution. CRC Press. ISBN 0-7503-0987-3.
Dugas, René (1988). A History of Mechanics. Courier Dover Publications. ISBN 0-486-65632-2.
Feld, Jacob; Carper, Kenneth L. (1997). Construction Failure. John Wiley & Sons. ISBN 0-471-57477-5.
Galilei, Galileo. (translators: Crew, Henry; de Salvio, Alfonso) (1954). Dialogues Concerning Two New Sciences. Courier Dover Publications. ISBN 0-486-60099-8
Kirby, Richard Shelton (1990). Engineering in History. Courier Dover Publications. ISBN 0-486-26412-2.
Heyman, Jacques (1998). Structural Analysis: A Historical Approach. Cambridge University Press. ISBN 0-521-62249-2.
Labrum, E.A. (1994). Civil Engineering Heritage. Thomas Telford. ISBN 0-7277-1970-X.
Lewis, Peter R. (2004). Beautiful Bridge of the Silvery Tay. Tempus.
Mir, Ali (2001). Art of the Skyscraper: the Genius of Fazlur Khan. Rizzoli International Publications. ISBN 0-8478-2370-9.
Rozhanskaya, Mariam; Levinova, I. S. (1996). ""Statics"" in Morelon, Régis & Rashed, Roshdi (1996). Encyclopedia of the History of Arabic Science, vol. 2–3, Routledge. ISBN 0-415-02063-8
Whitbeck, Caroline (1998). Ethics in Engineering Practice and Research. Cambridge University Press. ISBN 0-521-47944-4.
Hoogenboom P.C.J. (1998). ""Discrete Elements and Nonlinearity in Design of Structural Concrete Walls"", Section 1.3 Historical Overview of Structural Concrete Modelling, ISBN 90-901184-3-8.
Nedwell, P.J.; Swamy, R.N.(ed) (1994). Ferrocement:Proceedings of the Fifth International Symposium. Taylor & Francis. ISBN 0-419-19700-1.


== External links ==

Structural Engineering Association – International
National Council of Structural Engineers Associations
Structural Engineering Institute, an institute of the American Society of Civil Engineers
Structurae database of structures
Structuremag The Definition of Structural Engineering
The EN Eurocodes are a series of 10 European Standards, EN 1990 – EN 1999, providing a common approach for the design of buildings and other civil engineering works and construction products","pandas(index=127, _1=127, text='structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the \'bones and muscles\' that create the form and shape of man-made structures. structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. the structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. they can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  see glossary of structural engineering. structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.   == history ==  structural engineering dates back to 2700 b.c.e. when the step pyramid for pharaoh djoser was built by imhotep, the first engineer in history known by name. pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).the structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. the limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 mpa (mpa = pa × 106). therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid\'s geometry. throughout ancient and medieval history most architectural design and construction were carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. no theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of \'what had worked before\'. knowledge was retained by guilds and seldom supplanted by advances. structures were repetitive, and increases in scale were incremental.no record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of a structural engineer only really took shape with the industrial revolution and the re-invention of concrete (see history of concrete. the physical sciences underlying structural engineering began to be understood in the renaissance and have since developed into computer-based applications pioneered in the 1970s. catenaries derive their strength from their form and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). they are almost always cable or fabric structures. a fabric structure acts as a catenary in two directions.   == materials ==  structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads. it also involves a knowledge of corrosion engineering to avoid for example galvanic coupling of dissimilar materials. common structural materials are:  iron: wrought iron, cast iron concrete: reinforced concrete, prestressed concrete alloy: steel, stainless steel masonry timber: hardwood, softwood aluminium composite materials: plywood other structural materials: adobe, bamboo, carbon fibre, fiber reinforced plastic, mudbrick, roofing materials   == see also ==   == notes ==   == references == hibbeler, r. c. (2010). structural analysis. prentice-hall. blank, alan; mcevoy, michael; plank, roger (1993). architecture and construction in steel. taylor & francis. isbn 0-419-17660-8. hewson, nigel r. (2003). prestressed concrete bridges: design and construction. thomas telford. isbn 0-7277-2774-5. heyman, jacques (1999). the science of structural engineering. imperial college press. isbn 1-86094-189-3. hosford, william f. (2005). mechanical behavior of materials. cambridge university press. isbn 0-521-84670-6.   == further reading == blockley, david (2014). a very short introduction to structural engineering. oxford university press isbn 978-0-19967193-9. bradley, robert e.; sandifer, charles edward (2007). leonhard euler: life, work, and legacy. elsevier. isbn 0-444-52728-1. chapman, allan. (2005). england\'s leornardo: robert hooke and the seventeenth century\'s scientific revolution. crc press. isbn 0-7503-0987-3. dugas, rené (1988). a history of mechanics. courier dover publications. isbn 0-486-65632-2. feld, jacob; carper, kenneth l. (1997). construction failure. john wiley & sons. isbn 0-471-57477-5. galilei, galileo. (translators: crew, henry; de salvio, alfonso) (1954). dialogues concerning two new sciences. courier dover publications. isbn 0-486-60099-8 kirby, richard shelton (1990). engineering in history. courier dover publications. isbn 0-486-26412-2. heyman, jacques (1998). structural analysis: a historical approach. cambridge university press. isbn 0-521-62249-2. labrum, e.a. (1994). civil engineering heritage. thomas telford. isbn 0-7277-1970-x. lewis, peter r. (2004). beautiful bridge of the silvery tay. tempus. mir, ali (2001). art of the skyscraper: the genius of fazlur khan. rizzoli international publications. isbn 0-8478-2370-9. rozhanskaya, mariam; levinova, i. s. (1996). ""statics"" in morelon, régis & rashed, roshdi (1996). encyclopedia of the history of arabic science, vol. 2–3, routledge. isbn 0-415-02063-8 whitbeck, caroline (1998). ethics in engineering practice and research. cambridge university press. isbn 0-521-47944-4. hoogenboom p.c.j. (1998). ""discrete elements and nonlinearity in design of structural concrete walls"", section 1.3 historical overview of structural concrete modelling, isbn 90-901184-3-8. nedwell, p.j.; swamy, r.n.(ed) (1994). ferrocement:proceedings of the fifth international symposium. taylor & francis. isbn 0-419-19700-1.   == external links ==  structural engineering association – international national council of structural engineers associations structural engineering institute, an institute of the american society of civil engineers structurae database of structures structuremag the definition of structural engineering the en eurocodes are a series of 10 european standards, en 1990 – en 1999, providing a common approach for the design of buildings and other civil engineering works and construction products')"
128,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.  
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place. This inventory or database must include information on population, land use, economic activity, transportation facilities and services, travel patterns and volumes, laws and ordinances, regional financial resources, and community values and expectations. These inventories help the engineer create business models to complete accurate forecasts of the future conditions of the system.
Operations and management involve traffic engineering, so that vehicles move smoothly on the road or track.  Older techniques include signs, signals, markings, and tolling.  Newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. Human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.


== Highway engineering ==

Engineers in this specialization:

Handle the planning, design, construction, and operation of highways, roads, and other vehicular facilities as well as their related bicycle and pedestrian realms
Estimate the transportation needs of the public and then secure the funding for projects
Analyze locations of high traffic volumes and high collisions for safety and capacity
Use engineering principles to improve the transportation system
Utilize the three design controls, which are the drivers, the vehicles, and the roadways themselves


== Railroad engineering ==

Railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or monorails).  Typical tasks include determining horizontal and vertical alignment design, station location and design, and construction cost estimating.  Railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control.
Railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands.  In the United States, railway engineers work with elected officials in Washington, D.C. on rail transportation issues to make sure that the rail system meets the country's transportation needs.


== Port and harbor engineering ==

Port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities.


== Airport engineering ==
Airport engineers design and construct airports.  Airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. These engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.


== See also ==


== References ==


== External links ==
 Media related to Transport engineering at Wikimedia Commons
http://www.ite.org Institute of Transportation Engineers, a professional society for transportation engineers
http://www.itsa.org ITS America
http://www.asce.org ASCE","pandas(index=128, _1=128, text=""transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport. the planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  more sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system. a review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. the national council of examiners for engineering and surveying (ncees) list online the safety protocols, geometric design requirements, and signal timing. transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. the facilities support air, highway, railroad, pipeline, water, and even space transportation. the design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track). before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place. this inventory or database must include information on population, land use, economic activity, transportation facilities and services, travel patterns and volumes, laws and ordinances, regional financial resources, and community values and expectations. these inventories help the engineer create business models to complete accurate forecasts of the future conditions of the system. operations and management involve traffic engineering, so that vehicles move smoothly on the road or track.  older techniques include signs, signals, markings, and tolling.  newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.   == highway engineering ==  engineers in this specialization:  handle the planning, design, construction, and operation of highways, roads, and other vehicular facilities as well as their related bicycle and pedestrian realms estimate the transportation needs of the public and then secure the funding for projects analyze locations of high traffic volumes and high collisions for safety and capacity use engineering principles to improve the transportation system utilize the three design controls, which are the drivers, the vehicles, and the roadways themselves   == railroad engineering ==  railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or monorails).  typical tasks include determining horizontal and vertical alignment design, station location and design, and construction cost estimating.  railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control. railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands.  in the united states, railway engineers work with elected officials in washington, d.c. on rail transportation issues to make sure that the rail system meets the country's transportation needs.   == port and harbor engineering ==  port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities.   == airport engineering == airport engineers design and construct airports.  airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. these engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.   == see also ==   == references ==   == external links == media related to transport engineering at wikimedia commons http://www.ite.org institute of transportation engineers, a professional society for transportation engineers http://www.itsa.org its america http://www.asce.org asce"")"
129,"Avionics are the electronic systems used on aircraft, artificial satellites, and spacecraft. Avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. These can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform. The term avionics is a portmanteau of the words aviation and electronics.


== History ==
The term ""avionics"" was coined in 1949 by  Philip J. Klass, senior editor at Aviation Week & Space Technology magazine as a portmanteau of ""aviation electronics"".Radio communication was first used in aircraft just prior to World War I.   The first airborne radios were in zeppelins, but the military sparked development of light radio sets that could be carried by heavier-than-air craft, so that aerial reconnaissance biplanes could report their observations immediately in case they were shot down.  The first experimental radio transmission from an airplane was conducted by the US Navy August 1910. The first aircraft radios transmitted by radiotelegraphy, so they required two-seat aircraft with a second crewman to tap on a telegraph key to spell out messages by Morse code.   During World War I, AM voice two way radio sets were made possible in 1917 by the development of the triode vacuum tube, which were simple enough that the pilot in a single seat aircraft could use it while flying.
Radar, the central technology used today in aircraft navigation and air traffic control, was developed by several nations, mainly in secret, as an air defense system in the 1930s during the runup to World War II.   Many modern avionics have their origins in World War II wartime developments. For example, autopilot systems that are commonplace today began as specialized systems to help bomber planes fly steadily enough to hit precision targets from high altitudes.  Britain's 1940 decision to share its radar technology with its US ally, particularly the magnetron vacuum tube, in the famous Tizard Mission, significantly shortened the war. Modern avionics is a substantial portion of military aircraft spending. Aircraft like the F‑15E and the now retired F‑14 have roughly 20 percent of their budget spent on avionics. Most modern helicopters now have budget splits of 60/40 in favour of avionics.The civilian market has also seen a growth in cost of avionics. Flight control systems (fly-by-wire) and new navigation needs brought on by tighter airspaces, have pushed up development costs. The major change has been the recent boom in consumer flying. As more people begin to use planes as their primary method of transportation, more elaborate methods of controlling aircraft safely in these high restrictive airspaces have been invented.


=== Modern avionics ===
Avionics plays a heavy role in modernization initiatives like the Federal Aviation Administration's (FAA) Next Generation Air Transportation System project in the United States and the Single European Sky ATM Research (SESAR) initiative in Europe. The Joint Planning and Development Office put forth a roadmap for avionics in six areas:
Published Routes and Procedures – Improved navigation and routing
Negotiated Trajectories – Adding data communications to create preferred routes dynamically
Delegated Separation – Enhanced situational awareness in the air and on the ground
LowVisibility/CeilingApproach/Departure – Allowing operations with weather constraints with less ground infrastructure
Surface Operations – To increase safety in approach and departure
ATM Efficiencies – Improving the ATM process


=== Market ===
The Aircraft Electronics Association reports $1.73 billion avionics sales for the first three quarters of 2017 in business and general aviation, a 4.1% yearly improvement: 73.5% came from North America, forward-fit represented 42.3% while 57.7% were retrofits as the U.S. deadline of January 1, 2020 for mandatory ADS-B out approach.


== Aircraft avionics ==
The cockpit of an aircraft is a typical location for avionic equipment, including control, monitoring, communication, navigation, weather, and anti-collision systems. The majority of aircraft power their avionics using 14- or 28‑volt DC electrical systems; however, larger, more sophisticated aircraft (such as airliners or military combat aircraft) have AC systems operating at 400 Hz, 115 volts AC. There are several major vendors of flight avionics, including Panasonic Avionics Corporation, Honeywell (which now owns Bendix/King), Universal Avionics Systems Corporation, Rockwell Collins (now Collins Aerospace), Thales Group, GE Aviation Systems, Garmin, Raytheon, Parker Hannifin, UTC Aerospace Systems (now Collins Aerospace), Selex ES (now Leonardo S.p.A.), Shadin Avionics and Avidyne Corporation.
International standards for avionics equipment are prepared by the Airlines Electronic Engineering Committee (AEEC) and published by ARINC.


=== Communications ===
Communications connect the flight deck to the ground and the flight deck to the passengers. On‑board communications are provided by public-address systems and aircraft intercoms.
The VHF aviation communication system works on the airband of 118.000 MHz to 136.975 MHz. Each channel is spaced from the adjacent ones by 8.33 kHz in Europe, 25 kHz elsewhere. VHF is also used for line of sight communication such as aircraft-to-aircraft and aircraft-to-ATC. Amplitude modulation (AM) is used, and the conversation is performed in simplex mode.  Aircraft communication can also take place using HF (especially for trans-oceanic flights) or satellite communication.


=== Navigation ===

Air navigation is the determination of position and direction on or above the surface of the Earth. Avionics can use satellite navigation systems (such as GPS and WAAS), INS( inertial navigation system), ground-based radio navigation systems (such as VOR or LORAN), or any combination thereof. Some navigation systems such as GPS calculate the position automatically and display it to the flight crew on moving map displays. Older ground-based Navigation systems such as VOR or LORAN requires a pilot or navigator to plot the intersection of signals on a paper map to determine an aircraft's location; modern systems calculate the position automatically and display it to the flight crew on moving map displays.


=== Monitoring ===

The first hints of glass cockpits emerged in the 1970s when flight-worthy cathode ray tube (CRT) screens began to replace electromechanical displays, gauges and instruments. A ""glass"" cockpit refers to the use of computer monitors instead of gauges and other analog displays. Aircraft were getting progressively more displays, dials and information dashboards that eventually competed for space and pilot attention. In the 1970s, the average aircraft had more than 100 cockpit instruments and controls.
Glass cockpits started to come into being with the Gulfstream G‑IV private jet in 1985. One of the key challenges in glass cockpits is to balance how much control is automated and how much the pilot should do manually. Generally they try to automate flight operations while keeping the pilot constantly informed.


=== Aircraft flight-control system ===

Aircraft have means of automatically controlling flight. Autopilot was first invented by Lawrence Sperry during World War I to fly bomber planes steady enough to hit accurate targets from 25,000 feet. When it was first adopted by the U.S. military, a Honeywell engineer sat in the back seat with bolt cutters to disconnect the autopilot in case of emergency. Nowadays most commercial planes are equipped with aircraft flight control systems in order to reduce pilot error and workload at landing or takeoff.The first simple commercial auto-pilots were used to control heading and altitude and had limited authority on things like thrust and flight control surfaces. In helicopters, auto-stabilization was used in a similar way. The first systems were electromechanical. The advent of fly by wire and electro-actuated flight surfaces (rather than the traditional hydraulic) has increased safety. As with displays and instruments, critical devices that were electro-mechanical had a finite life. With safety critical systems, the software is very strictly tested.


=== Fuel Systems ===
Fuel Quantity Indication System (FQIS) monitors the amount of fuel aboard. Using various sensors, such as capacitance tubes, temperature sensors, densitometers & level sensors, the FQIS computer calculates the mass of fuel remaining on board.
Fuel Control and Monitoring System (FCMS) reports fuel remaining on board in a similar manner, but, by controlling pumps & valves, also manages fuel transfers around various tanks.

Refuelling control to upload to a certain total mass of fuel and distribute it automatically.
Transfers during flight to the tanks that feed the engines. E.G. from fuselage to wing tanks
Centre of gravity control transfers from the tail (Trim) tanks forward to the wings as fuel is expended
Maintaining fuel in the wing tips (to help stop the wings bending due to lift in flight) & transferring to the main tanks after landing
Controlling fuel jettison during an emergency to reduce the aircraft weight.


=== Collision-avoidance systems ===

To supplement air traffic control, most large transport aircraft and many smaller ones use a traffic alert and collision avoidance system (TCAS), which can detect the location of nearby aircraft, and provide instructions for avoiding a midair collision. Smaller aircraft may use simpler traffic alerting systems such as TPAS, which are passive (they do not actively interrogate the transponders of other aircraft) and do not provide advisories for conflict resolution.
To help avoid controlled flight into terrain (CFIT), aircraft use systems such as ground-proximity warning systems (GPWS), which use radar altimeters as a key element. One of the major weaknesses of GPWS is the lack of ""look-ahead"" information, because it only provides altitude above terrain ""look-down"". In order to overcome this weakness, modern aircraft use a terrain awareness warning system (TAWS).


=== Flight recorders ===

Commercial aircraft cockpit data recorders, commonly known as ""black boxes"", store flight information and audio from the cockpit. They are often recovered from an aircraft after a crash to determine control settings and other parameters during the incident.


=== Weather systems ===

Weather systems such as weather radar (typically Arinc 708 on commercial aircraft) and lightning detectors are important for aircraft flying at night or in instrument meteorological conditions, where it is not possible for pilots to see the weather ahead.  Heavy precipitation (as sensed by radar) or severe turbulence (as sensed by lightning activity) are both indications of strong convective activity and severe turbulence, and weather systems allow pilots to deviate around these areas.
Lightning detectors like the Stormscope or Strikefinder have become inexpensive enough that they are practical for light aircraft. In addition to radar and lightning detection, observations and extended radar pictures (such as NEXRAD) are now available through satellite data connections, allowing pilots to see weather conditions far beyond the range of their own in-flight systems. Modern displays allow weather information to be integrated with moving maps, terrain, and traffic onto a single screen, greatly simplifying navigation.
Modern weather systems also include wind shear and turbulence detection and terrain and traffic warning systems. In‑plane weather avionics are especially popular in Africa, India, and other countries where air-travel is a growing market, but ground support is not as well developed.


=== Aircraft management systems ===
There has been a progression towards centralized control of the multiple complex systems fitted to aircraft, including engine monitoring and management. Health and usage monitoring systems (HUMS) are integrated with aircraft management computers to give maintainers early warnings of parts that will need replacement.
The integrated modular avionics concept proposes an integrated architecture with application software portable across an assembly of common hardware modules. It has been used in fourth generation jet fighters and the latest generation of airliners.


== Mission or tactical avionics ==
Military aircraft have been designed either to deliver a weapon or to be the eyes and ears of other weapon systems. The vast array of sensors available to the military is used for whatever tactical means required. As with aircraft management, the bigger sensor platforms (like the E‑3D, JSTARS, ASTOR, Nimrod MRA4, Merlin HM Mk 1) have mission-management computers.
Police and EMS aircraft also carry sophisticated tactical sensors.


=== Military communications ===
While aircraft communications provide the backbone for safe flight, the tactical systems are designed to withstand the rigors of the battle field. UHF, VHF Tactical (30–88 MHz) and SatCom systems combined with ECCM methods, and cryptography secure the communications. Data links such as Link 11, 16, 22 and BOWMAN, JTRS and even TETRA provide the means of transmitting data (such as images, targeting information etc.).


=== Radar ===
Airborne radar was one of the first tactical sensors. The benefit of altitude providing range has meant a significant focus on airborne radar technologies. Radars include airborne early warning (AEW), anti-submarine warfare (ASW), and even weather radar (Arinc 708) and ground tracking/proximity radar.
The military uses radar in fast jets to help pilots fly at low levels. While the civil market has had weather radar for a while, there are strict rules about using it to navigate the aircraft.


=== Sonar ===
Dipping sonar fitted to a range of military helicopters allows the helicopter to protect shipping assets from submarines or surface threats. Maritime support aircraft can drop active and passive sonar devices (sonobuoys) and these are also used to determine the location of enemy submarines.


=== Electro-Optics ===
Electro-optic systems include devices such as the head-up display (HUD), forward looking infrared (FLIR), infra-red search and track and other passive infrared devices (Passive infrared sensor). These are all used to provide imagery and information to the flight crew. This imagery is used for everything from search and rescue to navigational aids and target acquisition.


=== ESM/DAS ===
Electronic support measures and defensive aids systems are used extensively to gather information about threats or possible threats. They can be used to launch devices (in some cases automatically) to counter direct threats against the aircraft. They are also used to determine the state of a threat and identify it.


=== Aircraft networks ===
The avionics systems in military, commercial and advanced models of civilian aircraft are interconnected using an avionics databus. Common avionics databus protocols, with their primary application, include:

Aircraft Data Network (ADN): Ethernet derivative for Commercial Aircraft
Avionics Full-Duplex Switched Ethernet (AFDX): Specific implementation of ARINC 664 (ADN) for Commercial Aircraft
ARINC 429: Generic Medium-Speed Data Sharing for Private and Commercial Aircraft
ARINC 664: See ADN above
ARINC 629: Commercial Aircraft (Boeing 777)
ARINC 708: Weather Radar for Commercial Aircraft
ARINC 717: Flight Data Recorder for Commercial Aircraft
ARINC 825: CAN bus for commercial aircraft (for example Boeing 787 and Airbus A350)
Commercial Standard Digital Bus
IEEE 1394b: Military Aircraft
MIL-STD-1553: Military Aircraft
MIL-STD-1760: Military Aircraft
TTP – Time-Triggered Protocol: Boeing 787, Airbus A380, Fly-By-Wire Actuation Platforms from Parker Aerospace
TTEthernet – Time-Triggered Ethernet: Orion spacecraft


== See also ==
ACARS
Acronyms and abbreviations in avionics
ARINC
Avionics software
DO-178C
Emergency locator beacon
Emergency position-indicating radiobeacon station
Flight recorder
Integrated modular avionics


== Notes ==


== Further reading ==
Avionics: Development and Implementation by Cary R. Spitzer (Hardcover – December 15, 2006)
Principles of Avionics, 4th Edition by Albert Helfrick, Len Buckwalter, and Avionics Communications Inc. (Paperback – July 1, 2007)
Avionics Training: Systems, Installation, and Troubleshooting by Len Buckwalter (Paperback – June 30, 2005)
Avionics Made Simple, by Mouhamed Abdulla, Ph.D.; Jaroslav V. Svoboda, Ph.D. and Luis Rodrigues, Ph.D. (Coursepack – Dec. 2005 - ISBN 978-0-88947-908-1).


== External links ==
Avionics in Commercial Aircraft
Aircraft Electronics Association (AEA)
Pilot's Guide to Avionics
The Avionic Systems Standardisation Committee
Space Shuttle Avionics
Aviation Today Avionics magazine
RAES Avionics homepage","pandas(index=129, _1=129, text='avionics are the electronic systems used on aircraft, artificial satellites, and spacecraft. avionic systems include communications, navigation, the display and management of multiple systems, and the hundreds of systems that are fitted to aircraft to perform individual functions. these can be as simple as a searchlight for a police helicopter or as complicated as the tactical system for an airborne early warning platform. the term avionics is a portmanteau of the words aviation and electronics.   == history == the term ""avionics"" was coined in 1949 by  philip j. klass, senior editor at aviation week & space technology magazine as a portmanteau of ""aviation electronics"".radio communication was first used in aircraft just prior to world war i.   the first airborne radios were in zeppelins, but the military sparked development of light radio sets that could be carried by heavier-than-air craft, so that aerial reconnaissance biplanes could report their observations immediately in case they were shot down.  the first experimental radio transmission from an airplane was conducted by the us navy august 1910. the first aircraft radios transmitted by radiotelegraphy, so they required two-seat aircraft with a second crewman to tap on a telegraph key to spell out messages by morse code.   during world war i, am voice two way radio sets were made possible in 1917 by the development of the triode vacuum tube, which were simple enough that the pilot in a single seat aircraft could use it while flying. radar, the central technology used today in aircraft navigation and air traffic control, was developed by several nations, mainly in secret, as an air defense system in the 1930s during the runup to world war ii.   many modern avionics have their origins in world war ii wartime developments. for example, autopilot systems that are commonplace today began as specialized systems to help bomber planes fly steadily enough to hit precision targets from high altitudes.  britain\'s 1940 decision to share its radar technology with its us ally, particularly the magnetron vacuum tube, in the famous tizard mission, significantly shortened the war. modern avionics is a substantial portion of military aircraft spending. aircraft like the f‑15e and the now retired f‑14 have roughly 20 percent of their budget spent on avionics. most modern helicopters now have budget splits of 60/40 in favour of avionics.the civilian market has also seen a growth in cost of avionics. flight control systems (fly-by-wire) and new navigation needs brought on by tighter airspaces, have pushed up development costs. the major change has been the recent boom in consumer flying. as more people begin to use planes as their primary method of transportation, more elaborate methods of controlling aircraft safely in these high restrictive airspaces have been invented. the avionics systems in military, commercial and advanced models of civilian aircraft are interconnected using an avionics databus. common avionics databus protocols, with their primary application, include:  aircraft data network (adn): ethernet derivative for commercial aircraft avionics full-duplex switched ethernet (afdx): specific implementation of arinc 664 (adn) for commercial aircraft arinc 429: generic medium-speed data sharing for private and commercial aircraft arinc 664: see adn above arinc 629: commercial aircraft (boeing 777) arinc 708: weather radar for commercial aircraft arinc 717: flight data recorder for commercial aircraft arinc 825: can bus for commercial aircraft (for example boeing 787 and airbus a350) commercial standard digital bus ieee 1394b: military aircraft mil-std-1553: military aircraft mil-std-1760: military aircraft ttp – time-triggered protocol: boeing 787, airbus a380, fly-by-wire actuation platforms from parker aerospace ttethernet – time-triggered ethernet: orion spacecraft   == see also == acars acronyms and abbreviations in avionics arinc avionics software do-178c emergency locator beacon emergency position-indicating radiobeacon station flight recorder integrated modular avionics   == notes ==   == further reading == avionics: development and implementation by cary r. spitzer (hardcover – december 15, 2006) principles of avionics, 4th edition by albert helfrick, len buckwalter, and avionics communications inc. (paperback – july 1, 2007) avionics training: systems, installation, and troubleshooting by len buckwalter (paperback – june 30, 2005) avionics made simple, by mouhamed abdulla, ph.d.; jaroslav v. svoboda, ph.d. and luis rodrigues, ph.d. (coursepack – dec. 2005 - isbn 978-0-88947-908-1).   == external links == avionics in commercial aircraft aircraft electronics association (aea) pilot\'s guide to avionics the avionic systems standardisation committee space shuttle avionics aviation today avionics magazine raes avionics homepage')"
130,"Electronic paper, also sometimes electronic ink, e-ink or electrophoretic display, are display devices that mimic the appearance of ordinary ink on paper. Unlike conventional flat panel displays that emit light, electronic paper displays reflect light like paper. This may make them more comfortable to read, and provide a wider viewing angle than most light-emitting displays. The contrast ratio in electronic displays available as of 2008 approaches newspaper, and newly (2008) developed displays are slightly better. An ideal e-paper display can be read in direct sunlight without the image appearing to fade.
Many electronic paper technologies hold static text and images indefinitely without electricity. Flexible electronic paper uses plastic substrates and plastic electronics for the display backplane. Applications of electronic visual displays include electronic shelf labels and digital signage, time tables at bus stations, electronic billboards, smartphone displays, and e-readers able to display digital versions of books and magazines.


== Technologies ==


=== Gyricon ===
Electronic paper was first developed in the 1970s by Nick Sheridon at Xerox's Palo Alto Research Center. The first electronic paper, called Gyricon, consisted of polyethylene spheres between 75 and 106 micrometers across. Each sphere is a Janus particle composed of negatively charged black plastic on one side and positively charged white plastic on the other (each bead is thus a dipole). The spheres are embedded in a transparent silicone sheet, with each sphere suspended in a bubble of oil so that it can rotate freely. The polarity of the voltage applied to each pair of electrodes then determines whether the white or black side is face-up, thus giving the pixel a white or black appearance.
At the FPD 2008 exhibition, Japanese company Soken demonstrated a wall with electronic wall-paper using this technology. In 2007, the Estonian company Visitret Displays was developing this kind of display using polyvinylidene fluoride (PVDF) as the material for the spheres, dramatically improving the video speed and decreasing the control voltage needed.


=== Electrophoretic ===

In the simplest implementation of an electrophoretic display, titanium dioxide (titania) particles approximately one micrometer in diameter are dispersed in a hydrocarbon oil. A dark-colored dye is also added to the oil, along with surfactants and charging agents that cause the particles to take on an electric charge. This mixture is placed between two parallel, conductive plates separated by a gap of 10 to 100 micrometres. When a voltage is applied across the two plates, the particles migrate electrophoretically to the plate that bears the opposite charge from that on the particles. When the particles are located at the front (viewing) side of the display, it appears white, because the light is scattered back to the viewer by the high-index titania particles. When the particles are located at the rear side of the display, it appears dark, because the incident light is absorbed by the colored dye. If the rear electrode is divided into a number of small picture elements (pixels), then an image can be formed by applying the appropriate voltage to each region of the display to create a pattern of reflecting and absorbing regions.
An electrophoretic display is also known as an EPD. They are typically addressed using MOSFET-based thin-film transistor (TFT) technology. TFTs are required to form a high-density image in an EPD. A common application for TFT-based EPDs are e-readers. Electrophoretic displays are considered prime examples of the electronic paper category, because of their paper-like appearance and low power consumption. Examples of commercial electrophoretic displays include the high-resolution active matrix displays used in the Amazon Kindle, Barnes & Noble Nook, Sony Reader, Kobo eReader, and iRex iLiad e-readers. These displays are constructed from an electrophoretic imaging film manufactured by E Ink Corporation. A mobile phone that used the technology is the Motorola Fone.
Electrophoretic Display technology has also been developed by SiPix and Bridgestone/Delta. SiPix is now part of E Ink Corporation. The SiPix design uses a flexible 0.15 mm Microcup architecture, instead of E Ink's 0.04 mm diameter microcapsules. Bridgestone Corp.'s Advanced Materials Division cooperated with Delta Optoelectronics Inc. in developing Quick Response Liquid Powder Display technology.Electrophoretic displays can be manufactured using the Electronics on Plastic by Laser Release (EPLaR) process developed by Philips Research to enable existing AM-LCD manufacturing plants to create flexible plastic displays.


==== Microencapsulated electrophoretic display ====

An electrophoretic display forms images by rearranging charged pigment particles with an applied electric field.

In the 1990s another type of electronic ink based on a microencapsulated electrophoretic display was conceived and prototyped by a team of undergraduates at MIT as described in their Nature paper. J.D. Albert, Barrett Comiskey, Joseph Jacobson, Jeremy Rubin and Russ Wilcox co-founded E Ink Corporation in 1997 to commercialize the technology. E ink subsequently formed a partnership with Philips Components two years later to develop and market the technology. In 2005, Philips sold the electronic paper business as well as its related patents to Prime View International. ""It has for many years been an ambition of researchers in display media to create a flexible low-cost system that is the electronic analog of paper. In this context, microparticle-based displays have long intrigued researchers. Switchable contrast in such displays is achieved by the electromigration of highly scattering or absorbing microparticles (in the size range 0.1–5 μm), quite distinct from the molecular-scale properties that govern the behavior of the more familiar liquid-crystal displays. Micro-particle-based displays possess intrinsic bistability, exhibit extremely low power d.c. field addressing and have demonstrated high contrast and reflectivity. These features, combined with a near-lambertian viewing characteristic, result in an 'ink on paper' look. But such displays have to date suffered from short lifetimes and difficulty in manufacture. Here we report the synthesis of an electrophoretic ink based on the microencapsulation of an electrophoretic dispersion. The use of a microencapsulated electrophoretic medium solves the lifetime issues and permits the fabrication of a bistable electronic display solely by means of printing. This system may satisfy the practical requirements of electronic paper.""
This used tiny microcapsules filled with electrically charged white particles suspended in a colored oil. In early versions, the underlying circuitry controlled whether the white particles were at the top of the capsule (so it looked white to the viewer) or at the bottom of the capsule (so the viewer saw the color of the oil). This was essentially a reintroduction of the well-known electrophoretic display technology, but microcapsules meant the display could be made on flexible plastic sheets instead of glass.
One early version of the electronic paper consists of a sheet of very small transparent capsules, each about 40 micrometers across. Each capsule contains an oily solution containing black dye (the electronic ink), with numerous white titanium dioxide particles suspended within. The particles are slightly negatively charged, and each one is naturally white.
The screen holds microcapsules in a layer of liquid polymer, sandwiched between two arrays of electrodes, the upper of which is transparent. The two arrays are aligned to divide the sheet into pixels, and each pixel corresponds to a pair of electrodes situated on either side of the sheet. The sheet is laminated with transparent plastic for protection, resulting in an overall thickness of 80 micrometers, or twice that of ordinary paper.
The network of electrodes connects to display circuitry, which turns the electronic ink 'on' and 'off' at specific pixels by applying a voltage to specific electrode pairs. A negative charge to the surface electrode repels the particles to the bottom of local capsules, forcing the black dye to the surface and turning the pixel black. Reversing the voltage has the opposite effect. It forces the particles to the surface, turning the pixel white. A more recent implementation of this concept requires only one layer of electrodes beneath the microcapsules. These are commercially referred to as Active Matrix Electrophoretic Displays (AMEPD).


=== Electrowetting ===

Electrowetting display (EWD) is based on controlling the shape of a confined water/oil interface by an applied voltage. With no voltage applied, the (colored) oil forms a flat film between the water and a hydrophobic (water-repellent) insulating coating of an electrode, resulting in a colored pixel. When a voltage is applied between the electrode and the water, the interfacial tension between the water and the coating changes. As a result, the stacked state is no longer stable, causing the water to move the oil aside. This makes a partly transparent pixel, or, if a reflective white surface is under the switchable element, a white pixel. Because of the small pixel size, the user only experiences the average reflection, which provides a high-brightness, high-contrast switchable element.
Displays based on electrowetting provide several attractive features. The switching between white and colored reflection is fast enough to display video content. It's a low-power, low-voltage technology, and displays based on the effect can be made flat and thin. The reflectivity and contrast are better than or equal to other reflective display types and approach the visual qualities of paper. In addition, the technology offers a unique path toward high-brightness full-color displays, leading to displays that are four times brighter than reflective LCDs and twice as bright as other emerging technologies. Instead of using red, green, and blue (RGB) filters or alternating segments of the three primary colors, which effectively result in only one-third of the display reflecting light in the desired color, electrowetting allows for a system in which one sub-pixel can switch two different colors independently.
This results in the availability of two-thirds of the display area to reflect light in any desired color. This is achieved by building up a pixel with a stack of two independently controllable colored oil films plus a color filter.
The colors are cyan, magenta, and yellow, which is a subtractive system, comparable to the principle used in inkjet printing. Compared to LCD, brightness is gained because no polarisers are required.


==== Electrofluidic ====
Electrofluidic display is a variation of an electrowetting display. Electrofluidic displays place an aqueous pigment dispersion inside a tiny reservoir. The reservoir comprises <5-10% of the viewable pixel area and therefore the pigment is substantially hidden from view. Voltage is used to electromechanically pull the pigment out of the reservoir and spread it as a film directly behind the viewing substrate. As a result, the display takes on color and brightness similar to that of conventional pigments printed on paper. When voltage is removed liquid surface tension causes the pigment dispersion to rapidly recoil into the reservoir. The technology can potentially provide >85 % white state reflectance for electronic paper.The core technology was invented at the Novel Devices Laboratory at the University of Cincinnati. The technology is currently being commercialized by Gamma Dynamics.


=== Interferometric modulator (Mirasol) ===

The technology used in electronic visual displays that can create various colors via interference of reflected light. The color is selected with an electrically switched light modulator comprising a microscopic cavity that is switched on and off using driver integrated circuits similar to those used to address liquid crystal displays (LCD).


=== Plasmonic electronic display ===
Plasmonic nanostructures with conductive polymers have also been suggested as one kind of electronic paper. The material has two parts. The first part is a highly reflective metasurface made by metal-insulator-metal films tens of nanometers in thickness including nanoscale holes. The metasurfaces can reflect different colors depending on the thickness of the insulator. The standard RGB color schema can be used as pixels for full-color displays. The second part is a polymer with optical absorption controllable by an electrochemical potential. After growing the polymer on the plasmonic metasurfaces, the reflection of the metasurfaces can be modulated by the applied voltage. This technology presents broad range colors, high polarization-independent reflection (>50 %), strong contrast (>30 %), the fast response time (hundreds of ms), and long-term stability. In addition, it has ultralow power consumption (< 0.5 mW/cm2) and potential for high resolution (>10000 dpi). Since the ultrathin metasurfaces are flexible and the polymer is soft, the whole system can be bent. Desired future improvements for this technology include bistability, cheaper materials and implementation with TFT arrays.


=== Other technologies ===
Other research efforts into e-paper have involved using organic transistors embedded into flexible substrates, including attempts to build them into conventional paper.
Simple color e-paper consists of a thin colored optical filter added to the monochrome technology described above. The array of pixels is divided into triads, typically consisting of the standard cyan, magenta and yellow, in the same way as CRT monitors (although using subtractive primary colors as opposed to additive primary colors). The display is then controlled like any other electronic color display.


== History ==
E Ink Corporation of E Ink Holdings Inc. released the first colored E Ink displays to be used in a marketed product. The Ectaco Jetbook Color was released in 2012 as the first colored electronic ink device, which used E Ink's Triton display technology. E Ink in early 2015 also announced another color electronic ink technology called Prism. This new technology is a color changing film that can be used for e-reader, but Prism is also marketed as a film that can be integrated into architectural design such as ""wall, ceiling panel, or entire room instantly."" The disadvantage of these current color displays is that they are considerably more expensive than standard E Ink displays. The JetBook Color costs roughly nine times more than other popular e-readers such as the Amazon Kindle. As of January 2015, Prism had not been announced to be used in the plans for any e-reader devices.


== Applications ==

Several companies are simultaneously developing electronic paper and ink. While the technologies used by each company provide many of the same features, each has its own distinct technological advantages. All electronic paper technologies face the following general challenges:

A method for encapsulation
An ink or active material to fill the encapsulation
Electronics to activate the inkElectronic ink can be applied to flexible or rigid materials. For flexible displays, the base requires a thin, flexible material tough enough to withstand considerable wear, such as extremely thin plastic. The method of how the inks are encapsulated and then applied to the substrate is what distinguishes each company from others. These processes are complex and are carefully guarded industry secrets. Nevertheless, making electronic paper is less complex and costly than LCDs.
There are many approaches to electronic paper, with many companies developing technology in this area. Other technologies being applied to electronic paper include modifications of liquid crystal displays, electrochromic displays, and the electronic equivalent of an Etch A Sketch at Kyushu University. Advantages of electronic paper include low power usage (power is only drawn when the display is updated), flexibility and better readability than most displays. Electronic ink can be printed on any surface, including walls, billboards, product labels and T-shirts. The ink's flexibility would also make it possible to develop rollable displays for electronic devices.


=== Wristwatches ===
In December 2005, Seiko released the first electronic ink based watch called the Spectrum SVRD001 wristwatch, which has a flexible electrophoretic display and in March 2010 Seiko released a second generation of this famous electronic ink watch with an active matrix display. The Pebble smart watch (2013) uses a low-power memory LCD manufactured by Sharp for its e-paper display.In 2019, Fossil launched a hybrid smartwatch called the Hybrid HR, integrating an always on electronic ink display with physical hands and dial to simulate the look of a traditional analog watch.


=== E-book readers ===

In 2004 Sony released the Librié in Japan, the first e-book reader with an electronic paper E Ink display. In September 2006, Sony released the PRS-500 Sony Reader e-book reader in the USA. On October 2, 2007, Sony announced the PRS-505, an updated version of the Reader. In November 2008, Sony released the PRS-700BC, which incorporated a backlight and a touchscreen.
In late 2007, Amazon began producing and marketing the Amazon Kindle, an e-book reader with an e-paper display. In February 2009, Amazon released the Kindle 2 and in May 2009 the larger Kindle DX was announced. In July 2010 the third-generation Kindle was announced, with notable design changes. The fourth generation of Kindle, called Touch, was announced in September 2011 that was the Kindle's first departure from keyboards and page turn buttons in favor of touchscreens. In September 2012, Amazon announced the fifth generation of the Kindle called the Paperwhite, which incorporates a LED frontlight and a higher contrast display.In November 2009, Barnes and Noble launched the Barnes & Noble Nook, running an Android operating system. It differs from other e-readers in having a replaceable battery, and a separate touch-screen color LCD below the main electronic paper reading screen.
In 2017, Sony and reMarkable offered e-books tailored for writing with a smart stylus.In 2020, Onyx released the first frontlit 13.3 inch electronic paper Android tablet, the Boox Max Lumi. At the end of the same year, Bigme released the first 10.3 inch color electronic paper Android tablet, the Bigme B1 Pro. This was also the first large electronic paper tablet to support 4g cellular data.


=== Newspapers ===
In February 2006, the Flemish daily De Tijd distributed an electronic version of the paper to select subscribers in a limited marketing study, using a pre-release version of the iRex iLiad. This was the first recorded application of electronic ink to newspaper publishing.
The French daily Les Échos announced the official launch of an electronic version of the paper on a subscription basis, in September 2007. Two offers were available, combining a one-year subscription and a reading device. The offer included either a light (176g) reading device (adapted for Les Echos by Ganaxa) or the iRex iLiad. Two different processing platforms were used to deliver readable information of the daily, one based on the newly developed GPP electronic ink platform from Ganaxa, and the other one developed internally by Les Echos.


=== Displays embedded in smart cards ===
Flexible display cards enable financial payment cardholders to generate a one-time password to reduce online banking and transaction fraud. Electronic paper offers a flat and thin alternative to existing key fob tokens for data security. The world's first ISO compliant smart card with an embedded display was developed by Innovative Card Technologies and nCryptone in 2005. The cards were manufactured by Nagra ID.


=== Status displays ===

Some devices, like USB flash drives, have used electronic paper to display status information, such as available storage space. Once the image on the electronic paper has been set, it requires no power to maintain, so the readout can be seen even when the flash drive is not plugged in.


=== Mobile phones ===
Motorola's low-cost mobile phone, the Motorola F3, uses an alphanumeric black-and-white electrophoretic display.
The Samsung Alias 2 mobile phone incorporates electronic ink from E Ink into the keypad, which allows the keypad to change character sets and orientation while in different display modes.
On December 12, 2012, Yota Devices announced the first ""YotaPhone"" prototype and was later released in December 2013, a unique double-display smartphone. It has a 4.3-inch, HD LCD on the front and an electronic ink display on the back.
On May and June 2020, Hisense released the hisense A5c and A5 pro cc, the first color electronic ink smartphones. With a single color display, with toggable front light running android 9 and Android 10.


=== Electronic shelf labels ===

E-paper based electronic shelf labels (ESL) are used to digitally display the prices of goods at retail stores. Electronic-paper-based labels are updated via two-way infrared or radio technology.


=== Public transport timetables ===

E-paper displays at bus or trams stops can be remotely updated. Compared to LED or liquid crystal displays (LCDs), they consume lower energy and the text or graphics stays visible during a power failure. Compared to LCDs, it is well visible also under full sunshine.


=== Digital signage ===

Because of its energy-saving properties, electronic paper has proved a technology suited to digital signage applications.


=== Computer monitor ===
Electronic paper is used on computer monitors like the 13.3 inch Dasung Paperlike 3 HD and 25.3 inch Paperlike 253.


=== Laptop ===
Some laptops like Lenovo ThinkBook Plus use electronic paper as a secondary screen.


=== Other ===
Other proposed applications include clothes, digital photo frames, information boards, and keyboards. Keyboards with dynamically changeable keys are useful for less represented languages, non-standard keyboard layouts such as Dvorak, or for special non-alphabetical applications such as video editing or games.
The reMarkable is a writer tablet for reading and taking notes.


== See also ==
E-book
Electrofluidic
Flexible electronics
History of display technology


== References ==


== Further reading ==
Electric paper, New Scientist, 2003
E-paper may offer video images, New Scientist, 2003
Paper comes alive New Scientist, 2003
Most flexible electronic paper yet revealed, New Scientist, 2004
Roll-up digital displays move closer to market New Scientist, 2005


== External links ==
Wired article on E Ink-Philips partnership, and background
Bosner, Kevin. How Electronic Ink Will Work at HowStuffWorks, retrieved 2007-08-26
MIT ePaper Project
Tanaka, Naoki (2007-12-06). ""Fuji Xerox Exhibits Color Electronic Paper w/ Optical Writing System"". Japan: Tech-On. Retrieved 2007-12-10.
Fujitsu Develops World's First Film Substrate-based Bendable Color Electronic Paper featuring Image Memory Function","pandas(index=130, _1=130, text='electronic paper, also sometimes electronic ink, e-ink or electrophoretic display, are display devices that mimic the appearance of ordinary ink on paper. unlike conventional flat panel displays that emit light, electronic paper displays reflect light like paper. this may make them more comfortable to read, and provide a wider viewing angle than most light-emitting displays. the contrast ratio in electronic displays available as of 2008 approaches newspaper, and newly (2008) developed displays are slightly better. an ideal e-paper display can be read in direct sunlight without the image appearing to fade. many electronic paper technologies hold static text and images indefinitely without electricity. flexible electronic paper uses plastic substrates and plastic electronics for the display backplane. applications of electronic visual displays include electronic shelf labels and digital signage, time tables at bus stations, electronic billboards, smartphone displays, and e-readers able to display digital versions of books and magazines.   == technologies == other proposed applications include clothes, digital photo frames, information boards, and keyboards. keyboards with dynamically changeable keys are useful for less represented languages, non-standard keyboard layouts such as dvorak, or for special non-alphabetical applications such as video editing or games. the remarkable is a writer tablet for reading and taking notes.   == see also == e-book electrofluidic flexible electronics history of display technology   == references ==   == further reading == electric paper, new scientist, 2003 e-paper may offer video images, new scientist, 2003 paper comes alive new scientist, 2003 most flexible electronic paper yet revealed, new scientist, 2004 roll-up digital displays move closer to market new scientist, 2005   == external links == wired article on e ink-philips partnership, and background bosner, kevin. how electronic ink will work at howstuffworks, retrieved 2007-08-26 mit epaper project tanaka, naoki (2007-12-06). ""fuji xerox exhibits color electronic paper w/ optical writing system"". japan: tech-on. retrieved 2007-12-10. fujitsu develops world\'s first film substrate-based bendable color electronic paper featuring image memory function')"
131,"Electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. It emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use.
Electrical engineering is now divided into a wide range of fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, and electronics. Many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, and electrical materials science.Electrical engineers typically hold a degree in electrical engineering or electronic engineering.  Practising engineers may have professional certification and be members of a professional body or an international standards organization. These include the International Electrotechnical Commission (IEC), the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET) (formerly the IEE).
Electrical engineers work in a very wide range of industries and the skills required are likewise variable. These range from circuit theory to the management skills of a project manager.  The tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.


== History ==

Electricity has been a subject of scientific interest since at least the early 17th century. William Gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. He is credited with establishing the term ""electricity"". He also designed the versorium: a device that detects the presence of statically charged objects. In 1762 Swedish professor Johan Wilcke invented a device later named electrophorus that produced a static electric charge. By 1800 Alessandro Volta had developed the voltaic pile, a forerunner of the electric battery.


=== 19th century ===

In the 19th century, research into the subject started to intensify. Notable developments in this century include the work of Hans Christian Ørsted who discovered in 1820 that an electric current produces a magnetic field that will deflect a compass needle, of William Sturgeon who, in 1825 invented the electromagnet, of Joseph Henry and Edward Davy who invented the electrical relay in 1835, of Georg Ohm, who in 1827 quantified the relationship between the electric current and potential difference in a conductor, of Michael Faraday (the discoverer of electromagnetic induction in 1831), and of James Clerk Maxwell, who in 1873 published a unified theory of electricity and magnetism in his treatise Electricity and Magnetism.In 1782, Georges-Louis Le Sage developed and presented in Berlin probably the world's first form of electric telegraphy, using 24 different wires, one for each letter of the alphabet. This telegraph connected two rooms. It was an electrostatic telegraph that moved gold leaf through electrical conduction.
In 1795, Francisco Salva Campillo proposed an electrostatic telegraph system. Between 1803–1804, he worked on electrical telegraphy and in 1804, he presented his report at the Royal Academy of Natural Sciences and Arts of Barcelona. Salva's electrolyte telegraph system was very innovative though it was greatly influenced by and based upon two new discoveries made in Europe in 1800 – Alessandro Volta's electric battery for generating an electric current and William Nicholson and Anthony Carlyle's electrolysis of water. Electrical telegraphy may be considered the first example of electrical engineering. Electrical engineering became a profession in the later 19th century.  Practitioners had created a global electric telegraph network, and the first professional electrical engineering institutions were founded in the UK and USA to support the new discipline. Francis Ronalds created an electric telegraph system in 1816 and documented his vision of how the world could be transformed by electricity. Over 50 years later, he joined the new Society of Telegraph Engineers (soon to be renamed the Institution of Electrical Engineers) where he was regarded by other members as the first of their cohort. By the end of the 19th century, the world had been forever changed by the rapid communication made possible by the engineering development of land-lines, submarine cables, and, from about 1890, wireless telegraphy.
Practical applications and advances in such fields created an increasing need for standardised units of measure. They led to the international standardization of the units volt, ampere, coulomb, ohm, farad, and henry. This was achieved at an international conference in Chicago in 1893. The publication of these standards formed the basis of future advances in standardisation in various industries, and in many countries, the definitions were immediately recognized in relevant legislation.During these years, the study of electricity was largely considered to be a subfield of physics since the early electrical technology was considered electromechanical in nature. The Technische Universität Darmstadt founded the world's first department of electrical engineering in 1882 and introduced the first degree course in electrical engineering in 1883. The first electrical engineering degree program in the United States was started at Massachusetts Institute of Technology (MIT) in the physics department under Professor Charles Cross,  though it was Cornell University to produce the world's first electrical engineering graduates in 1885. The first course in electrical engineering was taught in 1883 in Cornell's Sibley College of Mechanical Engineering and Mechanic Arts. It was not until about 1885 that Cornell President Andrew Dickson White established the first Department of Electrical Engineering in the United States. In the same year, University College London founded the first chair of electrical engineering in Great Britain. Professor Mendell P. Weinbach at University of Missouri soon followed suit by establishing the electrical engineering department in 1886. Afterwards, universities and institutes of technology gradually started to offer electrical engineering programs to their students all over the world.
During these decades use of electrical engineering increased dramatically. In 1882, Thomas Edison switched on the world's first large-scale electric power network that provided 110 volts — direct current (DC) — to 59 customers on Manhattan Island in New York City. In 1884, Sir Charles Parsons invented the steam turbine allowing for more efficient electric power generation. Alternating current, with its ability to transmit power more efficiently over long distances via the use of transformers, developed rapidly in the 1880s and 1890s with transformer designs by Károly Zipernowsky, Ottó Bláthy and Miksa Déri (later called ZBD transformers), Lucien Gaulard, John Dixon Gibbs and William Stanley, Jr.. Practical AC motor designs including induction motors were independently invented by Galileo Ferraris and Nikola Tesla and further developed into a practical three-phase form by Mikhail Dolivo-Dobrovolsky and Charles Eugene Lancelot Brown. Charles Steinmetz and Oliver Heaviside contributed to the theoretical basis of alternating current engineering. The spread in the use of AC set off in the United States what has been called the war of the currents between a George Westinghouse backed AC system and a Thomas Edison backed DC power system, with AC being adopted as the overall standard.


=== Early 20th century ===

During the development of radio, many scientists and inventors contributed to radio technology and electronics. The mathematical work of James Clerk Maxwell during the 1850s had shown the relationship of different forms of electromagnetic radiation including the possibility of invisible airborne waves (later called ""radio waves""). In his classic physics experiments of 1888, Heinrich Hertz proved Maxwell's theory by transmitting radio waves with a spark-gap transmitter, and detected them by using simple electrical devices. Other physicists experimented with these new waves and in the process developed devices for transmitting and detecting them. In 1895, Guglielmo Marconi began work on a way to adapt the known methods of transmitting and detecting these ""Hertzian waves"" into a purpose built commercial wireless telegraphic system. Early on, he sent wireless signals over a distance of one and a half miles. In December 1901, he sent wireless waves that were not affected by the curvature of the Earth. Marconi later transmitted the wireless signals across the Atlantic between Poldhu, Cornwall, and St. John's, Newfoundland, a distance of 2,100 miles (3,400 km).Millimetre wave communication was first investigated by Jagadish Chandra Bose during 1894–1896, when he reached an extremely high frequency of up to 60 GHz in his experiments. He also introduced the use of semiconductor junctions to detect radio waves, when he patented the radio crystal detector in 1901.In 1897, Karl Ferdinand Braun introduced the cathode ray tube as part of an oscilloscope, a crucial enabling technology for electronic television. John Fleming invented the first radio tube, the diode, in 1904. Two years later, Robert von Lieben and Lee De Forest independently developed the amplifier tube, called the triode.In 1920, Albert Hull developed the magnetron which would eventually lead to the development of the microwave oven in 1946 by Percy Spencer. In 1934, the British military began to make strides toward radar (which also uses the magnetron) under the direction of Dr Wimperis, culminating in the operation of the first radar station at Bawdsey in August 1936.In 1941, Konrad Zuse presented the Z3, the world's first fully functional and programmable computer using electromechanical parts.  In 1943, Tommy Flowers designed and built the Colossus, the world's first fully functional, electronic, digital and programmable computer. In 1946, the ENIAC (Electronic Numerical Integrator and Computer) of John Presper Eckert and John Mauchly followed, beginning the computing era. The arithmetic performance of these machines allowed engineers to develop completely new technologies and achieve new objectives.In 1948 Claude Shannon publishes ""A Mathematical Theory of Communication"" which mathematically describes the passage of information with uncertainty (electrical noise).


=== Solid-state electronics ===

The first working transistor was a point-contact transistor invented by John Bardeen and Walter Houser Brattain while working under William Shockley at the Bell Telephone Laboratories (BTL) in 1947. They then invented the bipolar junction transistor in 1948. While early junction transistors were relatively bulky devices that were difficult to manufacture on a mass-production basis, they opened the door for more compact devices.The surface passivation process, which electrically stabilized silicon surfaces via thermal oxidation, was developed by Mohamed M. Atalla at BTL in 1957. This led to the development of the monolithic integrated circuit chip. The first integrated circuits were the hybrid integrated circuit invented by Jack Kilby at Texas Instruments in 1958 and the monolithic integrated circuit chip invented by Robert Noyce at Fairchild Semiconductor in 1959.The MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) was invented by Mohamed Atalla and Dawon Kahng at BTL in 1959. It was the first truly compact transistor that could be miniaturised and mass-produced for a wide range of uses. It revolutionized the electronics industry, becoming the most widely used electronic device in the world. The MOSFET is the basic element in most modern electronic equipment, and has been central to the electronics revolution, the microelectronics revolution, and the Digital Revolution. The MOSFET has thus been credited as the birth of modern electronics, and possibly the most important invention in electronics.The MOSFET made it possible to build high-density integrated circuit chips. Atalla first proposed the concept of the MOS integrated circuit (MOS IC) chip in 1960, followed by Kahng in 1961. The earliest experimental MOS IC chip to be fabricated was built by Fred Heiman and Steven Hofstein at RCA Laboratories in 1962. MOS technology enabled Moore's law, the doubling of transistors on an IC chip every two years, predicted by Gordon Moore in 1965. Silicon-gate MOS technology was developed by Federico Faggin at Fairchild in 1968. Since then, the MOSFET has been the basic building block of modern electronics. The mass-production of silicon MOSFETs and MOS integrated circuit chips, along with continuous MOSFET scaling miniaturization at an exponential pace (as predicted by Moore's law), has since led to revolutionary changes in technology, economy, culture and thinking.The Apollo program which culminated in landing astronauts on the Moon with Apollo 11 in 1969 was enabled by NASA's adoption of advances in semiconductor electronic technology, including MOSFETs in the Interplanetary Monitoring Platform (IMP) and silicon integrated circuit chips in the Apollo Guidance Computer (AGC).The development of MOS integrated circuit technology in the 1960s led to the invention of the microprocessor in the early 1970s. The first single-chip microprocessor was the Intel 4004, released in 1971. It began with the ""Busicom Project"" as Masatoshi Shima's three-chip CPU design in 1968, before Sharp's Tadashi Sasaki conceived of a single-chip CPU design, which he discussed with Busicom and Intel in 1968. The Intel 4004 was then designed and realized by Federico Faggin at Intel with his silicon-gate MOS technology, along with Intel's Marcian Hoff and Stanley Mazor and Busicom's Masatoshi Shima. The microprocessor led to the development of microcomputers and personal computers, and the microcomputer revolution.


== Subfields ==
Electrical engineering has many subdisciplines, the most common of which are listed below. Although there are electrical engineers who focus exclusively on one of these subdisciplines, many deal with a combination of them. Sometimes certain fields, such as electronic engineering and computer engineering, are considered separate disciplines in their own right.


=== Power ===

Power engineering deals with the generation, transmission, and distribution of electricity as well as the design of a range of related devices. These include transformers, electric generators, electric motors, high voltage engineering, and power electronics. In many regions of the world, governments maintain an electrical network called a power grid that connects a variety of generators together with users of their energy. Users purchase electrical energy from the grid, avoiding the costly exercise of having to generate their own. Power engineers may work on the design and maintenance of the power grid as well as the power systems that connect to it. Such systems are called on-grid power systems and may supply the grid with additional power, draw power from the grid, or do both. Power engineers may also work on systems that do not connect to the grid, called off-grid power systems, which in some cases are preferable to on-grid systems. The future includes Satellite controlled power systems, with feedback in real time to prevent power surges and prevent blackouts.


=== Control ===

Control engineering focuses on the modeling of a diverse range of dynamic systems and the design of controllers that will cause these systems to behave in the desired manner. To implement such controllers, electrical engineers may use electronic circuits, digital signal processors, microcontrollers, and programmable logic controllers (PLCs). Control engineering has a wide range of applications from the flight and propulsion systems of commercial airliners to the cruise control present in many modern automobiles. It also plays an important role in industrial automation.
Control engineers often utilize feedback when designing control systems. For example, in an automobile with cruise control the vehicle's speed is continuously monitored and fed back to the system which adjusts the motor's power output accordingly. Where there is regular feedback, control theory can be used to determine how the system responds to such feedback.
Electrical engineers also work in robotics to design autonomous systems using control algorithms which interpret sensory feedback to control actuators that move robots such as autonomous vehicles, autonomous drones and others used in a variety of industries.


=== Electronics ===

Electronic engineering involves the design and testing of electronic circuits that use the properties of components such as resistors, capacitors, inductors, diodes, and transistors to achieve a particular functionality. The tuned circuit, which allows the user of a radio to filter out all but a single station, is just one example of such a circuit. Another example to research is a pneumatic signal conditioner.
Prior to the Second World War, the subject was commonly known as radio engineering and basically was restricted to aspects of communications and radar, commercial radio, and early television. Later, in post-war years, as consumer devices began to be developed, the field grew to include modern television, audio systems, computers, and microprocessors. In the mid-to-late 1950s, the term radio engineering gradually gave way to the name electronic engineering.
Before the invention of the integrated circuit in 1959, electronic circuits were constructed from discrete components that could be manipulated by humans. These discrete circuits consumed much space and power and were limited in speed, although they are still common in some applications. By contrast, integrated circuits packed a large number—often millions—of tiny electrical components, mainly transistors, into a small chip around the size of a coin. This allowed for the powerful computers and other electronic devices we see today.


=== Microelectronics and nanoelectronics ===

Microelectronics engineering deals with the design and microfabrication of very small electronic circuit components for use in an integrated circuit or sometimes for use on their own as a general electronic component. The most common microelectronic components are semiconductor transistors, although all main electronic components (resistors, capacitors etc.) can be created at a microscopic level.
Nanoelectronics is the further scaling of devices down to nanometer levels. Modern devices are already in the nanometer regime, with below 100 nm processing having been standard since around 2002.Microelectronic components are created by chemically fabricating wafers of semiconductors such as silicon (at higher frequencies, compound semiconductors like gallium arsenide and indium phosphide) to obtain the desired transport of electronic charge and control of current. The field of microelectronics involves a significant amount of chemistry and material science and requires the electronic engineer working in the field to have a very good working knowledge of the effects of quantum mechanics.


=== Signal processing ===

Signal processing deals with the analysis and manipulation of signals. Signals can be either analog, in which case the signal varies continuously according to the information, or digital, in which case the signal varies according to a series of discrete values representing the information. For analog signals, signal processing may involve the amplification and filtering of audio signals for audio equipment or the modulation and demodulation of signals for telecommunications. For digital signals, signal processing may involve the compression, error detection and error correction of digitally sampled signals.Signal Processing is a very mathematically oriented and intensive area forming the core of digital signal processing and it is rapidly expanding with new applications in every field of electrical engineering such as communications, control, radar, audio engineering, broadcast engineering, power electronics, and biomedical engineering as many already existing analog systems are replaced with their digital counterparts. Analog signal processing is still important in the design of many control systems.
DSP processor ICs are found in many types of modern electronic devices, such as digital television sets, radios, Hi-Fi audio equipment, mobile phones, multimedia players, camcorders and digital cameras, automobile control systems, noise cancelling headphones, digital spectrum analyzers, missile guidance systems, radar systems, and telematics systems. In such products, DSP may be responsible for noise reduction, speech recognition or synthesis, encoding or decoding digital media, wirelessly transmitting or receiving data, triangulating position using GPS, and other kinds of image processing, video processing, audio processing, and speech processing.


=== Telecommunications ===

Telecommunications engineering focuses on the transmission of information across a communication channel such as a coax cable, optical fiber or free space. Transmissions across free space require information to be encoded in a carrier signal to shift the information to a carrier frequency suitable for transmission; this is known as modulation. Popular analog modulation techniques include amplitude modulation and frequency modulation. The choice of modulation affects the cost and performance of a system and these two factors must be balanced carefully by the engineer.
Once the transmission characteristics of a system are determined, telecommunication engineers design the transmitters and receivers needed for such systems. These two are sometimes combined to form a two-way communication device known as a transceiver. A key consideration in the design of transmitters is their power consumption as this is closely related to their signal strength. Typically, if the power of the transmitted signal is insufficient once the signal arrives at the receiver's antenna(s), the information contained in the signal will be corrupted by noise.


=== Instrumentation ===

Instrumentation engineering deals with the design of devices to measure physical quantities such as pressure, flow, and temperature. The design of such instruments requires a good understanding of physics that often extends beyond electromagnetic theory. For example, flight instruments measure variables such as wind speed and altitude to enable pilots the control of aircraft analytically. Similarly, thermocouples use the Peltier-Seebeck effect to measure the temperature difference between two points.Often instrumentation is not used by itself, but instead as the sensors of larger electrical systems. For example, a thermocouple might be used to help ensure a furnace's temperature remains constant. For this reason, instrumentation engineering is often viewed as the counterpart of control.


=== Computers ===

Computer engineering deals with the design of computers and computer systems. This may involve the design of new hardware, the design of PDAs, tablets, and supercomputers, or the use of computers to control an industrial plant. Computer engineers may also work on a system's software. However, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. Desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and DVD players.


== Related disciplines ==

Mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. Such combined systems are known as electromechanical systems and have widespread adoption. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles.
Electronic systems design is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.The term mechatronics is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. Already, such small devices, known as Microelectromechanical systems (MEMS), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. In the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.Biomedical engineering is another related discipline, concerned with the design of medical equipment. This includes fixed equipment such as ventilators, MRI scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts.
Aerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.


== Education ==

Electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. The same fundamental principles are taught in all programs, though emphasis may vary according to title. The length of study for such a degree is usually four or five years and the completed degree may be designated as a Bachelor of Science in Electrical/Electronics Engineering Technology, Bachelor of Engineering, Bachelor of Science, Bachelor of Technology, or Bachelor of Applied Science, depending on the university. The bachelor's degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. Initially such topics cover most, if not all, of the subdisciplines of electrical engineering. At some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.

At many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a Bachelor of Engineering (Electrical and Electronic), but in others, electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.Some electrical engineers choose to study for a postgraduate degree such as a Master of Engineering/Master of Science (MEng/MSc), a Master of Engineering Management, a Doctor of Philosophy (PhD) in Engineering, an Engineering Doctorate (Eng.D.), or an Engineer's degree. The master's and engineer's degrees may consist of either research, coursework or a mixture of the two. The Doctor of Philosophy and Engineering Doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. In the United Kingdom and some other European countries, Master of Engineering is often considered to be an undergraduate degree of slightly longer duration than the Bachelor of Engineering rather than a standalone postgraduate degree.


== Professional practice ==

In most countries, a bachelor's degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. After completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. Once certified the engineer is designated the title of Professional Engineer (in the United States, Canada and South Africa), Chartered Engineer or Incorporated Engineer (in India, Pakistan, the United Kingdom, Ireland and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (in much of the European Union).

The advantages of licensure vary depending upon location. For example, in the United States and Canada ""only a licensed engineer may seal engineering work for public and private clients"". This requirement is enforced by state and provincial legislation such as Quebec's Engineers Act. In other countries, no such legislation exists. Practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. In this way these organizations play an important role in maintaining ethical standards for the profession. Even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. In cases where an engineer's work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. An engineer's work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law.
Professional bodies of note for electrical engineers include the Institute of Electrical and Electronics Engineers (IEEE) and the Institution of Engineering and Technology (IET).  The IEEE claims to produce 30% of the world's literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. The IET publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in Europe. Obsolescence of technical skills is a serious concern for electrical engineers. Membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. An MIET(Member of the Institution of Engineering and Technology) is recognised in Europe as an Electrical and computer (technology) engineer.In Australia, Canada, and the United States electrical engineers make up around 0.25% of the labor force.


== Tools and work ==
From the Global Positioning System to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. They design, develop, test, and supervise the deployment of electrical systems and electronic devices. For example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.

Fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. Today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. Nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.

Although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. For example, quantum mechanics and solid state physics might be relevant to an engineer working on VLSI (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. Even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. Perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.

A wide range of instrumentation is used by electrical engineers.  For simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice.  Where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument.  In RF engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used.  In some disciplines, safety can be a particular concern with instrumentation.  For instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. Power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. Many disciplines of electrical engineering use tests specific to their discipline.  Audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise.  Likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.

For many engineers, technical work accounts for only a fraction of the work they do. A lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. Many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. Most engineering projects involve some form of documentation and strong written communication skills are therefore very important.
The workplaces of engineers are just as varied as the types of work they do. Electrical engineers may be found in the pristine lab environment of a fabrication plant, on board a Naval ship, the offices of a consulting firm or on site at a mine. During their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.Electrical engineering has an intimate relationship with the physical sciences.  For instance, the physicist Lord Kelvin played a major role in the engineering of the first transatlantic telegraph cable. Conversely, the engineer Oliver Heaviside produced major work on the mathematics of transmission on telegraph cables. Electrical engineers are often required on major science projects.  For instance, large particle accelerators such as CERN need electrical engineers to deal with many aspects of the project including the power distribution, the instrumentation, and the manufacture and installation of the superconducting electromagnets.


== See also ==


== Notes ==


== References ==

BibliographyAbramson, Albert (1955). Electronic Motion Pictures: A History of the Television Camera. University of California Press.
Bayoumi, Magdy A.; Swartzlander, Jr., Earl E. (31 October 1994). VLSI Signal Processing Technology. Springer. ISBN 978-0-7923-9490-7.
Bhushan, Bharat (1997). Micro/Nanotribology and Its Applications. Springer. ISBN 978-0-7923-4386-8.
Bissell, Chris (25 July 1996). Control Engineering, 2nd Edition. CRC Press. ISBN 978-0-412-57710-9.
Chandrasekhar, Thomas (1 December 2006). Analog Communication (Jntu). Tata McGraw-Hill Education. ISBN 978-0-07-064770-1.
Chaturvedi, Pradeep (1997). Sustainable Energy Supply in Asia: Proceedings of the International Conference, Asia Energy Vision 2020, Organised by the Indian Member Committee, World Energy Council Under the Institution of Engineers (India), During November 15–17, 1996 at New Delhi. Concept Publishing Company. ISBN 978-81-7022-631-4.
Dodds, Christopher; Kumar, Chandra; Veering, Bernadette (March 2014). Oxford Textbook of Anaesthesia for the Elderly Patient. Oxford University Press. ISBN 978-0-19-960499-9.
Fairman, Frederick Walker (11 June 1998). Linear Control Theory: The State Space Approach. John Wiley & Sons. ISBN 978-0-471-97489-5.
Fredlund, D. G.; Rahardjo, H.; Fredlund, M. D. (30 July 2012). Unsaturated Soil Mechanics in Engineering Practice. Wiley. ISBN 978-1-118-28050-8.
Grant, Malcolm Alister; Bixley, Paul F (1 April 2011). Geothermal Reservoir Engineering. Academic Press. ISBN 978-0-12-383881-0.
Grigsby, Leonard L. (16 May 2012). Electric Power Generation, Transmission, and Distribution, Third Edition. CRC Press. ISBN 978-1-4398-5628-4.
Heertje, Arnold; Perlman, Mark (1990). Evolving technology and market structure: studies in Schumpeterian economics. University of Michigan Press. ISBN 978-0-472-10192-4.
Huurdeman, Anton A. (31 July 2003). The Worldwide History of Telecommunications. John Wiley & Sons. ISBN 978-0-471-20505-0.
Iga, Kenichi; Kokubun, Yasuo (12 December 2010). Encyclopedic Handbook of Integrated Optics. CRC Press. ISBN 978-1-4200-2781-5.
Jalote, Pankaj (31 January 2006). An Integrated Approach to Software Engineering. Springer. ISBN 978-0-387-28132-2.
Khanna, Vinod Kumar (1 January 2009). Digital Signal Processing. S. Chand. ISBN 978-81-219-3095-6.
Lambourne, Robert J. A. (1 June 2010). Relativity, Gravitation and Cosmology. Cambridge University Press. ISBN 978-0-521-13138-4.
Leitgeb, Norbert (6 May 2010). Safety of Electromedical Devices: Law – Risks – Opportunities. Springer. ISBN 978-3-211-99683-6.
Leondes, Cornelius T. (8 August 2000). Energy and Power Systems. CRC Press. ISBN 978-90-5699-677-2.
Mahalik, Nitaigour Premchand (2003). Mechatronics: Principles, Concepts and Applications. Tata McGraw-Hill Education. ISBN 978-0-07-048374-3.
Maluf, Nadim; Williams, Kirt (1 January 2004). Introduction to Microelectromechanical Systems Engineering. Artech House. ISBN 978-1-58053-591-5.
Manolakis, Dimitris G.; Ingle, Vinay K. (21 November 2011). Applied Digital Signal Processing: Theory and Practice. Cambridge University Press. ISBN 978-1-139-49573-8.
Martini, L., ""BSCCO-2233 multilayered conductors"", in Superconducting Materials for High Energy Colliders, pp. 173–181, World Scientific, 2001 ISBN 981-02-4319-7.
Martinsen, Orjan G.; Grimnes, Sverre (29 August 2011). Bioimpedance and Bioelectricity Basics. Academic Press. ISBN 978-0-08-056880-5.
McDavid, Richard A.; Echaore-McDavid, Susan (1 January 2009). Career Opportunities in Engineering. Infobase Publishing. ISBN 978-1-4381-1070-7.
Merhari, Lhadi (3 March 2009). Hybrid Nanocomposites for Nanotechnology: Electronic, Optical, Magnetic and Biomedical Applications. Springer. ISBN 978-0-387-30428-1.
Mook, William Moyer (2008). The Mechanical Response of Common Nanoscale Contact Geometries. ISBN 978-0-549-46812-7.
Naidu, S. M.; Kamaraju, V. (2009). High Voltage Engineering. Tata McGraw-Hill Education. ISBN 978-0-07-066928-4.
Obaidat, Mohammad S.; Denko, Mieso; Woungang, Isaac (9 June 2011). Pervasive Computing and Networking. John Wiley & Sons. ISBN 978-1-119-97043-9.
Rosenberg, Chaim M. (2008). America at the Fair: Chicago's 1893 World's Columbian Exposition. Arcadia Publishing. ISBN 978-0-7385-2521-1.
Schmidt, Rüdiger, ""The LHC accelerator and its challenges"", in Kramer M.; Soler, F.J.P. (eds), Large Hadron Collider Phenomenology, pp. 217–250, CRC Press, 2004 ISBN 0-7503-0986-5.
Severs, Jeffrey; Leise, Christopher (24 February 2011). Pynchon's Against the Day: A Corrupted Pilgrim's Guide. Lexington Books. ISBN 978-1-61149-065-7.
Shetty, Devdas; Kolk, Richard (14 September 2010). Mechatronics System Design, SI Version. Cengage Learning. ISBN 978-1-133-16949-9.
Smith, Brian W. (January 2007). Communication Structures. Thomas Telford. ISBN 978-0-7277-3400-6.
Sullivan, Dennis M. (24 January 2012). Quantum Mechanics for Electrical Engineers. John Wiley & Sons. ISBN 978-0-470-87409-7.
Taylor, Allan (2008). Energy Industry. Infobase Publishing. ISBN 978-1-4381-1069-1.
Thompson, Marc (12 June 2006). Intuitive Analog Circuit Design. Newnes. ISBN 978-0-08-047875-3.
Tobin, Paul (1 January 2007). PSpice for Digital Communications Engineering. Morgan & Claypool Publishers. ISBN 978-1-59829-162-9.
Tunbridge, Paul (1992). Lord Kelvin, His Influence on Electrical Measurements and Units. IET. ISBN 978-0-86341-237-0.
Tuzlukov, Vyacheslav (12 December 2010). Signal Processing Noise. CRC Press. ISBN 978-1-4200-4111-8.
Walker, Denise (2007). Metals and Non-metals. Evans Brothers. ISBN 978-0-237-53003-7.
Wildes, Karl L.; Lindgren, Nilo A. (1 January 1985). A Century of Electrical Engineering and Computer Science at MIT, 1882–1982. MIT Press. p. 19. ISBN 978-0-262-23119-0.
Zhang, Yan; Hu, Honglin; Luo, Jijun (27 June 2007). Distributed Antenna Systems: Open Architecture for Future Wireless Communications. CRC Press. ISBN 978-1-4200-4289-4.


== Further reading ==
Adhami, Reza; Meenen, Peter M.; Hite, Denis (2007). Fundamental Concepts in Electrical and Computer Engineering with Practical Design Problems. Universal-Publishers. ISBN 978-1-58112-971-7.
Bober, William; Stevens, Andrew (27 August 2012). Numerical and Analytical Methods with MATLAB for Electrical Engineers. CRC Press. ISBN 978-1-4398-5429-7.
Bobrow, Leonard S. (1996). Fundamentals of Electrical Engineering. Oxford University Press. ISBN 978-0-19-510509-4.
Chen, Wai Kai (16 November 2004). The Electrical Engineering Handbook. Academic Press. ISBN 978-0-08-047748-0.
Ciuprina, G.; Ioan, D. (30 May 2007). Scientific Computing in Electrical Engineering. Springer. ISBN 978-3-540-71980-9.
Faria, J. A. Brandao (15 September 2008). Electromagnetic Foundations of Electrical Engineering. John Wiley & Sons. ISBN 978-0-470-69748-1.
Jones, Lincoln D. (July 2004). Electrical Engineering: Problems and Solutions. Dearborn Trade Publishing. ISBN 978-1-4195-2131-7.
Karalis, Edward (18 September 2003). 350 Solved Electrical Engineering Problems. Dearborn Trade Publishing. ISBN 978-0-7931-8511-5.
Krawczyk, Andrzej; Wiak, S. (1 January 2002). Electromagnetic Fields in Electrical Engineering. IOS Press. ISBN 978-1-58603-232-6.
Laplante, Phillip A. (31 December 1999). Comprehensive Dictionary of Electrical Engineering. Springer. ISBN 978-3-540-64835-2.
Leon-Garcia, Alberto (2008). Probability, Statistics, and Random Processes for Electrical Engineering. Prentice Hall. ISBN 978-0-13-147122-1.
Malaric, Roman (2011). Instrumentation and Measurement in Electrical Engineering. Universal-Publishers. ISBN 978-1-61233-500-1.
Sahay, Kuldeep; Sahay, Shivendra Pathak, Kuldeep (1 January 2006). Basic Concepts of Electrical Engineering. New Age International. ISBN 978-81-224-1836-1.
Srinivas, Kn (1 January 2007). Basic Electrical Engineering. I. K. International Pvt Ltd. ISBN 978-81-89866-34-1.


== External links ==

International Electrotechnical Commission (IEC)
MIT OpenCourseWare in-depth look at Electrical Engineering – online courses with video lectures.
IEEE Global History Network A wiki-based site with many resources about the history of IEEE, its members, their professions and electrical and informational technologies and sciences.","pandas(index=131, _1=131, text='electrical engineering is an engineering discipline concerned with the study, design and application of equipment, devices and systems which use electricity, electronics, and electromagnetism. it emerged as an identifiable occupation in the latter half of the 19th century after commercialization of the electric telegraph, the telephone, and electrical power generation, distribution and use. electrical engineering is now divided into a wide range of fields, including computer engineering, systems engineering, power engineering, telecommunications, radio-frequency engineering, signal processing, instrumentation, and electronics. many of these disciplines overlap with other engineering branches, spanning a huge number of specializations including hardware engineering, power electronics, electromagnetics and waves, microwave engineering, nanotechnology, electrochemistry, renewable energies, mechatronics, and electrical materials science.electrical engineers typically hold a degree in electrical engineering or electronic engineering.  practising engineers may have professional certification and be members of a professional body or an international standards organization. these include the international electrotechnical commission (iec), the institute of electrical and electronics engineers (ieee) and the institution of engineering and technology (iet) (formerly the iee). electrical engineers work in a very wide range of industries and the skills required are likewise variable. these range from circuit theory to the management skills of a project manager.  the tools and equipment that an individual engineer may need are similarly variable, ranging from a simple voltmeter to sophisticated design and manufacturing software.   == history ==  electricity has been a subject of scientific interest since at least the early 17th century. william gilbert was a prominent early electrical scientist, and was the first to draw a clear distinction between magnetism and static electricity. he is credited with establishing the term ""electricity"". he also designed the versorium: a device that detects the presence of statically charged objects. in 1762 swedish professor johan wilcke invented a device later named electrophorus that produced a static electric charge. by 1800 alessandro volta had developed the voltaic pile, a forerunner of the electric battery. computer engineering deals with the design of computers and computer systems. this may involve the design of new hardware, the design of pdas, tablets, and supercomputers, or the use of computers to control an industrial plant. computer engineers may also work on a system\'s software. however, the design of complex software systems is often the domain of software engineering, which is usually considered a separate discipline. desktop computers represent a tiny fraction of the devices a computer engineer might work on, as computer-like architectures are now found in a range of devices including video game consoles and dvd players.   == related disciplines ==  mechatronics is an engineering discipline which deals with the convergence of electrical and mechanical systems. such combined systems are known as electromechanical systems and have widespread adoption. examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various subsystems of aircraft and automobiles. electronic systems design is the subject within electrical engineering that deals with the multi-disciplinary design issues of complex electrical and mechanical systems.the term mechatronics is typically used to refer to macroscopic systems but futurists have predicted the emergence of very small electromechanical devices. already, such small devices, known as microelectromechanical systems (mems), are used in automobiles to tell airbags when to deploy, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high definition printing. in the future it is hoped the devices will help build tiny implantable medical devices and improve optical communication.biomedical engineering is another related discipline, concerned with the design of medical equipment. this includes fixed equipment such as ventilators, mri scanners, and electrocardiograph monitors as well as mobile equipment such as cochlear implants, artificial pacemakers, and artificial hearts. aerospace engineering and robotics an example is the most recent electric propulsion and ion propulsion.   == education ==  electrical engineers typically possess an academic degree with a major in electrical engineering, electronics engineering, electrical engineering technology, or electrical and electronic engineering. the same fundamental principles are taught in all programs, though emphasis may vary according to title. the length of study for such a degree is usually four or five years and the completed degree may be designated as a bachelor of science in electrical/electronics engineering technology, bachelor of engineering, bachelor of science, bachelor of technology, or bachelor of applied science, depending on the university. the bachelor\'s degree generally includes units covering physics, mathematics, computer science, project management, and a variety of topics in electrical engineering. initially such topics cover most, if not all, of the subdisciplines of electrical engineering. at some schools, the students can then choose to emphasize one or more subdisciplines towards the end of their courses of study.  at many schools, electronic engineering is included as part of an electrical award, sometimes explicitly, such as a bachelor of engineering (electrical and electronic), but in others, electrical and electronic engineering are both considered to be sufficiently broad and complex that separate degrees are offered.some electrical engineers choose to study for a postgraduate degree such as a master of engineering/master of science (meng/msc), a master of engineering management, a doctor of philosophy (phd) in engineering, an engineering doctorate (eng.d.), or an engineer\'s degree. the master\'s and engineer\'s degrees may consist of either research, coursework or a mixture of the two. the doctor of philosophy and engineering doctorate degrees consist of a significant research component and are often viewed as the entry point to academia. in the united kingdom and some other european countries, master of engineering is often considered to be an undergraduate degree of slightly longer duration than the bachelor of engineering rather than a standalone postgraduate degree.   == professional practice ==  in most countries, a bachelor\'s degree in engineering represents the first step towards professional certification and the degree program itself is certified by a professional body. after completing a certified degree program the engineer must satisfy a range of requirements (including work experience requirements) before being certified. once certified the engineer is designated the title of professional engineer (in the united states, canada and south africa), chartered engineer or incorporated engineer (in india, pakistan, the united kingdom, ireland and zimbabwe), chartered professional engineer (in australia and new zealand) or european engineer (in much of the european union).  the advantages of licensure vary depending upon location. for example, in the united states and canada ""only a licensed engineer may seal engineering work for public and private clients"". this requirement is enforced by state and provincial legislation such as quebec\'s engineers act. in other countries, no such legislation exists. practically all certifying bodies maintain a code of ethics that they expect all members to abide by or risk expulsion. in this way these organizations play an important role in maintaining ethical standards for the profession. even in jurisdictions where certification has little or no legal bearing on work, engineers are subject to contract law. in cases where an engineer\'s work fails he or she may be subject to the tort of negligence and, in extreme cases, the charge of criminal negligence. an engineer\'s work must also comply with numerous other rules and regulations, such as building codes and legislation pertaining to environmental law. professional bodies of note for electrical engineers include the institute of electrical and electronics engineers (ieee) and the institution of engineering and technology (iet).  the ieee claims to produce 30% of the world\'s literature in electrical engineering, has over 360,000 members worldwide and holds over 3,000 conferences annually. the iet publishes 21 journals, has a worldwide membership of over 150,000, and claims to be the largest professional engineering society in europe. obsolescence of technical skills is a serious concern for electrical engineers. membership and participation in technical societies, regular reviews of periodicals in the field and a habit of continued learning are therefore essential to maintaining proficiency. an miet(member of the institution of engineering and technology) is recognised in europe as an electrical and computer (technology) engineer.in australia, canada, and the united states electrical engineers make up around 0.25% of the labor force.   == tools and work == from the global positioning system to electric power generation, electrical engineers have contributed to the development of a wide range of technologies. they design, develop, test, and supervise the deployment of electrical systems and electronic devices. for example, they may work on the design of telecommunication systems, the operation of electric power stations, the lighting and wiring of buildings, the design of household appliances, or the electrical control of industrial machinery.  fundamental to the discipline are the sciences of physics and mathematics as these help to obtain both a qualitative and quantitative description of how such systems will work. today most engineering work involves the use of computers and it is commonplace to use computer-aided design programs when designing electrical systems. nevertheless, the ability to sketch ideas is still invaluable for quickly communicating with others.  although most electrical engineers will understand basic circuit theory (that is the interactions of elements such as resistors, capacitors, diodes, transistors, and inductors in a circuit), the theories employed by engineers generally depend upon the work they do. for example, quantum mechanics and solid state physics might be relevant to an engineer working on vlsi (the design of integrated circuits), but are largely irrelevant to engineers working with macroscopic electrical systems. even circuit theory may not be relevant to a person designing telecommunication systems that use off-the-shelf components. perhaps the most important technical skills for electrical engineers are reflected in university programs, which emphasize strong numerical skills, computer literacy, and the ability to understand the technical language and concepts that relate to electrical engineering.  a wide range of instrumentation is used by electrical engineers.  for simple control circuits and alarms, a basic multimeter measuring voltage, current, and resistance may suffice.  where time-varying signals need to be studied, the oscilloscope is also an ubiquitous instrument.  in rf engineering and high frequency telecommunications, spectrum analyzers and network analyzers are used.  in some disciplines, safety can be a particular concern with instrumentation.  for instance, medical electronics designers must take into account that much lower voltages than normal can be dangerous when electrodes are directly in contact with internal body fluids. power transmission engineering also has great safety concerns due to the high voltages used; although voltmeters may in principle be similar to their low voltage equivalents, safety and calibration issues make them very different. many disciplines of electrical engineering use tests specific to their discipline.  audio electronics engineers use audio test sets consisting of a signal generator and a meter, principally to measure level but also other parameters such as harmonic distortion and noise.  likewise, information technology have their own test sets, often specific to a particular data format, and the same is true of television broadcasting.  for many engineers, technical work accounts for only a fraction of the work they do. a lot of time may also be spent on tasks such as discussing proposals with clients, preparing budgets and determining project schedules. many senior engineers manage a team of technicians or other engineers and for this reason project management skills are important. most engineering projects involve some form of documentation and strong written communication skills are therefore very important. the workplaces of engineers are just as varied as the types of work they do. electrical engineers may be found in the pristine lab environment of a fabrication plant, on board a naval ship, the offices of a consulting firm or on site at a mine. during their working life, electrical engineers may find themselves supervising a wide range of individuals including scientists, electricians, computer programmers, and other engineers.electrical engineering has an intimate relationship with the physical sciences.  for instance, the physicist lord kelvin played a major role in the engineering of the first transatlantic telegraph cable. conversely, the engineer oliver heaviside produced major work on the mathematics of transmission on telegraph cables. electrical engineers are often required on major science projects.  for instance, large particle accelerators such as cern need electrical engineers to deal with many aspects of the project including the power distribution, the instrumentation, and the manufacture and installation of the superconducting electromagnets.   == see also ==   == notes ==   == references ==  bibliographyabramson, albert (1955). electronic motion pictures: a history of the television camera. university of california press. bayoumi, magdy a.; swartzlander, jr., earl e. (31 october 1994). vlsi signal processing technology. springer. isbn 978-0-7923-9490-7. bhushan, bharat (1997). micro/nanotribology and its applications. springer. isbn 978-0-7923-4386-8. bissell, chris (25 july 1996). control engineering, 2nd edition. crc press. isbn 978-0-412-57710-9. chandrasekhar, thomas (1 december 2006). analog communication (jntu). tata mcgraw-hill education. isbn 978-0-07-064770-1. chaturvedi, pradeep (1997). sustainable energy supply in asia: proceedings of the international conference, asia energy vision 2020, organised by the indian member committee, world energy council under the institution of engineers (india), during november 15–17, 1996 at new delhi. concept publishing company. isbn 978-81-7022-631-4. dodds, christopher; kumar, chandra; veering, bernadette (march 2014). oxford textbook of anaesthesia for the elderly patient. oxford university press. isbn 978-0-19-960499-9. fairman, frederick walker (11 june 1998). linear control theory: the state space approach. john wiley & sons. isbn 978-0-471-97489-5. fredlund, d. g.; rahardjo, h.; fredlund, m. d. (30 july 2012). unsaturated soil mechanics in engineering practice. wiley. isbn 978-1-118-28050-8. grant, malcolm alister; bixley, paul f (1 april 2011). geothermal reservoir engineering. academic press. isbn 978-0-12-383881-0. grigsby, leonard l. (16 may 2012). electric power generation, transmission, and distribution, third edition. crc press. isbn 978-1-4398-5628-4. heertje, arnold; perlman, mark (1990). evolving technology and market structure: studies in schumpeterian economics. university of michigan press. isbn 978-0-472-10192-4. huurdeman, anton a. (31 july 2003). the worldwide history of telecommunications. john wiley & sons. isbn 978-0-471-20505-0. iga, kenichi; kokubun, yasuo (12 december 2010). encyclopedic handbook of integrated optics. crc press. isbn 978-1-4200-2781-5. jalote, pankaj (31 january 2006). an integrated approach to software engineering. springer. isbn 978-0-387-28132-2. khanna, vinod kumar (1 january 2009). digital signal processing. s. chand. isbn 978-81-219-3095-6. lambourne, robert j. a. (1 june 2010). relativity, gravitation and cosmology. cambridge university press. isbn 978-0-521-13138-4. leitgeb, norbert (6 may 2010). safety of electromedical devices: law – risks – opportunities. springer. isbn 978-3-211-99683-6. leondes, cornelius t. (8 august 2000). energy and power systems. crc press. isbn 978-90-5699-677-2. mahalik, nitaigour premchand (2003). mechatronics: principles, concepts and applications. tata mcgraw-hill education. isbn 978-0-07-048374-3. maluf, nadim; williams, kirt (1 january 2004). introduction to microelectromechanical systems engineering. artech house. isbn 978-1-58053-591-5. manolakis, dimitris g.; ingle, vinay k. (21 november 2011). applied digital signal processing: theory and practice. cambridge university press. isbn 978-1-139-49573-8. martini, l., ""bscco-2233 multilayered conductors"", in superconducting materials for high energy colliders, pp. 173–181, world scientific, 2001 isbn 981-02-4319-7. martinsen, orjan g.; grimnes, sverre (29 august 2011). bioimpedance and bioelectricity basics. academic press. isbn 978-0-08-056880-5. mcdavid, richard a.; echaore-mcdavid, susan (1 january 2009). career opportunities in engineering. infobase publishing. isbn 978-1-4381-1070-7. merhari, lhadi (3 march 2009). hybrid nanocomposites for nanotechnology: electronic, optical, magnetic and biomedical applications. springer. isbn 978-0-387-30428-1. mook, william moyer (2008). the mechanical response of common nanoscale contact geometries. isbn 978-0-549-46812-7. naidu, s. m.; kamaraju, v. (2009). high voltage engineering. tata mcgraw-hill education. isbn 978-0-07-066928-4. obaidat, mohammad s.; denko, mieso; woungang, isaac (9 june 2011). pervasive computing and networking. john wiley & sons. isbn 978-1-119-97043-9. rosenberg, chaim m. (2008). america at the fair: chicago\'s 1893 world\'s columbian exposition. arcadia publishing. isbn 978-0-7385-2521-1. schmidt, rüdiger, ""the lhc accelerator and its challenges"", in kramer m.; soler, f.j.p. (eds), large hadron collider phenomenology, pp. 217–250, crc press, 2004 isbn 0-7503-0986-5. severs, jeffrey; leise, christopher (24 february 2011). pynchon\'s against the day: a corrupted pilgrim\'s guide. lexington books. isbn 978-1-61149-065-7. shetty, devdas; kolk, richard (14 september 2010). mechatronics system design, si version. cengage learning. isbn 978-1-133-16949-9. smith, brian w. (january 2007). communication structures. thomas telford. isbn 978-0-7277-3400-6. sullivan, dennis m. (24 january 2012). quantum mechanics for electrical engineers. john wiley & sons. isbn 978-0-470-87409-7. taylor, allan (2008). energy industry. infobase publishing. isbn 978-1-4381-1069-1. thompson, marc (12 june 2006). intuitive analog circuit design. newnes. isbn 978-0-08-047875-3. tobin, paul (1 january 2007). pspice for digital communications engineering. morgan & claypool publishers. isbn 978-1-59829-162-9. tunbridge, paul (1992). lord kelvin, his influence on electrical measurements and units. iet. isbn 978-0-86341-237-0. tuzlukov, vyacheslav (12 december 2010). signal processing noise. crc press. isbn 978-1-4200-4111-8. walker, denise (2007). metals and non-metals. evans brothers. isbn 978-0-237-53003-7. wildes, karl l.; lindgren, nilo a. (1 january 1985). a century of electrical engineering and computer science at mit, 1882–1982. mit press. p. 19. isbn 978-0-262-23119-0. zhang, yan; hu, honglin; luo, jijun (27 june 2007). distributed antenna systems: open architecture for future wireless communications. crc press. isbn 978-1-4200-4289-4.   == further reading == adhami, reza; meenen, peter m.; hite, denis (2007). fundamental concepts in electrical and computer engineering with practical design problems. universal-publishers. isbn 978-1-58112-971-7. bober, william; stevens, andrew (27 august 2012). numerical and analytical methods with matlab for electrical engineers. crc press. isbn 978-1-4398-5429-7. bobrow, leonard s. (1996). fundamentals of electrical engineering. oxford university press. isbn 978-0-19-510509-4. chen, wai kai (16 november 2004). the electrical engineering handbook. academic press. isbn 978-0-08-047748-0. ciuprina, g.; ioan, d. (30 may 2007). scientific computing in electrical engineering. springer. isbn 978-3-540-71980-9. faria, j. a. brandao (15 september 2008). electromagnetic foundations of electrical engineering. john wiley & sons. isbn 978-0-470-69748-1. jones, lincoln d. (july 2004). electrical engineering: problems and solutions. dearborn trade publishing. isbn 978-1-4195-2131-7. karalis, edward (18 september 2003). 350 solved electrical engineering problems. dearborn trade publishing. isbn 978-0-7931-8511-5. krawczyk, andrzej; wiak, s. (1 january 2002). electromagnetic fields in electrical engineering. ios press. isbn 978-1-58603-232-6. laplante, phillip a. (31 december 1999). comprehensive dictionary of electrical engineering. springer. isbn 978-3-540-64835-2. leon-garcia, alberto (2008). probability, statistics, and random processes for electrical engineering. prentice hall. isbn 978-0-13-147122-1. malaric, roman (2011). instrumentation and measurement in electrical engineering. universal-publishers. isbn 978-1-61233-500-1. sahay, kuldeep; sahay, shivendra pathak, kuldeep (1 january 2006). basic concepts of electrical engineering. new age international. isbn 978-81-224-1836-1. srinivas, kn (1 january 2007). basic electrical engineering. i. k. international pvt ltd. isbn 978-81-89866-34-1.   == external links ==  international electrotechnical commission (iec) mit opencourseware in-depth look at electrical engineering – online courses with video lectures. ieee global history network a wiki-based site with many resources about the history of ieee, its members, their professions and electrical and informational technologies and sciences.')"
132,"In telecommunication, communications-electronics (C-E) is the specialized field concerned with the use of electronic devices and systems for the acquisition or acceptance, processing, storage, display, analysis, protection, disposition, and transfer of information.  
C-E includes the wide range of responsibilities and actions relating to:

Electronic devices and systems used in the transfer of ideas and perceptions;
Electronic sensors and sensory systems used in the acquisition of information devoid of semantic influence;
Electronic devices and systems intended to allow friendly forces to operate in hostile environments and to deny to hostile forces the effective use of electromagnetic resources.


== Electronic Communications Equipment ==
Communication electronics radio equipment has been a rapidly growing industry for more than a century. Homeland Security in the USA is one of the reasons for the fast growth. Since the invention of the “solid state” transistor in the 1950s and the TTL (transistor-transistor logic) that led to the development of the IC (integrated circuit) in the 1960s the growth in the field of electronics has been phenomenal. As now witnessed in the “radio communications” field. The latest trend is to send conventional LMR (land-mobile-radio) signals over the Internet (Internet Protocol) this is called RoIP (Radio over Internet Protocol), which is just like VoIP (Voice over Internet Protocol) but uses the radio. By sending signals over the Internet it allows radios to be connected together all over the world. Hence: the “Communications Revolution”.","pandas(index=132, _1=132, text='in telecommunication, communications-electronics (c-e) is the specialized field concerned with the use of electronic devices and systems for the acquisition or acceptance, processing, storage, display, analysis, protection, disposition, and transfer of information. c-e includes the wide range of responsibilities and actions relating to:  electronic devices and systems used in the transfer of ideas and perceptions; electronic sensors and sensory systems used in the acquisition of information devoid of semantic influence; electronic devices and systems intended to allow friendly forces to operate in hostile environments and to deny to hostile forces the effective use of electromagnetic resources.   == electronic communications equipment == communication electronics radio equipment has been a rapidly growing industry for more than a century. homeland security in the usa is one of the reasons for the fast growth. since the invention of the “solid state” transistor in the 1950s and the ttl (transistor-transistor logic) that led to the development of the ic (integrated circuit) in the 1960s the growth in the field of electronics has been phenomenal. as now witnessed in the “radio communications” field. the latest trend is to send conventional lmr (land-mobile-radio) signals over the internet (internet protocol) this is called roip (radio over internet protocol), which is just like voip (voice over internet protocol) but uses the radio. by sending signals over the internet it allows radios to be connected together all over the world. hence: the “communications revolution”.')"
133,"In electrical engineering, two conductors are said to be inductively coupled or magnetically coupled  when they are configured such that a change in current through one wire induces a voltage across the ends of the other wire through electromagnetic induction.  A changing current through the first wire creates a changing magnetic field around it by Ampere's circuital law.  The changing magnetic field induces an electromotive force (EMF or voltage) in the second wire by Faraday's law of induction.  The amount of inductive coupling between two conductors is measured by their mutual inductance.  
The coupling between two wires can be increased by winding them into coils and placing them close together on a common axis, so the magnetic field of one coil passes through the other coil.  Coupling can also be increased by a magnetic core of a ferromagnetic material like iron or ferrite in the coils, which increases the magnetic flux.   The two coils may be physically contained in a single unit, as in the primary and secondary windings of a transformer, or may be separated.  Coupling may be intentional or unintentional. Unintentional inductive coupling can cause signals from one circuit to be induced into a nearby circuit, this is called cross-talk, and is a form of electromagnetic interference.
An inductively coupled transponder consists of a solid state transceiver chip connected to a large coil that functions as an antenna.  When brought within the oscillating magnetic field of a reader unit, the transceiver is powered up by energy inductively coupled into its antenna and transfers data back to the reader unit inductively.  
Magnetic coupling between two magnets can also be used to mechanically transfer power without contact, as in the magnetic gear.


== Uses ==
Inductive coupling is widely used throughout electrical technology; examples include:

Electric motors and generators
Inductive charging products
Induction cookers and induction heating systems
Induction loop communication systems
Metal detectors
Radio-frequency identification
Transformers
Wireless power transfer


== Low-frequency induction ==
Low-frequency induction can be a dangerous form of inductive coupling when it happens inadvertently. For example, if metal long-distance pipeline is installed along a right of way in parallel with a high-voltage power line, the power line can induce current on the pipe. Since the pipe is a conductor, insulated by its protective coating from the earth, it acts as a secondary winding for a long, drawn out transformer whose primary winding is the power line. Voltages induced on the pipe are then a hazard to people operating valves or otherwise touching metal parts of the metal pipeline.Reducing low-frequency magnetic fields may be necessary when dealing with electronics, as sensitive circuits in close proximity to an instrument with a power transformer could begin to display 60Hz pickup. Twisted wires are an effective way of reducing the interference as signals induced in the successive twists cancel. Using magnetic shielding is also an effective way of reducing unwanted inductive coupling, though moving the source of the magnetic field away from sensitive electronics is the simplest solution if possible. Although induced currents can be harmful, they can also be helpful. Electrical distribution line engineers use inductive coupling to tap power for cameras on towers and at substations that allow remote monitoring of the facilities. Using this they can watch from anywhere and not need to worry about changing camera batteries or solar panel maintenance.


== References ==","pandas(index=133, _1=133, text=""in electrical engineering, two conductors are said to be inductively coupled or magnetically coupled  when they are configured such that a change in current through one wire induces a voltage across the ends of the other wire through electromagnetic induction.  a changing current through the first wire creates a changing magnetic field around it by ampere's circuital law.  the changing magnetic field induces an electromotive force (emf or voltage) in the second wire by faraday's law of induction.  the amount of inductive coupling between two conductors is measured by their mutual inductance. the coupling between two wires can be increased by winding them into coils and placing them close together on a common axis, so the magnetic field of one coil passes through the other coil.  coupling can also be increased by a magnetic core of a ferromagnetic material like iron or ferrite in the coils, which increases the magnetic flux.   the two coils may be physically contained in a single unit, as in the primary and secondary windings of a transformer, or may be separated.  coupling may be intentional or unintentional. unintentional inductive coupling can cause signals from one circuit to be induced into a nearby circuit, this is called cross-talk, and is a form of electromagnetic interference. an inductively coupled transponder consists of a solid state transceiver chip connected to a large coil that functions as an antenna.  when brought within the oscillating magnetic field of a reader unit, the transceiver is powered up by energy inductively coupled into its antenna and transfers data back to the reader unit inductively. magnetic coupling between two magnets can also be used to mechanically transfer power without contact, as in the magnetic gear.   == uses == inductive coupling is widely used throughout electrical technology; examples include:  electric motors and generators inductive charging products induction cookers and induction heating systems induction loop communication systems metal detectors radio-frequency identification transformers wireless power transfer   == low-frequency induction == low-frequency induction can be a dangerous form of inductive coupling when it happens inadvertently. for example, if metal long-distance pipeline is installed along a right of way in parallel with a high-voltage power line, the power line can induce current on the pipe. since the pipe is a conductor, insulated by its protective coating from the earth, it acts as a secondary winding for a long, drawn out transformer whose primary winding is the power line. voltages induced on the pipe are then a hazard to people operating valves or otherwise touching metal parts of the metal pipeline.reducing low-frequency magnetic fields may be necessary when dealing with electronics, as sensitive circuits in close proximity to an instrument with a power transformer could begin to display 60hz pickup. twisted wires are an effective way of reducing the interference as signals induced in the successive twists cancel. using magnetic shielding is also an effective way of reducing unwanted inductive coupling, though moving the source of the magnetic field away from sensitive electronics is the simplest solution if possible. although induced currents can be harmful, they can also be helpful. electrical distribution line engineers use inductive coupling to tap power for cameras on towers and at substations that allow remote monitoring of the facilities. using this they can watch from anywhere and not need to worry about changing camera batteries or solar panel maintenance.   == references =="")"
134,"In physics and electrical engineering the reflection coefficient is a parameter that describes how much of a wave is reflected by an impedance discontinuity in the transmission medium.  It is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors.  For example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the electromagnetic wave is reflected by an impedance. The reflection coefficient is closely related to the transmission coefficient. The reflectance of a system is also sometimes called a ""reflection coefficient"".

Different specialties have different applications for the term.


== Transmission lines ==

In telecommunications and transmission line theory, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave.  The voltage and current at any point along a transmission line can always be resolved into forward and reflected traveling waves given a specified reference impedance Z0. The reference impedance used is typically the characteristic impedance of a transmission line that's involved, but one can speak of reflection coefficient without  any actual transmission line being present. In terms of the forward and reflected waves determined by the voltage and current, the reflection coefficient is defined as the complex ratio of the voltage of the reflected wave (
  
    
      
        
          V
          
            −
          
        
      
    
    {\displaystyle V^{-}}
  ) to that of the incident wave (
  
    
      
        
          V
          
            +
          
        
      
    
    {\displaystyle V^{+}}
  ). This is typically represented with a 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
   (capital gamma) and can be written as: 

  
    
      
        Γ
        =
        
          
            
              V
              
                −
              
            
            
              V
              
                +
              
            
          
        
      
    
    {\displaystyle \Gamma ={\frac {V^{-}}{V^{+}}}}
  It can as well be defined using the currents associated with the reflected and forward waves, but introducing a minus sign to account for the opposite orientations of the two currents:

  
    
      
        Γ
        =
        −
        
          
            
              I
              
                −
              
            
            
              I
              
                +
              
            
          
        
        =
        
          
            
              V
              
                −
              
            
            
              V
              
                +
              
            
          
        
      
    
    {\displaystyle \Gamma =-{\frac {I^{-}}{I^{+}}}={\frac {V^{-}}{V^{+}}}}
  The reflection coefficient may also be established using other field or circuit pairs of quantities whose product defines power resolvable into a forward and reverse wave. For instance, with electromagnetic plane waves, one uses the ratio of the electric fields of the reflected to that of the forward wave (or magnetic fields, again with a minus sign); the ratio of each wave's electric field E to its magnetic field H is again an impedance  Z0 (equal to the impedance of free space in a vacuum). Similarly in acoustics one uses the acoustic pressure and velocity respectively.

In the accompanying figure, a signal source with internal impedance 
  
    
      
        
          Z
          
            S
          
        
        
      
    
    {\displaystyle Z_{S}\,}
   possibly followed by a transmission line of characteristic impedance 
  
    
      
        
          Z
          
            S
          
        
        
      
    
    {\displaystyle Z_{S}\,}
   is represented by its Thévenin equivalent, driving the load 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
  . For a real (resistive) source impedance 
  
    
      
        
          Z
          
            S
          
        
      
    
    {\displaystyle Z_{S}}
  , if we define 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
   using the reference impedance 
  
    
      
        
          Z
          
            0
          
        
      
    
    {\displaystyle Z_{0}}
  =
  
    
      
        
          Z
          
            S
          
        
        
      
    
    {\displaystyle Z_{S}\,}
   then the source's maximum power is delivered to a load 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
  =
  
    
      
        
          Z
          
            0
          
        
      
    
    {\displaystyle Z_{0}}
  , in which case 
  
    
      
        Γ
        =
        0
      
    
    {\displaystyle \Gamma =0}
   implying no reflected power. More generally, the squared-magnitude of the reflection coefficient 
  
    
      
        
          |
        
        Γ
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle |\Gamma |^{2}}
    denotes the proportion of that power that is ""reflected"" and absorbed by the source, with the power actually delivered to the load thus reduced by 
  
    
      
        1
        −
        
          |
        
        Γ
        
          
            |
          
          
            2
          
        
      
    
    {\displaystyle 1-|\Gamma |^{2}}
  .
Anywhere along an intervening (lossless) transmission line of characteristic impedance 
  
    
      
        
          Z
          
            0
          
        
      
    
    {\displaystyle Z_{0}}
  , the magnitude of the reflection coefficient 
  
    
      
        
          |
        
        Γ
        
          |
        
      
    
    {\displaystyle |\Gamma |}
   will remain the same (the powers of the forward and reflected waves stay the same) but with a different phase. In the case of a short circuited load (
  
    
      
        
          Z
          
            L
          
        
        =
        0
      
    
    {\displaystyle Z_{L}=0}
  ), one finds 
  
    
      
        Γ
        =
        −
        1
      
    
    {\displaystyle \Gamma =-1}
   at the load. This implies the reflected wave having a 180° phase shift (phase reversal) with the voltages of the two waves being opposite at that point and adding to zero (as a short circuit demands).


=== Relation to load impedance ===
The reflection coefficient corresponds directly to a specific impedance as seen at the point it is measured. A load impedance of 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
   (using a reference impedance 
  
    
      
        
          Z
          
            0
          
        
        
      
    
    {\displaystyle Z_{0}\,}
  ) corresponds to a reflection coefficient of

  
    
      
        Γ
        =
        
          
            
              
                Z
                
                  L
                
              
              −
              
                Z
                
                  0
                
              
            
            
              
                Z
                
                  L
                
              
              +
              
                Z
                
                  0
                
              
            
          
        
      
    
    {\displaystyle \Gamma ={Z_{L}-Z_{0} \over Z_{L}+Z_{0}}}
   .If that load, 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
  , were measured not directly but through a transmission line, then the magnitude of the reflection coefficient is identical (as are the powers in the forward and reflected waves). However its phase will have shifted according to 

  
    
      
        
          Γ
          ′
        
        =
        Γ
        
          e
          
            −
            i
            
            2
            ϕ
          
        
      
    
    {\displaystyle \Gamma '=\Gamma e^{-i\,2\phi }}
  where 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   is the electrical length (expressed as phase) of that length of transmission line at the frequency considered. Note that the phase of the reflection coefficient is changed by twice the phase length of the attached transmission line. That is to take into account not only the phase delay of the reflected wave, but the phase shift that had first been applied to the forward wave, with the reflection coefficient being the quotient of these. The reflection coefficient so measured,  
  
    
      
        
          Γ
          ′
        
      
    
    {\displaystyle \Gamma '}
  , corresponds to an impedance which is generally dissimilar to 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
   present at the far side of the transmission line. 
The complex reflection coefficient (in the region 
  
    
      
        
          |
        
        Γ
        
          |
        
        ≤
        1
      
    
    {\displaystyle |\Gamma |\leq 1}
  , corresponding to passive loads) may be displayed graphically using a Smith chart. The Smith chart is a polar plot of 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  , therefore the magnitude of 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
   is given directly by the distance of a point to the center (with the edge of the Smith chart corresponding to 
  
    
      
        
          |
        
        Γ
        
          |
        
        =
        1
      
    
    {\displaystyle |\Gamma |=1}
  ). Its evolution along a transmission line is likewise described by a rotation of 
  
    
      
        2
        ϕ
      
    
    {\displaystyle 2\phi }
   around the chart's center. Using the scales on a Smith chart, the resulting impedance (normalized to 
  
    
      
        
          Z
          
            0
          
        
      
    
    {\displaystyle Z_{0}}
  ) can directly be read. Before the advent of modern electronic computers, the Smith chart was of particular use as a sort of  analog computer for this purpose.


=== Standing  wave ratio ===

The standing wave ratio (SWR) is determined solely by the magnitude of the reflection coefficient: 

  
    
      
        S
        W
        R
        =
        
          
            
              1
              +
              
                |
              
              Γ
              
                |
              
            
            
              1
              −
              
                |
              
              Γ
              
                |
              
            
          
        
      
    
    {\displaystyle SWR={1+|\Gamma | \over 1-|\Gamma |}}
   .Along a lossless transmission line of characteristic impedance Z0, the SWR signifies the ratio of the voltage (or current) maxima to minima (or what it would be if the transmission line were long enough to produce them). The above calculation assumes that  
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
   has been calculated using Z0 as the reference impedance. Since it uses only the magnitude of 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  , the SWR intentionally ignores the specific value of the load impedance ZL responsible for it, but only the magnitude of the resulting impedance mismatch. That SWR remains the same wherever measured along a transmission line (looking towards the load) since the addition of a transmission line length to a load 
  
    
      
        
          Z
          
            L
          
        
      
    
    {\displaystyle Z_{L}}
   only changes the phase, not magnitude of 
  
    
      
        Γ
      
    
    {\displaystyle \Gamma }
  . While having a one-to-one correspondence with reflection coefficient, SWR is the most commonly used figure of merit in describing the mismatch affecting a radio antenna or antenna system. It is most often measured at the transmitter side of a transmission line, but having, as explained, the same value as would be measured at the antenna (load) itself.


== Seismology ==

Reflection coefficient is used in feeder testing for reliability of medium.


== Optics and microwaves ==

In optics and electromagnetics in general, ""reflection coefficient"" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. Typically, the reflectance is represented by a capital R, while the amplitude reflection coefficient is represented by a lower-case r. These related concepts are covered by Fresnel equations in classical optics.


== Acoustics ==

Acousticians use reflection coefficients to understand the effect of different materials on their acoustic environments.


== See also ==
Microwave
Mismatch loss
Reflections of signals on conducting lines
Scattering parameters
Transmission coefficient
Target strength
Hagen-Rubens relation


== References ==
 This article incorporates public domain material from the General Services Administration document: ""Federal Standard 1037C"". (in support of MIL-STD-188)
Bogatin, Eric (2004). Signal Integrity - Simplified. Upper Saddle River, New Jersey: Pearson Education, Inc. ISBN 0-13-066946-6. Figure 8-2 and Eqn. 8-1 Pg. 279


== External links ==
Flash tutorial for understanding reflection A flash program that shows how a reflected wave is generated, the reflection coefficient and VSWR
Application for drawing standing wave diagrams including the reflection coefficient, input impedance, SWR, etc.","pandas(index=134, _1=134, text='in physics and electrical engineering the reflection coefficient is a parameter that describes how much of a wave is reflected by an impedance discontinuity in the transmission medium.  it is equal to the ratio of the amplitude of the reflected wave to the incident wave, with each expressed as phasors.  for example, it is used in optics to calculate the amount of light that is reflected from a surface with a different index of refraction, such as a glass surface, or in an electrical transmission line to calculate how much of the electromagnetic wave is reflected by an impedance. the reflection coefficient is closely related to the transmission coefficient. the reflectance of a system is also sometimes called a ""reflection coefficient"".  different specialties have different applications for the term.   == transmission lines ==  in telecommunications and transmission line theory, the reflection coefficient is the ratio of the complex amplitude of the reflected wave to that of the incident wave.  the voltage and current at any point along a transmission line can always be resolved into forward and reflected traveling waves given a specified reference impedance z0. the reference impedance used is typically the characteristic impedance of a transmission line that\'s involved, but one can speak of reflection coefficient without  any actual transmission line being present. in terms of the forward and reflected waves determined by the voltage and current, the reflection coefficient is defined as the complex ratio of the voltage of the reflected wave (     v  −      . while having a one-to-one correspondence with reflection coefficient, swr is the most commonly used figure of merit in describing the mismatch affecting a radio antenna or antenna system. it is most often measured at the transmitter side of a transmission line, but having, as explained, the same value as would be measured at the antenna (load) itself.   == seismology ==  reflection coefficient is used in feeder testing for reliability of medium.   == optics and microwaves ==  in optics and electromagnetics in general, ""reflection coefficient"" can refer to either the amplitude reflection coefficient described here, or the reflectance, depending on context. typically, the reflectance is represented by a capital r, while the amplitude reflection coefficient is represented by a lower-case r. these related concepts are covered by fresnel equations in classical optics.   == acoustics ==  acousticians use reflection coefficients to understand the effect of different materials on their acoustic environments.   == see also == microwave mismatch loss reflections of signals on conducting lines scattering parameters transmission coefficient target strength hagen-rubens relation   == references == this article incorporates public domain material from the general services administration document: ""federal standard 1037c"". (in support of mil-std-188) bogatin, eric (2004). signal integrity - simplified. upper saddle river, new jersey: pearson education, inc. isbn 0-13-066946-6. figure 8-2 and eqn. 8-1 pg. 279   == external links == flash tutorial for understanding reflection a flash program that shows how a reflected wave is generated, the reflection coefficient and vswr application for drawing standing wave diagrams including the reflection coefficient, input impedance, swr, etc.')"
135,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.Commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. Incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. For example, asking whether a kilogram is larger than an hour is meaningless.
Any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
The concept of physical dimension, and of dimensional analysis, was introduced by Joseph Fourier in 1822.


== Concrete numbers and base units ==
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof.
A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units.
Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). The newton is defined as 1 N = 1 kg⋅m⋅s−2.


=== Percentages and derivatives ===
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as ""hundredths"", since 1% = 1/100.
Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:

position (x) has the dimension L (length);
derivative of position with respect to time (dx/dt, velocity) has dimension LT−1—length from position, time due to the gradient;
the second derivative (d2x/dt2 = d(dx/dt) / dt, acceleration) has dimension LT−2.In economics, one distinguishes between stocks and flows: a stock has units of ""units"" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of ""units/time"" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.


== Conversion factor ==

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and 100 kPa = 1 bar. The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to 100 kPa / 1 bar = 1.  Since any quantity can be multiplied by 1 without changing it, the expression ""100 kPa / 1 bar"" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, 5 bar × 100 kPa / 1 bar = 500 kPa because 5 × 100 / 1 = 500, and bar/bar cancels out, so 5 bar = 500 kPa.


== Dimensional homogeneity ==

The most basic rule of dimensional analysis is that of dimensional homogeneity.
Only commensurable quantities (physical quantities having the same dimension) may be compared, equated, added, or subtracted.However, the dimensions form an abelian group under multiplication, so:

One may take ratios of incommensurable quantities (quantities with different dimensions), and multiply or divide them.For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometre being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful expression only quantities of the same dimension can be added, subtracted, or compared. For example, if mman, mrat and Lman denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression mman + mrat is meaningful, but the heterogeneous expression mman + Lman is meaningless. However, mman/L2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
This has the implication that most mathematical functions, particularly the transcendental functions, must have a dimensionless quantity, a pure number, as the argument and must return a dimensionless number as a result. This is clear because many transcendental functions can be expressed as an infinite power series with dimensionless coefficients.

  
    
      
        f
        (
        x
        )
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        =
        
          a
          
            0
          
        
        +
        
          a
          
            1
          
        
        x
        +
        
          a
          
            2
          
        
        
          x
          
            2
          
        
        +
        
          a
          
            3
          
        
        
          x
          
            3
          
        
        +
        ⋯
      
    
    {\displaystyle f(x)=\sum _{n=0}^{\infty }a_{n}x^{n}=a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}+\cdots }
  All powers of x must have the same dimension for the terms to be commensurable. But if x is not dimensionless, then the different powers of x will have different, incommensurable dimensions. However, power functions including root functions may have a dimensional argument and will return a result having dimension that is the same power applied to the argument dimension. This is because power functions and root functions are, loosely, just an expression of multiplication of quantities.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension L2MT−2, they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.


== The factor-label method for converting units ==
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:

  
    
      
        
          
            
              10
               
              
                
                  mile
                
              
            
            
              1
               
              
                
                  hour
                
              
            
          
        
        ×
        
          
            
              1609.344
              
                 meter
              
            
            
              1
               
              
                
                  mile
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  hour
                
              
            
            
              3600
              
                 second
              
            
          
        
        =
        4.4704
         
        
          
            meter
            second
          
        
        .
      
    
    {\displaystyle {\frac {10\ {\cancel {\text{mile}}}}{1\ {\cancel {\text{hour}}}}}\times {\frac {1609.344{\text{ meter}}}{1\ {\cancel {\text{mile}}}}}\times {\frac {1\ {\cancel {\text{hour}}}}{3600{\text{ second}}}}=4.4704\ {\frac {\text{meter}}{\text{second}}}.}
  Each conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as ""mile"" is the numerator in the original fraction and 
  
    
      
        1
         
        
          mile
        
        =
        1609.344
         
        
          meter
        
      
    
    {\displaystyle 1\ {\text{mile}}=1609.344\ {\text{meter}}}
  , ""mile"" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields 
  
    
      
        
          
            
              1
               
              
                mile
              
            
            
              1
               
              
                mile
              
            
          
        
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle {\frac {1\ {\text{mile}}}{1\ {\text{mile}}}}={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  , which when simplified results in the dimensionless 
  
    
      
        1
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle 1={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  . Multiplying any quantity (physical quantity or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units mile and hour, 10 miles per hour converts to 4.4704 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., 
  
    
      
        
          
            
              NO
            
            
              x
            
          
        
      
    
    {\displaystyle \color {Blue}{\ce {NO}}_{x}}
  ) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of 
  
    
      
        
          
            NO
          
          
            x
          
        
      
    
    {\displaystyle {\ce {NO}}_{x}}
   by using the following information as shown below:

NOx concentration
= 10 parts per million by volume = 10 ppmv = 10 volumes/106 volumes
NOx molar mass
= 46 kg/kmol = 46 g/mol
Flow rate of flue gas
= 20 cubic meters per minute = 20 m3/min
The flue gas exits the furnace at 0 °C temperature and 101.325 kPa absolute pressure.
The molar volume of a gas at 0 °C temperature and 101.325 kPa is 22.414 m3/kmol.
  
    
      
        
          
            
              1000
               
              
                
                  g
                   
                  NO
                
                
                  x
                
              
            
            
              1
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              46
               
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              22.414
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              10
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              
                10
                
                  6
                
              
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
          
        
        ×
        
          
            
              20
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
            
              1
               
              
                
                  minute
                
              
            
          
        
        ×
        
          
            
              60
               
              
                
                  minute
                
              
            
            
              1
               
              
                hour
              
            
          
        
        =
        24.63
         
        
          
            
              
                g
                 
                NO
              
              
                x
              
            
            hour
          
        
      
    
    {\displaystyle {\frac {1000\ {\ce {g\ NO}}_{x}}{1{\cancel {{\ce {kg\ NO}}_{x}}}}}\times {\frac {46\ {\cancel {{\ce {kg\ NO}}_{x}}}}{1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}}\times {\frac {1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}{22.414\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}}\times {\frac {10\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}{10^{6}\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}}\times {\frac {20\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}{1\ {\cancel {\ce {minute}}}}}\times {\frac {60\ {\cancel {\ce {minute}}}}{1\ {\ce {hour}}}}=24.63\ {\frac {{\ce {g\ NO}}_{x}}{\ce {hour}}}}
  After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.


=== Checking equations that involve dimensions ===
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.
For example, check the Universal Gas Law equation of PV = nRT, when:

the pressure P is in pascals (Pa)
the volume V is in cubic meters (m3)
the amount of substance n is in moles (mol)
the universal gas law constant R is 8.3145 Pa⋅m3/(mol⋅K)
the temperature T is in kelvins (K)
  
    
      
        
          Pa
          ⋅
          
            m
            
              3
            
          
        
        =
        
          
            
              
                mol
              
            
            1
          
        
        ×
        
          
            
              Pa
              ⋅
              
                m
                
                  3
                
              
            
            
              
                
                  
                    mol
                  
                
              
               
              
                
                  
                    K
                  
                
              
            
          
        
        ×
        
          
            
              
                K
              
            
            1
          
        
      
    
    {\displaystyle {\ce {Pa.m^3}}={\frac {\cancel {{\ce {mol}}}}{1}}\times {\frac {{\ce {Pa.m^3}}}{{\cancel {{\ce {mol}}}}\ {\cancel {{\ce {K}}}}}}\times {\frac {\cancel {{\ce {K}}}}{1}}}
  As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions — dimensional  adjusters — that can then be assigned physical significance. It is important to point out that such ‘mathematical manipulation’ is   neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck's constant, a fundamental constant of the universe, was ‘discovered’ as a purely mathematical abstraction or representation that built on the Rayleigh-Jeans Equation for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment – not earlier.


=== Limitations ===
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (
  
    
      
        x
        ↦
        a
        x
        +
        b
      
    
    {\displaystyle x\mapsto ax+b}
  , rather than a linear transform 
  
    
      
        x
        ↦
        a
        x
      
    
    {\displaystyle x\mapsto ax}
  ) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature T[F] in degrees Fahrenheit to a numerical quantity value T[C] in degrees Celsius, this formula may be used:

T[C] = (T[F] − 32) × 5/9.To convert T[C] in degrees Celsius to T[F] in degrees Fahrenheit, this formula may be used:

T[F] = (T[C] × 9/5) + 32.


== Applications ==
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.


=== Mathematics ===
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an n-ball (the solid ball in n dimensions), or the area of its surface, the n-sphere: being an n-dimensional figure, the volume scales as 
  
    
      
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle x^{n},}
   while the surface area, being 
  
    
      
        (
        n
        −
        1
        )
      
    
    {\displaystyle (n-1)}
  -dimensional, scales as 
  
    
      
        
          x
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle x^{n-1}.}
   Thus the volume of the n-ball in terms of the radius is 
  
    
      
        
          C
          
            n
          
        
        
          r
          
            n
          
        
        ,
      
    
    {\displaystyle C_{n}r^{n},}
   for some constant 
  
    
      
        
          C
          
            n
          
        
        .
      
    
    {\displaystyle C_{n}.}
   Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.


=== Finance, economics, and accounting ===
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

For example, the P/E ratio has dimensions of time (units of years), and can be interpreted as ""years of earnings to earn the price paid"".
In economics, debt-to-GDP ratio also has units of years (debt has units of currency, GDP has units of currency/year).
In financial analysis, some bond duration types also have dimension of time (unit of years) and can be interpreted as ”years to balance point between interest payments and nominal repayment”.
Velocity of money has units of 1/years (GDP/money supply has units of currency/year over currency): how often a unit of currency circulates per year.
Interest rates are often expressed as a percentage, but more properly percent per annum, which has dimensions of 1/years.


=== Fluid mechanics ===
In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:

Reynolds number (Re), generally important in all types of fluid problems:

  
    
      
        
          R
          e
        
        =
        
          
            
              ρ
              
              u
              d
            
            μ
          
        
      
    
    {\displaystyle \mathrm {Re} ={\frac {\rho \,ud}{\mu }}}
  .
Froude number (Fr), modeling flow with a free surface:

  
    
      
        
          F
          r
        
        =
        
          
            u
            
              g
              
              L
            
          
        
        .
      
    
    {\displaystyle \mathrm {Fr} ={\frac {u}{\sqrt {g\,L}}}.}
  
Euler number (Eu), used in problems in which pressure is of interest:

  
    
      
        
          E
          u
        
        =
        
          
            
              Δ
              p
            
            
              ρ
              
                u
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle \mathrm {Eu} ={\frac {\Delta p}{\rho u^{2}}}.}
  
Mach number (Ma), important in high speed flows where the velocity approaches or exceeds the local speed of sound:

  
    
      
        
          M
          a
        
        =
        
          
            u
            c
          
        
        ,
      
    
    {\displaystyle \mathrm {Ma} ={\frac {u}{c}},}
   where: c is the local speed of sound.


== History ==
The origins of dimensional analysis have been disputed by historians.The first written application of dimensional analysis has been credited to an article of François Daviet at the Turin Academy of Science. Daviet had the master Lagrange as teacher. 
His fundamental works are contained in acta of the Academy dated 1799.This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p.39). In the second edition of 1833, Poisson explicitly introduces the term dimension instead of the Daviet homogeneity.
In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like F = ma should be independent of the units employed to measure the physical variables.
Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be ""the three fundamental units"", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant G is taken as unity, thereby defining M = L3T−2. By assuming a form of Coulomb's law in which Coulomb's constant ke is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were Q = L3/2M1/2T−1, which, after substituting his M = L3T−2 equation for mass, results in charge having the same dimensions as mass, viz. Q = L3T−2.
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize.  It was used for the first time (Pesic 2005) in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue.  Rayleigh first published the technique in his 1877 book The Theory of Sound.The original meaning of the word dimension, in Fourier's Theorie de la Chaleur, was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT−2, instead of just the exponents.


== Mathematical formulation ==
The Buckingham π theorem describes how every physically meaningful equation involving n variables can be equivalently rewritten as an equation of n − m dimensionless parameters, where m is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.


=== Definition ===
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The dimension of a physical quantity is more fundamental than some scale unit used to express the amount of that physical quantity.  For example, mass is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.
There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity Q is given by 

  
    
      
        
          dim
        
         
        
          Q
        
        =
        
          
            
              L
            
          
          
            a
          
        
        
          
            
              M
            
          
          
            b
          
        
        
          
            
              T
            
          
          
            c
          
        
        
          
            
              I
            
          
          
            d
          
        
        
          
            
              Θ
            
          
          
            e
          
        
        
          
            
              N
            
          
          
            f
          
        
        
          
            
              J
            
          
          
            g
          
        
      
    
    {\displaystyle {\text{dim}}~{Q}={\mathsf {L}}^{a}{\mathsf {M}}^{b}{\mathsf {T}}^{c}{\mathsf {I}}^{d}{\mathsf {\Theta }}^{e}{\mathsf {N}}^{f}{\mathsf {J}}^{g}}
  where a, b, c, d, e, f, g are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electric current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.
As examples, the dimension of the physical quantity speed v is

  
    
      
        
          dim
        
         
        v
        =
        
          
            length
            time
          
        
        =
        
          
            
              L
            
            
              T
            
          
        
        =
        
          
            
              L
              T
            
          
          
            −
            1
          
        
      
    
    {\displaystyle {\text{dim}}~v={\frac {\text{length}}{\text{time}}}={\frac {\mathsf {L}}{\mathsf {T}}}={\mathsf {LT}}^{-1}}
  and the dimension of the physical quantity force F is

  
    
      
        
          dim
        
         
        F
        =
        
          mass
        
        ×
        
          acceleration
        
        =
        
          mass
        
        ×
        
          
            length
            
              
                time
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
            
            
              
                
                  T
                
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
              T
            
          
          
            −
            2
          
        
      
    
    {\displaystyle {\text{dim}}~F={\text{mass}}\times {\text{acceleration}}={\text{mass}}\times {\frac {\text{length}}{{\text{time}}^{2}}}={\frac {\mathsf {ML}}{{\mathsf {T}}^{2}}}={\mathsf {MLT}}^{-2}}
  The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.
There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.


=== Mathematical properties ===

The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; L0 = 1, and the inverse to L is 1/L or L−1. L raised to any rational power p is a member of the group, having an inverse of L−p or 1/Lp.  The operation of the group is multiplication, having the usual rules for handling exponents (Ln × Lm = Ln+m).
This group can be described as a vector space over the rational numbers, with for example dimensional symbol MiLjTk corresponding to the vector (i, j, k). When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., m) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., πm}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity X can be expressed in the general form

  
    
      
        X
        =
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        (
        
          π
          
            i
          
        
        
          )
          
            
              k
              
                i
              
            
          
        
        
        .
      
    
    {\displaystyle X=\prod _{i=1}^{m}(\pi _{i})^{k_{i}}\,.}
  Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

  
    
      
        f
        (
        
          π
          
            1
          
        
        ,
        
          π
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          π
          
            m
          
        
        )
        =
        0
        
        .
      
    
    {\displaystyle f(\pi _{1},\pi _{2},...,\pi _{m})=0\,.}
  Knowing this restriction can be a powerful tool for obtaining new insight into the system.


=== Mechanics ===
The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T2], L, M, while the latter can be expressed as M, L, [T = (ML/F)1/2].
On the other hand, length, velocity and time (L, V, T) do not form a set of base dimensions for mechanics, for two reasons:

There is no way to obtain mass – or anything derived from it, such as force – without introducing another base dimension (thus, they do not span the space).
Velocity, being expressible in terms of length and time (V = L/T), is redundant (the set is not linearly independent).


=== Other fields of physics and chemistry ===
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge.  In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ.  In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ 6.02×1023 mol−1) is defined as a base dimension, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.


=== Polynomials and transcendental functions ===
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities.  (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does not hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials (xn) of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for x2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for x2 + x, the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless.  For example,

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          (
          
            −
            9.8
             
            
              
                meter
                
                  
                    second
                  
                  
                    2
                  
                
              
            
          
          )
        
        ⋅
        
          t
          
            2
          
        
        +
        
          (
          
            500
             
            
              
                meter
                second
              
            
          
          )
        
        ⋅
        t
        .
      
    
    {\displaystyle {\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot t^{2}+\left(500\ {\frac {\text{meter}}{\text{second}}}\right)\cdot t.}
  This is the height to which an object rises in time t if the acceleration of gravity is 9.8 meter per second per second and the initial upward speed is 500 meter per second.  It is not necessary for t to be in seconds.  For example, suppose t = 0.01 minutes.  Then the first term would be

  
    
      
        
          
            
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                
                  (
                  
                    −
                    9.8
                     
                    
                      
                        meter
                        
                          
                            second
                          
                          
                            2
                          
                        
                      
                    
                  
                  )
                
                ⋅
                (
                0.01
                
                   minute
                
                
                  )
                  
                    2
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                
                  
                    (
                    
                      
                        minute
                        second
                      
                    
                    )
                  
                  
                    2
                  
                
                ⋅
                
                  meter
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                ⋅
                
                  60
                  
                    2
                  
                
                ⋅
                
                  meter
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot (0.01{\text{ minute}})^{2}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\left({\frac {\text{minute}}{\text{second}}}\right)^{2}\cdot {\text{meter}}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\cdot 60^{2}\cdot {\text{meter}}.\end{aligned}}}
  


=== Incorporating units ===
The value of a dimensional physical quantity Z is written as the product of a unit [Z] within the dimension and a dimensionless numerical factor, n.

  
    
      
        Z
        =
        n
        ×
        [
        Z
        ]
        =
        n
        [
        Z
        ]
      
    
    {\displaystyle Z=n\times [Z]=n[Z]}
  When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

  
    
      
        1
         
        
          
            ft
          
        
        =
        0.3048
         
        
          
            m
          
        
         
      
    
    {\displaystyle 1\ {\mbox{ft}}=0.3048\ {\mbox{m}}\ }
    is identical to 
  
    
      
        1
        =
        
          
            
              0.3048
               
              
                
                  m
                
              
            
            
              1
               
              
                
                  ft
                
              
            
          
        
        .
         
      
    
    {\displaystyle 1={\frac {0.3048\ {\mbox{m}}}{1\ {\mbox{ft}}}}.\ }
  The factor 
  
    
      
        0.3048
         
        
          
            
              m
            
            
              ft
            
          
        
      
    
    {\displaystyle 0.3048\ {\frac {\mbox{m}}{\mbox{ft}}}}
   is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.


=== Position vs displacement ===

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:

adding two displacements should yield a new displacement (walking ten paces then twenty paces gets you thirty paces forward),
adding a displacement to a position should yield a new position (walking one block down the street from an intersection gets you to the next intersection),
subtracting two positions should yield a displacement,
but one may not add two positions.This illustrates the subtle distinction between affine quantities (ones modeled by an affine space, such as position) and vector quantities (ones modeled by a vector space, such as displacement).

Vector quantities may be added to each other, yielding a new vector quantity, and a vector quantity may be added to a suitable affine quantity (a vector space acts on an affine space), yielding a new affine quantity.
Affine quantities cannot be added, but may be subtracted, yielding relative quantities which are vectors, and these relative differences may then be added to each other or to an affine quantity.Properly then, positions have dimension of affine length, while displacements have dimension of vector length. To assign a number to an affine unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a vector unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,

−273.15 °C ≘ 0 K = 0 °R ≘ −459.67 °F,where the symbol ≘ means corresponds to, since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.
For temperature differences,

1 K = 1 °C ≠ 1 °F = 1 °R.(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.


=== Orientation and frame of reference ===
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a direction. (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.


== Examples ==


=== A simple example: period of a harmonic oscillator ===
What is the period of oscillation T of a mass m attached to an ideal linear spring with spring constant k suspended in gravity of strength g? That period is the solution for T of some dimensionless equation in the variables T, m, k, and g.
The four quantities have the following dimensions:  T  [T];  m  [M]; k [M/T2]; and  g [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, 
  
    
      
        
          G
          
            1
          
        
      
    
    {\displaystyle G_{1}}
   = 
  
    
      
        
          T
          
            2
          
        
        k
        
          /
        
        m
      
    
    {\displaystyle T^{2}k/m}
   [T2 · M/T2 / M = 1], and putting 
  
    
      
        
          G
          
            1
          
        
        =
        C
      
    
    {\displaystyle G_{1}=C}
   for some dimensionless constant C gives the dimensionless equation sought.  The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term ""group"" means ""collection"" rather than mathematical group.  They are often called dimensionless numbers as well.
Note that the variable g does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines g with k, m, and T, because g is the only quantity that involves the dimension L. This implies that in this problem the g is irrelevant. Dimensional analysis can sometimes yield strong statements about the irrelevance of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of g: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way:  
  
    
      
        T
        =
        κ
        
          
            
              
                m
                k
              
            
          
        
      
    
    {\displaystyle T=\kappa {\sqrt {\tfrac {m}{k}}}}
  , for some dimensionless constant κ (equal to 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\sqrt {C}}}
   from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (g, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be ""complete"" – although it still may involve unknown dimensionless constants, such as κ.


=== A more complex example: energy of a vibrating wire ===
Consider the case of a vibrating wire of length ℓ (L) vibrating with an amplitude A (L).  The wire has a linear density ρ (M/L) and is under tension s (ML/T2), and we want to know the energy E (ML2/T2) in the wire.  Let π1 and π2 be two dimensionless products of powers of the variables chosen, given by

  
    
      
        
          
            
              
                
                  π
                  
                    1
                  
                
              
              
                
                =
                
                  
                    E
                    
                      A
                      s
                    
                  
                
              
            
            
              
                
                  π
                  
                    2
                  
                
              
              
                
                =
                
                  
                    ℓ
                    A
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\pi _{1}&={\frac {E}{As}}\\\pi _{2}&={\frac {\ell }{A}}.\end{aligned}}}
  The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation

  
    
      
        F
        
          (
          
            
              
                E
                
                  A
                  s
                
              
            
            ,
            
              
                ℓ
                A
              
            
          
          )
        
        =
        0
        ,
      
    
    {\displaystyle F\left({\frac {E}{As}},{\frac {\ell }{A}}\right)=0,}
  where F is some unknown function, or, equivalently as

  
    
      
        E
        =
        A
        s
        f
        
          (
          
            
              ℓ
              A
            
          
          )
        
        ,
      
    
    {\displaystyle E=Asf\left({\frac {\ell }{A}}\right),}
  where f is some other unknown function.  Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension.  Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function f.  But our experiments are simpler than in the absence of dimensional analysis.  We'd perform none to verify that the energy is proportional to the tension.  Or perhaps we might guess that the energy is proportional to ℓ, and so infer that E = ℓs.  The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex.  Consider, for example, a small pebble sitting on the bed of a river.  If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water.  At what critical velocity will this occur?  Sorting out the guessed variables is not so easy as before.  But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.


=== A third example: demand versus capacity for a rotating disc ===

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness t (L) and radius R (L).  The disc has a density ρ (M/L3), rotates at an angular velocity ω (T−1) and this leads to a stress S (ML−1T−2) in the material.  There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid.  As the disc becomes thicker relative to the radius then the plane stress solution breaks down.  If the disc is restrained axially on its free faces then a state of plane strain will occur.  However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case.  An engineer might, therefore, be interested in establishing a relationship between the five variables.  Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:

demand/capacity = ρR2ω2/S
thickness/radius or aspect ratio = t/RThrough the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure.  As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs


== Extensions ==


=== Huntley's extension: directed dimensions and quantity of matter ===
Huntley (Huntley 1967) has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank 
  
    
      
        m
      
    
    {\displaystyle m}
   of the dimensional matrix. He introduced two approaches to doing so:

The magnitudes of the components of a vector are to be considered dimensionally independent. For example, rather than an undifferentiated length dimension L, we may have Lx represent dimension in the x-direction, and so forth. This requirement stems ultimately from the requirement that each component of a physically meaningful equation (scalar, vector, or tensor) must be dimensionally consistent.
Mass as a measure of the quantity of matter is to be considered dimensionally independent from mass as a measure of inertia.As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   and a horizontal velocity component 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
  , both dimensioned as LT−1, R, the distance travelled, having dimension L, and g the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range R may be written:

  
    
      
        R
        ∝
        
          V
          
            x
          
          
            a
          
        
        
        
          V
          
            y
          
          
            b
          
        
        
        
          g
          
            c
          
        
        .
        
      
    
    {\displaystyle R\propto V_{\text{x}}^{a}\,V_{\text{y}}^{b}\,g^{c}.\,}
  Or dimensionally

  
    
      
        
          
            L
          
        
        =
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            a
            +
            b
          
        
        
          
            (
            
              
                
                  L
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
        
      
    
    {\displaystyle {\mathsf {L}}=\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{a+b}\left({\frac {\mathsf {L}}{{\mathsf {T}}^{2}}}\right)^{c}\,}
  from which we may deduce that 
  
    
      
        a
        +
        b
        +
        c
        =
        1
      
    
    {\displaystyle a+b+c=1}
   and 
  
    
      
        a
        +
        b
        +
        2
        c
        =
        0
      
    
    {\displaystyle a+b+2c=0}
  , which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
   will be dimensioned as LxT−1, 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   as LyT−1, R as Lx and g as LyT−2. The dimensional equation becomes:

  
    
      
        
          
            
              L
            
          
          
            
              x
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      x
                    
                  
                
                
                  T
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
      
    
    {\displaystyle {\mathsf {L}}_{\mathrm {x} }=\left({\frac {{\mathsf {L}}_{\mathrm {x} }}{\mathsf {T}}}\right)^{a}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{\mathsf {T}}}\right)^{b}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{{\mathsf {T}}^{2}}}\right)^{c}}
  and we may solve completely as 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
  , 
  
    
      
        b
        =
        1
      
    
    {\displaystyle b=1}
   and 
  
    
      
        c
        =
        −
        1
      
    
    {\displaystyle c=-1}
  . The increase in deductive power gained by the use of directed length dimensions is apparent.
In his second approach, Huntley holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity (a) proportional to inertial mass, but (b) not implicating inertial properties. No further restrictions are added to its definition.
For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables

  
    
      
        
          
            
              m
              ˙
            
          
        
      
    
    {\displaystyle {\dot {m}}}
   the mass flow rate with dimension MT−1

  
    
      
        
          p
          
            x
          
        
      
    
    {\displaystyle p_{\text{x}}}
   the pressure gradient along the pipe with dimension ML−2T−2
ρ the density with dimension ML−3
η the dynamic fluid viscosity with dimension ML−1T−1
r the radius of the pipe with dimension LThere are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be 
  
    
      
        
          π
          
            1
          
        
        =
        
          
            
              m
              ˙
            
          
        
        
          /
        
        η
        r
      
    
    {\displaystyle \pi _{1}={\dot {m}}/\eta r}
   and 
  
    
      
        
          π
          
            2
          
        
        =
        
          p
          
            
              x
            
          
        
        ρ
        
          r
          
            5
          
        
        
          /
        
        
          
            
              
                m
                ˙
              
            
          
          
            2
          
        
      
    
    {\displaystyle \pi _{2}=p_{\mathrm {x} }\rho r^{5}/{\dot {m}}^{2}}
   and we may express the dimensional equation as

  
    
      
        C
        =
        
          π
          
            1
          
        
        
          π
          
            2
          
          
            a
          
        
        =
        
          (
          
            
              
                
                  m
                  ˙
                
              
              
                η
                r
              
            
          
          )
        
        
          
            (
            
              
                
                  
                    p
                    
                      
                        x
                      
                    
                  
                  ρ
                  
                    r
                    
                      5
                    
                  
                
                
                  
                    
                      
                        m
                        ˙
                      
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
      
    
    {\displaystyle C=\pi _{1}\pi _{2}^{a}=\left({\frac {\dot {m}}{\eta r}}\right)\left({\frac {p_{\mathrm {x} }\rho r^{5}}{{\dot {m}}^{2}}}\right)^{a}}
  where C and a are undetermined constants. If we draw a distinction between inertial mass with dimension 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{\text{i}}}
   and quantity of matter with dimension 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{\text{m}}}
  , then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

  
    
      
        C
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              ρ
              
                r
                
                  4
                
              
            
            
              η
              
                
                  
                    m
                    ˙
                  
                
              
            
          
        
      
    
    {\displaystyle C={\frac {p_{\mathrm {x} }\rho r^{4}}{\eta {\dot {m}}}}}
  where now only C is an undetermined constant (found to be equal to 
  
    
      
        π
        
          /
        
        8
      
    
    {\displaystyle \pi /8}
   by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements (a) and (b) he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.
Huntley's concept of directed length dimensions however has some serious limitations:

It does not deal well with vector equations involving the cross product,
nor does it handle well the use of angles as physical variables.It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the ""symmetry"" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of ""symmetry"" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?
Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.


=== Siano's extension: orientational analysis ===
Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin (x, y) = (0, 0) at a speed v and angle θ above the x-axis, with the force of gravity directed along the negative y-axis. It is desired to find the range R, at which point the mass returns to the x-axis. Conventional analysis will yield the dimensionless variable π = R g/v2, but offers no insight into the relationship between R and θ.
Siano (1985-I, 1985-II) has suggested that the directed dimensions of Huntley be replaced by using orientational symbols 1x 1y 1z to denote vector directions, and an orientationless symbol 10. Thus, Huntley's Lx becomes L 1x with L specifying the dimension of length, and 1x specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own.  Along with the requirement that 1i−1 = 1i, the following multiplication table for the orientation symbols results:

  
    
      
        
          
            
              
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  
                    1
                    
                      z
                    
                  
                
              
            
            
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
            
            
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
            
            
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
            
            
              
                
                  
                    1
                    
                      z
                    
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{c|cccc}&\mathbf {1_{0}} &\mathbf {1_{\text{x}}} &\mathbf {1_{\text{y}}} &\mathbf {1_{\text{z}}} \\\hline \mathbf {1_{0}} &1_{0}&1_{\text{x}}&1_{\text{y}}&1_{\text{z}}\\\mathbf {1_{\text{x}}} &1_{\text{x}}&1_{0}&1_{\text{z}}&1_{\text{y}}\\\mathbf {1_{\text{y}}} &1_{\text{y}}&1_{\text{z}}&1_{0}&1_{\text{x}}\\\mathbf {1_{\text{z}}} &1_{\text{z}}&1_{\text{y}}&1_{\text{x}}&1_{0}\end{array}}}
  Note that the orientational symbols form a group (the Klein four-group or ""Viergruppe""). In this system, scalars always have the same orientation as the identity element, independent of the ""symmetry of the problem"".  Physical quantities that are vectors have the orientation expected:  a force or a velocity in the z-direction has the orientation of 1z.  For angles, consider an angle θ that lies in the z-plane.  Form a right triangle in the z-plane with θ being one of the acute angles.  The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y.  Since (using ~ to indicate orientational equivalence) tan(θ) = θ + ... ~ 1y/1x we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable.  Analogous reasoning forces the conclusion that sin(θ) has orientation 1z while cos(θ) has orientation 10.  These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form a cos(θ) + b sin(θ), where a and b are real scalars. Note that an expression such as 
  
    
      
        sin
        ⁡
        (
        θ
        +
        π
        
          /
        
        2
        )
        =
        cos
        ⁡
        (
        θ
        )
      
    
    {\displaystyle \sin(\theta +\pi /2)=\cos(\theta )}
   is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:

  
    
      
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            +
            b
            
            
              1
              
                z
              
            
          
          )
        
        =
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            b
            
            
              1
              
                z
              
            
          
          )
        
        +
        sin
        ⁡
        
          (
          
            b
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            a
            
            
              1
              
                z
              
            
          
          )
        
        ,
      
    
    {\displaystyle \sin \left(a\,1_{\text{z}}+b\,1_{\text{z}}\right)=\sin \left(a\,1_{\text{z}})\cos(b\,1_{\text{z}}\right)+\sin \left(b\,1_{\text{z}})\cos(a\,1_{\text{z}}\right),}
  which for 
  
    
      
        a
        =
        θ
      
    
    {\displaystyle a=\theta }
   and 
  
    
      
        b
        =
        π
        
          /
        
        2
      
    
    {\displaystyle b=\pi /2}
   yields 
  
    
      
        sin
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        +
        [
        π
        
          /
        
        2
        ]
        
        
          1
          
            z
          
        
        )
        =
        
          1
          
            z
          
        
        cos
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        )
      
    
    {\displaystyle \sin(\theta \,1_{\text{z}}+[\pi /2]\,1_{\text{z}})=1_{\text{z}}\cos(\theta \,1_{\text{z}})}
  . Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is 
  
    
      
        
          1
          
            0
          
        
      
    
    {\displaystyle 1_{0}}
  .
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems.  In this approach one sets up the dimensional equation and solves it as far as one can.  If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral.  This puts it into ""normal form"".  The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension 1z and the range of the projectile R will be of the form:

  
    
      
        R
        =
        
          g
          
            a
          
        
        
        
          v
          
            b
          
        
        
        
          θ
          
            c
          
        
        
           which means 
        
        
          
            L
          
        
        
        
          1
          
            
              x
            
          
        
        ∼
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                  
                    1
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
        
          1
          
            
              z
            
          
          
            c
          
        
        .
        
      
    
    {\displaystyle R=g^{a}\,v^{b}\,\theta ^{c}{\text{ which means }}{\mathsf {L}}\,1_{\mathrm {x} }\sim \left({\frac {{\mathsf {L}}\,1_{\text{y}}}{{\mathsf {T}}^{2}}}\right)^{a}\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{b}\,1_{\mathsf {z}}^{c}.\,}
  Dimensional homogeneity will now correctly yield a = −1 and b = 2, and orientational homogeneity requires that 
  
    
      
        
          1
          
            x
          
        
        
          /
        
        (
        
          1
          
            y
          
          
            a
          
        
        
          1
          
            z
          
          
            c
          
        
        )
        =
        
          1
          
            z
          
          
            c
            +
            1
          
        
        =
        1
      
    
    {\displaystyle 1_{x}/(1_{y}^{a}1_{z}^{c})=1_{z}^{c+1}=1}
  . In other words, that c must be an odd integer. In fact the required function of theta will be sin(θ)cos(θ) which is a series consisting of odd powers of θ.
It is seen that the Taylor series of sin(θ) and cos(θ) are orientationally homogeneous using the above multiplication table, while expressions like cos(θ) + sin(θ) and exp(θ) are not, and are (correctly) deemed unphysical.
Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.


== Dimensionless concepts ==


=== Constants ===

The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
   in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation.  Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity.  This observation can allow one to sometimes make ""back of the envelope"" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.


=== Formalisms ===
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
   ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on ""dimensional grounds"" that the non-analytical part of the free energy per lattice site should be 
  
    
      
        ∼
        1
        
          /
        
        
          ξ
          
            d
          
        
      
    
    {\displaystyle \sim 1/\xi ^{d}}
   where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: c, ħ, and G, in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants ħ, c, and G (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit 
  
    
      
        c
        →
        ∞
      
    
    {\displaystyle c\rightarrow \infty }
  ,  
  
    
      
        ℏ
        →
        0
      
    
    {\displaystyle \hbar \rightarrow 0}
   and 
  
    
      
        G
        →
        0
      
    
    {\displaystyle G\rightarrow 0}
  . In problems involving a gravitational field the latter limit should be taken such that the field stays finite.


== Dimensional equivalences ==
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.


=== SI units ===


=== Natural units ===

If c = ħ = 1, where c is the speed of light and ħ is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length L, mass M and time T can be expressed (dimensionally) as a power of energy E, because length, mass and time can be expressed using speed v, action S, and energy E:

  
    
      
        M
        =
        
          
            E
            
              v
              
                2
              
            
          
        
        ,
        
        L
        =
        
          
            
              S
              v
            
            E
          
        
        ,
        
        t
        =
        
          
            S
            E
          
        
      
    
    {\displaystyle M={\frac {E}{v^{2}}},\quad L={\frac {Sv}{E}},\quad t={\frac {S}{E}}}
  though speed and action are dimensionless (v = c = 1 and S = ħ = 1) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:

  
    
      
        
          
            
              E
            
          
          
            n
          
        
        =
        
          
            
              M
            
          
          
            p
          
        
        
          
            
              L
            
          
          
            q
          
        
        
          
            
              T
            
          
          
            r
          
        
        =
        
          
            
              E
            
          
          
            p
            −
            q
            −
            r
          
        
      
    
    {\displaystyle {\mathsf {E}}^{n}={\mathsf {M}}^{p}{\mathsf {L}}^{q}{\mathsf {T}}^{r}={\mathsf {E}}^{p-q-r}}
  This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge e though other choices are possible.


== See also ==
Buckingham π theorem
Dimensionless numbers in fluid mechanics
Fermi estimate — used to teach dimensional analysis
Rayleigh's method of dimensional analysis
Similitude (model) — an application of dimensional analysis
System of measurement


=== Related areas of math ===
Covariance and contravariance of vectors
Exterior algebra
Geometric algebra
Quantity calculus


=== Programming languages ===
Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML,  and later in F#. There are implementations for Haskell, OCaml, and Rust, Python, and a code checker for Fortran.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.


== Notes ==


== References ==
Barenblatt, G. I. (1996), Scaling, Self-Similarity, and Intermediate Asymptotics, Cambridge, UK: Cambridge University Press, ISBN 978-0-521-43522-2
Bhaskar, R.; Nigam, Anil (1990), ""Qualitative Physics Using Dimensional Analysis"", Artificial Intelligence, 45 (1–2): 73–111, doi:10.1016/0004-3702(90)90038-2
Bhaskar, R.; Nigam, Anil (1991), ""Qualitative Explanations of Red Giant Formation"", The Astrophysical Journal, 372: 592–6, Bibcode:1991ApJ...372..592B, doi:10.1086/170003
Boucher; Alves (1960), ""Dimensionless Numbers"", Chemical Engineering Progress, 55: 55–64
Bridgman, P. W. (1922), Dimensional Analysis, Yale University Press, ISBN 978-0-548-91029-0
Buckingham, Edgar (1914), ""On Physically Similar Systems: Illustrations of the Use of Dimensional Analysis"", Physical Review, 4 (4): 345–376, Bibcode:1914PhRv....4..345B, doi:10.1103/PhysRev.4.345, hdl:10338.dmlcz/101743
Drobot, S. (1953–1954), ""On the foundations of dimensional analysis"" (PDF), Studia Mathematica, 14: 84–99, doi:10.4064/sm-14-1-84-99
Gibbings, J.C. (2011), Dimensional Analysis, Springer, ISBN 978-1-84996-316-9
Hart, George W. (1994), ""The theory of dimensioned matrices"",  in Lewis, John G. (ed.), Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, SIAM, pp. 186–190, ISBN 978-0-89871-336-7 As postscript
Hart, George W. (1995), Multidimensional Analysis: Algebras and Systems for Science and Engineering, Springer-Verlag, ISBN 978-0-387-94417-3
Huntley, H. E. (1967), Dimensional Analysis, Dover, LOC 67-17978
Klinkenberg, A. (1955), ""Dimensional systems and systems of units in physics with special reference to chemical engineering: Part I. The principles according to which dimensional systems and systems of units are constructed"", Chemical Engineering Science, 4 (3): 130–140, 167–177, doi:10.1016/0009-2509(55)80004-8
Langhaar, Henry L. (1951), Dimensional Analysis and Theory of Models, Wiley, ISBN 978-0-88275-682-0
Mendez, P.F.; Ordóñez, F. (September 2005), ""Scaling Laws From Statistical Data and Dimensional Analysis"", Journal of Applied Mechanics, 72 (5): 648–657, Bibcode:2005JAM....72..648M, CiteSeerX 10.1.1.422.610, doi:10.1115/1.1943434
Moody, L. F. (1944), ""Friction Factors for Pipe Flow"", Transactions of the American Society of Mechanical Engineers, 66 (671)
Murphy, N. F. (1949), ""Dimensional Analysis"", Bulletin of the Virginia Polytechnic Institute, 42 (6)
Perry, J. H.;  et al. (1944), ""Standard System of Nomenclature for Chemical Engineering Unit Operations"", Transactions of the American Institute of Chemical Engineers, 40 (251)
Pesic, Peter (2005), Sky in a Bottle, MIT Press, pp. 227–8, ISBN 978-0-262-16234-0
Petty, G. W. (2001), ""Automated computation and consistency checking of physical dimensions and units in scientific programs"", Software – Practice and Experience, 31 (11): 1067–76, doi:10.1002/spe.401, S2CID 206506776
Porter, Alfred W. (1933), The Method of Dimensions (3rd ed.), Methuen
J. W. Strutt (3rd Baron Rayleigh) (1915), ""The Principle of Similitude"", Nature, 95 (2368): 66–8, Bibcode:1915Natur..95...66R, doi:10.1038/095066c0
Siano, Donald (1985), ""Orientational Analysis – A Supplement to Dimensional Analysis – I"", Journal of the Franklin Institute, 320 (6): 267–283, doi:10.1016/0016-0032(85)90031-6
Siano, Donald (1985), ""Orientational Analysis, Tensor Analysis and The Group Properties of the SI Supplementary Units – II"", Journal of the Franklin Institute, 320 (6): 285–302, doi:10.1016/0016-0032(85)90032-8
Silberberg, I. H.; McKetta, J. J. Jr. (1953), ""Learning How to Use Dimensional Analysis"", Petroleum Refiner, 32 (4): 5, (5): 147, (6): 101, (7): 129
Van Driest, E. R. (March 1946), ""On Dimensional Analysis and the Presentation of Data in Fluid Flow Problems"", Journal of Applied Mechanics, 68 (A–34)
Whitney, H. (1968), ""The Mathematics of Physical Quantities, Parts I and II"", American Mathematical Monthly, 75 (2): 115–138, 227–256, doi:10.2307/2315883, JSTOR 2315883
Vignaux, GA (1992),  Erickson, Gary J.; Neudorfer, Paul O. (eds.), Dimensional Analysis in Data Modelling, Kluwer Academic, ISBN 978-0-7923-2031-9
Kasprzak, Wacław; Lysik, Bertold; Rybaczuk, Marek (1990), Dimensional Analysis in the Identification of Mathematical Models, World Scientific, ISBN 978-981-02-0304-7


== External links ==
List of dimensions for variety of physical quantities
Unicalc Live web calculator doing units conversion by dimensional analysis
A C++ implementation of compile-time dimensional analysis in the Boost open-source libraries
Buckingham’s pi-theorem
Quantity System calculator for units conversion based on dimensional approach
Units, quantities, and fundamental constants project dimensional analysis maps
Bowley, Roger (2009). ""[ ] Dimensional Analysis"". Sixty Symbols. Brady Haran for the University of Nottingham.
Dureisseix, David (2019). An introduction to dimensional analysis (lecture). INSA Lyon.


=== Converting units ===
Unicalc Live web calculator doing units conversion by dimensional analysis
Math Skills Review
U.S. EPA tutorial
A Discussion of Units
Short Guide to Unit Conversions
Canceling Units Lesson
Chapter 11: Behavior of Gases Chemistry: Concepts and Applications, Denton Independent School District
Air Dispersion Modeling Conversions and Formulas
www.gnu.org/software/units  free program, very practical","pandas(index=135, _1=135, text='in engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. the conversion of units from one dimensional unit to another is often easier within the metric or si system than in others, due to the regular 10-base in all units. dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. for example, asking whether a kilogram is larger than an hour is meaningless. any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. it also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation. the concept of physical dimension, and of dimensional analysis, was introduced by joseph fourier in 1822.   == concrete numbers and base units == many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof. a set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. for example, units for length and time are normally chosen as base units. units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units. sometimes the names of units obscure the fact that they are derived units. for example, a newton (n) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). the newton is defined as 1 n = 1 kg⋅m⋅s−2. unicalc live web calculator doing units conversion by dimensional analysis math skills review u.s. epa tutorial a discussion of units short guide to unit conversions canceling units lesson chapter 11: behavior of gases chemistry: concepts and applications, denton independent school district air dispersion modeling conversions and formulas www.gnu.org/software/units  free program, very practical')"
136,"Hydrology (from Greek: ὕδωρ, ""hýdōr"" meaning ""water"" and λόγος, ""lógos"" meaning ""study"") is the scientific study of the movement, distribution, and management of water on Earth and other planets, including the water cycle, water resources, and environmental watershed sustainability. A practitioner of hydrology is called a hydrologist. Hydrologists are scientists studying earth or environmental science, civil or environmental engineering, and physical geography. Using various analytical methods and scientific techniques, they collect and analyze data to help solve water related problems such as environmental preservation, natural disasters, and water management.Hydrology subdivides into surface water hydrology, groundwater hydrology (hydrogeology), and marine hydrology. Domains of hydrology include hydrometeorology, surface hydrology, hydrogeology, drainage-basin management, and water quality, where water plays the central role.
Oceanography and meteorology are not included because water is only one of many important aspects within those fields.
Hydrological research can inform environmental engineering, policy, and planning.


== Branches ==
Chemical hydrology is the study of the chemical characteristics of water.
Ecohydrology is the study of interactions between organisms and the hydrologic cycle.
Hydrogeology is the study of the presence and movement of groundwater.
Hydrogeochemistry is the study of how terrestrial water dissolves minerals weathering and this effect on water chemistry.
Hydroinformatics is the adaptation of information technology to hydrology and water resources applications.
Hydrometeorology is the study of the transfer of water and energy between land and water body surfaces and the lower atmosphere.
Isotope hydrology is the study of the isotopic signatures of water.
Surface hydrology is the study of hydrologic processes that operate at or near Earth's surface.
Drainage basin management covers water storage, in the form of reservoirs, and floods protection.
Water quality includes the chemistry of water in rivers and lakes, both of pollutants and natural solutes.


== Applications ==
Calculation of rainfall.
Calculating surface runoff and precipitation.
Determining the water balance of a region.
Determining the agricultural water balance.
Designing riparian restoration projects.
Mitigating and predicting flood, landslide and drought risk.
Real-time flood forecasting and flood warning.
Designing irrigation schemes and managing agricultural productivity.
Part of the hazard module in catastrophe modeling.
Providing drinking water.
Designing dams for water supply or hydroelectric power generation.
Designing bridges.
Designing sewers and urban drainage system.
Analyzing the impacts of antecedent moisture on sanitary sewer systems.
Predicting geomorphologic changes, such as erosion or sedimentation.
Assessing the impacts of natural and anthropogenic environmental change on water resources.
Assessing contaminant transport risk and establishing environmental policy guidelines.
Estimating the water resource potential of river basins.


== History ==

Hydrology has been a subject of investigation and engineering for millennia. For example, about 4000 BC the Nile was dammed to improve agricultural productivity of previously barren lands. Mesopotamian towns were protected from flooding with high earthen walls. Aqueducts were built by the Greeks and Ancient Romans, while the history of China shows they built irrigation and flood control works. The ancient Sinhalese used hydrology to build complex irrigation works in Sri Lanka, also known for invention of the Valve Pit which allowed construction of large reservoirs, anicuts and canals which still function.
Marcus Vitruvius, in the first century BC, described a philosophical theory of the hydrologic cycle, in which precipitation falling in the mountains infiltrated the Earth's surface and led to streams and springs in the lowlands. With the adoption of a more scientific approach, Leonardo da Vinci and Bernard Palissy independently reached an accurate representation of the hydrologic cycle. It was not until the 17th century that hydrologic variables began to be quantified.
Pioneers of the modern science of hydrology include Pierre Perrault, Edme Mariotte and Edmund Halley. By measuring rainfall, runoff, and drainage area, Perrault showed that rainfall was sufficient to account for the flow of the Seine. Mariotte combined velocity and river cross-section measurements to obtain a discharge, again in the Seine. Halley showed that the evaporation from the Mediterranean Sea was sufficient to account for the outflow of rivers flowing into the sea.Advances in the 18th century included the Bernoulli piezometer and Bernoulli's equation, by Daniel Bernoulli, and the Pitot tube, by Henri Pitot. The 19th century saw development in groundwater hydrology, including Darcy's law, the Dupuit-Thiem well formula, and Hagen-Poiseuille's capillary flow equation.
Rational analyses began to replace empiricism in the 20th century, while governmental agencies began their own hydrological research programs. Of particular importance were Leroy Sherman's unit hydrograph, the infiltration theory of Robert E. Horton, and C.V. Theis' aquifer test/equation describing well hydraulics.
Since the 1950s, hydrology has been approached with a more theoretical basis than in the past, facilitated by advances in the physical understanding of hydrological processes and by the advent of computers and especially geographic information systems (GIS). (See also GIS and hydrology)


== Themes ==

The central theme of hydrology is that water circulates throughout the Earth through different pathways and at different rates. The most vivid image of this is in the evaporation of water from the ocean, which forms clouds. These clouds drift over the land and produce rain. The rainwater flows into lakes, rivers, or aquifers. The water in lakes, rivers, and aquifers then either evaporates back to the atmosphere or eventually flows back to the ocean, completing a cycle. Water changes its state of being several times throughout this cycle.
The areas of research within hydrology concern the movement of water between its various states, or within a given state, or simply quantifying the amounts in these states in a given region. Parts of hydrology concern developing methods for directly measuring these flows or amounts of water, while others concern modeling these processes either for scientific knowledge or for making a prediction in practical applications.


=== Groundwater ===

Ground water is water beneath Earth's surface, often pumped for drinking water. Groundwater hydrology (hydrogeology) considers quantifying groundwater flow and solute transport. Problems in describing the saturated zone include the characterization of aquifers in terms of flow direction, groundwater pressure and, by inference, groundwater depth (see: aquifer test). Measurements here can be made using a piezometer. Aquifers are also described in terms of hydraulic conductivity, storativity and transmissivity. There are a number of geophysical methods for characterising aquifers. There are also problems in characterising the vadose zone (unsaturated zone).


=== Infiltration ===

Infiltration is the process by which water enters the soil. Some of the water is absorbed, and the rest percolates down to the water table. The infiltration capacity, the maximum rate at which the soil can absorb water, depends on several factors. The layer that is already saturated provides a resistance that is proportional to its thickness, while that plus the depth of water above the soil provides the driving force (hydraulic head). Dry soil can allow rapid infiltration by capillary action; this force diminishes as the soil becomes wet. Compaction reduces the porosity and the pore sizes. Surface cover increases capacity by retarding runoff, reducing compaction and other processes. Higher temperatures reduce viscosity, increasing infiltration.


=== Soil moisture ===

Soil moisture can be measured in various ways; by capacitance probe, time domain reflectometer or Tensiometer. Other methods include solute sampling and geophysical methods.


=== Surface water flow ===

Hydrology considers quantifying surface water flow and solute transport, although the treatment of flows in large rivers is sometimes considered as a distinct topic of hydraulics or hydrodynamics. Surface water flow can include flow both in recognizable river channels and otherwise. Methods for measuring flow once the water has reached a river include the stream gauge (see: discharge), and tracer techniques. Other topics include chemical transport as part of surface water, sediment transport and erosion.
One of the important areas of hydrology is the interchange between rivers and aquifers. Groundwater/surface water interactions in streams and aquifers can be complex and the direction of net water flux (into surface water or into the aquifer) may vary spatially along a stream channel and over time at any particular location, depending on the relationship between stream stage and groundwater levels.


=== Precipitation and evaporation ===

In some considerations, hydrology is thought of as starting at the land-atmosphere boundary and so it is important to have adequate knowledge of both precipitation and evaporation. Precipitation can be measured in various ways: disdrometer for precipitation characteristics at a fine time scale; radar for cloud properties, rain rate estimation, hail and snow detection; rain gauge for routine accurate measurements of rain and snowfall; satellite for rainy area identification, rain rate estimation, land-cover/land-use, and soil moisture, for example.
Evaporation is an important part of the water cycle. It is partly affected by humidity, which can be measured by a sling psychrometer. It is also affected by the presence of snow, hail, and ice and can relate to dew, mist and fog. Hydrology considers evaporation of various forms: from water surfaces; as transpiration
from plant surfaces in natural and agronomic ecosystems. Direct measurement of evaporation can be obtained using Simon's evaporation pan.
Detailed studies of evaporation involve boundary layer considerations as well as momentum, heat flux, and energy budgets.


=== Remote sensing ===

Remote sensing of hydrologic processes can provide information on locations where in situ sensors may be unavailable or sparse. It also enables observations over large spatial extents. Many of the variables constituting the terrestrial water balance, for example surface water storage, soil moisture, precipitation, evapotranspiration, and snow and ice, are measurable using remote sensing at various spatial-temporal resolutions and accuracies. Sources of remote sensing include land-based sensors, airborne sensors and satellite sensors which can capture microwave, thermal and near-infrared data or use lidar, for example.


=== Water quality ===

In hydrology, studies of water quality concern organic and inorganic compounds, and both dissolved and sediment material. In addition, water quality is affected by the interaction of dissolved oxygen with organic material and various chemical transformations that may take place. Measurements of water quality may involve either in-situ methods, in which analyses take place on-site, often automatically, and laboratory-based analyses and may include microbiological analysis.


=== Integrating measurement and modelling ===
Budget analyses
Parameter estimation
Scaling in time and space
Data assimilation
Quality control of data – see for example Double mass analysis


=== Prediction ===
Observations of hydrologic processes are used to make predictions of the future behavior of hydrologic systems (water flow, water quality). One of the major current concerns in hydrologic research is ""Prediction in Ungauged Basins"" (PUB), i.e. in basins where no or only very few data exist.


=== Statistical hydrology ===
By analyzing the statistical properties of hydrologic records, such as rainfall or river flow, hydrologists can estimate future hydrologic phenomena. When making assessments of how often relatively rare events will occur, analyses are made in terms of the return period of such events. Other quantities of interest include the average flow in a river, in a year or by season.
These estimates are important for engineers and economists so that proper risk analysis can be performed to influence investment decisions in future infrastructure and to determine the yield reliability characteristics of water supply systems. Statistical information is utilized to formulate operating rules for large dams forming part of systems which include agricultural, industrial and residential demands.


=== Modeling ===

Hydrological models are simplified, conceptual representations of a part of the hydrologic cycle. They are primarily used for hydrological prediction and for understanding hydrological processes, within the general field of scientific modeling. Two major types of hydrological models can be distinguished:
Models based on data. These models are black box systems, using mathematical and statistical concepts to link a certain input (for instance rainfall) to the model output (for instance runoff). Commonly used techniques are regression, transfer functions, and system identification. The simplest of these models may be linear models, but it is common to deploy non-linear components to represent some general aspects of a catchment's response without going deeply into the real physical processes involved. An example of such an aspect is the well-known behavior that a catchment will respond much more quickly and strongly when it is already wet than when it is dry.
Models based on process descriptions. These models try to represent the physical processes observed in the real world. Typically, such models contain representations of surface runoff, subsurface flow, evapotranspiration, and channel flow, but they can be far more complicated. Within this category, models can be divided into conceptual and deterministic. Conceptual models link simplified representations of the hydrological processes in an area, whereas deterministic models seek to resolve as much of the physics of a system as possible. These models can be subdivided into single-event models and continuous simulation models.Recent research in hydrological modeling tries to have a more global approach to the understanding of the behavior of hydrologic systems to make better predictions and to face the major challenges in water resources management.


=== Transport ===

Water movement is a significant means by which other material, such as soil, gravel, boulders or pollutants, are transported from place to place. Initial input to receiving waters may arise from a point source discharge or a line source or area source, such as surface runoff. Since the 1960s rather complex mathematical models have been developed, facilitated by the availability of high-speed computers. The most common pollutant classes analyzed are nutrients, pesticides, total dissolved solids and sediment.


== Organizations ==


=== Intergovernmental organizations ===
International Hydrological Programme (IHP)


=== International research bodies ===
International Water Management Institute (IWMI)
UN-IHE Delft Institute for Water Education


=== National research bodies ===
Centre for Ecology and Hydrology – UK
Centre for Water Science, Cranfield University, UK
eawag – aquatic research, ETH Zürich, Switzerland
Institute of Hydrology, Albert-Ludwigs-University of Freiburg, Germany
United States Geological Survey – Water Resources of the United States
NOAA's National Weather Service – Office of Hydrologic Development, USA
US Army Corps of Engineers Hydrologic Engineering Center, USA
Hydrologic Research Center, USA
NOAA Economics and Social Sciences, United States
University of Oklahoma Center for Natural Hazards and Disasters Research, USA
National Hydrology Research Centre, Canada
National Institute of Hydrology, India


=== National and international societies ===
American Institute of Hydrology (AIH)
Geological Society of America (GSA) – Hydrogeology Division
American Geophysical Union (AGU) – Hydrology Section
National Ground Water Association (NGWA)
American Water Resources Association
Consortium of Universities for the Advancement of Hydrologic Science, Inc. (CUAHSI)
International Association of Hydrological Sciences (IAHS)
Statistics in Hydrology Working Group (subgroup of IAHS)
German Hydrological Society (DHG: Deutsche Hydrologische Gesellschaft)
Italian Hydrological Society (SII-IHS) – http://www.sii-ihs.it
Nordic Association for Hydrology
British Hydrological Society
Russian Geographical Society (Moscow Center) – Hydrology Commission
International Association for Environmental Hydrology
International Association of Hydrogeologists
Society of Hydrologists and Meteorologists – Nepal 


=== Basin- and catchment-wide overviews ===
Connected Waters Initiative, University of New South Wales – Investigating and raising awareness of groundwater and water resource issues in Australia
Murray Darling Basin Initiative, Department of Environment and Heritage, Australia


== Research journals ==
International Journal of Hydrology Science and Technology
Hydrological Processes, ISSN 1099-1085 (electronic) 0885-6087 (paper), John Wiley & Sons
Hydrology Research, ISSN 0029-1277, IWA Publishing  (formerly Nordic Hydrology)
Journal of Hydroinformatics, ISSN 1464-7141, IWA Publishing
Journal of Hydrologic Engineering, ISSN 0733-9496, ASCE Publication
Journal of Hydrology
Water Research
Water Resources Research
Hydrological Sciences Journal - Journal of the International Association of Hydrological Sciences (IAHS) ISSN 0262-6667 (Print), ISSN 2150-3435 (Online)


== See also ==

Other water-related fieldsOceanography is the more general study of water in the oceans and estuaries.
Meteorology is the more general study of the atmosphere and of weather, including precipitation as snow and rainfall.
Limnology is the study of lakes, rivers and wetlands ecosystems. It covers the biological, chemical, physical, geological, and other attributes of all inland waters (running and standing waters, both fresh and saline, natural or man-made).
Water resources are sources of water that are useful or potentially useful. Hydrology studies the availability of those resources, but usually not their uses.


== References ==


== Further reading ==


== External links ==
Hydrology.nl – Portal to international hydrology and water resources
Decision tree to choose an uncertainty method for hydrological and hydraulic modelling
Experimental Hydrology Wiki","pandas(index=136, _1=136, text='hydrology (from greek: ὕδωρ, ""hýdōr"" meaning ""water"" and λόγος, ""lógos"" meaning ""study"") is the scientific study of the movement, distribution, and management of water on earth and other planets, including the water cycle, water resources, and environmental watershed sustainability. a practitioner of hydrology is called a hydrologist. hydrologists are scientists studying earth or environmental science, civil or environmental engineering, and physical geography. using various analytical methods and scientific techniques, they collect and analyze data to help solve water related problems such as environmental preservation, natural disasters, and water management.hydrology subdivides into surface water hydrology, groundwater hydrology (hydrogeology), and marine hydrology. domains of hydrology include hydrometeorology, surface hydrology, hydrogeology, drainage-basin management, and water quality, where water plays the central role. oceanography and meteorology are not included because water is only one of many important aspects within those fields. hydrological research can inform environmental engineering, policy, and planning.   == branches == chemical hydrology is the study of the chemical characteristics of water. ecohydrology is the study of interactions between organisms and the hydrologic cycle. hydrogeology is the study of the presence and movement of groundwater. hydrogeochemistry is the study of how terrestrial water dissolves minerals weathering and this effect on water chemistry. hydroinformatics is the adaptation of information technology to hydrology and water resources applications. hydrometeorology is the study of the transfer of water and energy between land and water body surfaces and the lower atmosphere. isotope hydrology is the study of the isotopic signatures of water. surface hydrology is the study of hydrologic processes that operate at or near earth\'s surface. drainage basin management covers water storage, in the form of reservoirs, and floods protection. water quality includes the chemistry of water in rivers and lakes, both of pollutants and natural solutes.   == applications == calculation of rainfall. calculating surface runoff and precipitation. determining the water balance of a region. determining the agricultural water balance. designing riparian restoration projects. mitigating and predicting flood, landslide and drought risk. real-time flood forecasting and flood warning. designing irrigation schemes and managing agricultural productivity. part of the hazard module in catastrophe modeling. providing drinking water. designing dams for water supply or hydroelectric power generation. designing bridges. designing sewers and urban drainage system. analyzing the impacts of antecedent moisture on sanitary sewer systems. predicting geomorphologic changes, such as erosion or sedimentation. assessing the impacts of natural and anthropogenic environmental change on water resources. assessing contaminant transport risk and establishing environmental policy guidelines. estimating the water resource potential of river basins.   == history ==  hydrology has been a subject of investigation and engineering for millennia. for example, about 4000 bc the nile was dammed to improve agricultural productivity of previously barren lands. mesopotamian towns were protected from flooding with high earthen walls. aqueducts were built by the greeks and ancient romans, while the history of china shows they built irrigation and flood control works. the ancient sinhalese used hydrology to build complex irrigation works in sri lanka, also known for invention of the valve pit which allowed construction of large reservoirs, anicuts and canals which still function. marcus vitruvius, in the first century bc, described a philosophical theory of the hydrologic cycle, in which precipitation falling in the mountains infiltrated the earth\'s surface and led to streams and springs in the lowlands. with the adoption of a more scientific approach, leonardo da vinci and bernard palissy independently reached an accurate representation of the hydrologic cycle. it was not until the 17th century that hydrologic variables began to be quantified. pioneers of the modern science of hydrology include pierre perrault, edme mariotte and edmund halley. by measuring rainfall, runoff, and drainage area, perrault showed that rainfall was sufficient to account for the flow of the seine. mariotte combined velocity and river cross-section measurements to obtain a discharge, again in the seine. halley showed that the evaporation from the mediterranean sea was sufficient to account for the outflow of rivers flowing into the sea.advances in the 18th century included the bernoulli piezometer and bernoulli\'s equation, by daniel bernoulli, and the pitot tube, by henri pitot. the 19th century saw development in groundwater hydrology, including darcy\'s law, the dupuit-thiem well formula, and hagen-poiseuille\'s capillary flow equation. rational analyses began to replace empiricism in the 20th century, while governmental agencies began their own hydrological research programs. of particular importance were leroy sherman\'s unit hydrograph, the infiltration theory of robert e. horton, and c.v. theis\' aquifer test/equation describing well hydraulics. since the 1950s, hydrology has been approached with a more theoretical basis than in the past, facilitated by advances in the physical understanding of hydrological processes and by the advent of computers and especially geographic information systems (gis). (see also gis and hydrology)   == themes ==  the central theme of hydrology is that water circulates throughout the earth through different pathways and at different rates. the most vivid image of this is in the evaporation of water from the ocean, which forms clouds. these clouds drift over the land and produce rain. the rainwater flows into lakes, rivers, or aquifers. the water in lakes, rivers, and aquifers then either evaporates back to the atmosphere or eventually flows back to the ocean, completing a cycle. water changes its state of being several times throughout this cycle. the areas of research within hydrology concern the movement of water between its various states, or within a given state, or simply quantifying the amounts in these states in a given region. parts of hydrology concern developing methods for directly measuring these flows or amounts of water, while others concern modeling these processes either for scientific knowledge or for making a prediction in practical applications. connected waters initiative, university of new south wales – investigating and raising awareness of groundwater and water resource issues in australia murray darling basin initiative, department of environment and heritage, australia   == research journals == international journal of hydrology science and technology hydrological processes, issn 1099-1085 (electronic) 0885-6087 (paper), john wiley & sons hydrology research, issn 0029-1277, iwa publishing  (formerly nordic hydrology) journal of hydroinformatics, issn 1464-7141, iwa publishing journal of hydrologic engineering, issn 0733-9496, asce publication journal of hydrology water research water resources research hydrological sciences journal - journal of the international association of hydrological sciences (iahs) issn 0262-6667 (print), issn 2150-3435 (online)   == see also ==  other water-related fieldsoceanography is the more general study of water in the oceans and estuaries. meteorology is the more general study of the atmosphere and of weather, including precipitation as snow and rainfall. limnology is the study of lakes, rivers and wetlands ecosystems. it covers the biological, chemical, physical, geological, and other attributes of all inland waters (running and standing waters, both fresh and saline, natural or man-made). water resources are sources of water that are useful or potentially useful. hydrology studies the availability of those resources, but usually not their uses.   == references ==   == further reading ==   == external links == hydrology.nl – portal to international hydrology and water resources decision tree to choose an uncertainty method for hydrological and hydraulic modelling experimental hydrology wiki')"
137,"Mold health issues are potentially harmful effects of molds (US usage; British English ""moulds"") and their mycotoxins. However, recent research has shown these adverse health effects stem not just from molds, but also other microbial agents and biotoxins associated with dampness, mold, and water-damaged buildings, such as gram-negative bacteria that produce endotoxins, and actinomycetes and their associated exotoxins.Molds and many related microbial agents are ubiquitous in the biosphere, and mold spores are a common component of household and workplace dust. While the vast majority of molds are not hazardous to humans, a few are known to be, and reaction to molds can vary between individuals, from relatively minor allergic reactions through to severe multi-system inflammatory effects, and even neurological problems and death. The United States Centers for Disease Control and Prevention (CDC) reported in its June 2006 report, 'Mold Prevention Strategies and Possible Health Effects in the Aftermath of Hurricanes and Major Floods,' that ""excessive exposure to mold-contaminated materials can cause adverse health effects in susceptible persons regardless of the type of mold or the extent of contamination."" Mold spores and associated toxins can cause harm primarily via inhalation, ingestion, and contact. In abnormally high quantities, they can present especially hazardous health risks to humans after prolonged exposure, with three generally accepted mechanisms of harm and a fourth probable mechanism:

Allergic reactions
Fungal infection (mycosis)
Toxicity (poisoning by mycotoxins)
Innate immune activation.


== Health effects ==
Studies have shown that people who are atopic (sensitive), already suffer from allergies, asthma, or compromised immune systems and occupy damp or moldy buildings are at an increased risk of health problems such as inflammatory responses to mold spores, metabolites such as mycotoxins, and other components. Other problems are respiratory and/or immune system responses including respiratory symptoms, respiratory infections, exacerbation of asthma, and rarely hypersensitivity pneumonitis, allergic alveolitis, chronic rhinosinusitis and allergic fungal sinusitis. A person's reaction to mold depends on their sensitivity and other health conditions, the amount of mold present, length of exposure, and the type of mold or mold products.
Some molds also produce mycotoxins, which can pose serious health risks to humans and animals. The colloquial term ""toxic mold"" (or more accurately, toxigenic mold) refers to molds that produce mycotoxins known to harm humans, such as Stachybotrys chartarum, not to all molds.  Exposure to high levels of mycotoxins can lead to neurological problems and, in some cases, death. Prolonged exposure, e.g., daily workplace exposure, can be particularly harmful.
The five most common genera of indoor molds are Cladosporium, Penicillium, Aspergillus, Alternaria, and Trichoderma.
Damp environments that allow mold to grow can also allow the proliferation of bacteria and release volatile organic compounds.


=== Symptoms of mold exposure ===
Symptoms of mold exposure can include:
Nasal and sinus congestion, runny nose
Respiratory problems, such as wheezing and difficulty breathing, chest tightness
Cough
Throat irritation
Sneezing / Sneezing fits


==== Health effects linking to asthma ====
Adverse respiratory health effects are associated with occupancy in buildings with moisture and mold damage. Infants may develop respiratory symptoms due to exposure to a specific type of fungal mold, called Penicillium. Signs that an infant may have mold-related respiratory problems include (but are not limited to) a persistent cough and wheeze. Increased exposure increases the probability of developing respiratory symptoms during their first year of life.  Studies have shown that a correlation exists between the probability of developing asthma and increased exposure to Penicillium. The levels are deemed ‘no mold’  to ‘low level’, from ‘low’  to ‘intermediate’, and ‘intermediate’  to ‘high’.Mold exposures have a variety of health effects depending on the person. Some people are more sensitive to mold than others. Exposure to mold can cause several health issues such as; throat irritation, nasal stuffiness, eye irritation, cough, and wheezing, as well as skin irritation in some cases.  Exposure to mold may also cause heightened sensitivity depending on the time and nature of exposure.  People at higher risk for mold allergies are people with chronic lung illnesses and weak immune systems, which can often result in more severe reactions when exposed to mold.There has been sufficient evidence that damp indoor environments are correlated with upper respiratory tract symptoms such as coughing, and wheezing in people with asthma.


=== Flood-specific mold health effects ===
Among children and adolescents, the most common health effect post-flooding was lower respiratory tract symptoms, though there was a lack of association with measurements of total fungi. Another study found that these respiratory symptoms were positively associated with exposure to water damaged homes, exposure included being inside without participating in clean up.  Despite lower respiratory effects among all children, there was a significant difference in health outcomes between children with pre-existing conditions and children without.  Children with pre-existing conditions were at greater risk that can likely be attributed to the greater disruption of care in the face of flooding and natural disaster.Although mold is the primary focus post flooding for residents, the effects of dampness alone must also be considered. According to the Institute of Medicine, there is a significant association between dampness in the home and wheeze, cough, and upper respiratory symptoms. A later analysis determined that 30% to 50% of asthma-related health outcomes are associated with not only mold, but also dampness in buildings. Another health effect associated with dampness and mold is Sick Building Syndrome (SBS), which is defined by manifestations of symptomatic illness as a result of poor indoor air quality and pollutant exposures. Signs of potentially illness-causing buildings include condensation on the windows, high humidity in the bathrooms, a moldy odor, or water leakage.While there is a proven correlation between mold exposure and the development of upper and lower respiratory syndromes, there are still fewer incidences of negative health effects than one might expect.  Barbeau and colleagues suggested that studies do not show a greater impact from mold exposure for several reasons: 1) the types of health effects are not severe and are therefore not caught; 2) people whose homes have flooded find alternative housing to prevent exposure; 3) self-selection, the healthier people participated in mold clean-up and were less likely to get sick; 4) exposures were time-limited as result of remediation efforts and; 5) the lack of access to health care post-flooding may result in fewer illnesses being discovered and reported for their association with mold. There are also certain notable scientific limitations in studying the exposure effects of dampness and molds on individuals because there are currently no known biomarkers that can prove that a person was exclusively exposed to molds. Thus, it is currently impossible to prove correlation between mold exposure and symptoms.


== Mold-associated conditions ==
Health problems associated with high levels of airborne mold spores include allergic reactions, asthma episodes, irritations of the eye, nose and throat, sinus congestion, and other respiratory problems. Several studies and reviews have suggested that childhood exposure to dampness and mold might contribute to the development of asthma. For example, residents of homes with mold are at an elevated risk for both respiratory infections and bronchitis. When mold spores are inhaled by an immunocompromised individual, some mold spores may begin to grow on living tissue, attaching to cells along the respiratory tract and causing further problems.  Generally, when this occurs, the illness is an epiphenomenon and not the primary pathology. Also, mold may produce mycotoxins, either before or after exposure to humans, potentially causing toxicity.


=== Fungal infection ===

A serious health threat from mold exposure for immunocompromised individuals is systemic fungal infection (systemic mycosis). Immunocompromised individuals exposed to high levels of mold, or individuals with chronic exposure may become infected. Sinuses and digestive tract infections are most common; lung and skin infections are also possible. Mycotoxins may or may not be produced by the invading mold.
Dermatophytes are the parasitic fungi that cause skin infections such as athlete's foot and tinea cruris. Most dermatophyte fungi take the form of mold, as opposed to a yeast, with an appearance (when cultured) that is similar to other molds.
Opportunistic infection by molds such as Talaromyces marneffei and Aspergillus fumigatus is a common cause of illness and death among immunocompromised people, including people with AIDS or asthma.


=== Mold-induced hypersensitivity ===
The most common form of hypersensitivity is caused by the direct exposure to inhaled mold spores that can be dead or alive or hyphal fragments which can lead to allergic asthma or allergic rhinitis.  The most common effects are rhinorrhea (runny nose), watery eyes, coughing and asthma attacks.  Another form of hypersensitivity is hypersensitivity pneumonitis.  Exposure can occur at home, at work or in other settings.   It is predicted that about 5% of people have some airway symptoms due to allergic reactions to molds in their lifetimes.Hypersensitivity may also be a reaction toward an established fungal infection in allergic bronchopulmonary aspergillosis.


=== Mycotoxin toxicity ===

Molds excrete toxic compounds called mycotoxins, secondary metabolites produced by fungi under certain environmental conditions. These environmental conditions affect the production of mycotoxins at the transcription level. Temperature, water activity and pH, strongly influence mycotoxin biosynthesis by increasing the level of transcription within the fungal spore. It has also been found that low levels of fungicides can boost mycotoxin synthesis. Certain mycotoxins can be harmful or lethal to humans and animals when exposure is high enough.The common house mold, Trichoderma longibrachiatum, produces small toxic peptides containing amino acids not found in common proteins, like alpha-aminoisobutyric acid, called trilongins (up to 10% w/w). Their toxicity is due to absorption into cells and production of nano-channels that obstruct vital ion channels that ferry potassium and sodium ions across the cell membrane. This affects in the cells action potential profile, as seen in cardiomyocytes, pneumocytes and neurons leading to conduction defects. Trilongins are highly resistant to heat and antimicrobials making primary prevention the only management option.Extreme exposure to very high levels of mycotoxins can lead to neurological problems and, in some cases, death; fortunately, such exposures rarely to never occur in normal exposure scenarios, even in residences with serious mold problems. Prolonged exposure, such as daily workplace exposure, can be particularly harmful.It is thought that all molds may produce mycotoxins, and thus all molds may be potentially toxic if large enough quantities are ingested, or the human becomes exposed to extreme quantities of mold. Mycotoxins are not produced all the time, but only under specific growing conditions. Mycotoxins are harmful or lethal to humans and animals only when exposure is high enough.Mycotoxins can be found on the mold spore and mold fragments, and therefore they can also be found on the substrate upon which the mold grows.  Routes of entry for these insults can include ingestion, dermal exposure, and inhalation.
Aflatoxin is an example of a mycotoxin. It is a cancer-causing poison produced by certain fungi in or on foods and feeds, especially in field corn and peanuts.Toxic effects from mold were thought to be the result of exposure to the mycotoxins of some mold species, such as Stachybotrys chartarum. In 1927, Ismailson, a Soviet scientist, noted a form of mycotoxicosis in employees in a binder twine factory. In the 1940s, ""Stachybotryotoxicosis"" was identified in Ukraine as a new disease in humans in close contact with moldy hay, including inhalation of the associated dust, which caused, among other symptoms, a ""haemorrhagic exúdate"". Following cases of pulmonary hemorrhage in infants in Cleveland, Ohio in 1993–94, several related studies suggested a causal relationship between exposure to S. chartarum and the disease. An anonymous panel from within the CDC revisited the cases and argued that the link was not proven. Subsequent studies with mice and rats exposed to S. chartarum and associated mycotoxins showed that pulmonary hemorrhage could occur, suggesting the link is plausible. The American Academy of Pediatrics also found the link plausible, and subsequent analysis and case studies with humans have further noted the association. As well, a 1987 report by the United States Army Medical Research Institute of Infectious Diseases suggested that the effects of ""trichothecene mycotoxins are more than 10 times greater via inhalation than via intravenous exposure."" The presumed mechanism of action is that Stachybotrys produces a compound, stachylysin, which is a hemolysin that disintegrates (lyses) red blood cells.


=== Innate immune activation ===
The health hazards produced by mold have been associated with sick building syndrome (SBS), but previously, controversy existed around whether studies had sufficiently demonstrated that indoor exposures to these common organisms posed a significant threat. In 1986, a study noted an airborne outbreak of toxicosis from trichothecenes associated with Stachybotrys atra in a Chicago house affecting a family including their maid; symptoms included diarrhea, headaches, fatigue, dermatitis, malaise, and severe leg pains, which resolved following remediation of the mold contamination. This study drew attention to how mycotoxins in indoor environments might impact health. In the early 2000s, several small studies concluded that individuals with significant dampness and mold exposure displayed cognitive and neurological deficits on par with mild-to-moderate traumatic brain injury along with immunological changes. These studies were criticised for their methodologies, such as by not showing a possible mechanism of action for the harm, and not controlling for the possibility of malingering by mold-exposed individuals involved in litigation, although the associated critiques were also problematic. Researchers also contested whether the amount of spores that could be breathed in by humans would be sufficient to cause a toxic effect and that no association between spore counts and adverse health effects existed. However, when also considering spore fragments (that have more surface area to carry mycotoxins) as well as whole spores, the amount of exposure was estimated to be 1,000x to 1,000,000x higher than previously thought. Moreover, inhalational exposure ""provides a pathway to the central nervous system along the olfactory and trigeminal nerve axons in the nasal sensory epithelium that bypasses the blood–brain barrier.""Despite these early studies, a 2003 position paper by the American College of Occupational and Environmental Medicine (ACOEM) claimed the link between mold and building-related symptoms was ""weak and unproven"". Further to this, the Center for Legal Policy at The Manhattan Institute paid $40,000 to Globaltox (later, Veritox), a company associated with two of the same authors of the ACOEM paper, to produce a ""lay translation"" of their study that would be ""more assessable ... to judges"". This lay paper claimed that the notion that human health could be adversely affected by inhaled molds or their toxins was ""junk science"" and was referenced in legal cases in the United States to deny related legal claims. The United States Chamber of Commerce, the largest lobbying group in the U.S., also promoted this paper (and is still doing so as of 2020).A 2006 position paper by the American Academy of Allergy, Asthma, and Immunology (AAAAI) maintained a similarly sceptical position as the ACOEM paper in denying that mold in indoor environments could cause severe effects. 
In 2008, the United States Government Accountability Office published a report on indoor mold, reviewing the literature to date and acknowledging the possibility of immune and toxic effects, while calling for further research. By 2009, the WHO noted a strong association between dampness and inflammatory responses, while also recognising that ""synergistic interactions among microbial agents"" might make it ""difficult to detect and implicate specific exposures in the causation of damp building-associated adverse health effects."" Gram-negative bacteria, which create endotoxins known to produce inflammatory responses, might also be partly responsible, as might actinomycetes and their associated exotoxins. While it may be difficult to determine the relative contributions of the molds, bacteria, and dust particles to which people are exposed, studies clearly show that such combinations activate stronger, synergistic immune responses than predicted by adding the effects of the individual stimuli.Later in 2009, a carefully controlled, seminal study published by Kilburn demonstrated that mold exposure was associated with extensive adverse effects on multiple physiological systems. He compared the responses of 105 mold-exposed individuals to those of 202 unexposed controls, as well as those of 100 people exposed to a wide variety of chemicals. Rather than asking people how they felt, Kilburn measured physiological and mental function. He found highly significant abnormalities in the responses of mold-exposed individuals compared to controls on 12 of the 14 physiological functions quantified and 10 of the 13 psychological tests administered. These abnormalities included extreme problems with balance correlated with cerebellar abnormalities, decreased grip strength, impaired color vision, impaired visual fields, slowed reaction times, slowed performance on perceptual motor tasks, impaired memory, and decreased performance on problem-solving tasks as well as a variety of respiratory problems. Chemical-exposed individuals had similar abnormalities.Like many researchers, Kilburn attributed the adverse effects of mold exposure primarily to the toxins some molds produce. Currently available data suggest mold’s effects are more the result of chronic activation of the immune system, leading to chronic inflammation. Such immune activation does not necessarily require toxin exposure; rather, exposure to non-toxic mold stimuli or fungal skeletal elements is sufficient to activate immune responses and trigger inflammation. Nineteen innate-immune pattern-recognition receptors have been identified that recognize common components of fungal cell walls or fungal RNA/DNA, activating inflammatory responses. Studies exposing mice to controlled doses of S. chartarum spores show activation of the innate immune system, along with neural, cognitive, and emotional dysfunction, even when mycotoxins were removed and mice were exposed only to spore skeletal elements.In 2012, a ten-year longitudinal study found that dampness and mold seemed to be an underlying cause of sick-building syndrome. A 2018 review of 16 associated studies, including Kilburn's, concluded that people exposed to molds and mycotoxins had ""symptoms affecting multiple organs, including the lungs, musculoskeletal system, as well as the central and peripheral nervous systems"" and also noted that such exposure has now been implicated in the pathogenesis of autism-spectrum disorder. An in vitro study of human neurological system cells showed damage caused by inflammatory and immune processes (along with disruption of the blood-brain barrier) in response to mycotoxins at exposure levels that would be expected in water-damaged buildings. Ex vivo studies of human peripheral blood mononuclear cells showed inflammatory and innate immune responses upon exposure to specific molds and mycotoxins, such as S. chartarum (and an associated mycotoxin, Satratoxin G) and various strains of Aspergillus. Furthermore, children living in water-damaged homes show systemic inflammation, immune activation, and probably poorer cognitive function, too. Tellingly, many of the affected biomarkers, hormones, and pathways in individuals affected by inhaled mycotoxins are consistent with studies of ingested mycotoxins, such as trichothecene exposure. Two studies using volumetric MRIs have suggested that affected individuals display structural changes in the brain, associated with the forebrain parenchymal, cortical gray matter, pallidum volumes, and the caudate nucleus. Correlating to this, Dale Bredesen, a neurodegenerative researcher, has noted a subtype of Alzheimer's Disease associated with this chronic inflammatory response, calling it an ""unrecognized – and treatable – epidemic"". Arnold R. Eiser, professor emeritus of medicine at Drexel University College of Medicine, has suggested that environmental factors such as dampness and mold may be a contributing factor for why Finland has the highest death rate from dementia in the world.The WHO estimates the prevalence of significant dampness and mold in buildings to be at least 20%, while other estimates of US homes suggest a prevalence as high as 47%. Sleeping disorders are also associated with exposure to dampness and mold, consistent with the decrease in α-melanocyte stimulating hormone (α-MSH) associated with this syndrome. Patients may also present with psychological symptoms given the neuroinflammatory markers and growth factors involved.


== Exposure sources and prevention ==
The primary sources of mold exposure are from the indoor air in buildings with substantial mold growth and the ingestion of food with mold growths.


=== Air ===

While mold and related microbial agents can be found both inside and outside, specific factors can lead to significantly higher levels of these microbes, creating a potential health hazard. Several notable factors are water damage in buildings, the use of building materials which provide a suitable substrate and source of food to amplify mold growth, relative humidity, and energy-efficient building designs, which can prevent proper circulation of outside air and create a unique ecology in the built environment. A common issue with mold hazards in the household can be the placement of furniture, resulting in a lack of ventilation of the nearby wall. The simplest method of avoiding mold in a home so affected is to move the furniture in question.
Prevention of mold exposure and its ensuing health issues begins with the prevention of mold growth in the first place by avoiding a mold-supporting environment. Extensive flooding and water damage can support extensive mold growth. Following hurricanes, homes with greater flood damage, especially those with more than 3 feet (0.91 m) of indoor flooding, demonstrated far higher levels of mold growth compared with homes with little or no flooding.It is useful to perform an assessment of the location and extent of the mold hazard in a structure. Various practices of remediation can be followed to mitigate mold issues in buildings, the most important of which is to reduce moisture levels. Removal of affected materials after the source of moisture has been reduced and/or eliminated may be necessary, as some materials cannot be remediated. Thus, the concept of mold growth, assessment, and remediation is essential in preventing health issues arising due to the presence of dampness and mold.
Molds may excrete liquids or low-volatility gases, but the concentrations are so low that frequently they cannot be detected even with sensitive analytical sampling techniques. Sometimes, these by-products are detectable by odor, in which case they are referred to as ""ergonomic odors"", meaning the odors are noticeable but do not indicate toxicologically significant exposures.


=== Food ===

Molds that are often found on meat and poultry include members of the genera Alternaria, Aspergillus, Botrytis, Cladosporium, Fusarium, Geotrichum, Mortierella, Mucor, Neurospora, Paecilomyces, Penicillium, and Rhizopus.  Grain crops in particular incur considerable losses both in field and storage due to pathogens, post-harvest spoilage, and insect damage. A number of common microfungi are important agents of post-harvest spoilage, notably members of the genera Aspergillus, Fusarium, and Penicillium. A number of these produce mycotoxins (soluble, non-volatile toxins produced by a range of microfungi that demonstrate specific and potent toxic properties on human and animal cells) that can render foods unfit for consumption. When ingested, inhaled, or absorbed through skin, mycotoxins may cause or contribute to a range of effects from reduced appetite and general malaise to acute illness or death in rare cases. Mycotoxins may also contribute to cancer. Dietary exposure to the mycotoxin aflatoxin B1, commonly produced by growth of the fungus Aspergillus flavus on improperly stored ground nuts in many areas of the developing world, is known to independently (and synergistically with Hepatitis B virus) induce liver cancer. Mycotoxin-contaminated grain and other food products have a significant impact on human and animal health globally. According to the World Health Organization, roughly 25% of the world's food may be contaminated by mycotoxins.Prevention of mold exposure from food is generally to consume food that has no mold growths on it. Also, mold growth in the first place can be prevented by the same concept of mold growth, assessment, and remediation that prevents air exposure. Also, it is especially useful to clean the inside of the refrigerator and to ensure dishcloths, towels, sponges, and mops are clean.Ruminants are considered to have increased resistance to some mycotoxins, presumably due to the superior mycotoxin-degrading capabilities of their gut microbiota.  The passage of mycotoxins through the food chain may also have important consequences on human health. For example, in China in December 2011, high levels of carcinogen aflatoxin M1 in Mengniu brand milk were found to be associated with the consumption of mold-contaminated feed by dairy cattle.


=== Bedding ===
Bacteria, fungi, allergens, and particle-bound semi-volatile organic compounds (SVOCs) can all be found in bedding and pillows with possible consequences for human health given the high amount of exposure each day. Over 47 species of fungi have been identified in pillows, although the typical range of species found in a single pillow varied between four and sixteen. Compared to feather pillows, synthetic pillows typically display a slightly greater variety of fungal species and significantly higher levels of β‐(1,3)‐glucan, which can cause inflammatory responses. The authors concluded that these and related results suggest feather bedding might be a more appropriate choice for asthmatics than synthetics. Some newer bedding products incorporate silver nanoparticles due to their antibacterial, antifungal, and antiviral properties; however, the long-term safety of this additional exposure to these nanoparticles is relatively unknown, and a conservative approach to the use of these products is recommended.


=== Flooding ===
Flooding in houses causes a unique opportunity for mold growth, which may be attributed to adverse health effects in people exposed to the mold, especially children and adolescents. In a study on the health effects of mold exposure after hurricanes Katrina and Rita, the predominant types of mold were Aspergillus, Penicillium, and Cladosporium with indoor spore counts ranging from 6,142 – 735,123 spores m−3. Molds isolated following flooding were different from mold previously reported for non-water damaged homes in the area. Further research found that homes with greater than three feet of indoor flooding demonstrated significantly higher levels of mold than those with little or no flooding.


==== Mitigation ====
Recommended strategies to prevent mold include avoiding mold-contamination; utilization of environmental controls; the use of personal protective equipment (PPE), including skin and eye protection and respiratory protection; and environmental controls such as ventilation and suppression of dust.  When mold cannot be prevented, the CDC recommends clean-up protocol including first taking emergency action to stop water intrusion. Second, they recommend determining the extent of water damage and mold contamination. And third, they recommend planning remediation activities such as establishing containment and protection for workers and occupants; eliminating water or moisture sources if possible; decontaminating or removing damaged materials and drying any wet materials; evaluating whether space has been successfully remediated; and reassembling the space to control sources of moisture.


== History ==
In 1698, the physician Sir John Floyer published the first edition of A Treatise of the Asthma, the first English testbook on the malady. In it, he describes how dampness and mold could trigger an asthmatic attack, specifically, ""damp houses and fenny [boggy] countries"". He also writes of an asthmatic ""who fell into a violent fit by going into a Wine-Cellar"", presumably due to the ""fumes"" in the air.In the 1930s, mold was identified as the cause behind the mysterious deaths of farm animals in Russia and other countries. Stachybotrys chartarum was found growing on the wet grain used for animal feed.  Illness and death also occurred in humans when starving peasants ate large quantities of rotten food grains and cereals heavily overgrown with the Stachybotrys mold.In the 1970s, building construction techniques changed in response to changing economic realities, including the energy crisis. As a result, homes, and buildings became more airtight.  Also, cheaper materials such as drywall came into common use. The newer building materials reduced the drying potential of the structures, making moisture problems more prevalent.  This combination of increased moisture and suitable substrates contributed to increased mold growth inside buildings.In April 2015, a tree fell on the home of Lucy Wicks, a Federal Liberal MP of Australia, causing water damage. Afterwards, she fell ill with neurological symptoms, environmental sensitivity, and fatigue, and was eventually diagnosed with chronic inflammatory response syndrome (CIRS) (see Innate immune activation). This led to a federal inquiry into biotoxin-related illnesses in 2018. It is currently unclear if the Department of Health have made any changes to the Australian healthcare system in response to the Inquiry and subsequent report. But the Australian Government formally responded to the recommendations made by the inquiry in March 2020, and the National Construction Code was updated in 2019 to implement higher standards regarding condensation management in building design.As of 2020, the US Centers for Disease Control and Prevention (CDC) are yet to acknowledge or warn the public of the multi-system, chronic inflammatory, and innate immune system effects that can occur from exposure to dampness, mold, and water-damaged buildings.Today, the US Food and Drug Administration and the agriculture industry closely monitor mold and mycotoxin levels in grains and foodstuffs to keep the contamination of animal feed and human food supplies below specific levels. In 2005, Diamond Pet Foods, a US pet food manufacturer, experienced a significant rise in the number of corn shipments containing elevated levels of aflatoxin. This mold toxin eventually made it into the pet food supply, and dozens of dogs and cats died before the company was forced to recall affected products.


== Litigation ==
In 1999, an Austin, Texas, woman was awarded $32 million when she sued her insurer over mold damage in her 22-room mansion.In 2001, a jury awarded a couple and their eight-year-old son $2.7 million, plus attorney's fees and costs, in a toxic mold-related personal injury lawsuit against the owners and managers of their apartment in Sacramento, California.In 2002, the U.S. International Trade Commission reported that, according to one estimate, US insurers paid over $3 billion in mold-related lawsuits, more than double the previous year's total.In 2003, there were over 10,000 mold-related lawsuits pending in US state courts according to the Insurance Information Institute. Most were filed in states with high humidity, but suits were on the rise in other states as well. Notably that year, The Tonight Show co-host Ed McMahon received $7.2 million from insurers and others to settle his lawsuit alleging that toxic mold in his Beverly Hills home made him and his wife ill and killed their dog. Also that year, environmental activist Erin Brockovich received settlements of $430,000 from two parties and an undisclosed amount from a third party to settle her lawsuit alleging toxic mold in her Agoura Hills, California, home.By 2004, many mold litigation settlements were for amounts well past $100,000.In 2005, the U.S. International Trade Commission reported that toxic mold showed signs of being the ""new asbestos"" in terms of claims paid.In 2006, a Manhattan Beach, California, family received a $22.6 million settlement in a toxic mold case. The family had asserted that moldy lumber had caused severe medical problems in their child.  That same year, Hilton Hotels received $25 million in settlement of its lawsuit over mold growth in the Hilton Hawaiian Village's Kalia Tower.In 2010, a jury awarded $1.2 million in damages in a lawsuit against a landlord for neglecting to repair a mold-infested house in Laguna Beach, California. The lawsuit asserted that a child in the home suffered from severe respiratory problems for several years as a result of the mold.In 2011, in North Pocono, Pennsylvania, a jury awarded two homeowners $4.3 million in a toxic mold verdict.In 2012, a key appellate court in Manhattan found a consensus in the scientific literature for a causal relationship between the presence of mold and resultant illness.


== Policy ==
While there is a national policy in the United States regarding mold, each state is responsible for independently creating and administering its policy. For example, following Hurricane Harvey, the governor of Texas sought to expand the emergency response to allow mold-remediation companies to come from out of state.Under Section 17920.3 of the California Health & Safety Code, visible mold growth and dampness of habitable rooms can be sufficient for a home to be declared as a ""substandard building"", offering legal recourse for those affected, such as tenants in moldy apartments. Notably, California recognizes by law not only that dampness and mold exacerbate asthma but can cause its development.


== See also ==

Environmental engineering
Environmental health
Occupational asthma
Occupational safety and health


== References ==


== Further reading ==


== External links ==
CDC.gov Mold
US EPA: Mold Information – U.S. Environmental Protection Agency
US EPA: EPA Publication #402-K-02-003 ""A Brief Guide to Mold, Moisture, and Your Home""
NIBS: Whole Building Design Guide: Air Decontamination
NPIC: Mold Pest Control Information – National Pesticide Information Center
Mycotoxins in grains and the food supply:
indianacrop.org
cropwatch.unl.edu
agbiopubs.sdstate.edu (PDF)
Dunning, Brian (November 24, 2015). ""Skeptoid #494: Black Mold: Peril or Prosaic?"". Skeptoid.","pandas(index=137, _1=137, text='mold health issues are potentially harmful effects of molds (us usage; british english ""moulds"") and their mycotoxins. however, recent research has shown these adverse health effects stem not just from molds, but also other microbial agents and biotoxins associated with dampness, mold, and water-damaged buildings, such as gram-negative bacteria that produce endotoxins, and actinomycetes and their associated exotoxins.molds and many related microbial agents are ubiquitous in the biosphere, and mold spores are a common component of household and workplace dust. while the vast majority of molds are not hazardous to humans, a few are known to be, and reaction to molds can vary between individuals, from relatively minor allergic reactions through to severe multi-system inflammatory effects, and even neurological problems and death. the united states centers for disease control and prevention (cdc) reported in its june 2006 report, \'mold prevention strategies and possible health effects in the aftermath of hurricanes and major floods,\' that ""excessive exposure to mold-contaminated materials can cause adverse health effects in susceptible persons regardless of the type of mold or the extent of contamination."" mold spores and associated toxins can cause harm primarily via inhalation, ingestion, and contact. in abnormally high quantities, they can present especially hazardous health risks to humans after prolonged exposure, with three generally accepted mechanisms of harm and a fourth probable mechanism:  allergic reactions fungal infection (mycosis) toxicity (poisoning by mycotoxins) innate immune activation.   == health effects == studies have shown that people who are atopic (sensitive), already suffer from allergies, asthma, or compromised immune systems and occupy damp or moldy buildings are at an increased risk of health problems such as inflammatory responses to mold spores, metabolites such as mycotoxins, and other components. other problems are respiratory and/or immune system responses including respiratory symptoms, respiratory infections, exacerbation of asthma, and rarely hypersensitivity pneumonitis, allergic alveolitis, chronic rhinosinusitis and allergic fungal sinusitis. a person\'s reaction to mold depends on their sensitivity and other health conditions, the amount of mold present, length of exposure, and the type of mold or mold products. some molds also produce mycotoxins, which can pose serious health risks to humans and animals. the colloquial term ""toxic mold"" (or more accurately, toxigenic mold) refers to molds that produce mycotoxins known to harm humans, such as stachybotrys chartarum, not to all molds.  exposure to high levels of mycotoxins can lead to neurological problems and, in some cases, death. prolonged exposure, e.g., daily workplace exposure, can be particularly harmful. the five most common genera of indoor molds are cladosporium, penicillium, aspergillus, alternaria, and trichoderma. damp environments that allow mold to grow can also allow the proliferation of bacteria and release volatile organic compounds. recommended strategies to prevent mold include avoiding mold-contamination; utilization of environmental controls; the use of personal protective equipment (ppe), including skin and eye protection and respiratory protection; and environmental controls such as ventilation and suppression of dust.  when mold cannot be prevented, the cdc recommends clean-up protocol including first taking emergency action to stop water intrusion. second, they recommend determining the extent of water damage and mold contamination. and third, they recommend planning remediation activities such as establishing containment and protection for workers and occupants; eliminating water or moisture sources if possible; decontaminating or removing damaged materials and drying any wet materials; evaluating whether space has been successfully remediated; and reassembling the space to control sources of moisture.   == history == in 1698, the physician sir john floyer published the first edition of a treatise of the asthma, the first english testbook on the malady. in it, he describes how dampness and mold could trigger an asthmatic attack, specifically, ""damp houses and fenny [boggy] countries"". he also writes of an asthmatic ""who fell into a violent fit by going into a wine-cellar"", presumably due to the ""fumes"" in the air.in the 1930s, mold was identified as the cause behind the mysterious deaths of farm animals in russia and other countries. stachybotrys chartarum was found growing on the wet grain used for animal feed.  illness and death also occurred in humans when starving peasants ate large quantities of rotten food grains and cereals heavily overgrown with the stachybotrys mold.in the 1970s, building construction techniques changed in response to changing economic realities, including the energy crisis. as a result, homes, and buildings became more airtight.  also, cheaper materials such as drywall came into common use. the newer building materials reduced the drying potential of the structures, making moisture problems more prevalent.  this combination of increased moisture and suitable substrates contributed to increased mold growth inside buildings.in april 2015, a tree fell on the home of lucy wicks, a federal liberal mp of australia, causing water damage. afterwards, she fell ill with neurological symptoms, environmental sensitivity, and fatigue, and was eventually diagnosed with chronic inflammatory response syndrome (cirs) (see innate immune activation). this led to a federal inquiry into biotoxin-related illnesses in 2018. it is currently unclear if the department of health have made any changes to the australian healthcare system in response to the inquiry and subsequent report. but the australian government formally responded to the recommendations made by the inquiry in march 2020, and the national construction code was updated in 2019 to implement higher standards regarding condensation management in building design.as of 2020, the us centers for disease control and prevention (cdc) are yet to acknowledge or warn the public of the multi-system, chronic inflammatory, and innate immune system effects that can occur from exposure to dampness, mold, and water-damaged buildings.today, the us food and drug administration and the agriculture industry closely monitor mold and mycotoxin levels in grains and foodstuffs to keep the contamination of animal feed and human food supplies below specific levels. in 2005, diamond pet foods, a us pet food manufacturer, experienced a significant rise in the number of corn shipments containing elevated levels of aflatoxin. this mold toxin eventually made it into the pet food supply, and dozens of dogs and cats died before the company was forced to recall affected products.   == litigation == in 1999, an austin, texas, woman was awarded $32 million when she sued her insurer over mold damage in her 22-room mansion.in 2001, a jury awarded a couple and their eight-year-old son $2.7 million, plus attorney\'s fees and costs, in a toxic mold-related personal injury lawsuit against the owners and managers of their apartment in sacramento, california.in 2002, the u.s. international trade commission reported that, according to one estimate, us insurers paid over $3 billion in mold-related lawsuits, more than double the previous year\'s total.in 2003, there were over 10,000 mold-related lawsuits pending in us state courts according to the insurance information institute. most were filed in states with high humidity, but suits were on the rise in other states as well. notably that year, the tonight show co-host ed mcmahon received $7.2 million from insurers and others to settle his lawsuit alleging that toxic mold in his beverly hills home made him and his wife ill and killed their dog. also that year, environmental activist erin brockovich received settlements of $430,000 from two parties and an undisclosed amount from a third party to settle her lawsuit alleging toxic mold in her agoura hills, california, home.by 2004, many mold litigation settlements were for amounts well past $100,000.in 2005, the u.s. international trade commission reported that toxic mold showed signs of being the ""new asbestos"" in terms of claims paid.in 2006, a manhattan beach, california, family received a $22.6 million settlement in a toxic mold case. the family had asserted that moldy lumber had caused severe medical problems in their child.  that same year, hilton hotels received $25 million in settlement of its lawsuit over mold growth in the hilton hawaiian village\'s kalia tower.in 2010, a jury awarded $1.2 million in damages in a lawsuit against a landlord for neglecting to repair a mold-infested house in laguna beach, california. the lawsuit asserted that a child in the home suffered from severe respiratory problems for several years as a result of the mold.in 2011, in north pocono, pennsylvania, a jury awarded two homeowners $4.3 million in a toxic mold verdict.in 2012, a key appellate court in manhattan found a consensus in the scientific literature for a causal relationship between the presence of mold and resultant illness.   == policy == while there is a national policy in the united states regarding mold, each state is responsible for independently creating and administering its policy. for example, following hurricane harvey, the governor of texas sought to expand the emergency response to allow mold-remediation companies to come from out of state.under section 17920.3 of the california health & safety code, visible mold growth and dampness of habitable rooms can be sufficient for a home to be declared as a ""substandard building"", offering legal recourse for those affected, such as tenants in moldy apartments. notably, california recognizes by law not only that dampness and mold exacerbate asthma but can cause its development.   == see also ==  environmental engineering environmental health occupational asthma occupational safety and health   == references ==   == further reading ==   == external links == cdc.gov mold us epa: mold information – u.s. environmental protection agency us epa: epa publication #402-k-02-003 ""a brief guide to mold, moisture, and your home"" nibs: whole building design guide: air decontamination npic: mold pest control information – national pesticide information center mycotoxins in grains and the food supply: indianacrop.org cropwatch.unl.edu agbiopubs.sdstate.edu (pdf) dunning, brian (november 24, 2015). ""skeptoid #494: black mold: peril or prosaic?"". skeptoid.')"
138,"The design of experiments (DOE, DOX, or experimental design) is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variation. The term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation.
In its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as ""input variables"" or ""predictor variables."" The change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as ""output variables"" or ""response variables."" The experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. Experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources.  There are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment.
Main concerns in experimental design include the establishment of validity, reliability, and replicability. For example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. Related concerns include achieving appropriate levels of statistical power and sensitivity.
Correctly designed experiments advance knowledge in the natural and social sciences and engineering. Other applications include marketing and policy making. The study of the design of experiments is an important topic in metascience.


== History ==


=== Statistical experiments, following Charles S. Peirce ===

A theory of statistical inference was developed by Charles S. Peirce in ""Illustrations of the Logic of Science"" (1877–1878) and  ""A Theory of Probable Inference"" (1883), two publications that emphasized the importance of randomization-based inference in statistics.


==== Randomized experiments ====

Charles S. Peirce randomly assigned volunteers to a blinded, repeated-measures design to evaluate their ability to discriminate weights.
Peirce's experiment inspired other researchers in psychology and education, which developed a research tradition of randomized experiments in laboratories and specialized textbooks in the 1800s.


==== Optimal designs for regression models ====

Charles S. Peirce also contributed the first English-language publication on an optimal design for regression models in 1876. A pioneering optimal design for polynomial regression was suggested by Gergonne in 1815. In 1918, Kirstine Smith published optimal designs for polynomials of degree six (and less).


=== Sequences of experiments ===

The use of a sequence of experiments, where the design of each may depend on the results of previous experiments, including the possible decision to stop experimenting, is within the scope of sequential analysis, a field that was pioneered by Abraham Wald in the context of sequential tests of statistical hypotheses. Herman Chernoff wrote an overview of optimal sequential designs, while adaptive designs have been surveyed by S. Zacks. One specific type of sequential design is the ""two-armed bandit"", generalized to the multi-armed bandit, on which early work was done by Herbert Robbins in 1952.


== Fisher's principles ==
A methodology for designing experiments was proposed by Ronald Fisher, in his innovative books: The Arrangement of Field Experiments (1926) and The Design of Experiments (1935).  Much of his pioneering work dealt with agricultural applications of statistical methods.  As a mundane example, he described how to test the lady tasting tea hypothesis, that a certain lady could distinguish by flavour alone whether the milk or the tea was first placed in the cup. These methods have been broadly adapted in biological, psychological, and agricultural research.
Comparison
In some fields of study it is not possible to have independent measurements to a traceable metrology standard.  Comparisons between treatments are much more valuable and are usually preferable, and often compared against a scientific control or traditional treatment that acts as baseline.Randomization
Random assignment is the process of assigning individuals at random to groups or to different groups in an experiment, so that each individual of the population has the same chance of becoming a participant in the study. The random assignment of individuals to groups (or conditions within a group) distinguishes a rigorous, ""true"" experiment from an observational study or ""quasi-experiment"". There is an extensive body of mathematical theory that explores the consequences of making the allocation of units to treatments by means of some random mechanism (such as tables of random numbers, or the use of randomization devices such as playing cards or dice). Assigning units to treatments at random tends to mitigate confounding, which makes effects due to factors other than the treatment to appear to result from the treatment.The risks associated with random allocation (such as having a serious imbalance in a key characteristic between a treatment group and a control group) are calculable and hence can be managed down to an acceptable level by using enough experimental units. However, if the population is divided into several subpopulations that somehow differ, and the research requires each subpopulation to be equal in size, stratified sampling can be used. In that way, the units in each subpopulation are randomized, but not the whole sample. The results of an experiment can be generalized reliably from the experimental units to a larger statistical population of units only if the experimental units are a random sample from the larger population; the probable error of such an extrapolation depends on the sample size, among other things.Statistical replication
Measurements are usually subject to variation and measurement uncertainty; thus they are repeated and full experiments are replicated to help identify the sources of variation, to better estimate the true effects of treatments, to further strengthen the experiment's reliability and validity, and to add to the existing knowledge of the topic. However, certain conditions must be met before the replication of the experiment is commenced: the original research question has been published in a peer-reviewed journal or widely cited, the researcher is independent of the original experiment, the researcher must first try to replicate the original findings using the original data, and the write-up should state that the study conducted is a replication study that tried to follow the original study as strictly as possible.Blocking
Blocking is the non-random arrangement of experimental units into groups (blocks) consisting of units that are similar to one another. Blocking reduces known but irrelevant sources of variation between units and thus allows greater precision in the estimation of the source of variation under study.Orthogonality
Orthogonality concerns the forms of comparison (contrasts) that can be legitimately and efficiently carried out. Contrasts can be represented by vectors and sets of orthogonal contrasts are uncorrelated and independently distributed if the data are normal. Because of this independence, each orthogonal treatment provides different information to the others. If there are T treatments and T – 1 orthogonal contrasts, all the information that can be captured from the experiment is obtainable from the set of contrasts.Factorial experiments
Use of factorial experiments instead of the one-factor-at-a-time method.  These are efficient at evaluating the effects and possible interactions of several factors (independent variables). Analysis of experiment design is built on the foundation of the analysis of variance, a collection of models that partition the observed variance into components, according to what factors the experiment must estimate or test.


== Example ==

This example of design experiments is attributed to Harold Hotelling, building on examples from Frank Yates. The experiments designed in this example involve combinatorial designs.Weights of eight objects are measured using a pan balance and set of standard weights.  Each weighing measures the weight difference between objects in the left pan and any objects in the right pan by adding calibrated weights to the lighter pan until the balance is in equilibrium. Each measurement has a random error.  The average error is zero; the standard deviations of the probability distribution of the errors is the same number σ on different weighings; errors on different weighings are independent.  Denote the true weights by

  
    
      
        
          θ
          
            1
          
        
        ,
        …
        ,
        
          θ
          
            8
          
        
        .
        
      
    
    {\displaystyle \theta _{1},\dots ,\theta _{8}.\,}
  We consider two different experiments:

Weigh each object in one pan, with the other pan empty.  Let Xi be the measured weight of the object, for i = 1, ..., 8.
Do the eight weighings according to the following schedule and let Yi be the measured difference for i = 1, ..., 8:
  
    
      
        
          
            
              
              
                
                  left pan
                
              
              
                
                  right pan
                
              
            
            
              
                
                  1st weighing:
                
              
              
                1
                 
                2
                 
                3
                 
                4
                 
                5
                 
                6
                 
                7
                 
                8
              
              
                
                  (empty)
                
              
            
            
              
                
                  2nd:
                
              
              
                1
                 
                2
                 
                3
                 
                8
                 
              
              
                4
                 
                5
                 
                6
                 
                7
              
            
            
              
                
                  3rd:
                
              
              
                1
                 
                4
                 
                5
                 
                8
                 
              
              
                2
                 
                3
                 
                6
                 
                7
              
            
            
              
                
                  4th:
                
              
              
                1
                 
                6
                 
                7
                 
                8
                 
              
              
                2
                 
                3
                 
                4
                 
                5
              
            
            
              
                
                  5th:
                
              
              
                2
                 
                4
                 
                6
                 
                8
                 
              
              
                1
                 
                3
                 
                5
                 
                7
              
            
            
              
                
                  6th:
                
              
              
                2
                 
                5
                 
                7
                 
                8
                 
              
              
                1
                 
                3
                 
                4
                 
                6
              
            
            
              
                
                  7th:
                
              
              
                3
                 
                4
                 
                7
                 
                8
                 
              
              
                1
                 
                2
                 
                5
                 
                6
              
            
            
              
                
                  8th:
                
              
              
                3
                 
                5
                 
                6
                 
                8
                 
              
              
                1
                 
                2
                 
                4
                 
                7
              
            
          
        
      
    
    {\displaystyle {\begin{array}{lcc}&{\text{left pan}}&{\text{right pan}}\\\hline {\text{1st weighing:}}&1\ 2\ 3\ 4\ 5\ 6\ 7\ 8&{\text{(empty)}}\\{\text{2nd:}}&1\ 2\ 3\ 8\ &4\ 5\ 6\ 7\\{\text{3rd:}}&1\ 4\ 5\ 8\ &2\ 3\ 6\ 7\\{\text{4th:}}&1\ 6\ 7\ 8\ &2\ 3\ 4\ 5\\{\text{5th:}}&2\ 4\ 6\ 8\ &1\ 3\ 5\ 7\\{\text{6th:}}&2\ 5\ 7\ 8\ &1\ 3\ 4\ 6\\{\text{7th:}}&3\ 4\ 7\ 8\ &1\ 2\ 5\ 6\\{\text{8th:}}&3\ 5\ 6\ 8\ &1\ 2\ 4\ 7\end{array}}}
  Then the estimated value of the weight θ1 is
  
    
      
        
          
            
              
                θ
                ^
              
            
          
          
            1
          
        
        =
        
          
            
              
                Y
                
                  1
                
              
              +
              
                Y
                
                  2
                
              
              +
              
                Y
                
                  3
                
              
              +
              
                Y
                
                  4
                
              
              −
              
                Y
                
                  5
                
              
              −
              
                Y
                
                  6
                
              
              −
              
                Y
                
                  7
                
              
              −
              
                Y
                
                  8
                
              
            
            8
          
        
        .
      
    
    {\displaystyle {\widehat {\theta }}_{1}={\frac {Y_{1}+Y_{2}+Y_{3}+Y_{4}-Y_{5}-Y_{6}-Y_{7}-Y_{8}}{8}}.}
  Similar estimates can be found for the weights of the other items. For example
  
    
      
        
          
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    2
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      +
                      
                        Y
                        
                          2
                        
                      
                      −
                      
                        Y
                        
                          3
                        
                      
                      −
                      
                        Y
                        
                          4
                        
                      
                      +
                      
                        Y
                        
                          5
                        
                      
                      +
                      
                        Y
                        
                          6
                        
                      
                      −
                      
                        Y
                        
                          7
                        
                      
                      −
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    3
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      +
                      
                        Y
                        
                          2
                        
                      
                      −
                      
                        Y
                        
                          3
                        
                      
                      −
                      
                        Y
                        
                          4
                        
                      
                      −
                      
                        Y
                        
                          5
                        
                      
                      −
                      
                        Y
                        
                          6
                        
                      
                      +
                      
                        Y
                        
                          7
                        
                      
                      +
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    4
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      −
                      
                        Y
                        
                          2
                        
                      
                      +
                      
                        Y
                        
                          3
                        
                      
                      −
                      
                        Y
                        
                          4
                        
                      
                      +
                      
                        Y
                        
                          5
                        
                      
                      −
                      
                        Y
                        
                          6
                        
                      
                      +
                      
                        Y
                        
                          7
                        
                      
                      −
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    5
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      −
                      
                        Y
                        
                          2
                        
                      
                      +
                      
                        Y
                        
                          3
                        
                      
                      −
                      
                        Y
                        
                          4
                        
                      
                      −
                      
                        Y
                        
                          5
                        
                      
                      +
                      
                        Y
                        
                          6
                        
                      
                      −
                      
                        Y
                        
                          7
                        
                      
                      +
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    6
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      −
                      
                        Y
                        
                          2
                        
                      
                      −
                      
                        Y
                        
                          3
                        
                      
                      +
                      
                        Y
                        
                          4
                        
                      
                      +
                      
                        Y
                        
                          5
                        
                      
                      −
                      
                        Y
                        
                          6
                        
                      
                      −
                      
                        Y
                        
                          7
                        
                      
                      +
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    7
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      −
                      
                        Y
                        
                          2
                        
                      
                      −
                      
                        Y
                        
                          3
                        
                      
                      +
                      
                        Y
                        
                          4
                        
                      
                      −
                      
                        Y
                        
                          5
                        
                      
                      +
                      
                        Y
                        
                          6
                        
                      
                      +
                      
                        Y
                        
                          7
                        
                      
                      −
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
            
              
                
                  
                    
                      
                        θ
                        ^
                      
                    
                  
                  
                    8
                  
                
              
              
                
                =
                
                  
                    
                      
                        Y
                        
                          1
                        
                      
                      +
                      
                        Y
                        
                          2
                        
                      
                      +
                      
                        Y
                        
                          3
                        
                      
                      +
                      
                        Y
                        
                          4
                        
                      
                      +
                      
                        Y
                        
                          5
                        
                      
                      +
                      
                        Y
                        
                          6
                        
                      
                      +
                      
                        Y
                        
                          7
                        
                      
                      +
                      
                        Y
                        
                          8
                        
                      
                    
                    8
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\widehat {\theta }}_{2}&={\frac {Y_{1}+Y_{2}-Y_{3}-Y_{4}+Y_{5}+Y_{6}-Y_{7}-Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{3}&={\frac {Y_{1}+Y_{2}-Y_{3}-Y_{4}-Y_{5}-Y_{6}+Y_{7}+Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{4}&={\frac {Y_{1}-Y_{2}+Y_{3}-Y_{4}+Y_{5}-Y_{6}+Y_{7}-Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{5}&={\frac {Y_{1}-Y_{2}+Y_{3}-Y_{4}-Y_{5}+Y_{6}-Y_{7}+Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{6}&={\frac {Y_{1}-Y_{2}-Y_{3}+Y_{4}+Y_{5}-Y_{6}-Y_{7}+Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{7}&={\frac {Y_{1}-Y_{2}-Y_{3}+Y_{4}-Y_{5}+Y_{6}+Y_{7}-Y_{8}}{8}}.\\[5pt]{\widehat {\theta }}_{8}&={\frac {Y_{1}+Y_{2}+Y_{3}+Y_{4}+Y_{5}+Y_{6}+Y_{7}+Y_{8}}{8}}.\end{aligned}}}
  The question of design of experiments is: which experiment is better?
The variance of the estimate X1 of θ1 is σ2 if we use the first experiment.  But if we use the second experiment, the variance of the estimate given above is σ2/8.  Thus the second experiment gives us 8 times as much precision for the estimate of a single item, and estimates all items simultaneously, with the same precision. What the second experiment achieves with eight would require 64 weighings if the items are weighed separately. However, note that the estimates for the items obtained in the second experiment have errors that correlate with each other.
Many problems of the design of experiments involve combinatorial designs, as in this example and others.


== Avoiding false positives ==

False positive conclusions, often resulting from the pressure to publish or the author's own confirmation bias, are an inherent hazard in many fields. A good way to prevent biases potentially leading to false positives in the data collection phase is to use a double-blind design. When a double-blind design is used, participants are randomly assigned to experimental groups but the researcher is unaware of what participants belong to which group. Therefore, the researcher can not affect the participants' response to the intervention.  
Experimental designs with undisclosed degrees of freedom are a problem. This can lead to conscious or unconscious ""p-hacking"": trying multiple things until you get the desired result.  It typically involves the manipulation – perhaps unconsciously – of the process of statistical analysis and the degrees of freedom until they return a figure below the p<.05 level of statistical significance. So the design of the experiment should include a clear statement proposing the analyses to be undertaken. P-hacking can be prevented by preregistering researches, in which researchers have to send their data analysis plan to the journal they wish to publish their paper in before they even start their data collection, so no data manipulation is possible (https://osf.io). Another way to prevent this is taking the double-blind design to the data-analysis phase, where the data are sent to a data-analyst unrelated to the research who scrambles up the data so there is no way to know which participants belong to before they are potentially taken away as outliers.
Clear and complete documentation of the experimental methodology is also important in order to support replication of results.


== Discussion topics when setting up an experimental design ==
An experimental design or randomized clinical trial requires careful consideration of several factors before actually doing the experiment. An experimental design is the laying out of a detailed experimental plan in advance of doing the experiment. Some of the following topics have already been discussed in the principles of experimental design section:

How many factors does the design have, and are the levels of these factors fixed or random?
Are control conditions needed, and what should they be?
Manipulation checks; did the manipulation really work?
What are the background variables?
What is the sample size. How many units must be collected for the experiment to be generalisable and have enough power?
What is the relevance of interactions between factors?
What is the influence of delayed effects of substantive factors on outcomes?
How do response shifts affect self-report measures?
How feasible is repeated administration of the same measurement instruments to the same units at different occasions, with a post-test and follow-up tests?
What about using a proxy pretest?
Are there lurking variables?
Should the client/patient, researcher or even the analyst of the data be blind to conditions?
What is the feasibility of subsequent application of different conditions to the same units?
How many of each control and noise factors should be taken into account?The independent variable of a study often has many levels or different groups. In a true experiment, researchers can have an experimental group, which is where their intervention testing the hypothesis is implemented, and a control group, which has all the same element as the experimental group, without the interventional element. Thus, when everything else except for one intervention is held constant, researchers can certify with some certainty that this one element is what caused the observed change. In some instances, having a control group is not ethical. This is sometimes solved using two different experimental groups. In some cases, independent variables cannot be manipulated, for example when testing the difference between two groups who have a different disease, or testing the difference between genders (obviously variables that would be hard or unethical to assign participants to). In these cases, a quasi-experimental design may be used.


== Causal attributions ==
In the pure experimental design, the independent (predictor) variable is manipulated by the researcher – that is – every participant of the research is chosen randomly from the population, and each participant chosen is assigned randomly to conditions of the independent variable. Only when this is done is it possible to certify with high probability that the reason for the differences in the outcome variables are caused by the different conditions. Therefore, researchers should choose the experimental design over other design types whenever possible. However, the nature of the independent variable does not always allow for manipulation. In those cases, researchers must be aware of not certifying about causal attribution when their design doesn't allow for it. For example, in observational designs, participants are not assigned randomly to conditions, and so if there are differences found in outcome variables between conditions, it is likely that there is something other than the differences between the conditions that causes the differences in outcomes, that is – a third variable. The same goes for studies with correlational design. (Adér & Mellenbergh, 2008).


== Statistical control ==
It is best that a process be in reasonable statistical control prior to conducting designed experiments.  When this is not possible, proper blocking, replication, and randomization allow for the careful conduct of designed experiments.
To control for nuisance variables, researchers institute control checks as additional measures.  Investigators should ensure that uncontrolled influences (e.g., source credibility perception) do not skew the findings of the study.  A manipulation check is one example of a control check.  Manipulation checks allow investigators to isolate the chief variables to strengthen support that these variables are operating as planned.
One of the most important requirements of experimental research designs is the necessity of eliminating the effects of spurious, intervening, and antecedent variables. In the most basic model, cause (X) leads to effect (Y). But there could be a third variable (Z) that influences (Y), and X might not be the true cause at all. Z is said to be a spurious variable and must be controlled for. The same is true for intervening variables (a variable in between the supposed cause (X) and the effect (Y)), and anteceding variables (a variable prior to the supposed cause (X) that is the true cause). When a third variable is involved and has not been controlled for, the relation is said to be a zero order relationship. In most practical applications of experimental research designs there are several causes (X1, X2, X3). In most designs, only one of these causes is manipulated at a time.


== Experimental designs after Fisher ==
Some efficient designs for estimating several main effects were found independently and in near succession by Raj Chandra Bose and K. Kishen in 1940 at the Indian Statistical Institute, but remained little known until the Plackett–Burman designs were published in Biometrika in 1946. About the same time, C. R. Rao introduced the concepts of orthogonal arrays as experimental designs. This concept played a central role in the development of Taguchi methods by Genichi Taguchi, which took place during his visit to Indian Statistical Institute in early 1950s. His methods were successfully applied and adopted by Japanese and Indian industries and subsequently were also embraced by US industry albeit with some reservations.
In 1950, Gertrude Mary Cox and William Gemmell Cochran published the book Experimental Designs, which became the major reference work on the design of experiments for statisticians for years afterwards.
Developments of the theory of linear models have encompassed and surpassed the cases that concerned early writers. Today, the theory rests on advanced topics in linear algebra, algebra and combinatorics.
As with other branches of statistics, experimental design is pursued using both frequentist and Bayesian approaches: In evaluating statistical procedures like experimental designs, frequentist statistics studies the sampling distribution while Bayesian statistics updates a probability distribution on the parameter space.
Some important contributors to the field of experimental designs are C. S. Peirce, R. A. Fisher, F. Yates, R. C. Bose, A. C. Atkinson, R. A. Bailey, D. R. Cox, G. E. P. Box, W. G. Cochran, W. T. Federer, V. V. Fedorov, A. S. Hedayat, J. Kiefer, O. Kempthorne, J. A. Nelder, Andrej Pázman, Friedrich Pukelsheim, D. Raghavarao, C. R. Rao, Shrikhande S. S., J. N. Srivastava, William J. Studden, G. Taguchi and H. P. Wynn.The textbooks of D. Montgomery, R. Myers, and G. Box/W. Hunter/J.S. Hunter have reached generations of students and practitioners.
Some discussion of experimental design in the context of system identification (model building for static or dynamic models) is given in and 


== Human participant constraints ==
Laws and ethical considerations preclude some carefully designed 
experiments with human subjects.  Legal constraints are dependent on 
jurisdiction.  Constraints may involve 
institutional review boards, informed consent 
and confidentiality affecting both clinical (medical) trials and 
behavioral and social science experiments.
In the field of toxicology, for example, experimentation is performed 
on laboratory animals with the goal of defining safe exposure limits 
for humans. Balancing
the constraints are views from the medical field. Regarding the randomization of patients, 
""... if no one knows which therapy is better, there is no ethical 
imperative to use one therapy or another."" (p 380) Regarding 
experimental design, ""...it is clearly not ethical to place subjects 
at risk to collect data in a poorly designed study when this situation 
can be easily avoided..."". (p 393)


== See also ==


== References ==


=== Sources ===


== External links ==
A chapter from a ""NIST/SEMATECH Handbook on Engineering Statistics"" at NIST
Box–Behnken designs from a ""NIST/SEMATECH Handbook on Engineering Statistics"" at NIST
Detailed mathematical developments of most common DoE in the Opera Magistris v3.6 online reference Chapter 15, section 7.4, ISBN 978-2-8399-0932-7.","pandas(index=138, _1=138, text='the design of experiments (doe, dox, or experimental design) is the design of any task that aims to describe and explain the variation of information under conditions that are hypothesized to reflect the variation. the term is generally associated with experiments in which the design introduces conditions that directly affect the variation, but may also refer to the design of quasi-experiments, in which natural conditions that influence the variation are selected for observation. in its simplest form, an experiment aims at predicting the outcome by introducing a change of the preconditions, which is represented by one or more independent variables, also referred to as ""input variables"" or ""predictor variables."" the change in one or more independent variables is generally hypothesized to result in a change in one or more dependent variables, also referred to as ""output variables"" or ""response variables."" the experimental design may also identify control variables that must be held constant to prevent external factors from affecting the results. experimental design involves not only the selection of suitable independent, dependent, and control variables, but planning the delivery of the experiment under statistically optimal conditions given the constraints of available resources.  there are multiple approaches for determining the set of design points (unique combinations of the settings of the independent variables) to be used in the experiment. main concerns in experimental design include the establishment of validity, reliability, and replicability. for example, these concerns can be partially addressed by carefully choosing the independent variable, reducing the risk of measurement error, and ensuring that the documentation of the method is sufficiently detailed. related concerns include achieving appropriate levels of statistical power and sensitivity. correctly designed experiments advance knowledge in the natural and social sciences and engineering. other applications include marketing and policy making. the study of the design of experiments is an important topic in metascience.   == history == == external links == a chapter from a ""nist/sematech handbook on engineering statistics"" at nist box–behnken designs from a ""nist/sematech handbook on engineering statistics"" at nist detailed mathematical developments of most common doe in the opera magistris v3.6 online reference chapter 15, section 7.4, isbn 978-2-8399-0932-7.')"
139,"Efficiency factor is a ratio of some measure of performance to an expected value.


== Data communication ==
In data communications, the factor is the ratio of the time to transmit a text automatically at a specified modulation rate to the time actually required to receive the same text at a specified maximum error rate. All of the communication facilities are assumed to be in the normal condition of adjustment and operation. The practical conditions of measurement should be specified, especially the duration of the measurement.
Telegraph communications may have different temporal efficiency factors for the two directions of transmission.


== Industrial engineering ==
In industrial engineering, the efficiency factor is the relationship between the allowance time and the time taken, in the form of percentage.
Efficiency factors are used in performance rating and remuneration calculation exercises. The efficiency factor is an extremely simple to use and readily comprehensible index, the prerequisite being exact time management for maintaining the allowed times.


== References ==","pandas(index=139, _1=139, text='efficiency factor is a ratio of some measure of performance to an expected value.   == data communication == in data communications, the factor is the ratio of the time to transmit a text automatically at a specified modulation rate to the time actually required to receive the same text at a specified maximum error rate. all of the communication facilities are assumed to be in the normal condition of adjustment and operation. the practical conditions of measurement should be specified, especially the duration of the measurement. telegraph communications may have different temporal efficiency factors for the two directions of transmission.   == industrial engineering == in industrial engineering, the efficiency factor is the relationship between the allowance time and the time taken, in the form of percentage. efficiency factors are used in performance rating and remuneration calculation exercises. the efficiency factor is an extremely simple to use and readily comprehensible index, the prerequisite being exact time management for maintaining the allowed times.   == references ==')"
140,"Operations research (British English: operational research) (OR) is a discipline that deals with the application of advanced analytical methods to help make better decisions. The term operational analysis is used in the British (and some British Commonwealth) military as an intrinsic part of capability development, management and assurance.  Operational analysis forms part of the Combined Operational Effectiveness and Investment Appraisals, which support British defence capability acquisition decision-making.
It is sometimes considered to be a sub-field of mathematical sciences. The terms management science and decision science are sometimes used as synonyms.Employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. Because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. Operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). Originating in military efforts before World War II, its techniques have grown to concern problems in a variety of industries.


== Overview ==
Operational research (OR) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, Markov decision processes, econometric methods, data envelopment analysis, neural networks, expert systems, decision analysis, and the analytic hierarchy process. Nearly all of these techniques involve the construction of mathematical models that attempt to describe the system. Because of the computational and statistical nature of most of these fields, OR also has strong ties to computer science and analytics. Operational researchers faced with a new problem must determine which of these techniques are most appropriate given the nature of the system, the goals for improvement, and constraints on time and computing power, or develop a new technique specific to the problem at hand (and, afterwards, to that type of problem).
The major sub-disciplines in modern operational research, as identified by the journal Operations Research, are:

Computing and information technologies
Financial engineering
Manufacturing, service sciences, and supply chain management
Policy modeling and public sector work
Revenue management
Simulation
Stochastic models
Transportation


== History ==
In the decades after the two world wars, the tools of operations research were more widely applied to problems in business, industry, and society. Since that time, operational research has expanded into a field widely used in industries ranging from petrochemicals to airlines, finance, logistics, and government, moving to a focus on the development of mathematical models that can be used to analyse and optimize complex systems, and has become an area of active academic and industrial research.


=== Historical origins ===
In the 17th century mathematicians like Christiaan Huygens (1629-1695) and Blaise Pascal (problem of points) tried to solve problems involving complex decisions with probability. Others in the 18th and 19th centuries solved these types of problems with combinatorics. Charles Babbage's research into the cost of transportation and sorting of mail led to England's universal ""Penny Post"" in 1840, and to studies into the dynamical behaviour of railway vehicles in defence of the GWR's broad gauge. Beginning in the 20th century, study of inventory management could be considered the origin of modern operations research with economic order quantity developed by Ford W. Harris in 1913. Operational research may have originated in the efforts of military planners during World War I (convoy theory and Lanchester's laws). Percy Bridgman brought operational research to bear on problems in physics in the 1920s and would later attempt to extend these to the social sciences.Modern operational research originated at the Bawdsey Research Station in the UK in 1937 as the result of an initiative of the station's superintendent, A. P. Rowe and Robert Watson-Watt. Rowe conceived the idea as a means to analyse and improve the working of the UK's early-warning radar system, code-named ""Chain Home"" (CH). Initially, Rowe analysed the operating of the radar equipment and its communication networks, expanding later to include the operating personnel's behaviour. This revealed unappreciated limitations of the CH network and allowed remedial action to be taken.Scientists in the United Kingdom (including Patrick Blackett (later Lord Blackett OM PRS), Cecil Gordon, Solly Zuckerman, (later Baron Zuckerman OM, KCB, FRS), C. H. Waddington, Owen Wansbrough-Jones, Frank Yates, Jacob Bronowski and Freeman Dyson), and in the United States (George Dantzig) looked for ways to make better decisions in such areas as logistics and training schedules


=== Second World War ===
The modern field of operational research arose during World War II. In the World War II era, operational research was defined as ""a scientific method of providing executive departments with a quantitative basis for decisions regarding the operations under their control"". Other names for it included operational analysis (UK Ministry of Defence from 1962) and quantitative management.During the Second World War close to 1,000 men and women in Britain were engaged in operational research. About 200 operational research scientists worked for the British Army.Patrick Blackett worked for several different organizations during the war. Early in the war while working for the Royal Aircraft Establishment (RAE) he set up a team known as the ""Circus"" which helped to reduce the number of anti-aircraft artillery rounds needed to shoot down an enemy aircraft from an average of over 20,000 at the start of the Battle of Britain to 4,000 in 1941.

In 1941, Blackett moved from the RAE to the Navy, after first working with RAF Coastal Command, in 1941 and then early in 1942 to the Admiralty. Blackett's team at Coastal Command's Operational Research Section (CC-ORS) included two future Nobel prize winners and many other people who went on to be pre-eminent in their fields. They undertook a number of crucial analyses that aided the war effort. Britain introduced the convoy system to reduce shipping losses, but while the principle of using warships to accompany merchant ships was generally accepted, it was unclear whether it was better for convoys to be small or large. Convoys travel at the speed of the slowest member, so small convoys can travel faster. It was also argued that small convoys would be harder for German U-boats to detect. On the other hand, large convoys could deploy more warships against an attacker. Blackett's staff showed that the losses suffered by convoys depended largely on the number of escort vessels present, rather than the size of the convoy. Their conclusion was that a few large convoys are more defensible than many small ones.
While performing an analysis of the methods used by RAF Coastal Command to hunt and destroy submarines, one of the analysts asked what colour the aircraft were. As most of them were from Bomber Command they were painted black for night-time operations. At the suggestion of CC-ORS a test was run to see if that was the best colour to camouflage the aircraft for daytime operations in the grey North Atlantic skies. Tests showed that aircraft painted white were on average not spotted until they were 20% closer than those painted black. This change indicated that 30% more submarines would be attacked and sunk for the same number of sightings. As a result of these findings Coastal Command changed their aircraft to using white undersurfaces.
Other work by the CC-ORS indicated that on average if the trigger depth of aerial-delivered depth charges (DCs) were changed from 100 to 25 feet, the kill ratios would go up. The reason was that if a U-boat saw an aircraft only shortly before it arrived over the target then at 100 feet the charges would do no damage (because the U-boat wouldn't have had time to descend as far as 100 feet), and if it saw the aircraft a long way from the target it had time to alter course under water so the chances of it being within the 20-foot kill zone of the charges was small. It was more efficient to attack those submarines close to the surface when the targets' locations were better known than to attempt their destruction at greater depths when their positions could only be guessed. Before the change of settings from 100 to 25 feet, 1% of submerged U-boats were sunk and 14% damaged. After the change, 7% were sunk and 11% damaged; if submarines were caught on the surface but had time to submerge just before being attacked, the numbers rose to 11% sunk and 15% damaged. Blackett observed ""there can be few cases where such a great operational gain had been obtained by such a small and simple change of tactics"".

Bomber Command's Operational Research Section (BC-ORS), analyzed a report of a survey carried out by RAF Bomber Command. For the survey, Bomber Command inspected all bombers returning from bombing raids over Germany over a particular period. All damage inflicted by German air defences was noted and the recommendation was given that armour be added in the most heavily damaged areas. This recommendation was not adopted because the fact that the aircraft were able to return with these areas damaged indicated the areas were not vital, and adding armour to non-vital areas where damage is acceptable reduces aircraft performance. Their suggestion to remove some of the crew so that an aircraft loss would result in fewer personnel losses, was also rejected by RAF command. Blackett's team made the logical recommendation that the armour be placed in the areas which were completely untouched by damage in the bombers which returned. They reasoned that the survey was biased, since it only included aircraft that returned to Britain. The areas untouched in returning aircraft were probably vital areas, which, if hit, would result in the loss of the aircraft. This story has been disputed, with a similar damage assessment study completed in the US by the Statistical Research Group at Columbia University, the result of work done by Abraham Wald.When Germany organized its air defences into the Kammhuber Line, it was realized by the British that if the RAF bombers were to fly in a bomber stream they could overwhelm the night fighters who flew in individual cells directed to their targets by ground controllers. It was then a matter of calculating the statistical loss from collisions against the statistical loss from night fighters to calculate how close the bombers should fly to minimize RAF losses.The ""exchange rate"" ratio of output to input was a characteristic feature of operational research. By comparing the number of flying hours put in by Allied aircraft to the number of U-boat sightings in a given area, it was possible to redistribute aircraft to more productive patrol areas. Comparison of exchange rates established ""effectiveness ratios"" useful in planning. The ratio of 60 mines laid per ship sunk was common to several campaigns: German mines in British ports, British mines on German routes, and United States mines in Japanese routes.Operational research doubled the on-target bomb rate of B-29s bombing Japan from the Marianas Islands by increasing the training ratio from 4 to 10 percent of flying hours; revealed that wolf-packs of three United States submarines were the most effective number to enable all members of the pack to engage targets discovered on their individual patrol stations; revealed that glossy enamel paint was more effective camouflage for night fighters than traditional dull camouflage paint finish, and a smooth paint finish increased airspeed by reducing skin friction.On land, the operational research sections of the Army Operational Research Group (AORG) of the Ministry of Supply (MoS) were landed in Normandy in 1944, and they followed British forces in the advance across Europe. They analyzed, among other topics, the effectiveness of artillery, aerial bombing and anti-tank shooting.


=== After World War II ===
In 1947 under the auspices of the British Association, a symposium was organised in Dundee. In his opening address Watson-Watts offered a definition of the aims of OR:

""to examine quantitatively whether the user organization is getting from the operation of its equipment the best attainable contribution to its overall objective.""With expanded techniques and growing awareness of the field at the close of the war, operational research was no longer limited to only operational, but was extended to encompass equipment procurement, training, logistics and infrastructure. Operations Research also grew in many areas other than the military once scientists learned to apply its principles to the civilian sector. With the development of the simplex algorithm for linear programming in 1947 and the development of computers over the next three decades, Operations Research can now ""solve problems with hundreds of thousands of variables and constraints. Moreover, the large volumes of data required for such problems can be stored and manipulated very efficiently."" Much of operations research (modernly known as 'analytics') relies upon stochastic variables and a therefore access to truly random numbers. Fortunately the cybernetics field also required the same level of randomness. The development of increasingly better random number generators has been a boon to both disciplines. Modern applications of operations research include city planning, football strategies, emergency planning, optimizing all facets of industry and economy, and undoubtedly with the likelihood of the inclusion of terrorist attack planning and definitely counter-terrorist attack planning.


== Problems addressed ==
critical path analysis or project planning: identifying those processes in a complex project which affect the overall duration of the project
Floorplanning: designing the layout of equipment in a factory or components on a computer chip to reduce manufacturing time (therefore reducing cost)
Network optimization: for instance, setup of telecommunications or power system networks to maintain quality of service during outages
Resource allocation problems
Facility location
Assignment Problems:
Assignment problem
Generalized assignment problem
Quadratic assignment problem
Weapon target assignment problem
Bayesian search theory: looking for a target
Optimal search
Routing, such as determining the routes of buses so that as few buses are needed as possible
Supply chain management: managing the flow of raw materials and products based on uncertain demand for the finished products
Project production activities: managing the flow of work activities in a capital project in response to system variability through operations research tools for variability reduction and buffer allocation using a combination of allocation of capacity, inventory and time
Efficient messaging and customer response tactics
Automation: automating or integrating robotic systems in human-driven operations processes
Globalization: globalizing operations processes in order to take advantage of cheaper materials, labor, land or other productivity inputs
Transportation: managing freight transportation and delivery systems (Examples: LTL shipping, intermodal freight transport, travelling salesman problem)
Scheduling:
Personnel staffing
Manufacturing steps
Project tasks
Network data traffic: these are known as queueing models or queueing systems.
Sports events and their television coverage
Blending of raw materials in oil refineries
Determining optimal prices, in many retail and B2B settings, within the disciplines of pricing science
Cutting stock problem: Cutting small items out of bigger ones.Operational research is also used extensively in government where evidence-based policy is used.


== Management science ==

In 1967 Stafford Beer characterized the field of management science as ""the business use of operations research"". Like operational research itself, management science (MS) is an interdisciplinary branch of applied mathematics devoted to optimal decision planning, with strong links with economics, business, engineering, and other sciences. It uses various scientific research-based principles, strategies, and analytical methods including mathematical modeling, statistics and numerical algorithms to improve an organization's ability to enact rational and meaningful management decisions by arriving at optimal or near optimal solutions to complex decision problems. Management scientists help businesses to achieve their goals using the scientific methods of operational research.
The management scientist's mandate is to use rational, systematic, science-based techniques to inform and improve decisions of all kinds. Of course, the techniques of management science are not restricted to business applications but may be applied to military, medical, public administration, charitable groups, political groups or community groups.
Management science is concerned with developing and applying models and concepts that may prove useful in helping to illuminate management issues and solve managerial problems, as well as designing and developing new and better models of organizational excellence.The application of these models within the corporate sector became known as management science.


=== Related fields ===
Some of the fields that have considerable overlap with Operations Research and Management Science include:


=== Applications ===
Applications are abundant such as in airlines, manufacturing companies, service organizations, military branches, and government. The range of problems and issues to which it has contributed insights and solutions is vast. It includes:
Scheduling (of airlines, trains, buses etc.)
Assignment (assigning crew to flights, trains or buses; employees to projects; commitment and dispatch of power generation facilities)
Facility location (deciding most appropriate location for new facilities such as warehouse; factory or fire station)
Hydraulics & Piping Engineering (managing flow of water from reservoirs)
Health Services (information and supply chain management)
Game Theory (identifying, understanding; developing strategies adopted by companies)
Urban Design
Computer Network Engineering (packet routing; timing; analysis)
Telecom & Data Communication Engineering (packet routing; timing; analysis)Management is also concerned with so-called 'soft-operational analysis' which concerns methods for strategic planning, strategic decision support, problem structuring methods. 
In dealing with these sorts of challenges, mathematical modeling and simulation may not be appropriate or may not suffice. Therefore, during the past 30 years, a number of non-quantified modeling methods have been developed. These include:
stakeholder based approaches including metagame analysis and drama theory
morphological analysis and various forms of influence diagrams
cognitive mapping
strategic choice
robustness analysis


== Societies and journals ==


=== Societies ===
The International Federation of Operational Research Societies (IFORS) is an umbrella organization for operational research societies worldwide, representing approximately 50 national societies including those in the US, UK, France, Germany, Italy, Canada, Australia, New Zealand, Philippines, India, Japan and South Africa. The constituent members of IFORS form regional groups, such as that in Europe, the Association of European Operational Research Societies (EURO). Other important operational research organizations are Simulation Interoperability Standards Organization (SISO) and Interservice/Industry Training, Simulation and Education Conference (I/ITSEC)In 2004 the US-based organization INFORMS began an initiative to market the OR profession better, including a website entitled The Science of Better which provides an introduction to OR and examples of successful applications of OR to industrial problems. This initiative has been adopted by the Operational Research Society in the UK, including a website entitled Learn about OR.


=== Journals of INFORMS ===
The Institute for Operations Research and the Management Sciences (INFORMS) publishes thirteen scholarly journals about operations research, including the top two journals in their class, according to 2005 Journal Citation Reports. They are:

Decision Analysis
Information Systems Research
INFORMS Journal on Computing
INFORMS Transactions on Education (an open access journal)
Interfaces
Management Science
Manufacturing & Service Operations Management
Marketing Science
Mathematics of Operations Research
Operations Research
Organization Science
Service Science
Transportation Science


=== Other journals ===
These are listed in alphabetical order of their titles.

4OR-A Quarterly Journal of Operations Research: jointly published the Belgian, French and Italian Operations Research Societies (Springer);
Decision Sciences published by Wiley-Blackwell on behalf of the Decision Sciences Institute
European Journal of Operational Research (EJOR): Founded in 1975 and is presently by far the largest operational research journal in the world, with its around 9,000 pages of published papers per year. In 2004, its total number of citations was the second largest amongst Operational Research and Management Science journals;
INFOR Journal: published and sponsored by the Canadian Operational Research Society;
International Journal of Operations Research and Information Systems (IJORIS):  an official publication of the Information Resources Management Association, published quarterly by IGI Global;
Journal of Defense Modeling and Simulation (JDMS): Applications, Methodology, Technology: a quarterly journal devoted to advancing the science of modeling and simulation as it relates to the military and defense.
Journal of the Operational Research Society (JORS): an official journal of The OR Society; this is the oldest continuously published journal of OR in the world, published by Taylor & Francis;
Military Operations Research (MOR): published by the Military Operations Research Society;
Omega - The International Journal of Management Science;
Operations Research Letters;
Opsearch: official journal of the Operational Research Society of India;
OR Insight: a quarterly journal of The OR Society, published by Palgrave;
Pesquisa Operacional, the official journal of the Brazilian Operations Research Society
Production and Operations Management, the official journal of the Production and Operations Management Society
TOP: the official journal of the Spanish Statistics and Operations Research Society.


== See also ==


== References ==


== Further reading ==


=== Classic books and articles ===
R. E. Bellman, Dynamic Programming, Princeton University Press, Princeton, 1957
Abraham Charnes, William W. Cooper, Management Models and Industrial Applications of Linear Programming, Volumes I and II, New York, John Wiley & Sons, 1961
Abraham Charnes, William W. Cooper, A. Henderson, An Introduction to Linear Programming, New York, John Wiley & Sons, 1953
C. West Churchman, Russell L. Ackoff & E. L. Arnoff, Introduction to Operations Research, New York: J. Wiley and Sons, 1957
George B. Dantzig, Linear Programming and Extensions, Princeton, Princeton University Press, 1963
Lester K. Ford, Jr., D. Ray Fulkerson, Flows in Networks, Princeton, Princeton University Press, 1962
Jay W. Forrester, Industrial Dynamics, Cambridge, MIT Press, 1961
L. V. Kantorovich, ""Mathematical Methods of Organizing and Planning Production"" Management Science, 4, 1960, 266–422
Ralph Keeney, Howard Raiffa, Decisions with Multiple Objectives: Preferences and Value Tradeoffs, New York, John Wiley & Sons, 1976
H. W. Kuhn, ""The Hungarian Method for the Assignment Problem,"" Naval Research Logistics Quarterly, 1–2, 1955, 83–97
H. W. Kuhn, A. W. Tucker, ""Nonlinear Programming,"" pp. 481–492 in Proceedings of the Second Berkeley Symposium on Mathematical Statistics and Probability
B. O. Koopman, Search and Screening: General Principles and Historical Applications, New York, Pergamon Press, 1980
Tjalling C. Koopmans, editor, Activity Analysis of Production and Allocation, New York, John Wiley & Sons, 1951
Charles C. Holt, Franco Modigliani, John F. Muth, Herbert A. Simon, Planning Production, Inventories, and Work Force, Englewood Cliffs, NJ, Prentice-Hall, 1960
Philip M. Morse, George E. Kimball, Methods of Operations Research, New York, MIT Press and John Wiley & Sons, 1951
Robert O. Schlaifer, Howard Raiffa, Applied Statistical Decision Theory, Cambridge, Division of Research, Harvard Business School, 1961


=== Classic textbooks ===
Frederick S. Hillier & Gerald J. Lieberman, Introduction to Operations Research, McGraw-Hill: Boston MA; 10th Edition, 2014
Taha, Hamdy A., ""Operations Research: An Introduction"", Pearson, 10th Edition, 2016
Robert J. Thierauf & Richard A. Grosse, ""Decision Making Through Operations Research"", John Wiley & Sons, INC, 1970
Harvey M. Wagner, Principles of Operations Research, Englewood Cliffs, Prentice-Hall, 1969


=== History ===
Saul I. Gass, Arjang A. Assad, An Annotated Timeline of Operations Research:  An Informal History. New York, Kluwer Academic Publishers, 2005.
Saul I. Gass (Editor), Arjang A. Assad (Editor), Profiles in Operations Research: Pioneers and Innovators. Springer, 2011
Maurice W. Kirby (Operational Research Society (Great Britain)). Operational Research in War and Peace: The British Experience from the 1930s to 1970, Imperial College Press, 2003. ISBN 1-86094-366-7, ISBN 978-1-86094-366-9
J. K. Lenstra, A. H. G. Rinnooy Kan, A. Schrijver (editors) History of Mathematical Programming: A Collection of Personal Reminiscences, North-Holland, 1991
Charles W. McArthur, Operations Analysis in the U.S. Army Eighth Air Force in World War II, History of Mathematics, Vol. 4, Providence, American Mathematical Society, 1990
C. H. Waddington, O. R. in World War 2: Operational Research Against the U-boat, London, Elek Science, 1973.


== External links ==
What is Operations Research?
International Federation of Operational Research Societies
The Institute for Operations Research and the Management Sciences (INFORMS)
Occupational Outlook Handbook, U.S. Department of Labor Bureau of Labor Statistics","pandas(index=140, _1=140, text='operations research (british english: operational research) (or) is a discipline that deals with the application of advanced analytical methods to help make better decisions. the term operational analysis is used in the british (and some british commonwealth) military as an intrinsic part of capability development, management and assurance.  operational analysis forms part of the combined operational effectiveness and investment appraisals, which support british defence capability acquisition decision-making. it is sometimes considered to be a sub-field of mathematical sciences. the terms management science and decision science are sometimes used as synonyms.employing techniques from other mathematical sciences, such as mathematical modeling, statistical analysis, and mathematical optimization, operations research arrives at optimal or near-optimal solutions to complex decision-making problems. because of its emphasis on human-technology interaction and because of its focus on practical applications, operations research has overlap with other disciplines, notably industrial engineering and operations management, and draws on psychology and organization science. operations research is often concerned with determining the extreme values of some real-world objective: the maximum (of profit, performance, or yield) or minimum (of loss, risk, or cost). originating in military efforts before world war ii, its techniques have grown to concern problems in a variety of industries.   == overview == operational research (or) encompasses the development and the use of a wide range of problem-solving techniques and methods applied in the pursuit of improved decision-making and efficiency, such as simulation, mathematical optimization, queueing theory and other stochastic-process models, markov decision processes, econometric methods, data envelopment analysis, neural networks, expert systems, decision analysis, and the analytic hierarchy process. nearly all of these techniques involve the construction of mathematical models that attempt to describe the system. because of the computational and statistical nature of most of these fields, or also has strong ties to computer science and analytics. operational researchers faced with a new problem must determine which of these techniques are most appropriate given the nature of the system, the goals for improvement, and constraints on time and computing power, or develop a new technique specific to the problem at hand (and, afterwards, to that type of problem). the major sub-disciplines in modern operational research, as identified by the journal operations research, are:  computing and information technologies financial engineering manufacturing, service sciences, and supply chain management policy modeling and public sector work revenue management simulation stochastic models transportation   == history == in the decades after the two world wars, the tools of operations research were more widely applied to problems in business, industry, and society. since that time, operational research has expanded into a field widely used in industries ranging from petrochemicals to airlines, finance, logistics, and government, moving to a focus on the development of mathematical models that can be used to analyse and optimize complex systems, and has become an area of active academic and industrial research. saul i. gass, arjang a. assad, an annotated timeline of operations research:  an informal history. new york, kluwer academic publishers, 2005. saul i. gass (editor), arjang a. assad (editor), profiles in operations research: pioneers and innovators. springer, 2011 maurice w. kirby (operational research society (great britain)). operational research in war and peace: the british experience from the 1930s to 1970, imperial college press, 2003. isbn 1-86094-366-7, isbn 978-1-86094-366-9 j. k. lenstra, a. h. g. rinnooy kan, a. schrijver (editors) history of mathematical programming: a collection of personal reminiscences, north-holland, 1991 charles w. mcarthur, operations analysis in the u.s. army eighth air force in world war ii, history of mathematics, vol. 4, providence, american mathematical society, 1990 c. h. waddington, o. r. in world war 2: operational research against the u-boat, london, elek science, 1973.   == external links == what is operations research? international federation of operational research societies the institute for operations research and the management sciences (informs) occupational outlook handbook, u.s. department of labor bureau of labor statistics')"
141,"Industrial ecology (IE) is the study of material and energy flows through industrial systems.  The global industrial economy can be modelled as a network of industrial processes that extract resources from the Earth and transform those resources into commodities which can be bought and sold to meet the needs of humanity.  Industrial ecology seeks to quantify the material flows and document the industrial processes that make modern society function.  Industrial ecologists are often concerned with the impacts that industrial activities have on the environment, with use of the planet's supply of natural resources, and with problems of waste disposal.  Industrial ecology is a young but growing multidisciplinary field of research which combines aspects of engineering, economics, sociology, toxicology and the natural sciences.
Industrial ecology has been defined as a ""systems-based, multidisciplinary discourse that seeks to understand emergent behaviour of complex integrated human/natural systems"". The field approaches issues of sustainability by examining problems from multiple perspectives, usually involving aspects of sociology, the environment, economy and technology.  The name comes from the idea that the analogy of natural systems should be used as an aid in understanding how to design sustainable industrial systems.


== Overview ==

Industrial ecology is concerned with the shifting of industrial process from linear (open loop) systems, in which resource and capital investments move through the system to become waste, to a closed loop system where wastes can become inputs for new processes.
Much of the research focuses on the following areas:
material and energy flow studies (""industrial metabolism"")
dematerialization and decarbonization
technological change and the environment
life-cycle planning, design and assessment
design for the environment (""eco-design"")
 extended producer responsibility (""product stewardship"")
 eco-industrial parks (""industrial symbiosis"") 
product-oriented environmental policy
eco-efficiencyIndustrial ecology seeks to understand the way in which industrial systems (for example a factory, an ecoregion, or national or global economy) interact with the biosphere.  Natural ecosystems provide a metaphor for understanding how different parts of industrial systems interact with one another, in an ""ecosystem"" based on resources and infrastructural capital rather than on natural capital. It seeks to exploit the idea that natural systems do not have waste in them to inspire sustainable design.
Along with more general energy conservation and material conservation goals, and redefining commodity markets and product stewardship relations strictly as a service economy, industrial ecology is one of the four objectives of Natural Capitalism.  This strategy discourages forms of amoral purchasing arising from ignorance of what goes on at a distance and implies a political economy that values natural capital highly and relies on more instructional capital to design and maintain each unique industrial ecology.


== History ==

Industrial ecology was popularized in 1989 in a Scientific American article by Robert Frosch and Nicholas E. Gallopoulos.  Frosch and Gallopoulos' vision was ""why would not our industrial system behave like an ecosystem, where the wastes of a species may be resource to another species? Why would not the outputs of an industry be the inputs of another, thus reducing use of raw materials, pollution, and saving on waste treatment?"" A notable example resides in a Danish industrial park in the city of Kalundborg.  Here several linkages of byproducts and waste heat can be found between numerous entities such as a large power plant, an oil refinery, a pharmaceutical plant, a plasterboard factory, an enzyme manufacturer, a waste company and the city itself. Another example is the Rantasalmi EIP in Rantasalmi, Finland. While this country has had previous organically formed EIP's, the park at Rantasalmi is Finland's first planned EIP.
The scientific field Industrial Ecology has grown quickly in recent years. The Journal of Industrial Ecology (since 1997), the International Society for Industrial Ecology (since 2001), and the journal Progress in Industrial Ecology (since 2004) give Industrial Ecology a strong and dynamic position in the international scientific community.  Industrial Ecology principles are also emerging in various policy realms such as the concept of the Circular Economy that is being promoted in China. Although the definition of the Circular Economy has yet to be formalized, generally the focus is on strategies such as creating a circular flow of materials, and cascading energy flows. An example of this would be using waste heat from one process to run another process that requires a lower temperature. The hope is that strategy such as this will create a more efficient economy with fewer pollutants and other unwanted by-products.


== Principles ==
One of the central principles of Industrial Ecology is the view that societal and technological systems are bounded within the biosphere, and do not exist outside it.  Ecology is used as a metaphor due to the observation that natural systems reuse materials and have a largely closed loop cycling of nutrients.  Industrial Ecology approaches problems with the hypothesis that by using similar principles as natural systems, industrial systems can be improved to reduce their impact on the natural environment as well. The table shows the general metaphor.

IE examines societal issues and their relationship with both technical systems and the environment. Through this holistic view , IE recognizes that solving problems must involve understanding the connections that exist between these systems, various aspects cannot be viewed in isolation. Often changes in one part of the overall system can propagate and cause changes in another part. Thus, you can only understand a problem if you look at its parts in relation to the whole. Based on this framework, IE looks at environmental issues with a systems thinking approach. A good IE example with these societal impacts can be found at the Blue Lagoon in Iceland. The Lagoon uses super-heated water from a local geothermal power plant to fill mineral-rich basins that have become recreational healing centers. In this sense the industrial process of energy production uses its wastewater to provide a crucial resource for the dependent recreational industry.
Take a city for instance. A city can be divided into commercial areas, residential areas, offices, services, infrastructures, and so forth. These are all sub-systems of the 'big city' system. Problems can emerge in one sub-system, but the solution has to be global. Let's say the price of housing is rising dramatically because there is too high a demand for housing. One solution would be to build new houses, but this will lead to more people living in the city, leading to the need for more infrastructure like roads, schools, more supermarkets, etc. This system is a simplified interpretation of reality whose behaviors can be 'predicted'.
In many cases, the systems IE deals with are complex systems. Complexity makes it difficult to understand the behavior of the system and may lead to rebound effects. Due to unforeseen behavioral change of users or consumers, a measure taken to improve environmental performance does not lead to any improvement or may even worsen the situation.
Moreover, life cycle thinking is also a very important principle in industrial ecology. It implies that all environmental impacts caused by a product, system, or project during its life cycle are taken into account. In this context life cycle includes

Raw material extraction
Material processing
Manufacture
Use
Maintenance
DisposalThe transport necessary between these stages is also taken into account as well as, if relevant, extra stages such as reuse, remanufacture, and recycle.
Adopting a life cycle approach is essential to avoid shifting environmental impacts from one life cycle stage to another. This is commonly referred to as problem shifting. For instance, during the re-design of a product, one can choose to reduce its weight, thereby decreasing use of resources. It is possible that the lighter materials used in the new product will be more difficult to dispose of. The environmental impacts of the product gained during the extraction phase are shifted to the disposal phase. Overall environmental improvements are thus null.
A final important principle of IE is its integrated approach or multidisciplinarity. IE takes into account three different disciplines: social sciences (including economics), technical sciences and environmental sciences. The challenge is to merge them into a single approach.


== Examples ==
The Kalundborg industrial park is located in Denmark. This industrial park is special because companies reuse each other's waste (which then becomes by-products). For example, the Energy E2 Asnæs Power Station produces gypsum as a by-product of the electricity generation process; this gypsum becomes a resource for the BPB Gyproc A/S which produces plasterboards. This is one example of a system inspired by the biosphere-technosphere metaphor: in ecosystems, the waste from one organism is used as inputs to other organisms; in industrial systems, waste from a company is used as a resource by others.
Apart from the direct benefit of incorporating waste into the loop, the use of an eco-industrial park can be a means of making renewable energy generating plants, like Solar PV, more economical and environmentally friendly. In essence, this assists the growth of the renewable energy industry and the environmental benefits that come with replacing fossil-fuels.Additional examples of industrial ecology include:

Substituting the fly ash byproduct of coal burning practices for cement in concrete production
Using second generation biofuels. An example of this is converting grease or cooking oil to biodiesels to fuel vehicles.
South Africa's National Cleaner Production Center (NCPC) was created in order to make the region's industries more efficient in terms of materials. Results of the use of sustainable methods will include lowered energy costs and improved waste management. The program assesses existing companies to implement change.


== Tools ==


== Future directions ==
The ecosystem metaphor popularized by Frosch and Gallopoulos has been a valuable creative tool for helping researchers look for novel solutions to difficult problems.  Recently, it has been pointed out that this metaphor is based largely on a model of classical ecology, and that advancements in understanding ecology based on complexity science have been made by researchers such as C. S. Holling, James J. Kay, and further advanced in terms of contemporary ecology by others. For industrial ecology, this may mean a shift from a more mechanistic view of systems, to one where sustainability is viewed as an emergent property of a complex system. To explore this further, several researchers are working with agent based modeling techniques
.Exergy analysis is performed in the field of industrial ecology to use energy more efficiently. The term exergy was coined by Zoran Rant in 1956, but the concept was developed by J. Willard Gibbs. In recent decades, utilization of exergy has spread outside physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics.


== Other examples ==
Another great example of industrial ecology both in practice and in potential is the Burnside Cleaner Production Centre in Burnside, Nova Scotia. They play a role in facilitating the 'greening' of over 1200 businesses that are located in Burnside, Eastern Canada's largest industrial park. The creation of waste exchange is a big part of what they work towards, which will promote strong industrial ecology relationships.


== See also ==


== References ==


== Further reading ==
The industrial green game: implications for environmental design and management, Deanna J Richards (Ed), National Academy Press, Washington DC, USA, 1997, ISBN 0-309-05294-7
'Handbook of Input-Output Economics in Industrial Ecology', Sangwon Suh (Ed), Springer, 2009, ISBN 978-1-4020-6154-7
Boons, Frank (2012). ""Freedom Versus Coercion in Industrial Ecology: Mind the Gap!"". Econ Journal Watch. 9 (2): 100–111.
Desrochers, Pierre (2012). ""Freedom Versus Coercion in Industrial Ecology: A Reply to Boons"". Econ Journal Watch. 9 (2): 78–99.


== External links ==
Articles and booksIndustrial Ecology: An Introduction
Industrial Ecology
Industrial Symbiosis Timeline
Journal of Industrial Ecology (Yale University on behalf of the School of Forestry and Environmental Studies).
Industrial Ecology research & articles from The Program for the Human Environment, The Rockefeller UniversityEducationIndustrial Ecology open online course (IEooc)
Erasmus Mundus Master's Programme in Industrial Ecology
Industrial ecology programme at the NTNU: Industrial Ecology Programme at NTNU, Trondheim – Norway
Industrial Ecology Master's Programme at Leiden University & TU Delft (Joint Degree), Leiden/Delft – The Netherlands
Center for Industrial Ecology at Yale University’s School of Forestry & Environmental Studies, New Haven – CT, USAResearch materialInventory of free and open software tools for industrial ecology researchNetworkInternational Society for Industrial Ecology – ISIE","pandas(index=141, _1=141, text='industrial ecology (ie) is the study of material and energy flows through industrial systems.  the global industrial economy can be modelled as a network of industrial processes that extract resources from the earth and transform those resources into commodities which can be bought and sold to meet the needs of humanity.  industrial ecology seeks to quantify the material flows and document the industrial processes that make modern society function.  industrial ecologists are often concerned with the impacts that industrial activities have on the environment, with use of the planet\'s supply of natural resources, and with problems of waste disposal.  industrial ecology is a young but growing multidisciplinary field of research which combines aspects of engineering, economics, sociology, toxicology and the natural sciences. industrial ecology has been defined as a ""systems-based, multidisciplinary discourse that seeks to understand emergent behaviour of complex integrated human/natural systems"". the field approaches issues of sustainability by examining problems from multiple perspectives, usually involving aspects of sociology, the environment, economy and technology.  the name comes from the idea that the analogy of natural systems should be used as an aid in understanding how to design sustainable industrial systems.   == overview ==  industrial ecology is concerned with the shifting of industrial process from linear (open loop) systems, in which resource and capital investments move through the system to become waste, to a closed loop system where wastes can become inputs for new processes. much of the research focuses on the following areas: material and energy flow studies (""industrial metabolism"") dematerialization and decarbonization technological change and the environment life-cycle planning, design and assessment design for the environment (""eco-design"") extended producer responsibility (""product stewardship"") eco-industrial parks (""industrial symbiosis"") product-oriented environmental policy eco-efficiencyindustrial ecology seeks to understand the way in which industrial systems (for example a factory, an ecoregion, or national or global economy) interact with the biosphere.  natural ecosystems provide a metaphor for understanding how different parts of industrial systems interact with one another, in an ""ecosystem"" based on resources and infrastructural capital rather than on natural capital. it seeks to exploit the idea that natural systems do not have waste in them to inspire sustainable design. along with more general energy conservation and material conservation goals, and redefining commodity markets and product stewardship relations strictly as a service economy, industrial ecology is one of the four objectives of natural capitalism.  this strategy discourages forms of amoral purchasing arising from ignorance of what goes on at a distance and implies a political economy that values natural capital highly and relies on more instructional capital to design and maintain each unique industrial ecology.   == history ==  industrial ecology was popularized in 1989 in a scientific american article by robert frosch and nicholas e. gallopoulos.  frosch and gallopoulos\' vision was ""why would not our industrial system behave like an ecosystem, where the wastes of a species may be resource to another species? why would not the outputs of an industry be the inputs of another, thus reducing use of raw materials, pollution, and saving on waste treatment?"" a notable example resides in a danish industrial park in the city of kalundborg.  here several linkages of byproducts and waste heat can be found between numerous entities such as a large power plant, an oil refinery, a pharmaceutical plant, a plasterboard factory, an enzyme manufacturer, a waste company and the city itself. another example is the rantasalmi eip in rantasalmi, finland. while this country has had previous organically formed eip\'s, the park at rantasalmi is finland\'s first planned eip. the scientific field industrial ecology has grown quickly in recent years. the journal of industrial ecology (since 1997), the international society for industrial ecology (since 2001), and the journal progress in industrial ecology (since 2004) give industrial ecology a strong and dynamic position in the international scientific community.  industrial ecology principles are also emerging in various policy realms such as the concept of the circular economy that is being promoted in china. although the definition of the circular economy has yet to be formalized, generally the focus is on strategies such as creating a circular flow of materials, and cascading energy flows. an example of this would be using waste heat from one process to run another process that requires a lower temperature. the hope is that strategy such as this will create a more efficient economy with fewer pollutants and other unwanted by-products.   == principles == one of the central principles of industrial ecology is the view that societal and technological systems are bounded within the biosphere, and do not exist outside it.  ecology is used as a metaphor due to the observation that natural systems reuse materials and have a largely closed loop cycling of nutrients.  industrial ecology approaches problems with the hypothesis that by using similar principles as natural systems, industrial systems can be improved to reduce their impact on the natural environment as well. the table shows the general metaphor.  ie examines societal issues and their relationship with both technical systems and the environment. through this holistic view , ie recognizes that solving problems must involve understanding the connections that exist between these systems, various aspects cannot be viewed in isolation. often changes in one part of the overall system can propagate and cause changes in another part. thus, you can only understand a problem if you look at its parts in relation to the whole. based on this framework, ie looks at environmental issues with a systems thinking approach. a good ie example with these societal impacts can be found at the blue lagoon in iceland. the lagoon uses super-heated water from a local geothermal power plant to fill mineral-rich basins that have become recreational healing centers. in this sense the industrial process of energy production uses its wastewater to provide a crucial resource for the dependent recreational industry. take a city for instance. a city can be divided into commercial areas, residential areas, offices, services, infrastructures, and so forth. these are all sub-systems of the \'big city\' system. problems can emerge in one sub-system, but the solution has to be global. let\'s say the price of housing is rising dramatically because there is too high a demand for housing. one solution would be to build new houses, but this will lead to more people living in the city, leading to the need for more infrastructure like roads, schools, more supermarkets, etc. this system is a simplified interpretation of reality whose behaviors can be \'predicted\'. in many cases, the systems ie deals with are complex systems. complexity makes it difficult to understand the behavior of the system and may lead to rebound effects. due to unforeseen behavioral change of users or consumers, a measure taken to improve environmental performance does not lead to any improvement or may even worsen the situation. moreover, life cycle thinking is also a very important principle in industrial ecology. it implies that all environmental impacts caused by a product, system, or project during its life cycle are taken into account. in this context life cycle includes  raw material extraction material processing manufacture use maintenance disposalthe transport necessary between these stages is also taken into account as well as, if relevant, extra stages such as reuse, remanufacture, and recycle. adopting a life cycle approach is essential to avoid shifting environmental impacts from one life cycle stage to another. this is commonly referred to as problem shifting. for instance, during the re-design of a product, one can choose to reduce its weight, thereby decreasing use of resources. it is possible that the lighter materials used in the new product will be more difficult to dispose of. the environmental impacts of the product gained during the extraction phase are shifted to the disposal phase. overall environmental improvements are thus null. a final important principle of ie is its integrated approach or multidisciplinarity. ie takes into account three different disciplines: social sciences (including economics), technical sciences and environmental sciences. the challenge is to merge them into a single approach.   == examples == the kalundborg industrial park is located in denmark. this industrial park is special because companies reuse each other\'s waste (which then becomes by-products). for example, the energy e2 asnæs power station produces gypsum as a by-product of the electricity generation process; this gypsum becomes a resource for the bpb gyproc a/s which produces plasterboards. this is one example of a system inspired by the biosphere-technosphere metaphor: in ecosystems, the waste from one organism is used as inputs to other organisms; in industrial systems, waste from a company is used as a resource by others. apart from the direct benefit of incorporating waste into the loop, the use of an eco-industrial park can be a means of making renewable energy generating plants, like solar pv, more economical and environmentally friendly. in essence, this assists the growth of the renewable energy industry and the environmental benefits that come with replacing fossil-fuels.additional examples of industrial ecology include:  substituting the fly ash byproduct of coal burning practices for cement in concrete production using second generation biofuels. an example of this is converting grease or cooking oil to biodiesels to fuel vehicles. south africa\'s national cleaner production center (ncpc) was created in order to make the region\'s industries more efficient in terms of materials. results of the use of sustainable methods will include lowered energy costs and improved waste management. the program assesses existing companies to implement change.   == tools ==   == future directions == the ecosystem metaphor popularized by frosch and gallopoulos has been a valuable creative tool for helping researchers look for novel solutions to difficult problems.  recently, it has been pointed out that this metaphor is based largely on a model of classical ecology, and that advancements in understanding ecology based on complexity science have been made by researchers such as c. s. holling, james j. kay, and further advanced in terms of contemporary ecology by others. for industrial ecology, this may mean a shift from a more mechanistic view of systems, to one where sustainability is viewed as an emergent property of a complex system. to explore this further, several researchers are working with agent based modeling techniques .exergy analysis is performed in the field of industrial ecology to use energy more efficiently. the term exergy was coined by zoran rant in 1956, but the concept was developed by j. willard gibbs. in recent decades, utilization of exergy has spread outside physics and engineering to the fields of industrial ecology, ecological economics, systems ecology, and energetics.   == other examples == another great example of industrial ecology both in practice and in potential is the burnside cleaner production centre in burnside, nova scotia. they play a role in facilitating the \'greening\' of over 1200 businesses that are located in burnside, eastern canada\'s largest industrial park. the creation of waste exchange is a big part of what they work towards, which will promote strong industrial ecology relationships.   == see also ==   == references ==   == further reading == the industrial green game: implications for environmental design and management, deanna j richards (ed), national academy press, washington dc, usa, 1997, isbn 0-309-05294-7 \'handbook of input-output economics in industrial ecology\', sangwon suh (ed), springer, 2009, isbn 978-1-4020-6154-7 boons, frank (2012). ""freedom versus coercion in industrial ecology: mind the gap!"". econ journal watch. 9 (2): 100–111. desrochers, pierre (2012). ""freedom versus coercion in industrial ecology: a reply to boons"". econ journal watch. 9 (2): 78–99.   == external links == articles and booksindustrial ecology: an introduction industrial ecology industrial symbiosis timeline journal of industrial ecology (yale university on behalf of the school of forestry and environmental studies). industrial ecology research & articles from the program for the human environment, the rockefeller universityeducationindustrial ecology open online course (ieooc) erasmus mundus master\'s programme in industrial ecology industrial ecology programme at the ntnu: industrial ecology programme at ntnu, trondheim – norway industrial ecology master\'s programme at leiden university & tu delft (joint degree), leiden/delft – the netherlands center for industrial ecology at yale university’s school of forestry & environmental studies, new haven – ct, usaresearch materialinventory of free and open software tools for industrial ecology researchnetworkinternational society for industrial ecology – isie')"
142,"Value engineering (VE) is a systematic method to improve the ""value"" of goods or products and services by using an examination of function. Value, as defined, is the ratio of function to cost. Value can therefore be manipulated by either improving the function or reducing the cost. It is a primary tenet of value engineering that basic functions be preserved and not be reduced as a consequence of pursuing value improvements. The term ""value management"" is sometimes used as a synonym of ""value engineering"", and both promote the planning and delivery of projects with improved performance The reasoning behind value engineering is as follows: if marketers expect a product to become practically or stylistically obsolete within a specific length of time, they can design it to only last for that specific lifetime. The products could be built with higher-grade components, but with value engineering they are not because this would impose an unnecessary cost on the manufacturer, and to a limited extent also an increased cost on the purchaser. Value engineering will reduce these costs. A company will typically use the least expensive components that satisfy the product's lifetime projections. 
Due to the very short life spans, however, which is often a result of this ""value engineering technique"", planned obsolescence has become associated with product deterioration and inferior quality. Vance Packard once claimed this practice gave engineering as a whole a bad name, as it directed creative engineering energies toward short-term market ends. Philosophers such as Herbert Marcuse and Jacque Fresco have also criticized the economic and societal implications of this model.


== History ==
Value engineering began at General Electric Co. during World War II. Because of the war, there were shortages of skilled labour, raw materials, and component parts. Lawrence Miles, Jerry Leftow, and Harry Erlicher at G.E. looked for acceptable substitutes. They noticed that these substitutions often reduced costs, improved the product, or both. What started out as an accident of necessity was turned into a systematic process. They called their technique ""value analysis"".


== Description ==
Value engineering is sometimes taught within the project management, industrial engineering or architecture body of knowledge as a technique in which the value of a system’s outputs is optimized by crafting a mix of performance (function) and costs. It is based on an analysis investigating systems, equipment, facilities, services, and supplies for providing necessary functions at the lowest life cycle cost while meeting the required targets in performance, reliability, quality, and safety. In most cases this practice identifies and removes unnecessary expenditures, thereby increasing the value for the manufacturer and/or their customers.
VE follows a structured thought process that is based exclusively on ""function"", i.e. what something ""does"" not what it is. For example a screw driver that is being used to stir a can of paint has a ""function"" of mixing the contents of a paint can and not the original connotation of securing a screw into a screw-hole. In value engineering ""functions"" are always described in a two word abridgment consisting of an active verb and measurable noun (what is being done – the verb – and what it is being done to – the noun) and to do so in the most non-prescriptive way possible. In the screw driver and can of paint example, the most basic function would be ""blend liquid"" which is less prescriptive than ""stir paint"" which can be seen to limit the action (by stirring) and to limit the application (only considers paint). This is the basis of what value engineering refers to as ""function analysis"".Value engineering uses rational logic (a unique ""how"" - ""why"" questioning technique) and the analysis of function to identify relationships that increase value.  It is considered a quantitative method similar to the scientific method, which focuses on hypothesis-conclusion approaches to test relationships, and operations research, which uses model building to identify predictive relationships.


== Legal terminology ==
In the United States, value engineering is specifically mandated for federal agencies by section 4306 of the National Defense Authorization Act for Fiscal Year 1996, which amended the Office of Federal Procurement Policy Act (41 U.S.C. 401 et seq.):

“Each executive agency shall establish and maintain cost-effective value engineering procedures and processes"".
""As used in this section, the term ‘value engineering’ means an analysis of the functions of a program, project, system, product, item of equipment, building, facility, service, or supply of an executive agency, performed by qualified agency or contractor personnel, directed at improving performance, reliability, quality, safety, and life cycle costs"".


== See also ==
Benefits realisation management
Cost
Cost engineering
Cost overrun
ISO 15686
Muntzing
Overengineering
Value theory


== References ==


== Further reading ==
Cooper, R. and Slagmulder, R. (1997): Target Costing and Value Engineering. 
""Value Optimization for Project and Performance Management by Robert B. Stewart, CVS-Life, FSAVE, PMP""


== External links ==
Lawrence D. Miles Value Foundation
SAVE International - American Value engineering society
wertanalyse.com - Many links regarding VE organizations and publications
The Canadian Society of Value Analysis - Value Engineering in Canada
Value Engineering’s History in Construction- American Institute of Architects - AIA
The Institute of Value Management, UK
the APTE method","pandas(index=142, _1=142, text='value engineering (ve) is a systematic method to improve the ""value"" of goods or products and services by using an examination of function. value, as defined, is the ratio of function to cost. value can therefore be manipulated by either improving the function or reducing the cost. it is a primary tenet of value engineering that basic functions be preserved and not be reduced as a consequence of pursuing value improvements. the term ""value management"" is sometimes used as a synonym of ""value engineering"", and both promote the planning and delivery of projects with improved performance the reasoning behind value engineering is as follows: if marketers expect a product to become practically or stylistically obsolete within a specific length of time, they can design it to only last for that specific lifetime. the products could be built with higher-grade components, but with value engineering they are not because this would impose an unnecessary cost on the manufacturer, and to a limited extent also an increased cost on the purchaser. value engineering will reduce these costs. a company will typically use the least expensive components that satisfy the product\'s lifetime projections. due to the very short life spans, however, which is often a result of this ""value engineering technique"", planned obsolescence has become associated with product deterioration and inferior quality. vance packard once claimed this practice gave engineering as a whole a bad name, as it directed creative engineering energies toward short-term market ends. philosophers such as herbert marcuse and jacque fresco have also criticized the economic and societal implications of this model.   == history == value engineering began at general electric co. during world war ii. because of the war, there were shortages of skilled labour, raw materials, and component parts. lawrence miles, jerry leftow, and harry erlicher at g.e. looked for acceptable substitutes. they noticed that these substitutions often reduced costs, improved the product, or both. what started out as an accident of necessity was turned into a systematic process. they called their technique ""value analysis"".   == description == value engineering is sometimes taught within the project management, industrial engineering or architecture body of knowledge as a technique in which the value of a system’s outputs is optimized by crafting a mix of performance (function) and costs. it is based on an analysis investigating systems, equipment, facilities, services, and supplies for providing necessary functions at the lowest life cycle cost while meeting the required targets in performance, reliability, quality, and safety. in most cases this practice identifies and removes unnecessary expenditures, thereby increasing the value for the manufacturer and/or their customers. ve follows a structured thought process that is based exclusively on ""function"", i.e. what something ""does"" not what it is. for example a screw driver that is being used to stir a can of paint has a ""function"" of mixing the contents of a paint can and not the original connotation of securing a screw into a screw-hole. in value engineering ""functions"" are always described in a two word abridgment consisting of an active verb and measurable noun (what is being done – the verb – and what it is being done to – the noun) and to do so in the most non-prescriptive way possible. in the screw driver and can of paint example, the most basic function would be ""blend liquid"" which is less prescriptive than ""stir paint"" which can be seen to limit the action (by stirring) and to limit the application (only considers paint). this is the basis of what value engineering refers to as ""function analysis"".value engineering uses rational logic (a unique ""how"" - ""why"" questioning technique) and the analysis of function to identify relationships that increase value.  it is considered a quantitative method similar to the scientific method, which focuses on hypothesis-conclusion approaches to test relationships, and operations research, which uses model building to identify predictive relationships.   == legal terminology == in the united states, value engineering is specifically mandated for federal agencies by section 4306 of the national defense authorization act for fiscal year 1996, which amended the office of federal procurement policy act (41 u.s.c. 401 et seq.):  “each executive agency shall establish and maintain cost-effective value engineering procedures and processes"". ""as used in this section, the term ‘value engineering’ means an analysis of the functions of a program, project, system, product, item of equipment, building, facility, service, or supply of an executive agency, performed by qualified agency or contractor personnel, directed at improving performance, reliability, quality, safety, and life cycle costs"".   == see also == benefits realisation management cost cost engineering cost overrun iso 15686 muntzing overengineering value theory   == references ==   == further reading == cooper, r. and slagmulder, r. (1997): target costing and value engineering. ""value optimization for project and performance management by robert b. stewart, cvs-life, fsave, pmp""   == external links == lawrence d. miles value foundation save international - american value engineering society wertanalyse.com - many links regarding ve organizations and publications the canadian society of value analysis - value engineering in canada value engineering’s history in construction- american institute of architects - aia the institute of value management, uk the apte method')"
143,"In the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. The creation of this field of statistics has been credited to Danish statistician Kirstine Smith.In the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum variance. A non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. In practical terms, optimal experiments can reduce the costs of experimentation.
The optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. Specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments.


== Advantages ==
Optimal designs offer three advantages over sub-optimal experimental designs:
Optimal designs reduce the costs of experimentation by allowing statistical models to be estimated with fewer experimental runs.
Optimal designs can accommodate multiple types of factors, such as process, mixture, and discrete factors.
Designs can be optimized when the design-space is constrained, for example, when the mathematical process-space contains factor-settings that are practically infeasible (e.g. due to safety concerns).


== Minimizing the variance of estimators ==
Experimental designs are evaluated using statistical criteria.It is known that the least squares estimator minimizes the variance of mean-unbiased estimators (under the conditions of the Gauss–Markov theorem). In the estimation theory for statistical models with one real parameter, the reciprocal of the variance of an (""efficient"") estimator is called the ""Fisher information"" for that estimator. Because of this reciprocity, minimizing the variance corresponds to maximizing the information.
When the statistical model has several parameters, however, the mean of the parameter-estimator is a vector and its variance is a matrix. The inverse matrix of the variance-matrix is called the ""information matrix"". Because the variance of the estimator of a parameter vector is a matrix, the problem of ""minimizing the variance"" is complicated. Using statistical theory, statisticians compress the  information-matrix using real-valued summary statistics; being real-valued functions, these ""information criteria"" can be maximized. The traditional optimality-criteria are invariants of the information matrix; algebraically, the traditional optimality-criteria are functionals of the eigenvalues of the information matrix.

A-optimality (""average"" or trace)
One criterion is A-optimality, which seeks to minimize the trace of the inverse of the information matrix. This criterion results in minimizing the average variance of the estimates of the regression coefficients.
C-optimality
This criterion minimizes the variance of a best linear unbiased estimator of a predetermined linear combination of model parameters.
D-optimality (determinant)
A popular criterion is D-optimality, which seeks to minimize |(X'X)−1|, or equivalently maximize the determinant of the information matrix X'X of the design. This criterion results in maximizing the differential Shannon information content of the parameter estimates.
E-optimality (eigenvalue)
Another design is E-optimality, which maximizes the minimum eigenvalue of the information matrix.
T-optimality
This criterion maximizes the trace of the information matrix.Other optimality-criteria are concerned with the variance of predictions:

G-optimality
A popular criterion is G-optimality, which seeks to minimize the maximum entry in the diagonal of the hat matrix X(X'X)−1X'. This has the effect of minimizing the maximum variance of the predicted values.
I-optimality (integrated)
A second criterion on prediction variance is I-optimality, which seeks to minimize  the average prediction variance over the design space.
V-optimality (variance)
A third criterion on prediction variance is V-optimality, which seeks to minimize the average prediction variance over a set of m specific points.


=== Contrasts ===
 
In many applications, the statistician is most concerned with a ""parameter of interest"" rather than with ""nuisance parameters"". More generally, statisticians consider linear combinations of parameters, which are estimated via linear combinations of treatment-means in the design of experiments and in the analysis of variance; such linear combinations are called contrasts. Statisticians can use appropriate optimality-criteria for such parameters of interest and for contrasts.


== Implementation ==
Catalogs of optimal designs occur in books and in software libraries.
In addition, major statistical systems like SAS and R have procedures for optimizing a design according to a user's specification. The experimenter must specify a model for the design and an optimality-criterion before the method can compute an optimal design.


== Practical considerations ==
Some advanced topics in optimal design require more statistical theory and practical knowledge in designing experiments.


=== Model dependence and robustness ===
Since the optimality criterion of most optimal designs is based on some function of the information matrix, the 'optimality' of a given design is model dependent: While an optimal design is best for that model, its performance may deteriorate on other models. On other models, an optimal design can be either better or worse than a non-optimal design. Therefore, it is important to benchmark the performance of designs under alternative models.


=== Choosing an optimality criterion and robustness ===
The choice of an appropriate optimality criterion requires some thought, and it is useful to benchmark the performance of designs with respect to several optimality criteria. Cornell writes that

since the [traditional optimality] criteria . . . are variance-minimizing criteria, . . .  a design that is optimal for a given model using one of the . . . criteria is usually near-optimal for the same model with respect to the other criteria.
Indeed, there are several classes of designs for which all the traditional optimality-criteria agree, according to the theory of ""universal optimality"" of Kiefer. The experience of practitioners like Cornell and the ""universal optimality"" theory of Kiefer suggest that robustness with respect to changes in the optimality-criterion is much greater than is robustness with respect to changes in the model.


==== Flexible optimality criteria and convex analysis ====
High-quality statistical software provide a combination of libraries of optimal designs or iterative methods for constructing approximately optimal designs, depending on the model specified and the optimality criterion. Users may use a standard optimality-criterion or may program a custom-made criterion.
All of the traditional optimality-criteria are convex (or concave) functions, and therefore optimal-designs are amenable to the mathematical theory of convex analysis and their computation can use specialized methods of convex minimization. The practitioner need not select exactly one traditional, optimality-criterion, but can specify a custom criterion. In particular, the practitioner can specify a convex criterion using the maxima of convex optimality-criteria and nonnegative combinations of optimality criteria (since these operations preserve convex functions). For convex optimality criteria, the Kiefer-Wolfowitz equivalence theorem allows the practitioner to verify that a given design is globally optimal. The Kiefer-Wolfowitz equivalence theorem is related with the Legendre-Fenchel conjugacy for convex functions.If an optimality-criterion lacks convexity, then finding a global optimum and verifying its optimality often are difficult.


=== Model uncertainty and Bayesian approaches ===


==== Model selection ====

When scientists wish to test several theories, then a statistician can design an experiment that allows optimal tests between specified models. Such ""discrimination experiments"" are especially important in the biostatistics supporting  pharmacokinetics and pharmacodynamics, following the work of Cox and Atkinson.


==== Bayesian experimental design ====

When practitioners need to consider multiple models, they can specify a probability-measure on the models and then select any design maximizing the expected value of such an experiment. Such probability-based optimal-designs are called optimal Bayesian designs. Such Bayesian designs are used especially for generalized linear models (where the response follows an exponential-family distribution).The use of a Bayesian design does not force statisticians to use Bayesian methods to analyze the data, however. Indeed, the ""Bayesian"" label for probability-based experimental-designs is disliked by some researchers.  Alternative terminology for ""Bayesian"" optimality includes ""on-average"" optimality or ""population"" optimality.


== Iterative experimentation ==
Scientific experimentation is an iterative process, and statisticians have developed several approaches to the optimal design of sequential experiments.


=== Sequential analysis ===

Sequential analysis was pioneered by Abraham Wald. In 1972, Herman Chernoff wrote an overview of optimal sequential designs, while adaptive designs were surveyed later by S. Zacks. Of course, much work on the optimal design of experiments is related to the theory of optimal decisions, especially the statistical decision theory of Abraham Wald.


=== Response-surface methodology ===

Optimal designs for response-surface models are discussed in the textbook by Atkinson, Donev and Tobias, and in the survey of Gaffke and Heiligers and in the mathematical text of Pukelsheim. The blocking of optimal designs is discussed in the textbook of Atkinson, Donev and Tobias and also in the monograph by Goos.
The earliest optimal designs were developed to estimate the parameters of regression models with continuous variables, for example, by J. D. Gergonne in 1815 (Stigler).  In English, two early contributions were made by Charles S. Peirce and Kirstine Smith.
Pioneering designs for multivariate response-surfaces were proposed by George E. P. Box. However, Box's designs have few optimality properties. Indeed, the Box–Behnken design requires excessive experimental runs when the number of variables exceeds three.
Box's ""central-composite"" designs require more experimental runs than do the optimal designs of Kôno.


=== System identification and stochastic approximation ===

The optimization of sequential experimentation is studied also in stochastic programming and in systems and control. Popular methods include stochastic approximation and other methods of stochastic optimization. Much of this research has been associated with the subdiscipline of system identification.
In computational optimal control, D. Judin & A. Nemirovskii and Boris Polyak has described methods that are more efficient than the (Armijo-style) step-size rules introduced by G. E. P. Box in response-surface methodology.Adaptive designs are used in clinical trials, and optimal adaptive designs are surveyed in the Handbook of Experimental Designs chapter by Shelemyahu Zacks.


== Specifying the number of experimental runs ==


=== Using a computer to find a good design ===
There are several methods of finding an optimal design, given an a priori restriction on the number of experimental runs or replications. Some of these methods are discussed by Atkinson, Donev and Tobias and in the paper by Hardin and Sloane. Of course, fixing the number of experimental runs a priori would be impractical.  Prudent statisticians examine the other optimal designs, whose number of experimental runs differ.


=== Discretizing probability-measure designs ===
In the mathematical theory on optimal experiments, an optimal design can be a probability measure that is supported on an infinite set of observation-locations. Such optimal probability-measure designs solve a mathematical problem that neglected to specify the cost of observations and experimental runs. Nonetheless, such optimal probability-measure designs can be discretized to furnish approximately optimal designs.In some cases, a finite set of observation-locations suffices to support an optimal design. Such a result was proved by Kôno and Kiefer in their works on response-surface designs for quadratic models. The Kôno–Kiefer analysis explains why optimal designs for response-surfaces can have discrete supports, which are very similar as do the less efficient designs that have been traditional in response surface methodology.


== History ==
In 1815, an article on optimal designs for polynomial regression was published by Joseph Diaz Gergonne, according to Stigler.
Charles S. Peirce proposed an economic theory of scientific experimentation in 1876, which sought to maximize the precision of the estimates. Peirce's optimal allocation immediately improved the accuracy of gravitational experiments and was used for decades by Peirce and his colleagues. In his 1882 published lecture at Johns Hopkins University, Peirce introduced experimental design with these words:

Logic will not undertake to inform you what kind of experiments you ought to make in order best to determine the acceleration of gravity, or the value of the Ohm; but it will tell you how to proceed to form a plan of experimentation.[....] Unfortunately practice generally precedes theory, and it is the usual fate of mankind to get things done in some boggling way first, and find out afterward how they could have been done much more easily and perfectly.

Kirstine Smith proposed optimal designs for polynomial models in 1918. (Kirstine Smith had been a student of the Danish statistician Thorvald N. Thiele and was working with Karl Pearson in London.)


== See also ==


== Notes ==


== References ==
Atkinson, A. C.; Donev, A. N.; Tobias, R. D. (2007). Optimum experimental designs, with SAS. Oxford University Press. pp. 511+xvi. ISBN 978-0-19-929660-6.
Chernoff, Herman (1972). Sequential analysis and optimal design. Society for Industrial and Applied Mathematics. ISBN 978-0-89871-006-9.
Fedorov, V. V. (1972). Theory of Optimal Experiments. Academic Press.
Fedorov, Valerii V.; Hackl, Peter (1997). Model-Oriented Design of Experiments. Lecture Notes in Statistics. 125. Springer-Verlag.
Goos, Peter (2002). The Optimal Design of Blocked and Split-plot Experiments. Lecture Notes in Statistics. 164. Springer.
Kiefer, Jack Carl (1985).  Brown; Olkin, Ingram; Sacks, Jerome;  et al. (eds.). Jack Carl Kiefer: Collected papers III—Design of experiments. Springer-Verlag and the Institute of Mathematical Statistics. pp. 718+xxv. ISBN 978-0-387-96004-3.
Logothetis, N.; Wynn, H. P. (1989). Quality through design: Experimental design, off-line quality control, and Taguchi's contributions. Oxford U. P. pp. 464+xi. ISBN 978-0-19-851993-5.
Nordström, Kenneth (May 1999). ""The life and work of Gustav Elfving"". Statistical Science. 14 (2): 174–196. doi:10.1214/ss/1009212244. JSTOR 2676737. MR 1722074.
Pukelsheim, Friedrich (2006). Optimal design of experiments. Classics in Applied Mathematics. 50 (republication with errata-list and new preface of Wiley (0-471-61971-X) 1993 ed.). Society for Industrial and Applied Mathematics. pp. 454+xxxii. ISBN 978-0-89871-604-7.
Shah, Kirti R. & Sinha, Bikas K. (1989). Theory of Optimal Designs. Lecture Notes in Statistics. 54. Springer-Verlag. pp. 171+viii. ISBN 978-0-387-96991-6.


== Further reading ==


=== Textbooks for practitioners and students ===


==== Textbooks emphasizing regression and response-surface methodology ====
The textbook by Atkinson, Donev and Tobias has been used for short courses for industrial practitioners as well as university courses.

Atkinson, A. C.; Donev, A. N.; Tobias, R. D. (2007). Optimum experimental designs, with SAS. Oxford University Press. pp. 511+xvi. ISBN 978-0-19-929660-6.
Logothetis, N.; Wynn, H. P. (1989). Quality through design: Experimental design, off-line quality control, and Taguchi's contributions. Oxford U. P. pp. 464+xi. ISBN 978-0-19-851993-5.


==== Textbooks emphasizing block designs ====
Optimal block designs are discussed by Bailey and by Bapat. The first chapter of Bapat's book reviews the linear algebra used by Bailey (or the advanced books below). Bailey's exercises and discussion of randomization both emphasize statistical concepts (rather than algebraic computations).

Bailey, R. A. (2008). Design of Comparative Experiments. Cambridge U. P. ISBN 978-0-521-68357-9. Draft available on-line. (Especially Chapter 11.8 ""Optimality"")
Bapat, R. B. (2000). Linear Algebra and Linear Models (Second ed.). Springer. ISBN 978-0-387-98871-9. (Chapter 5 ""Block designs and optimality"", pages 99–111)Optimal block designs are discussed in the advanced monograph by Shah and Sinha and in the survey-articles by Cheng and by Majumdar.


=== Books for professional statisticians and researchers ===
Chernoff, Herman (1972). Sequential Analysis and Optimal Design. SIAM. ISBN 978-0-89871-006-9.
Fedorov, V. V. (1972). Theory of Optimal Experiments. Academic Press.
Fedorov, Valerii V.; Hackl, Peter (1997). Model-Oriented Design of Experiments. 125. Springer-Verlag.
Goos, Peter (2002). The Optimal Design of Blocked and Split-plot Experiments. 164. Springer.
Goos, Peter & Jones, Bradley (2011). Optimal design of experiments: a case study approach. Chichester Wiley. p. 304. ISBN 978-0-470-74461-1.
Kiefer, Jack Carl. (1985).  Brown, Lawrence D.; Olkin, Ingram; Jerome Sacks; Wynn, Henry P (eds.). Jack Carl Kiefer Collected Papers III Design of Experiments. Springer-Verlag and the Institute of Mathematical Statistics. ISBN 978-0-387-96004-3.
Pukelsheim, Friedrich (2006). Optimal Design of Experiments. 50. Society for Industrial and Applied Mathematics. ISBN 978-0-89871-604-7. Republication with errata-list and new preface of Wiley (0-471-61971-X) 1993
Shah, Kirti R. & Sinha, Bikas K. (1989). Theory of Optimal Designs. 54. Springer-Verlag. ISBN 978-0-387-96991-6.


=== Articles and chapters ===
Chaloner, Kathryn & Verdinelli, Isabella (1995). ""Bayesian Experimental Design: A Review"". Statistical Science. 10 (3): 273–304. CiteSeerX 10.1.1.29.5355. doi:10.1214/ss/1177009939.
Ghosh, S.; Rao, C. R., eds. (1996). Design and Analysis of Experiments. Handbook of Statistics. 13. North-Holland. ISBN 978-0-444-82061-7.
""Model Robust Designs"". Design and Analysis of Experiments. Handbook of Statistics. pp. 1055–1099.
Cheng, C.-S. ""Optimal Design: Exact Theory"". Design and Analysis of Experiments. Handbook of Statistics. pp. 977–1006.
DasGupta, A. ""Review of Optimal Bayesian Designs"". Design and Analysis of Experiments. Handbook of Statistics. pp. 1099–1148.
Gaffke, N. & Heiligers, B. ""Approximate Designs for Polynomial Regression: Invariance, Admissibility, and Optimality"". Design and Analysis of Experiments. Handbook of Statistics. pp. 1149–1199.
Majumdar, D. ""Optimal and Efficient Treatment-Control Designs"". Design and Analysis of Experiments. Handbook of Statistics. pp. 1007–1054.
Stufken, J. ""Optimal Crossover Designs"". Design and Analysis of Experiments. Handbook of Statistics. pp. 63–90.
Zacks, S. ""Adaptive Designs for Parametric Models"". Design and Analysis of Experiments. Handbook of Statistics. pp. 151–180.
Kôno, Kazumasa (1962). ""Optimum designs for quadratic regression on k-cube"" (PDF). Memoirs of the Faculty of Science. Kyushu University. Series A. Mathematics. 16 (2): 114–122. doi:10.2206/kyushumfs.16.114.


=== Historical ===
Gergonne, J. D. (November 1974) [1815]. ""The application of the method of least squares to the interpolation of sequences"". Historia Mathematica (Translated by Ralph St. John and S. M. Stigler from the 1815 French ed.). 1 (4): 439–447. doi:10.1016/0315-0860(74)90034-2.
Stigler, Stephen M. (November 1974). ""Gergonne's 1815 paper on the design and analysis of polynomial regression experiments"". Historia Mathematica. 1 (4): 431–439. doi:10.1016/0315-0860(74)90033-0.
Peirce, C. S (1876). ""Note on the Theory of the Economy of Research"". Coast Survey Report: 197–201. (Appendix No. 14). NOAA PDF Eprint. Reprinted in Collected Papers of Charles Sanders Peirce. 7. 1958. paragraphs 139–157, and in Peirce, C. S. (July–August 1967). ""Note on the Theory of the Economy of Research"". Operations Research. 15 (4): 643–648. doi:10.1287/opre.15.4.643. JSTOR 168276.
Smith, Kirstine (1918). ""On the Standard Deviations of Adjusted and Interpolated Values of an Observed Polynomial Function and its Constants and the Guidance They Give Towards a Proper Choice of the Distribution of the Observations"". Biometrika. 12 (1/2): 1–85. doi:10.2307/2331929. JSTOR 2331929.","pandas(index=143, _1=143, text='in the design of experiments, optimal designs (or optimum designs) are a class of experimental designs that are optimal with respect to some statistical criterion. the creation of this field of statistics has been credited to danish statistician kirstine smith.in the design of experiments for estimating statistical models, optimal designs allow parameters to be estimated without bias and with minimum variance. a non-optimal design requires a greater number of experimental runs to estimate the parameters with the same precision as an optimal design. in practical terms, optimal experiments can reduce the costs of experimentation. the optimality of a design depends on the statistical model and is assessed with respect to a statistical criterion, which is related to the variance-matrix of the estimator. specifying an appropriate model and specifying a suitable criterion function both require understanding of statistical theory and practical knowledge with designing experiments.   == advantages == optimal designs offer three advantages over sub-optimal experimental designs: optimal designs reduce the costs of experimentation by allowing statistical models to be estimated with fewer experimental runs. optimal designs can accommodate multiple types of factors, such as process, mixture, and discrete factors. designs can be optimized when the design-space is constrained, for example, when the mathematical process-space contains factor-settings that are practically infeasible (e.g. due to safety concerns).   == minimizing the variance of estimators == experimental designs are evaluated using statistical criteria.it is known that the least squares estimator minimizes the variance of mean-unbiased estimators (under the conditions of the gauss–markov theorem). in the estimation theory for statistical models with one real parameter, the reciprocal of the variance of an (""efficient"") estimator is called the ""fisher information"" for that estimator. because of this reciprocity, minimizing the variance corresponds to maximizing the information. when the statistical model has several parameters, however, the mean of the parameter-estimator is a vector and its variance is a matrix. the inverse matrix of the variance-matrix is called the ""information matrix"". because the variance of the estimator of a parameter vector is a matrix, the problem of ""minimizing the variance"" is complicated. using statistical theory, statisticians compress the  information-matrix using real-valued summary statistics; being real-valued functions, these ""information criteria"" can be maximized. the traditional optimality-criteria are invariants of the information matrix; algebraically, the traditional optimality-criteria are functionals of the eigenvalues of the information matrix.  a-optimality (""average"" or trace) one criterion is a-optimality, which seeks to minimize the trace of the inverse of the information matrix. this criterion results in minimizing the average variance of the estimates of the regression coefficients. c-optimality this criterion minimizes the variance of a best linear unbiased estimator of a predetermined linear combination of model parameters. d-optimality (determinant) a popular criterion is d-optimality, which seeks to minimize |(x\'x)−1|, or equivalently maximize the determinant of the information matrix x\'x of the design. this criterion results in maximizing the differential shannon information content of the parameter estimates. e-optimality (eigenvalue) another design is e-optimality, which maximizes the minimum eigenvalue of the information matrix. t-optimality this criterion maximizes the trace of the information matrix.other optimality-criteria are concerned with the variance of predictions:  g-optimality a popular criterion is g-optimality, which seeks to minimize the maximum entry in the diagonal of the hat matrix x(x\'x)−1x\'. this has the effect of minimizing the maximum variance of the predicted values. i-optimality (integrated) a second criterion on prediction variance is i-optimality, which seeks to minimize  the average prediction variance over the design space. v-optimality (variance) a third criterion on prediction variance is v-optimality, which seeks to minimize the average prediction variance over a set of m specific points. gergonne, j. d. (november 1974) [1815]. ""the application of the method of least squares to the interpolation of sequences"". historia mathematica (translated by ralph st. john and s. m. stigler from the 1815 french ed.). 1 (4): 439–447. doi:10.1016/0315-0860(74)90034-2. stigler, stephen m. (november 1974). ""gergonne\'s 1815 paper on the design and analysis of polynomial regression experiments"". historia mathematica. 1 (4): 431–439. doi:10.1016/0315-0860(74)90033-0. peirce, c. s (1876). ""note on the theory of the economy of research"". coast survey report: 197–201. (appendix no. 14). noaa pdf eprint. reprinted in collected papers of charles sanders peirce. 7. 1958. paragraphs 139–157, and in peirce, c. s. (july–august 1967). ""note on the theory of the economy of research"". operations research. 15 (4): 643–648. doi:10.1287/opre.15.4.643. jstor 168276. smith, kirstine (1918). ""on the standard deviations of adjusted and interpolated values of an observed polynomial function and its constants and the guidance they give towards a proper choice of the distribution of the observations"". biometrika. 12 (1/2): 1–85. doi:10.2307/2331929. jstor 2331929.')"
144,"The Lang Factor is an estimated ratio of the total cost of creating a process within a plant, to the cost of all major technical components. It is widely used in industrial engineering to calculate the capital and operating costs of a plant.The factors were introduced by H. J. Lang and Dr Micheal Bird in Chemical Engineering magazine in 1947 as a method for estimating the total installation cost for plants and equipment.


== Industries ==
These factors are widely used in the refining and petrochemical industries to help estimate the cost of new facilities.  A typical multiplier for a new unit within a refinery would be in the range of 5.0.  When the purchase price of all the pumps, heat exchangers, pressure vessels, and other process equipment are multiplied by 5.0,  a rough estimate of the total installed cost of the plant, including equipment, materials, construction, and engineering will be achieved. The accuracy of this estimate method usually is +/- 35%.


== Guthrie factors ==
The factors change over time because construction labor, bulk materials (concrete, pipe, etc.), engineering design, indirect costs, and major process equipment prices often do not change at the same rate.
In the late 1960s and early 1970s Kenneth Guthrie further expanded on this concept, generating different factors for different types of process equipment (pumps, exchangers, vessels, etc.). These are sometimes referred to as ""Guthrie factors"".


== References ==","pandas(index=144, _1=144, text='the lang factor is an estimated ratio of the total cost of creating a process within a plant, to the cost of all major technical components. it is widely used in industrial engineering to calculate the capital and operating costs of a plant.the factors were introduced by h. j. lang and dr micheal bird in chemical engineering magazine in 1947 as a method for estimating the total installation cost for plants and equipment.   == industries == these factors are widely used in the refining and petrochemical industries to help estimate the cost of new facilities.  a typical multiplier for a new unit within a refinery would be in the range of 5.0.  when the purchase price of all the pumps, heat exchangers, pressure vessels, and other process equipment are multiplied by 5.0,  a rough estimate of the total installed cost of the plant, including equipment, materials, construction, and engineering will be achieved. the accuracy of this estimate method usually is/- 35%.   == guthrie factors == the factors change over time because construction labor, bulk materials (concrete, pipe, etc.), engineering design, indirect costs, and major process equipment prices often do not change at the same rate. in the late 1960s and early 1970s kenneth guthrie further expanded on this concept, generating different factors for different types of process equipment (pumps, exchangers, vessels, etc.). these are sometimes referred to as ""guthrie factors"".   == references ==')"
145,"Methods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes.


== Overview ==
Alternatively it can be described as the design of the productive process in which a person is involved. The task of the Methods engineer is to decide where humans will be utilized in the process of converting raw materials to finished products and how workers can most effectively perform their assigned tasks. The terms operation analysis, work design and simplification, and methods engineering and corporate re-engineering are frequently used interchangeably.Lowering costs and increasing reliability and productivity are the objectives of methods engineering. These objectives are met in a five step sequence as follows: Project selection,  data acquisition and presentation, data analysis, development of an ideal method based on the data analysis and, finally, presentation and implementation of the method.


== Methods engineering topics ==


=== Project selection ===
Methods engineers typically work on projects involving new product design, products with a high cost of production to profit ratio, and products associated with having poor quality issues. Different methods of project selection include the Pareto analysis, fish diagrams, Gantt charts, PERT charts, and job/work site analysis guides.


=== Data acquisition and presentation ===
Data that needs to be collected are specification sheets for the product, design drawings, quantity and delivery requirements, and projections as to how the product will perform or has performed in the market. The Gantt process chart can assist in the analysis of the man to machine interaction and it can aid in establishing the optimum number of workers and machines subject to the financial constraints of the operation. A flow diagram is frequently employed to represent the manufacturing process associated with the product.


=== Data analysis ===
Data analysis enables the methods engineer to make decisions about several things, including: purpose of the operation, part design characteristics, specifications and tolerances of parts, materials, manufacturing process design, setup and tooling, working conditions, material handling, plant layout, and workplace design. Knowing the specifics (who, what, when, where, why, and how) of product manufacturing assists in the development of an optimum manufacturing method.


=== Ideal method development ===
Equations of synchronous and random servicing as well as line balancing are used to determine the ideal worker to machine ratio for the process or product chosen. Synchronous servicing is defined as the process where a machine is assigned to more than one operator, and the assigned operators and machine are occupied during the whole operating cycle. Random servicing of a facility, as the name indicates, is defined as a servicing process with a random time of occurrence and need of servicing variables. Line balancing equations determine the ideal number of workers needed on a production line to enable it to work at capacity.


=== Presentation and methods implementation ===
The industrial process or operation can be optimized using a variety of available methods. Each method design has its advantages and disadvantages. The best overall method is chosen using selection criteria and concepts involving value engineering, cost-benefit analysis, crossover charts, and economic analysis. The outcome of the selection process is then presented to the company for implementation at the plant. This last step involves ""selling the idea"" to the company brass, a skill the methods engineer must develop in addition to the normal engineering qualifications.


== See also ==
Work design
Motion analysis


== References ==","pandas(index=145, _1=145, text='methods engineering is a subspecialty of industrial engineering and manufacturing engineering concerned with human integration in industrial production processes.   == overview == alternatively it can be described as the design of the productive process in which a person is involved. the task of the methods engineer is to decide where humans will be utilized in the process of converting raw materials to finished products and how workers can most effectively perform their assigned tasks. the terms operation analysis, work design and simplification, and methods engineering and corporate re-engineering are frequently used interchangeably.lowering costs and increasing reliability and productivity are the objectives of methods engineering. these objectives are met in a five step sequence as follows: project selection,  data acquisition and presentation, data analysis, development of an ideal method based on the data analysis and, finally, presentation and implementation of the method.   == methods engineering topics == the industrial process or operation can be optimized using a variety of available methods. each method design has its advantages and disadvantages. the best overall method is chosen using selection criteria and concepts involving value engineering, cost-benefit analysis, crossover charts, and economic analysis. the outcome of the selection process is then presented to the company for implementation at the plant. this last step involves ""selling the idea"" to the company brass, a skill the methods engineer must develop in addition to the normal engineering qualifications.   == see also == work design motion analysis   == references ==')"
146,"A time and motion study (or time-motion study) is a business efficiency technique combining the Time Study work of Frederick Winslow Taylor with the Motion Study work of Frank and Lillian Gilbreth (the same couple as is best known through the biographical 1950 film and book Cheaper by the Dozen). It is a major part of scientific management (Taylorism). After its first introduction, time study developed in the direction of establishing standard times, while motion study evolved into a technique for improving work methods. The two techniques became integrated and refined into a widely accepted method applicable to the improvement and upgrading of work systems. This integrated approach to work system improvement is known as methods engineering and it is applied today to industrial as well as service organizations, including banks, schools and hospitals.


== Time studies ==
Time study is a direct and continuous observation of a task, using a timekeeping device (e.g., decimal minute stopwatch, computer-assisted electronic stopwatch, and videotape camera) to record the time taken to accomplish a task and it is often used when:
there are repetitive work cycles of short to long duration,
wide variety of dissimilar work is performed, or
process control elements constitute a part of the cycle.The Industrial Engineering Terminology Standard, defines time study as ""a work measurement technique consisting of careful time measurement of the task with a time measuring instrument, adjusted for any observed variance from normal effort or pace and to allow adequate time for such items as foreign elements, unavoidable or machine delays, rest to overcome fatigue, and personal needs.""The systems of time and motion studies are frequently assumed to be interchangeable terms, descriptive of equivalent theories. However, the underlying principles and the rationale for the establishment of each respective method are dissimilar, despite originating within the same school of thought.
The application of science to business problems, and the use of time-study methods in standard setting and the planning of work, was pioneered by Frederick Winslow Taylor. Taylor liaised with factory managers and from the success of these discussions wrote several papers proposing the use of wage-contingent performance standards based on scientific time study. At its most basic level time studies involved breaking down each job into component parts, timing each part and rearranging the parts into the most efficient method of working. By counting and calculating, Taylor wanted to transform management, which was essentially an oral tradition, into a set of calculated and written techniques.Taylor and his colleagues placed emphasis on the content of a fair day's work, and sought to maximize productivity irrespective of the physiological cost to the worker. For example, Taylor thought unproductive time usage (soldiering) to be the deliberate attempt of workers to promote their best interests and to keep employers ignorant of how fast work could be carried out. This instrumental view of human behavior by Taylor prepared the path for human relations to supersede scientific management in terms of literary success and managerial application.


=== Direct time study procedure ===
Following is the procedure developed by Mikell Groover for a direct time study:
Define and document the standard method.
Divide the task into work elements.
These first two steps are conducted prior to the actual timing. They familiarize the analyst with the task and allow the analyst to attempt to improve the work procedure before defining the standard time.
Time the work elements to obtain the observed time for the task.
Evaluate the worker's pace relative to standard performance (performance rating), to determine the normal time.
Note that steps 3 and 4 are accomplished simultaneously.  During these steps, several different work cycles are timed, and each cycle performance is rated independently. Finally, the values collected at these steps are averaged to get the normalized time.
Apply an allowance to the normal time to compute the standard time. The allowance factors that are needed in the work are then added to compute the standard time for the task.


=== Conducting time studies ===
According to good practice guidelines for production studies  a comprehensive time study consists of:

Study goal setting;
Experimental design;
Time data collection;
Data analysis;
Reporting.Easy analysis of working areas
The collection of time data can be done in several ways, depending on study goal and environmental conditions. Time and motion data can be captured with a common stopwatch, a handheld computer or a video recorder.  There are a number of dedicated software packages used to turn a palmtop or a handheld PC into a time study device. As an alternative, time and motion data can be collected automatically from the memory of computer-control machines (i.e. automated time studies).


=== Criticisms ===
In response to Taylor's time studies and view of human nature, many strong criticisms and reactions were recorded. Unions, for example, regarded time study as a disguised tool of management designed to standardize and intensify the pace of production. Similarly, individuals such as Gilbreth (1909), Cadbury and Marshall heavily criticized Taylor and pervaded his work with subjectivity. For example, Cadbury in reply to Thompson stated that under scientific management employee skills and initiatives are passed from the individual to management, a view reiterated by Nyland. In addition, Taylor's critics condemned the lack of scientific substance in his time studies, in the sense that they relied heavily on individual interpretations of what workers actually do. However, the value in rationalizing production is indisputable and supported by academics such as Gantt, Ford and Munsterberg, and Taylor society members Mr C.G. Renold, Mr W.H. Jackson and Mr C.B. Thompson.
Proper time studies are based on repeated observation, so that motions performed on the same part differently by one or many workers can be recorded, to determine those values that are truly repetitive and measurable.


== Motion studies ==
In contrast to, and motivated by, Taylor's time study methods, the Gilbreths proposed a technical language, allowing for the analysis of the labor process in a scientific context. The Gilbreths made use of scientific insights to develop a study method based upon the analysis of ""work motions"", consisting in part of filming the details of a worker's activities and their body posture while recording the time. The films served two main purposes. One was the visual record of how work had been done, emphasizing areas for improvement. Secondly, the films also served the purpose of training workers about the best way to perform their work. This method allowed the Gilbreths to build on the best elements of these work flows and to create a standardized best practice.


== Taylor vs. the Gilbreths ==
Although for Taylor, motion studies remained subordinate to time studies, the attention he paid to the motion study technique demonstrated the seriousness with which he considered the Gilbreths’ method. The split with Taylor in 1914, on the basis of attitudes to workers, meant the Gilbreths had to argue contrary to the trade unionists, government commissions and Robert Hoxie who believed scientific management was unstoppable. The Gilbreths were charged with the task of proving that motion study particularly, and scientific management generally, increased industrial output in ways which improved and did not detract from workers' mental and physical strength. This was no simple task given the propaganda fuelling the Hoxie report and the consequent union opposition to scientific management. In addition, the Gilbreths credibility and academic success continued to be hampered by Taylor who held the view that motion studies were nothing more than a continuation of his work.
While both Taylor and the Gilbreths continue to be criticized for their respective work, it should be remembered that they were writing at a time of industrial reorganization and the emergence of large, complex organizations with new forms of technology. Furthermore, to equate scientific management merely with time and motion study and consequently labor control not only misconceives the scope of scientific management, but also misinterprets Taylor's incentives for proposing a different style of managerial thought.


== Health care time and motion study ==
A Health care time and motion study is used to research and track the efficiency and quality of health care workers.  In the case of nurses, numerous programs have been initiated to increase the percent of a shift nurses spend providing direct care to patients. Prior to interventions nurses were found to spend ~20% of their time doing direct care.  After focused intervention, some hospitals doubled that number, with some even exceeding 70% of shift time with patients, resulting in reduced errors, codes, and falls.


=== Methods ===
External observer: Someone visually follows the person being observed, either contemporaneously or via video recording.  This method presents additional expense as it usually requires a 1 to 1 ratio of research time to subject time.  An advantage is the data can be more consistent, complete, and accurate than with self-reporting.
Self-reporting: Self-reported studies require the target to record time and activity data.  This can be done contemporaneously by having subjects stop and start a timer when completing a task, through work sampling where the subject records what they are doing at determined or random intervals, or by having the subject journal activities at the end of the day. Self-reporting introduces errors that may not be present through other methods, including errors in temporal perception and memory, as well as the motivation to manipulate the data.
Automation: Motion can be tracked with GPS.  Documentation activities can be tracked through monitoring software embedded in the applications used to create documentation.  Badge scans can also create a log of activity.


== See also ==
Ergonomics
Human factors
Methods-time measurement
Memo motion
Predetermined motion time system
Standard time
Industrial Engineering
Evolutionary economics


== References ==","pandas(index=146, _1=146, text='a time and motion study (or time-motion study) is a business efficiency technique combining the time study work of frederick winslow taylor with the motion study work of frank and lillian gilbreth (the same couple as is best known through the biographical 1950 film and book cheaper by the dozen). it is a major part of scientific management (taylorism). after its first introduction, time study developed in the direction of establishing standard times, while motion study evolved into a technique for improving work methods. the two techniques became integrated and refined into a widely accepted method applicable to the improvement and upgrading of work systems. this integrated approach to work system improvement is known as methods engineering and it is applied today to industrial as well as service organizations, including banks, schools and hospitals.   == time studies == time study is a direct and continuous observation of a task, using a timekeeping device (e.g., decimal minute stopwatch, computer-assisted electronic stopwatch, and videotape camera) to record the time taken to accomplish a task and it is often used when: there are repetitive work cycles of short to long duration, wide variety of dissimilar work is performed, or process control elements constitute a part of the cycle.the industrial engineering terminology standard, defines time study as ""a work measurement technique consisting of careful time measurement of the task with a time measuring instrument, adjusted for any observed variance from normal effort or pace and to allow adequate time for such items as foreign elements, unavoidable or machine delays, rest to overcome fatigue, and personal needs.""the systems of time and motion studies are frequently assumed to be interchangeable terms, descriptive of equivalent theories. however, the underlying principles and the rationale for the establishment of each respective method are dissimilar, despite originating within the same school of thought. the application of science to business problems, and the use of time-study methods in standard setting and the planning of work, was pioneered by frederick winslow taylor. taylor liaised with factory managers and from the success of these discussions wrote several papers proposing the use of wage-contingent performance standards based on scientific time study. at its most basic level time studies involved breaking down each job into component parts, timing each part and rearranging the parts into the most efficient method of working. by counting and calculating, taylor wanted to transform management, which was essentially an oral tradition, into a set of calculated and written techniques.taylor and his colleagues placed emphasis on the content of a fair day\'s work, and sought to maximize productivity irrespective of the physiological cost to the worker. for example, taylor thought unproductive time usage (soldiering) to be the deliberate attempt of workers to promote their best interests and to keep employers ignorant of how fast work could be carried out. this instrumental view of human behavior by taylor prepared the path for human relations to supersede scientific management in terms of literary success and managerial application. external observer: someone visually follows the person being observed, either contemporaneously or via video recording.  this method presents additional expense as it usually requires a 1 to 1 ratio of research time to subject time.  an advantage is the data can be more consistent, complete, and accurate than with self-reporting. self-reporting: self-reported studies require the target to record time and activity data.  this can be done contemporaneously by having subjects stop and start a timer when completing a task, through work sampling where the subject records what they are doing at determined or random intervals, or by having the subject journal activities at the end of the day. self-reporting introduces errors that may not be present through other methods, including errors in temporal perception and memory, as well as the motivation to manipulate the data. automation: motion can be tracked with gps.  documentation activities can be tracked through monitoring software embedded in the applications used to create documentation.  badge scans can also create a log of activity.   == see also == ergonomics human factors methods-time measurement memo motion predetermined motion time system standard time industrial engineering evolutionary economics   == references ==')"
147,"Bayesian experimental design provides a general probability-theoretical framework from which other theories on experimental design can be derived. It is based on Bayesian inference to interpret the observations/data acquired during the experiment. This allows accounting for both any prior knowledge on the parameters to be determined as well as uncertainties in observations.
The theory of Bayesian experimental design is to a certain extent based on the theory for making optimal decisions under uncertainty. The aim when designing an experiment is to  maximize the expected utility of the experiment outcome. The utility is most commonly defined in terms of a measure of the accuracy of the information provided by the experiment (e.g. the Shannon information or the negative variance), but may also involve factors such as the financial cost of performing the experiment. What will be the optimal experiment design depends on the particular utility criterion chosen.


== Relations to more specialized optimal design theory ==


=== Linear theory ===
If the model is linear, the prior probability density function (PDF) is homogeneous and observational errors are normally distributed, the theory simplifies to the classical optimal experimental design theory.


=== Approximate normality ===
In numerous publications on Bayesian experimental design, it is (often implicitly) assumed that all posterior PDFs will be approximately normal. This allows for the expected utility to be calculated using linear theory, averaging over the space of model parameters, an approach reviewed in Chaloner & Verdinelli (1995). Caution must however be taken when applying this method, since approximate normality of all possible posteriors is difficult to verify, even in cases of normal observational errors and uniform prior PDF.


=== Posterior distribution ===
Recently, increased computational resources allow inference of the posterior distribution of model parameters, which can directly be used for experiment design. Vanlier et al. (2012) proposed an approach that uses the posterior predictive distribution to assess the effect of new measurements on prediction uncertainty, while Liepe et al. (2013) suggest maximizing the mutual information between parameters, predictions and potential new experiments.


== Mathematical formulation ==
Given a vector 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   of parameters to determine, a prior PDF 
  
    
      
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )}
   over those parameters and a PDF 
  
    
      
        p
        (
        y
        ∣
        θ
        ,
        ξ
        )
      
    
    {\displaystyle p(y\mid \theta ,\xi )}
   for making observation 
  
    
      
        y
      
    
    {\displaystyle y}
  , given parameter values 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   and an experiment design 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
  , the posterior PDF can be calculated using Bayes' theorem

  
    
      
        p
        (
        θ
        ∣
        y
        ,
        ξ
        )
        =
        
          
            
              p
              (
              y
              ∣
              θ
              ,
              ξ
              )
              p
              (
              θ
              )
            
            
              p
              (
              y
              ∣
              ξ
              )
            
          
        
        
        ,
      
    
    {\displaystyle p(\theta \mid y,\xi )={\frac {p(y\mid \theta ,\xi )p(\theta )}{p(y\mid \xi )}}\,,}
  where 
  
    
      
        p
        (
        y
        ∣
        ξ
        )
      
    
    {\displaystyle p(y\mid \xi )}
   is the marginal probability density in observation space

  
    
      
        p
        (
        y
        ∣
        ξ
        )
        =
        ∫
        p
        (
        θ
        )
        p
        (
        y
        ∣
        θ
        ,
        ξ
        )
        
        d
        θ
        
        .
      
    
    {\displaystyle p(y\mid \xi )=\int p(\theta )p(y\mid \theta ,\xi )\,d\theta \,.}
  The expected utility of an experiment with design 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
   can then be defined

  
    
      
        U
        (
        ξ
        )
        =
        ∫
        p
        (
        y
        ∣
        ξ
        )
        U
        (
        y
        ,
        ξ
        )
        
        d
        y
        ,
      
    
    {\displaystyle U(\xi )=\int p(y\mid \xi )U(y,\xi )\,dy,}
  where 
  
    
      
        U
        (
        y
        ,
        ξ
        )
      
    
    {\displaystyle U(y,\xi )}
   is some real-valued functional of the posterior PDF 
  
    
      
        p
        (
        θ
        ∣
        y
        ,
        ξ
        )
      
    
    {\displaystyle p(\theta \mid y,\xi )}
   after making observation 
  
    
      
        y
      
    
    {\displaystyle y}
   using an experiment design 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
  .


=== Gain in Shannon information as utility ===
Utility may be defined as the prior-posterior gain in Shannon information

  
    
      
        U
        (
        y
        ,
        ξ
        )
        =
        ∫
        log
        ⁡
        (
        p
        (
        θ
        ∣
        y
        ,
        ξ
        )
        )
        
        p
        (
        θ
        
          |
        
        y
        ,
        ξ
        )
        
        d
        θ
        −
        ∫
        log
        ⁡
        (
        p
        (
        θ
        )
        )
        
        p
        (
        θ
        )
        
        d
        θ
        
        .
      
    
    {\displaystyle U(y,\xi )=\int \log(p(\theta \mid y,\xi ))\,p(\theta |y,\xi )\,d\theta -\int \log(p(\theta ))\,p(\theta )\,d\theta \,.}
  Another possibility is to define the utility as

  
    
      
        U
        (
        y
        ,
        ξ
        )
        =
        
          D
          
            K
            L
          
        
        (
        p
        (
        θ
        ∣
        y
        ,
        ξ
        )
        ‖
        p
        (
        θ
        )
        )
        
        ,
      
    
    {\displaystyle U(y,\xi )=D_{KL}(p(\theta \mid y,\xi )\|p(\theta ))\,,}
  the Kullback–Leibler divergence of the prior from the posterior distribution.
Lindley (1956) noted that the expected utility will then be coordinate-independent and can be written in two forms

  
    
      
        
          
            
              
                U
                (
                ξ
                )
              
              
                
                =
                ∫
                ∫
                log
                ⁡
                (
                p
                (
                θ
                ∣
                y
                ,
                ξ
                )
                )
                
                p
                (
                θ
                ,
                y
                ∣
                ξ
                )
                
                d
                θ
                
                d
                y
                −
                ∫
                log
                ⁡
                (
                p
                (
                θ
                )
                )
                
                p
                (
                θ
                )
                
                d
                θ
              
            
            
              
              
                
                =
                ∫
                ∫
                log
                ⁡
                (
                p
                (
                y
                ∣
                θ
                ,
                ξ
                )
                )
                
                p
                (
                θ
                ,
                y
                ∣
                ξ
                )
                
                d
                y
                
                d
                θ
                −
                ∫
                log
                ⁡
                (
                p
                (
                y
                ∣
                ξ
                )
                )
                
                p
                (
                y
                ∣
                ξ
                )
                
                d
                y
                ,
              
            
          
        
        
      
    
    {\displaystyle {\begin{alignedat}{2}U(\xi )&=\int \int \log(p(\theta \mid y,\xi ))\,p(\theta ,y\mid \xi )\,d\theta \,dy-\int \log(p(\theta ))\,p(\theta )\,d\theta \\&=\int \int \log(p(y\mid \theta ,\xi ))\,p(\theta ,y\mid \xi )\,dy\,d\theta -\int \log(p(y\mid \xi ))\,p(y\mid \xi )\,dy,\end{alignedat}}\,}
  of which the latter can be evaluated without the need for evaluating individual posterior PDFs

  
    
      
        p
        (
        θ
        ∣
        y
        ,
        ξ
        )
      
    
    {\displaystyle p(\theta \mid y,\xi )}
   for all possible observations 
  
    
      
        y
      
    
    {\displaystyle y}
  . It is worth noting that the first term on the second equation line will not depend on the design 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
  , as long as the observational uncertainty doesn't. On the other hand, the integral of 
  
    
      
        p
        (
        θ
        )
        log
        ⁡
        p
        (
        θ
        )
      
    
    {\displaystyle p(\theta )\log p(\theta )}
   in the first form is constant for all 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
  , so if the goal is to choose the design with the highest utility, the term need not be computed at all. Several authors have considered numerical techniques for evaluating and optimizing this criterion, e.g. van den Berg, Curtis & Trampert (2003) and Ryan (2003).  Note that

  
    
      
        U
        (
        ξ
        )
        =
        I
        (
        θ
        ;
        y
        )
        
        ,
      
    
    {\displaystyle U(\xi )=I(\theta ;y)\,,}
  the expected information gain being exactly the mutual information  between the parameter θ and the observation y. An example of Bayesian design for linear dynamical model discrimination is given in Bania (2019). Since 
  
    
      
        I
        (
        θ
        ;
        y
        )
        
        ,
      
    
    {\displaystyle I(\theta ;y)\,,}
   was difficult to calculate, its lower bound has been used as a utility function. The lower bound is then maximized under the signal energy constraint. Proposed Bayesian design has been also compared with classical average D-optimal design. It was shown that the Bayesian design is superior to D-optimal design.
The Kelly criterion also describes such a utility function for a gambler seeking to maximize profit, which is used in gambling and information theory; Kelly's situation is identical to the foregoing, with the side information, or ""private wire"" taking the place of the experiment. 


== See also ==
Bayesian optimization
Optimal design
Active Learning


== References ==
Vanlier; Tiemann; Hilbers; van Riel (2012), ""A Bayesian approach to targeted experiment design"" (PDF), Bioinformatics, 28 (8): 1136–1142, doi:10.1093/bioinformatics/bts092, PMC 3324513, PMID 22368245Liepe; Filippi; Komorowski; Stumpf (2013), ""Maximizing the Information Content of Experiments in Systems Biology"", PLOS Computational Biology, 9 (1): e1002888, doi:10.1371/journal.pcbi.1002888, PMC 3561087, PMID 23382663van den Berg; Curtis; Trampert (2003), ""Optimal nonlinear Bayesian experimental design: an application to amplitude versus offset experiments"" (PDF), Geophysical Journal International, 155 (2): 411–421, doi:10.1046/j.1365-246x.2003.02048.x, archived from the original (PDF) on 2011-07-17Chaloner, Kathryn; Verdinelli, Isabella (1995), ""Bayesian experimental design: a review"" (PDF), Statistical Science, 10 (3): 273–304, doi:10.1214/ss/1177009939DasGupta, A. (1996), ""Review of optimal Bayes designs"" (PDF),  in Ghosh, S.; Rao, C. R. (eds.), Design and Analysis of Experiments, Handbook of Statistics, 13, North-Holland, pp. 1099–1148, ISBN 978-0-444-82061-7Lindley, D. V. (1956), ""On a measure of information provided by an experiment"", Annals of Mathematical Statistics, 27 (4): 986–1005, doi:10.1214/aoms/1177728069Ryan, K. J. (2003), ""Estimating Expected Information Gains for Experimental Designs With Application to the Random Fatigue-Limit Model"", Journal of Computational and Graphical Statistics, 12 (3): 585–603, doi:10.1198/1061860032012
Bania, P. (2019), ""Bayesian Input Design for Linear Dynamical Model Discrimination"", Entropy, 21 (4), doi:10.3390/e21040351","pandas(index=147, _1=147, text='bayesian experimental design provides a general probability-theoretical framework from which other theories on experimental design can be derived. it is based on bayesian inference to interpret the observations/data acquired during the experiment. this allows accounting for both any prior knowledge on the parameters to be determined as well as uncertainties in observations. the theory of bayesian experimental design is to a certain extent based on the theory for making optimal decisions under uncertainty. the aim when designing an experiment is to  maximize the expected utility of the experiment outcome. the utility is most commonly defined in terms of a measure of the accuracy of the information provided by the experiment (e.g. the shannon information or the negative variance), but may also involve factors such as the financial cost of performing the experiment. what will be the optimal experiment design depends on the particular utility criterion chosen.   == relations to more specialized optimal design theory == recently, increased computational resources allow inference of the posterior distribution of model parameters, which can directly be used for experiment design. vanlier et al. (2012) proposed an approach that uses the posterior predictive distribution to assess the effect of new measurements on prediction uncertainty, while liepe et al. (2013) suggest maximizing the mutual information between parameters, predictions and potential new experiments.   == mathematical formulation == given a vector    θ    was difficult to calculate, its lower bound has been used as a utility function. the lower bound is then maximized under the signal energy constraint. proposed bayesian design has been also compared with classical average d-optimal design. it was shown that the bayesian design is superior to d-optimal design. the kelly criterion also describes such a utility function for a gambler seeking to maximize profit, which is used in gambling and information theory; kelly\'s situation is identical to the foregoing, with the side information, or ""private wire"" taking the place of the experiment.   == see also == bayesian optimization optimal design active learning   == references == vanlier; tiemann; hilbers; van riel (2012), ""a bayesian approach to targeted experiment design"" (pdf), bioinformatics, 28 (8): 1136–1142, doi:10.1093/bioinformatics/bts092, pmc 3324513, pmid 22368245liepe; filippi; komorowski; stumpf (2013), ""maximizing the information content of experiments in systems biology"", plos computational biology, 9 (1): e1002888, doi:10.1371/journal.pcbi.1002888, pmc 3561087, pmid 23382663van den berg; curtis; trampert (2003), ""optimal nonlinear bayesian experimental design: an application to amplitude versus offset experiments"" (pdf), geophysical journal international, 155 (2): 411–421, doi:10.1046/j.1365-246x.2003.02048.x, archived from the original (pdf) on 2011-07-17chaloner, kathryn; verdinelli, isabella (1995), ""bayesian experimental design: a review"" (pdf), statistical science, 10 (3): 273–304, doi:10.1214/ss/1177009939dasgupta, a. (1996), ""review of optimal bayes designs"" (pdf),  in ghosh, s.; rao, c. r. (eds.), design and analysis of experiments, handbook of statistics, 13, north-holland, pp. 1099–1148, isbn 978-0-444-82061-7lindley, d. v. (1956), ""on a measure of information provided by an experiment"", annals of mathematical statistics, 27 (4): 986–1005, doi:10.1214/aoms/1177728069ryan, k. j. (2003), ""estimating expected information gains for experimental designs with application to the random fatigue-limit model"", journal of computational and graphical statistics, 12 (3): 585–603, doi:10.1198/1061860032012 bania, p. (2019), ""bayesian input design for linear dynamical model discrimination"", entropy, 21 (4), doi:10.3390/e21040351')"
148,"Maynard operation sequence technique (MOST) is a predetermined motion time system that is used primarily in industrial settings to set the standard time in which a worker should perform a task.  To calculate this, a task is broken down into individual motion elements, and each is assigned a numerical time value in units known as time measurement units, or TMUs, where 100,000 TMUs is equivalent to one hour.  All the motion element times are then added together and any allowances are added, and the result is the standard time.  It is more common in Asia whereas the original and more sophisticated Methods Time Measurement technique, better known as MTM, is a global standard.
The most commonly used form of MOST is BasicMOST, which was released in Sweden in 1972 and in the United States in 1974.  Two other variations were released in 1980, called MiniMOST and MaxiMOST.  The difference between the three is their level of focus—the motions recorded in BasicMOST are on the level of tens of TMUs, while MiniMOST uses individual TMUs and MaxiMOST uses hundreds of TMUs.  This allows for a variety of applications—MiniMOST is commonly used for short (less than about a minute), repetitive cycles, and MaxiMOST for longer (more than several minutes), non-repetitive operations.  BasicMost is in the position between them, and can be used accurately for operations ranging from less than a minute to about ten minutes.
Another variation of MOST is known as AdminMOST.  Originally developed and released under the name ClericalMOST in the 1970s, it was recently updated to include modern administrative tasks and renamed.  It is on the same level of focus as BasicMOST.
Up until 16bit programs stopped working with Windows, it was possible to use AutoMOST. AutoMOST was a knowledge based system employing decision trees. Developers created logic trees. These trees could then be used by non IE trained operators to generate Standard Times. The user answered a series of logic questions to route the logic and made inputs (number of parts fitted etc.). As they made their way through the tree, based on their route and inputs, AutoMOST would be gathering sub operation data to collate into the final time for the activity being measured. AutoMOST was able to pull in sub operation data from any of the base versions of MOST (Mini, Maxi or Basic)


== References ==
Zandin, Kjell B (2003). MOST Work Measurement Systems. New York City: Marcel Dekker. ISBN 0-8247-0953-5


== External links ==
https://web.archive.org/web/20091011105119/http://www.iiie-pune.com/most.htm
http://faculty.kfupm.edu.sa/SE/atahir/SE%20323/Chapter-10-Predetermined-Motion-Time-Systems.ppt
http://www.hpcnet.org/upload/directory/materials/7210_20050926134822.ppt
https://www.researchgate.net/publication/296443715_MOST_-_Advanced_Work_Measurement_Technique","pandas(index=148, _1=148, text='maynard operation sequence technique (most) is a predetermined motion time system that is used primarily in industrial settings to set the standard time in which a worker should perform a task.  to calculate this, a task is broken down into individual motion elements, and each is assigned a numerical time value in units known as time measurement units, or tmus, where 100,000 tmus is equivalent to one hour.  all the motion element times are then added together and any allowances are added, and the result is the standard time.  it is more common in asia whereas the original and more sophisticated methods time measurement technique, better known as mtm, is a global standard. the most commonly used form of most is basicmost, which was released in sweden in 1972 and in the united states in 1974.  two other variations were released in 1980, called minimost and maximost.  the difference between the three is their level of focus—the motions recorded in basicmost are on the level of tens of tmus, while minimost uses individual tmus and maximost uses hundreds of tmus.  this allows for a variety of applications—minimost is commonly used for short (less than about a minute), repetitive cycles, and maximost for longer (more than several minutes), non-repetitive operations.  basicmost is in the position between them, and can be used accurately for operations ranging from less than a minute to about ten minutes. another variation of most is known as adminmost.  originally developed and released under the name clericalmost in the 1970s, it was recently updated to include modern administrative tasks and renamed.  it is on the same level of focus as basicmost. up until 16bit programs stopped working with windows, it was possible to use automost. automost was a knowledge based system employing decision trees. developers created logic trees. these trees could then be used by non ie trained operators to generate standard times. the user answered a series of logic questions to route the logic and made inputs (number of parts fitted etc.). as they made their way through the tree, based on their route and inputs, automost would be gathering sub operation data to collate into the final time for the activity being measured. automost was able to pull in sub operation data from any of the base versions of most (mini, maxi or basic)   == references == zandin, kjell b (2003). most work measurement systems. new york city: marcel dekker. isbn 0-8247-0953-5   == external links == https://web.archive.org/web/20091011105119/http://www.iiie-pune.com/most.htm http://faculty.kfupm.edu.sa/se/atahir/se%20323/chapter-10-predetermined-motion-time-systems.ppt http://www.hpcnet.org/upload/directory/materials/7210_20050926134822.ppt https://www.researchgate.net/publication/296443715_most_-_advanced_work_measurement_technique')"
149,"An operational-level agreement (OLA) defines the interdependent relationships in support of a service-level agreement (SLA). The agreement describes the responsibilities of each internal support group toward other support groups, including the process and timeframe for delivery of their services. The objective of the OLA is to present a clear, concise and measurable description of the service provider's internal support relationships.
OLA is sometimes expanded to other phrases but they all have the same meaning:

organisational-level agreement
operating-level agreement
operations-level agreement.


== Note ==
OLA(s) are not a substitute for an SLA. The purpose of the OLA is to help ensure that the underpinning activities that are performed by a number of support team components are clearly aligned to provide the intended SLA.
If the underpinning OLA(s) are not in place, it is often very difficult for organisations to go back and engineer agreements between the support teams to deliver the SLA. OLA(s) have to be seen as the foundation of good practice and common agreement.


== See also ==
IT service management (ITSM)
ITIL
Service-level objective (WS-Agreement)


== References ==


== External links ==
ITSM Foundation (pdf)
Forsythe:ITSM Glossary","pandas(index=149, _1=149, text=""an operational-level agreement (ola) defines the interdependent relationships in support of a service-level agreement (sla). the agreement describes the responsibilities of each internal support group toward other support groups, including the process and timeframe for delivery of their services. the objective of the ola is to present a clear, concise and measurable description of the service provider's internal support relationships. ola is sometimes expanded to other phrases but they all have the same meaning:  organisational-level agreement operating-level agreement operations-level agreement.   == note == ola(s) are not a substitute for an sla. the purpose of the ola is to help ensure that the underpinning activities that are performed by a number of support team components are clearly aligned to provide the intended sla. if the underpinning ola(s) are not in place, it is often very difficult for organisations to go back and engineer agreements between the support teams to deliver the sla. ola(s) have to be seen as the foundation of good practice and common agreement.   == see also == it service management (itsm) itil service-level objective (ws-agreement)   == references ==   == external links == itsm foundation (pdf) forsythe:itsm glossary"")"
150,"In operations research and engineering, a criticality matrix is a representation (often graphical) of failure modes along with their probabilities and severities.


== Example ==
For example, an aircraft might have the following matrix:","pandas(index=150, _1=150, text='in operations research and engineering, a criticality matrix is a representation (often graphical) of failure modes along with their probabilities and severities.   == example == for example, an aircraft might have the following matrix:')"
151,"A pilot plant is a pre-commercial production system that employs new production technology and/or produces small volumes of new technology-based products, mainly for the purpose of learning about the new technology. The knowledge obtained is then used for design of full-scale production systems and commercial products, as well as for identification of further research objectives and support of investment decisions. Other (non-technical) purposes include gaining public support for new technologies and questioning government regulations. Pilot plant is a relative term in the sense that pilot plants are typically smaller than full-scale production plants, but are built in a range of sizes.  Also, as pilot plants are intended for learning, they typically are more flexible, possibly at the expense of economy. Some pilot plants are built in laboratories using stock lab equipment, while others require substantial engineering efforts, cost millions of dollars, and are custom-assembled and fabricated from process equipment, instrumentation and piping.   They can also be used to train personnel for a full-scale plant. Pilot plants tend to be smaller compared to demonstration plants.


== Terminology ==
A word similar to pilot plant is pilot line. Essentially, pilot plants and pilot lines perform the same functions, but 'pilot plant' is used in the context of (bio)chemical and advanced materials production systems, whereas 'pilot line' is used for new technology in general. The term 'kilo lab' is also used for small pilot plants referring to the expected output quantities.


== Risk management ==
Pilot plants are used to reduce the risk associated with construction of large process plants. They do so in several ways:

Computer simulations and semi-empirical methods are used to determine the limitations of the pilot scale system. These mathematical models are then tested in a physical pilot-scale plant. Various modeling methods are used for scale-up. These methods include:
Chemical similitude studies
Mathematical modeling
Aspen Plus/Aspen HYSYS modeling
Finite Elemental Analysis (FEA)
Computational Fluid Dynamics (CFD)
These theoretical modeling methods return the following:
Finalized mass and energy balances
Optimized system design and capacity
Equipment requirements
System limitations
The basis for determining the cost to build the pilot module
They are substantially less expensive to build than full-scale plants.  The business does not put as much capital at risk on a project that may be inefficient or unfeasible.  Further, design changes can be made more cheaply at the pilot scale and kinks in the process can be worked out before the large plant is constructed.
They provide valuable data for design of the full-scale plant.  Scientific data about reactions, material properties, corrosiveness, for instance, may be available, but it is difficult to predict the behavior of a process of any complexity. Engineering data from other process may be available, but this data can not always be clearly applied to the process of interest.  Designers use data from the pilot plant to refine their design of the production scale facility.If a system is well defined and the engineering parameters are known, pilot plants are not used.  For instance, a business that wants to expand production capacity by building a new plant that does the same thing as an existing plant may choose to not use a pilot plant.
Additionally, advances in process simulation on computers have increased the confidence of process designers and reduced the need for pilot plants.  However, they are still used as even state-of-the-art simulation cannot accurately predict the behavior of complex systems.


== Scale dependence of plant properties ==
As a system increases in size, system properties that depend on quantity of matter (with extensive properties) may change. The surface area to liquid ratio in a chemical plant is a good example of such a property. On a small chemical scale, in a flask, say, there is a relatively large surface area to liquid ratio. However, if the reaction in question is scaled up to fit in a 500-gallon tank, the surface area to liquid ratio becomes much smaller. As a result of this difference in surface area to liquid ratio, the exact nature of the thermodynamics and the reaction kinetics of the process change in a non-linear fashion. This is why a reaction in a beaker can behave vastly differently from the same reaction in a large-scale production process.


=== Other factors ===
Other factors that may change during the transformation to a production scale include:

Reaction kinetics
Chemical equilibrium
Material properties
Fluid dynamics
Thermodynamics
Equipment selection
Agitation
Uniformity / homogeneityAfter data has been collected from operation of a pilot plant, a larger production-scale facility may be built. Alternatively, a demonstration plant, which is typically bigger than a pilot plant, but smaller than a full-scale production plant, may be built to demonstrate the commercial feasibility of the process. Businesses sometimes continue to operate the pilot plant in order to test ideas for new products, new feedstocks, or different operating conditions.  Alternatively, they may be operated as production facilities, augmenting production from the main plant.
Recent trends try to keep the size of the plant a small as possible to save costs. This approach is called miniplant technology. The flow chemistry takes up this trend and uses flow miniplant technology for small-scale manufacturing.


== Bench scale vs pilot vs demonstration ==
The differences between bench scale, pilot scale and demonstration scale are strongly influenced by industry and application. Some industries use pilot plant and demonstration plant interchangeably. Some pilot plants are built as portable modules that can be easily transported as a contained unit. 
For batch processes, in the pharmaceutical industry for example, bench scale is typically conducted on samples 1–20 kg or less, whereas pilot scale testing is performed with samples of 20–100 kg. Demonstration scale is essentially operating the equipment at full commercial feed rates over extended time periods to prove operational stability.
For continuous processes, in the petroleum industry for example, bench scale systems are typically microreactor or CSTR systems with less than 1000 mL of catalyst, studying reactions and/or separations on a once-through basis. Pilot plants will typically have reactors with catalyst volume between 1 and 100 litres, and will often incorporate product separation and gas/liquid recycle with the goal of closing the mass balance. Demonstration plants, also referred to as semi-works plants, will study the viability of the process on a pre-commercial scale, with typical catalyst volumes in the 100 - 1000 litre range. The design of a demonstration scale plant for a continuous process will closely resemble that of the anticipated future commercial plant, albeit at a much lower throughput, and its goal is to study catalyst performance and operating lifetime over an extended period, while generating significant quantities of product for market testing.
In the development of new processes, the design and operation of the pilot and demonstration plant will often run in parallel with the design of the future commercial plant, and the results from pilot testing programs are key to optimizing the commercial plant flowsheet. It is common in cases where process technology has been successfully implemented that the savings at the commercial scale resulting from pilot testing will significantly outweigh the cost of the pilot plant itself.


== Steps to creating a custom pilot plant ==
Custom pilot plants are commonly designed either for research or commercial purposes. They can range in size from a small system with no automation and low flow, to a highly automated system producing relatively large amounts of products in a day. No matter the size, the steps to designing and fabricating a working pilot plant are the same. They are:

Pre-engineering - completing a process flow diagram (PFD), basic piping and instrumentation diagrams (P&ID's) and initial equipment layouts.
Engineering modeling and optimization - 2D and 3D models are created, using a simulation software to model the process parameters and scale the chemical processes. These modeling software help determine system limitations, non-linear chemical and physical changes, and potential equipment sizing. Mass and energy balances, finalized P&ID's and general arrangement drawings are produced.
Automation strategies for the system are developed (if needed). Controls system programming begins and will continue through fabrication and assembly
Fabrication and assembly - after an optimized design has been determined, the custom pilot is fabricated and assembled. Pilot plants can either be assembled on-site or off-site as modular skids that will be constructed and tested in a controlled environment.
Testing - testing of completed systems, including system controls, is conducted to ensure proper system function.
Installation and startup - if constructed offsite, pilot skids are installed onsite. After all equipment is in place, full system startup is completed by integrating the system with existing plant utilities and controls. Full operation is tested and affirmed.
Training - operator training is complete and full system documentation is handed over.


== See also ==
Operations research
Chemical engineering
Process engineering
Chemist


== Bibliography ==
M. Levin (Editor), Pharmaceutical Process Scale-Up (Drugs and the Pharmaceutical), Informa Healthcare, 3rd edition, ISBN 978-1616310011 (2011)
M. Lackner (Editor), Scale-up in Combustion, ProcessEng Engineering GmbH, Wien, ISBN 978-3-902655-04-2 (2009).
M. Zlokarnik, Scale-up in Chemical Engineering, Wiley-VCH Verlag GmbH & Co. KGaA, 2nd edition, ISBN 978-3527314218 (2006).
Richard Palluzi, Pilot Plants: Design, Construction and Operation, McGraw-Hill, February, 1992.
Richard Palluzi, Pilot Plants, Chemical Engineering, March, 1990.


== References ==","pandas(index=151, _1=151, text=""a pilot plant is a pre-commercial production system that employs new production technology and/or produces small volumes of new technology-based products, mainly for the purpose of learning about the new technology. the knowledge obtained is then used for design of full-scale production systems and commercial products, as well as for identification of further research objectives and support of investment decisions. other (non-technical) purposes include gaining public support for new technologies and questioning government regulations. pilot plant is a relative term in the sense that pilot plants are typically smaller than full-scale production plants, but are built in a range of sizes.  also, as pilot plants are intended for learning, they typically are more flexible, possibly at the expense of economy. some pilot plants are built in laboratories using stock lab equipment, while others require substantial engineering efforts, cost millions of dollars, and are custom-assembled and fabricated from process equipment, instrumentation and piping.   they can also be used to train personnel for a full-scale plant. pilot plants tend to be smaller compared to demonstration plants.   == terminology == a word similar to pilot plant is pilot line. essentially, pilot plants and pilot lines perform the same functions, but 'pilot plant' is used in the context of (bio)chemical and advanced materials production systems, whereas 'pilot line' is used for new technology in general. the term 'kilo lab' is also used for small pilot plants referring to the expected output quantities.   == risk management == pilot plants are used to reduce the risk associated with construction of large process plants. they do so in several ways:  computer simulations and semi-empirical methods are used to determine the limitations of the pilot scale system. these mathematical models are then tested in a physical pilot-scale plant. various modeling methods are used for scale-up. these methods include: chemical similitude studies mathematical modeling aspen plus/aspen hysys modeling finite elemental analysis (fea) computational fluid dynamics (cfd) these theoretical modeling methods return the following: finalized mass and energy balances optimized system design and capacity equipment requirements system limitations the basis for determining the cost to build the pilot module they are substantially less expensive to build than full-scale plants.  the business does not put as much capital at risk on a project that may be inefficient or unfeasible.  further, design changes can be made more cheaply at the pilot scale and kinks in the process can be worked out before the large plant is constructed. they provide valuable data for design of the full-scale plant.  scientific data about reactions, material properties, corrosiveness, for instance, may be available, but it is difficult to predict the behavior of a process of any complexity. engineering data from other process may be available, but this data can not always be clearly applied to the process of interest.  designers use data from the pilot plant to refine their design of the production scale facility.if a system is well defined and the engineering parameters are known, pilot plants are not used.  for instance, a business that wants to expand production capacity by building a new plant that does the same thing as an existing plant may choose to not use a pilot plant. additionally, advances in process simulation on computers have increased the confidence of process designers and reduced the need for pilot plants.  however, they are still used as even state-of-the-art simulation cannot accurately predict the behavior of complex systems.   == scale dependence of plant properties == as a system increases in size, system properties that depend on quantity of matter (with extensive properties) may change. the surface area to liquid ratio in a chemical plant is a good example of such a property. on a small chemical scale, in a flask, say, there is a relatively large surface area to liquid ratio. however, if the reaction in question is scaled up to fit in a 500-gallon tank, the surface area to liquid ratio becomes much smaller. as a result of this difference in surface area to liquid ratio, the exact nature of the thermodynamics and the reaction kinetics of the process change in a non-linear fashion. this is why a reaction in a beaker can behave vastly differently from the same reaction in a large-scale production process. other factors that may change during the transformation to a production scale include:  reaction kinetics chemical equilibrium material properties fluid dynamics thermodynamics equipment selection agitation uniformity / homogeneityafter data has been collected from operation of a pilot plant, a larger production-scale facility may be built. alternatively, a demonstration plant, which is typically bigger than a pilot plant, but smaller than a full-scale production plant, may be built to demonstrate the commercial feasibility of the process. businesses sometimes continue to operate the pilot plant in order to test ideas for new products, new feedstocks, or different operating conditions.  alternatively, they may be operated as production facilities, augmenting production from the main plant. recent trends try to keep the size of the plant a small as possible to save costs. this approach is called miniplant technology. the flow chemistry takes up this trend and uses flow miniplant technology for small-scale manufacturing.   == bench scale vs pilot vs demonstration == the differences between bench scale, pilot scale and demonstration scale are strongly influenced by industry and application. some industries use pilot plant and demonstration plant interchangeably. some pilot plants are built as portable modules that can be easily transported as a contained unit. for batch processes, in the pharmaceutical industry for example, bench scale is typically conducted on samples 1–20 kg or less, whereas pilot scale testing is performed with samples of 20–100 kg. demonstration scale is essentially operating the equipment at full commercial feed rates over extended time periods to prove operational stability. for continuous processes, in the petroleum industry for example, bench scale systems are typically microreactor or cstr systems with less than 1000 ml of catalyst, studying reactions and/or separations on a once-through basis. pilot plants will typically have reactors with catalyst volume between 1 and 100 litres, and will often incorporate product separation and gas/liquid recycle with the goal of closing the mass balance. demonstration plants, also referred to as semi-works plants, will study the viability of the process on a pre-commercial scale, with typical catalyst volumes in the 100 - 1000 litre range. the design of a demonstration scale plant for a continuous process will closely resemble that of the anticipated future commercial plant, albeit at a much lower throughput, and its goal is to study catalyst performance and operating lifetime over an extended period, while generating significant quantities of product for market testing. in the development of new processes, the design and operation of the pilot and demonstration plant will often run in parallel with the design of the future commercial plant, and the results from pilot testing programs are key to optimizing the commercial plant flowsheet. it is common in cases where process technology has been successfully implemented that the savings at the commercial scale resulting from pilot testing will significantly outweigh the cost of the pilot plant itself.   == steps to creating a custom pilot plant == custom pilot plants are commonly designed either for research or commercial purposes. they can range in size from a small system with no automation and low flow, to a highly automated system producing relatively large amounts of products in a day. no matter the size, the steps to designing and fabricating a working pilot plant are the same. they are:  pre-engineering - completing a process flow diagram (pfd), basic piping and instrumentation diagrams (p&id's) and initial equipment layouts. engineering modeling and optimization - 2d and 3d models are created, using a simulation software to model the process parameters and scale the chemical processes. these modeling software help determine system limitations, non-linear chemical and physical changes, and potential equipment sizing. mass and energy balances, finalized p&id's and general arrangement drawings are produced. automation strategies for the system are developed (if needed). controls system programming begins and will continue through fabrication and assembly fabrication and assembly - after an optimized design has been determined, the custom pilot is fabricated and assembled. pilot plants can either be assembled on-site or off-site as modular skids that will be constructed and tested in a controlled environment. testing - testing of completed systems, including system controls, is conducted to ensure proper system function. installation and startup - if constructed offsite, pilot skids are installed onsite. after all equipment is in place, full system startup is completed by integrating the system with existing plant utilities and controls. full operation is tested and affirmed. training - operator training is complete and full system documentation is handed over.   == see also == operations research chemical engineering process engineering chemist   == bibliography == m. levin (editor), pharmaceutical process scale-up (drugs and the pharmaceutical), informa healthcare, 3rd edition, isbn 978-1616310011 (2011) m. lackner (editor), scale-up in combustion, processeng engineering gmbh, wien, isbn 978-3-902655-04-2 (2009). m. zlokarnik, scale-up in chemical engineering, wiley-vch verlag gmbh & co. kgaa, 2nd edition, isbn 978-3527314218 (2006). richard palluzi, pilot plants: design, construction and operation, mcgraw-hill, february, 1992. richard palluzi, pilot plants, chemical engineering, march, 1990.   == references =="")"
152,"Axiomatic design is a systems design methodology using matrix methods to systematically analyze the transformation of customer needs into functional requirements, design parameters, and process variables.  Specifically, a set of functional requirements(FRs) are related to a set of design parameters (DPs) by a Design Matrix A:

  
    
      
        
          
            [
            
              
                
                  F
                  
                    R
                    
                      1
                    
                  
                
              
              
                
                  F
                  
                    R
                    
                      2
                    
                  
                
              
            
            ]
          
        
        =
        
          
            [
            
              
                
                  
                    A
                    
                      11
                    
                  
                
                
                  
                    A
                    
                      12
                    
                  
                
              
              
                
                  
                    A
                    
                      21
                    
                  
                
                
                  
                    A
                    
                      22
                    
                  
                
              
            
            ]
          
        
        
          
            [
            
              
                
                  D
                  
                    P
                    
                      1
                    
                  
                
              
              
                
                  D
                  
                    P
                    
                      2
                    
                  
                
              
            
            ]
          
        
      
    
    {\displaystyle {\begin{bmatrix}FR_{1}\\FR_{2}\end{bmatrix}}={\begin{bmatrix}A_{11}&A_{12}\\A_{21}&A_{22}\end{bmatrix}}{\begin{bmatrix}DP_{1}\\DP_{2}\end{bmatrix}}}
  The method gets its name from its use of design principles or design Axioms (i.e., given without proof) governing the analysis and decision making process in developing high quality product or system designs.  The two axioms used in Axiomatic Design (AD) are:

Axiom 1: The Independence Axiom.  Maintain the independence of the functional requirements (FRs).
Axiom 2: The Information Axiom.  Minimize the information content of the design.Axiomatic design is considered to be a design method that addresses fundamental issues in Taguchi methods.
Coupling is the term Axiomatic Design uses to describe a lack of independence between the FRs of the system as determined by the DPs.   I.e., if varying one DP has a resulting significant impact on two separate FRs, it is said the FRs are coupled.  Axiomatic Design introduces matrix analysis of the Design Matrix to both assess and mitigate the effects of coupling.
Axiom 2, the Information Axiom, provides a metric of the probability that a specific DP will deliver the functional performance required to satisfy the FR.  The metric is normalized to be summed up for the entire system being modeled. Systems with less functional performance risk (minimal information content) are preferred over alternative systems with higher information content.
The methodology was developed by Dr. Suh Nam Pyo at MIT, Department of Mechanical Engineering since the 1990s. A series of academic conferences have been held to present current developments of the methodology. 


== See also ==
Design structure matrix (DSM)
New product development (NPD)
Design for Six Sigma
Six Sigma
Taguchi methods
Axiomatic product development lifecycle (APDL)
C-K theory


== References ==


== External links ==
A discussion of the methodology is given here: 

Axiomatic Design for Complex Systems is a professional short course offered at MIT
Axiomatic Design Technology described by Axiomatic Design Solutions, Inc.Axiomatic Design Conferences:

""2021 International Conference on Axiomatic Design (ICAD 2021)"". NOVA School of Science and Technology. NOVA University of Lisbon, Portugal. Retrieved 21 January 2021.
""2019 International Conference on Axiomatic Design (ICAD)"". School of Mechanical and Manufacturing Engineering. University of New South Wales, Kensington, Australia. Retrieved 16 December 2018.Past proceedings of International Conferences on Axiomatic Design can be downloaded here:

ICAD2016
ICAD2015
ICAD2014
ICAD2013
ICAD2011
ICAD2009
ICAD2006
ICAD2004
ICAD2002
ICAD2000","pandas(index=152, _1=152, text='axiomatic design is a systems design methodology using matrix methods to systematically analyze the transformation of customer needs into functional requirements, design parameters, and process variables.  specifically, a set of functional requirements(frs) are related to a set of design parameters (dps) by a design matrix a:       [    f  r  1       f  r  2      ]   =   [     a  11      a  12        a  21      a  22      ]     [    d  p  1       d  p  2      ]      the method gets its name from its use of design principles or design axioms (i.e., given without proof) governing the analysis and decision making process in developing high quality product or system designs.  the two axioms used in axiomatic design (ad) are:  axiom 1: the independence axiom.  maintain the independence of the functional requirements (frs). axiom 2: the information axiom.  minimize the information content of the design.axiomatic design is considered to be a design method that addresses fundamental issues in taguchi methods. coupling is the term axiomatic design uses to describe a lack of independence between the frs of the system as determined by the dps.   i.e., if varying one dp has a resulting significant impact on two separate frs, it is said the frs are coupled.  axiomatic design introduces matrix analysis of the design matrix to both assess and mitigate the effects of coupling. axiom 2, the information axiom, provides a metric of the probability that a specific dp will deliver the functional performance required to satisfy the fr.  the metric is normalized to be summed up for the entire system being modeled. systems with less functional performance risk (minimal information content) are preferred over alternative systems with higher information content. the methodology was developed by dr. suh nam pyo at mit, department of mechanical engineering since the 1990s. a series of academic conferences have been held to present current developments of the methodology.   == see also == design structure matrix (dsm) new product development (npd) design for six sigma six sigma taguchi methods axiomatic product development lifecycle (apdl) c-k theory   == references ==   == external links == a discussion of the methodology is given here:  axiomatic design for complex systems is a professional short course offered at mit axiomatic design technology described by axiomatic design solutions, inc.axiomatic design conferences:  ""2021 international conference on axiomatic design (icad 2021)"". nova school of science and technology. nova university of lisbon, portugal. retrieved 21 january 2021. ""2019 international conference on axiomatic design (icad)"". school of mechanical and manufacturing engineering. university of new south wales, kensington, australia. retrieved 16 december 2018.past proceedings of international conferences on axiomatic design can be downloaded here:  icad2016 icad2015 icad2014 icad2013 icad2011 icad2009 icad2006 icad2004 icad2002 icad2000')"
153,"In statistics, response surface methodology (RSM) explores the relationships between several explanatory variables and one or more response variables.  The method was introduced by George E. P. Box and K. B. Wilson in 1951.  The main idea of RSM is to use a sequence of designed experiments to obtain an optimal response.  Box and Wilson suggest using a second-degree polynomial model to do this.  They acknowledge that this model is only an approximation, but they use it because such a model is easy to estimate and apply, even when little is known about the process.
Statistical approaches such as RSM can be employed to maximize the production of a special substance by optimization of operational factors. Of  late,  for formulation optimization, the RSM, using  proper design of experiments (DoE),  has become   extensively used. In contrast to conventional methods, the interaction among process variables can be determined by statistical techniques.


== Basic approach of response surface methodology ==
An easy way to estimate a first-degree polynomial model is to use a factorial experiment or a fractional factorial design.  This is sufficient to determine which explanatory variables affect the response variable(s) of interest.  Once it is suspected that only significant explanatory variables are left, then a more complicated design, such as a central composite design can be implemented to estimate a second-degree polynomial model, which is still only an approximation at best.  However, the second-degree model can be used to optimize (maximize, minimize, or attain a specific target for) the response variable(s) of interest.


== Important RSM properties and features ==
ORTHOGONALITY:
The property that allows individual effects of the k-factors to be estimated independently without (or with minimal) confounding. Also orthogonality provides minimum variance estimates of the model coefficient so that they are uncorrelated.
ROTATABILITY:
The property of rotating points of the design about the center of the factor space. The moments of the distribution of the design points are constant.
UNIFORMITY:
A third property of CCD designs used to control the number of center points is uniform precision (or Uniformity).


== Special geometries ==


=== Cube ===
Cubic designs are discussed by Kiefer, by Atkinson, Donev, and Tobias and by Hardin and Sloane.


=== Sphere ===
Spherical designs are discussed by Kiefer and by Hardin and Sloane.


=== Simplex geometry and mixture experiments ===
Mixture experiments are discussed in many books on the design of experiments, and in the response-surface methodology textbooks of Box and Draper and of Atkinson, Donev and Tobias. An extensive discussion and survey appears in the advanced textbook by John Cornell.


== Extensions ==


=== Multiple objective functions ===

Some extensions of response surface methodology deal with the multiple response problem.  Multiple response variables create difficulty because what is optimal for one response may not be optimal for other responses.  Other extensions are used to reduce variability in a single response while targeting a specific value, or attaining a near maximum or minimum while preventing variability in that response from getting too large.


== Practical concerns ==
Response surface methodology uses statistical models, and therefore practitioners need to be aware that even the best statistical model is an approximation to reality. In practice, both the models and the parameter values are unknown, and subject to uncertainty on top of ignorance. Of course, an estimated optimum point need not be optimum in reality, because of the errors of the estimates and of the inadequacies of the model.
Nonetheless, response surface methodology has an effective track-record of helping researchers improve products and services: For example, Box's original response-surface modeling enabled chemical engineers to improve a process that had been stuck at a saddle-point for years. The engineers had not been able to afford to fit a cubic three-level design to estimate a quadratic model, and their biased linear-models estimated the gradient to be zero. Box's design reduced the costs of experimentation so that a quadratic model could be fit, which led to a (long-sought) ascent direction.


== See also ==
Box–Behnken design
Central composite design
Gradient-enhanced kriging (GEK)
IOSO method based on response-surface methodology
Optimal designs
Plackett–Burman design
Polynomial and rational function modeling
Polynomial regression
Probabilistic design
Surrogate model


== References ==

Box, G.E.P.; Wilson, K.B. (1951). ""On the Experimental Attainment of Optimum Conditions"". Journal of the Royal Statistical Society: Series B. 13 (1): 1–45. doi:10.1111/j.2517-6161.1951.tb00067.x.
Box, G. E. P. and Draper, Norman. 2007. Response Surfaces, Mixtures, and Ridge Analyses, Second Edition [of Empirical Model-Building and Response Surfaces, 1987], Wiley.
Atkinson, A.C.; Donev, A.N.; Tobias, R.D. (2007). Optimum Experimental Designs, with SAS. Oxford University Press. pp. 511+xvi. ISBN 978-0-19-929660-6.
Cornell, John (2002). Experiments with Mixtures: Designs, Models, and the Analysis of Mixture Data (third ed.). Wiley. ISBN 978-0-471-07916-3.Goos, Peter] (2002). The Optimal Design of Blocked and Split-plot Experiments. Lecture Notes in Statistics. 164. Springer. ISBN 978-0-387-95515-5.
Kiefer, Jack Carl (1985).  L. D. Brown;  et al. (eds.). Jack Carl Kiefer Collected Papers III Design of Experiments. Springer-Verlag. ISBN 978-0-387-96004-3.
Pukelsheim, Friedrich (2006). Optimal Design of Experiments. SIAM]. ISBN 978-0-89871-604-7.
Hardin, R.H.; Sloane, N.J.A. (1993). ""A New Approach to the Construction of Optimal Designs"" (PDF). Journal of Statistical Planning and Inference. 37 (3): 339–369. doi:10.1016/0378-3758(93)90112-J.
Hardin, R.H.; Sloane, N.J.A. ""Computer-Generated Minimal (and Larger) Response Surface Designs: (I) The Sphere"" (PDF).
Hardin, R.H.; Sloane, N.J.A. ""Computer-Generated Minimal (and Larger) Response Surface Designs: (II) The Cube"" (PDF).
Ghosh, S.; Rao, C. R., eds. (1996). Design and Analysis of Experiments. Handbook of Statistics. 13. North-Holland. ISBN 978-0-444-82061-7.
Draper, Norman; Lin, Dennis K.J. ""Response surface designs"": 343–375. 
Gaffke, Norbert; Heiligers, Berthold (1996). ""30 Approximate designs for polynomial regression: Invariance, admissibility, and optimality"". Approximate designs for polynomial regression: Invariance, admissibility, and optimality. Handbook of Statistics. 13. pp. 1149–99. doi:10.1016/S0169-7161(96)13032-7. ISBN 9780444820617.


=== Historical ===
Gergonne, J. D. (1974) [1815]. ""The application of the method of least squares to the interpolation of sequences"". Historia Mathematica (Translated by Ralph St. John and S. M. Stigler from the 1815 French ed.). 1 (4): 439–447. doi:10.1016/0315-0860(74)90034-2.
Stigler, Stephen M. (1974). ""Gergonne's 1815 paper on the design and analysis of polynomial regression experiments"". Historia Mathematica. 1 (4): 431–9. doi:10.1016/0315-0860(74)90033-0.Peirce, C. S. (1876). ""Note on the Theory of the Economy of Research"" (PDF). Coast Survey Report. Appendix No. 14: 197–201.
Reprinted in Collected Papers of Charles Sanders Peirce. 7. 1958. paragraphs 139–157,
and in Peirce, C. S. (July–August 1967). ""Note on the Theory of the Economy of Research"". Operations Research. 15 (4): 643–8. doi:10.1287/opre.15.4.643. JSTOR 168276.
Smith, Kirstine (1918). ""On the Standard Deviations of Adjusted and Interpolated Values of an Observed Polynomial Function and its Constants and the Guidance They Give Towards a Proper Choice of the Distribution of the Observations"". Biometrika. 12 (1/2): 1–85. doi:10.2307/2331929. JSTOR 2331929.


== External links ==
Response surface designs","pandas(index=153, _1=153, text='in statistics, response surface methodology (rsm) explores the relationships between several explanatory variables and one or more response variables.  the method was introduced by george e. p. box and k. b. wilson in 1951.  the main idea of rsm is to use a sequence of designed experiments to obtain an optimal response.  box and wilson suggest using a second-degree polynomial model to do this.  they acknowledge that this model is only an approximation, but they use it because such a model is easy to estimate and apply, even when little is known about the process. statistical approaches such as rsm can be employed to maximize the production of a special substance by optimization of operational factors. of  late,  for formulation optimization, the rsm, using  proper design of experiments (doe),  has become   extensively used. in contrast to conventional methods, the interaction among process variables can be determined by statistical techniques.   == basic approach of response surface methodology == an easy way to estimate a first-degree polynomial model is to use a factorial experiment or a fractional factorial design.  this is sufficient to determine which explanatory variables affect the response variable(s) of interest.  once it is suspected that only significant explanatory variables are left, then a more complicated design, such as a central composite design can be implemented to estimate a second-degree polynomial model, which is still only an approximation at best.  however, the second-degree model can be used to optimize (maximize, minimize, or attain a specific target for) the response variable(s) of interest.   == important rsm properties and features == orthogonality: the property that allows individual effects of the k-factors to be estimated independently without (or with minimal) confounding. also orthogonality provides minimum variance estimates of the model coefficient so that they are uncorrelated. rotatability: the property of rotating points of the design about the center of the factor space. the moments of the distribution of the design points are constant. uniformity: a third property of ccd designs used to control the number of center points is uniform precision (or uniformity).   == special geometries == gergonne, j. d. (1974) [1815]. ""the application of the method of least squares to the interpolation of sequences"". historia mathematica (translated by ralph st. john and s. m. stigler from the 1815 french ed.). 1 (4): 439–447. doi:10.1016/0315-0860(74)90034-2. stigler, stephen m. (1974). ""gergonne\'s 1815 paper on the design and analysis of polynomial regression experiments"". historia mathematica. 1 (4): 431–9. doi:10.1016/0315-0860(74)90033-0.peirce, c. s. (1876). ""note on the theory of the economy of research"" (pdf). coast survey report. appendix no. 14: 197–201. reprinted in collected papers of charles sanders peirce. 7. 1958. paragraphs 139–157, and in peirce, c. s. (july–august 1967). ""note on the theory of the economy of research"". operations research. 15 (4): 643–8. doi:10.1287/opre.15.4.643. jstor 168276. smith, kirstine (1918). ""on the standard deviations of adjusted and interpolated values of an observed polynomial function and its constants and the guidance they give towards a proper choice of the distribution of the observations"". biometrika. 12 (1/2): 1–85. doi:10.2307/2331929. jstor 2331929.   == external links == response surface designs')"
154,"In industrial engineering, the standard time is the time required by an average skilled operator, working at
a normal pace, to perform a specified task using a prescribed method. It includes appropriate allowances to allow the person to recover from fatigue and, where necessary, an additional allowance to cover contingent elements which may occur but have not been observed.
Standard time =normal time +allowance 
Where; normal time =avg time *rating factor. 
(take rating factor between 1.1 and 1.2)


== Usage of the standard time ==
Time times for all operations are known. 

Staffing (or workforce planning): the number of workers required cannot accurately be determined unless the time required to process the existing work is known.
Line balancing (or production leveling): the correct number of workstations for optimum work flow depends on the processing time, or standard, at each workstation.
Materials requirement planning (MRP): MRP systems cannot operate properly without accurate work standards.
System simulation: simulation models cannot accurately simulate operation unless times for all operations are known.
Wage payment: comparing expected performance with actual performance requires the use of work standards.
Cost accounting: work standards are necessary for determining not only the labor component of costs, but also the correct allocation of production costs to specific products.
Employee evaluation: in order to assess whether individual employees are performing as well as they should, a performance standard is necessary against which to measure the level of performance.


== Techniques to establish a standard time ==
The standard time can be determined using the following techniques:
Time study,
Predetermined motion time system aka PMTS or PTS,
Standard data system,
Work sampling.


== Method of calculation ==
The Standard Time is the product of three factors:

Observed time: The time measured to complete the task.
Performance rating factor: The pace the person is working at. 90% is working slower than normal, 110% is working faster than normal, 100% is normal.  This factor is calculated by an experienced worker who is trained to observe and determine the rating.
Personal, fatigue, and delay (PFD) allowance.The standard time can then be calculated by using:

  
    
      
        
          Standard Time
        
        =
        (
        
          Observed Time
        
        )
        (
        
          Rating Factor
        
        )
        (
        1
        +
        
          PFD Allowance
        
        )
      
    
    {\displaystyle {\text{Standard Time}}=({\text{Observed Time}})({\text{Rating Factor}})(1+{\text{PFD Allowance}})}
  


== References ==

Groover, M. P. (2007). Work systems: the methods, measurement and management of work, Prentice Hall, ISBN 978-0-13-140650-6
Salvendy, G. (Ed.) (2001). Handbook of Industrial Engineering: Technology and Operations Management, third edition, John Wiley & Sons, Hoboken, NJ.
Zandin, K. (Ed.) (2001). Maynard's Industrial Engineering Handbook, fifth edition, McGraw-Hill, New York, NY.
Standard Performance","pandas(index=154, _1=154, text=""in industrial engineering, the standard time is the time required by an average skilled operator, working at a normal pace, to perform a specified task using a prescribed method. it includes appropriate allowances to allow the person to recover from fatigue and, where necessary, an additional allowance to cover contingent elements which may occur but have not been observed. standard time =normal timeallowance where; normal time =avg time *rating factor. (take rating factor between 1.1 and 1.2)   == usage of the standard time == time times for all operations are known.  staffing (or workforce planning): the number of workers required cannot accurately be determined unless the time required to process the existing work is known. line balancing (or production leveling): the correct number of workstations for optimum work flow depends on the processing time, or standard, at each workstation. materials requirement planning (mrp): mrp systems cannot operate properly without accurate work standards. system simulation: simulation models cannot accurately simulate operation unless times for all operations are known. wage payment: comparing expected performance with actual performance requires the use of work standards. cost accounting: work standards are necessary for determining not only the labor component of costs, but also the correct allocation of production costs to specific products. employee evaluation: in order to assess whether individual employees are performing as well as they should, a performance standard is necessary against which to measure the level of performance.   == techniques to establish a standard time == the standard time can be determined using the following techniques: time study, predetermined motion time system aka pmts or pts, standard data system, work sampling.   == method of calculation == the standard time is the product of three factors:  observed time: the time measured to complete the task. performance rating factor: the pace the person is working at. 90% is working slower than normal, 110% is working faster than normal, 100% is normal.  this factor is calculated by an experienced worker who is trained to observe and determine the rating. personal, fatigue, and delay (pfd) allowance.the standard time can then be calculated by using:      standard time  = (  observed time  ) (  rating factor  ) ( 1pfd allowance  )       == references ==  groover, m. p. (2007). work systems: the methods, measurement and management of work, prentice hall, isbn 978-0-13-140650-6 salvendy, g. (ed.) (2001). handbook of industrial engineering: technology and operations management, third edition, john wiley & sons, hoboken, nj. zandin, k. (ed.) (2001). maynard's industrial engineering handbook, fifth edition, mcgraw-hill, new york, ny. standard performance"")"
155,"Packaging engineering, also package engineering, packaging technology and packaging science, is a broad topic ranging from design conceptualization to product placement.  All steps along the manufacturing process, and more, must be taken into account in the design of the package for any given product.  Package engineering  is an interdisciplinary field integrating science, engineering, technology and management to protect and identify products for distribution, storage, sale, and use. It encompasses the process of design, evaluation, and production of packages.  It is a system integral to the value chain that impacts product quality, user satisfaction, distribution efficiencies, and safety. Package engineering includes industry-specific aspects of industrial engineering,  marketing,  materials science, industrial design and logistics.  Packaging engineers must interact with research and development, manufacturing, marketing, graphic design, regulatory, purchasing, planning and so on.  The package must sell and protect the product, while maintaining an efficient, cost-effective process cycle.Engineers develop packages from a wide variety of rigid and flexible materials.  Some materials have scores or creases to allow controlled folding into package shapes (sometimes resembling origami).  Packaging involves extrusion, thermoforming, molding and other processing technologies.  Packages are often developed for high speed fabrication, filling, processing, and shipment. Packaging engineers use principles of structural analysis and thermal analysis in their evaluations.


== Education ==
Some packaging engineers have backgrounds in other science, engineering, or design disciplines while some have college degrees specializing in this field.Formal packaging programs might be listed as package engineering, packaging science, packaging technology, etc.  BE, BS, MS, M.Tech and PhD programs are available.  Students in a packaging program typically begin with generalized science, business, and engineering classes before progressing into industry-specific topics such as shelf life stability, corrugated box design, cushioning, engineering design, labeling regulations,  project management, food safety, robotics, RFID tags, quality management, package testing, packaging machinery, tamper-evident methods, recycling, computer-aided design, etc.


== See also ==
Packaging and labelling
Packing problems
Queueing theory
Engineering economics
Manufacturing engineering


== Notes ==


== Bibliography ==
Yam, K. L., ""Encyclopedia of Packaging Technology"", John Wiley & Sons, 2009, ISBN 978-0-470-08704-6
Hanlon, Kelsey, and Forcinio,  ""Handbook of Package Engineering"", CRC Press, 1998","pandas(index=155, _1=155, text='packaging engineering, also package engineering, packaging technology and packaging science, is a broad topic ranging from design conceptualization to product placement.  all steps along the manufacturing process, and more, must be taken into account in the design of the package for any given product.  package engineering  is an interdisciplinary field integrating science, engineering, technology and management to protect and identify products for distribution, storage, sale, and use. it encompasses the process of design, evaluation, and production of packages.  it is a system integral to the value chain that impacts product quality, user satisfaction, distribution efficiencies, and safety. package engineering includes industry-specific aspects of industrial engineering,  marketing,  materials science, industrial design and logistics.  packaging engineers must interact with research and development, manufacturing, marketing, graphic design, regulatory, purchasing, planning and so on.  the package must sell and protect the product, while maintaining an efficient, cost-effective process cycle.engineers develop packages from a wide variety of rigid and flexible materials.  some materials have scores or creases to allow controlled folding into package shapes (sometimes resembling origami).  packaging involves extrusion, thermoforming, molding and other processing technologies.  packages are often developed for high speed fabrication, filling, processing, and shipment. packaging engineers use principles of structural analysis and thermal analysis in their evaluations.   == education == some packaging engineers have backgrounds in other science, engineering, or design disciplines while some have college degrees specializing in this field.formal packaging programs might be listed as package engineering, packaging science, packaging technology, etc.  be, bs, ms, m.tech and phd programs are available.  students in a packaging program typically begin with generalized science, business, and engineering classes before progressing into industry-specific topics such as shelf life stability, corrugated box design, cushioning, engineering design, labeling regulations,  project management, food safety, robotics, rfid tags, quality management, package testing, packaging machinery, tamper-evident methods, recycling, computer-aided design, etc.   == see also == packaging and labelling packing problems queueing theory engineering economics manufacturing engineering   == notes ==   == bibliography == yam, k. l., ""encyclopedia of packaging technology"", john wiley & sons, 2009, isbn 978-0-470-08704-6 hanlon, kelsey, and forcinio,  ""handbook of package engineering"", crc press, 1998')"
156,"The Manufacturing Engineering Centre (MEC) is an international R&D Centre of Excellence for Advanced Manufacturing and Information Technology.  The MEC was founded in 1996 under the directorship of Professor Duc Truong Pham. The Centre forms part of Cardiff University, which dates back to 1883 and is one of Britain's major civic universities.
The MEC's purpose is to conduct research and development in all major areas of Advanced Manufacturing and use the output to promote the introduction of new manufacturing technology and practice to industry. It was the first autonomous research centre created by Cardiff University.


== Research ==
The MEC conducts basic, strategic and applied research as well as technology transfer with partners from 22 countries in Europe, Asia and the Americas.  The research spans a broad spectrum of subjects, from Robotics and Microsystems, Sensor Systems, High-speed Automation and Intelligent Control, Rapid manufacturing, MicroManufacturing, Nanotechnology, Quality Engineering, Multimedia, Virtual Reality and Enterprise Information Management.
Since 1996, the Centre has received over £50 million in grants and contracts and has attracted hundreds of industrial partners.  In 2004, the MEC won two EC 6th Framework Networks of Excellence contracts totalling 15M Euros in value.  The two Networks of Excellence led by the MEC, I*PROMS and 4M, involve some 50 centres of excellence in the field of Advanced Manufacturing across the EU. 
As a Centre of Excellence for Technology and Industrial Collaboration (CETIC) sponsored by the Welsh Assembly Government (WAG) and the European Regional Development Fund (ERDF), the MEC has contributed significantly to the Welsh economy, having completed thousands of projects with local companies and helped to generate and safeguard jobs in the region.


== Awards ==
Under Professor Pham's leadership, the MEC was awarded the DTI University/Industry First Prize by the Secretary of State for Trade and Industry for its success in  building research partnerships with industry (March 1999), and the Queen's Anniversary Prize for Higher and Further Education in recognition of its contribution made to the economy (February 2001).


== References ==","pandas(index=156, _1=156, text=""the manufacturing engineering centre (mec) is an international r&d centre of excellence for advanced manufacturing and information technology.  the mec was founded in 1996 under the directorship of professor duc truong pham. the centre forms part of cardiff university, which dates back to 1883 and is one of britain's major civic universities. the mec's purpose is to conduct research and development in all major areas of advanced manufacturing and use the output to promote the introduction of new manufacturing technology and practice to industry. it was the first autonomous research centre created by cardiff university.   == research == the mec conducts basic, strategic and applied research as well as technology transfer with partners from 22 countries in europe, asia and the americas.  the research spans a broad spectrum of subjects, from robotics and microsystems, sensor systems, high-speed automation and intelligent control, rapid manufacturing, micromanufacturing, nanotechnology, quality engineering, multimedia, virtual reality and enterprise information management. since 1996, the centre has received over £50 million in grants and contracts and has attracted hundreds of industrial partners.  in 2004, the mec won two ec 6th framework networks of excellence contracts totalling 15m euros in value.  the two networks of excellence led by the mec, i*proms and 4m, involve some 50 centres of excellence in the field of advanced manufacturing across the eu. as a centre of excellence for technology and industrial collaboration (cetic) sponsored by the welsh assembly government (wag) and the european regional development fund (erdf), the mec has contributed significantly to the welsh economy, having completed thousands of projects with local companies and helped to generate and safeguard jobs in the region.   == awards == under professor pham's leadership, the mec was awarded the dti university/industry first prize by the secretary of state for trade and industry for its success in  building research partnerships with industry (march 1999), and the queen's anniversary prize for higher and further education in recognition of its contribution made to the economy (february 2001).   == references =="")"
157,"Manufacturing engineering is a branch of professional engineering that shares many common concepts and ideas with other fields of engineering such as mechanical, chemical, electrical, and industrial engineering. 
Manufacturing engineering requires the ability to plan the practices of manufacturing; to research and to develop tools, processes, machines and equipment; and to integrate the facilities and systems for producing quality products with the optimum expenditure of capital.The manufacturing or production engineer's primary focus is to turn raw material into an updated or new product in the most effective, efficient & economic way possible.


== Overview ==
Manufacturing Engineering is based on core industrial engineering and mechanical engineering skills, adding important elements from mechatronics, commerce, economics and business management.
This field also deals with the integration of different facilities and systems for producing quality products (with optimal expenditure) by applying the principles of physics and the results of manufacturing systems studies, such as the following: 

Manufacturing engineers develop and create physical artifacts, production processes, and technology. It is a very broad area which includes the design and development of products. Manufacturing engineering is considered to be a subdiscipline of industrial engineering/systems engineering and has very strong overlaps with mechanical engineering. Manufacturing engineers' success or failure directly impacts the advancement of technology and the spread of innovation. This field of manufacturing engineering emerged from tool and die discipline in the early 20th century.  It expanded greatly from the 1960s when industrialized countries introduced factories with:
1. Numerical control machine tools and automated systems of production.
2. Advanced statistical methods of quality control: These factories were pioneered by the American electrical engineer William Edwards Deming, who was initially ignored by his home country. The same methods of quality control later turned Japanese factories into world leaders in cost-effectiveness and production quality.
3. Industrial robots on the factory floor, introduced in the late 1970s: These computer-controlled welding arms and grippers could perform simple tasks such as attaching a car door quickly and flawlessly 24 hours a day. This cut costs and improved production speed.


== History ==
The history of manufacturing engineering can be traced to factories in the mid 19th century USA and 18th century UK. Although large home production sites and workshops were established in China, ancient Rome and the Middle East, the Venice Arsenal provides one of the first examples of a factory in the modern sense of the word. Founded in 1104 in the Republic of Venice several hundred years before the Industrial Revolution, this factory mass-produced ships on assembly lines using manufactured parts. The Venice Arsenal apparently produced nearly one ship every day and, at its height, employed 16,000 people.

Many historians regard Matthew Boulton's Soho Manufactory (established in 1761 in Birmingham) as the first modern factory. Similar claims can be made for John Lombe's silk mill in Derby (1721), or Richard Arkwright's Cromford Mill (1771). The Cromford Mill was purpose-built to accommodate the equipment it held and to take the material through the various manufacturing processes. One historian, Jack Weatherford, contends that the first factory was in Potosí. The Potosi factory took advantage of the abundant silver that was mined nearby and processed silver ingot slugs into coins.
British colonies in the 19th century built factories simply as buildings where a large number of workers gathered to perform hand labor, usually in textile production. This proved more efficient for the administration and distribution of materials to individual workers than earlier methods of manufacturing, such as cottage industries or the putting-out system.
Cotton mills used inventions such as the steam engine and the power loom to pioneer the industrial factories of the 19th century, where precision machine tools and replaceable parts allowed greater efficiency and less waste. This experience formed the basis for the later studies of manufacturing engineering.  Between 1820 and 1850, non-mechanized factories supplanted traditional artisan shops as the predominant form of manufacturing institution.
Henry Ford further revolutionized the factory concept and thus manufacturing engineering in the early 20th century with the innovation of mass production. Highly specialized workers situated alongside a series of rolling ramps would build up a product such as (in Ford's case) an automobile. This concept dramatically decreased production costs for virtually all manufactured goods and brought about the age of consumerism.


=== Modern developments ===
Modern manufacturing engineering studies include all intermediate processes required for the production and integration of a product's components.

Some industries, such as semiconductor and steel manufacturers use the term ""fabrication"" for these processes.

Automation is used in different processes of manufacturing such as machining and welding. Automated manufacturing refers to the application of automation to produce goods in a factory. The main advantages of automated manufacturing for the manufacturing process are realized with effective implementation of automation and include: higher consistency and quality, reduction of lead times, simplification of production, reduced handling, improved work flow, and improved worker morale.
Robotics is the application of mechatronics and automation to create robots, which are often used in manufacturing to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot). Robots are used extensively in manufacturing engineering.

Robots allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform economically, and to ensure better quality. Many companies employ assembly lines of robots, and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications.


== Education ==


=== Manufacturing Engineers ===
Manufacturing Engineers focus on the design, development and operation of integrated systems of production to obtain high quality & economically competitive products. These systems may include material handling equipment, machine tools, robots or even computers or networks of computers.


=== Certification Programs ===
Manufacturing engineers possess an associate's or bachelor's degree in engineering with a major in manufacturing engineering. The length of study for such a degree is usually two to five years followed by five more years of professional practice to qualify as a professional engineer. Working as a manufacturing engineering technologist involves a more applications-oriented qualification path.
Academic degrees for manufacturing engineers are usually the Associate or Bachelor of Engineering, [BE] or [BEng], and the Associate or Bachelor of Science, [BS] or [BSc]. For manufacturing technologists the required degrees are Associate or Bachelor of Technology [B.TECH] or Associate or Bachelor of Applied Science [BASc] in Manufacturing, depending upon the university. Master's degrees in engineering manufacturing include Master of Engineering [ME] or [MEng] in Manufacturing, Master of Science [M.Sc] in Manufacturing Management, Master of Science [M.Sc] in Industrial and Production Management, and Master of Science [M.Sc] as well as Master of Engineering [ME] in Design, which is a subdiscipline of manufacturing. Doctoral [PhD] or [DEng] level courses in manufacturing are also available depending on the university.
The undergraduate degree curriculum generally includes courses in physics, mathematics, computer science, project management, and specific topics in mechanical and manufacturing engineering. Initially such topics cover most, if not all, of the subdisciplines of manufacturing engineering. Students then choose to specialize in one or more subdisciplines towards the end of their degree work.


=== Syllabus ===
The Foundational Curriculum for a Bachelor's Degree of Manufacturing Engineering or Production Engineering includes below mentioned syllabus. This syllabus is closely related to Industrial Engineering and Mechanical Engineering,  but it differs by placing more emphasis on Manufacturing Science or Production Science. It includes the following areas:

Mathematics (Calculus, Differential Equations, Statistics and Linear Algebra)
Mechanics (Statics & Dynamics)
Solid Mechanics
Fluid Mechanics
Materials Science
Strength of Materials
Fluid Dynamics
Hydraulics
Pneumatics
HVAC (Heating, Ventilation & Air Conditioning)
Heat Transfer
Applied Thermodynamics
Energy Conversion
Instrumentation and Measurement
Engineering Drawing (Drafting) & Engineering Design
Engineering Graphics
Mechanism Design including Kinematics and Dynamics
Manufacturing Processes
Mechatronics
Circuit Analysis
Lean Manufacturing
Automation
Reverse Engineering
Quality Control
CAD(Computer aided Design which includes Solid Modelling) and CAM (Computer aided Manufacturing)A degree in Manufacturing Engineering typically differs from Mechanical Engineering in only a few specialized classes. Mechanical Engineering degrees focus more on the product design process and on complex products which requires more mathematical expertise.


== Manufacturing engineering certification ==
Certification and licensure:
In some countries, ""professional engineer"" is the term for registered or licensed engineers who are permitted to offer their professional services directly to the public. Professional Engineer, abbreviated (PE - USA) or (PEng - Canada), is the designation for licensure in North America. In order to qualify for this license, a candidate needs a bachelor's degree from an ABET recognized university in the USA, a passing score on a state examination, and four years of work experience usually gained via a structured internship. In the USA, more recent graduates have the option of dividing this licensure process into two segments. The Fundamentals of Engineering (FE) exam is often taken immediately after graduation and the Principles and Practice of Engineering exam is taken after four years of working in a chosen engineering field.
Society of Manufacturing Engineers (SME) certification (USA):
The SME administers qualifications specifically for the manufacturing industry. These are not degree level qualifications and are not recognized at the professional engineering level. The following discussion deals with qualifications in the USA only.  Qualified candidates for the Certified Manufacturing Technologist Certificate (CMfgT) must pass a three-hour, 130-question multiple-choice exam. The exam covers math, manufacturing processes, manufacturing management, automation, and related subjects. Additionally, a candidate must have at least four years of combined education and manufacturing-related work experience.
Certified Manufacturing Engineer (CMfgE) is an engineering qualification administered by the Society of Manufacturing Engineers, Dearborn, Michigan, USA.  Candidates qualifying for a Certified Manufacturing Engineer credential must pass a four-hour, 180 question multiple-choice exam which covers more in-depth topics than does the CMfgT exam. CMfgE candidates must also have eight years of combined education and manufacturing-related work experience, with a minimum of four years of work experience.
Certified Engineering Manager (CEM). The Certified Engineering Manager Certificate is also designed for engineers with eight years of combined education and manufacturing experience. The test is four hours long and has 160 multiple-choice questions. The CEM certification exam covers business processes, teamwork, responsibility, and other management-related categories.


== Modern tools ==

Many manufacturing companies, especially those in industrialized nations, have begun to incorporate computer-aided engineering (CAE) programs into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and ease of use in designing mating interfaces and tolerances.
Other CAE programs commonly used by product manufacturers include product life cycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM).
Using CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. No physical prototype need be created until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of relatively few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.
Just as manufacturing engineering is linked with other disciplines, such as mechatronics, multidisciplinary design optimization (MDO) is also being used with other CAE programs to automate and improve the iterative design process. MDO tools wrap around existing CAE processes, allowing product evaluation to continue even after the analyst goes home for the day. They also utilize sophisticated optimization algorithms to more intelligently explore possible designs, often finding better, innovative solutions to difficult multidisciplinary design problems.


== Manufacturing Engineering around the world ==
Manufacturing engineering is an extremely important discipline worldwide.  It goes by different names in different countries.  In the United States and the continental European Union it is commonly known as Industrial Engineering and in the United Kingdom and Australia it is called Manufacturing Engineering 


== Subdisciplines ==


=== Mechanics ===

Mechanics, in the most general sense, is the study of forces and their effects on matter. Typically, engineering mechanics is used to analyze and predict the acceleration and deformation (both elastic and plastic) of objects under known forces (also called loads) or stresses. Subdisciplines of mechanics include:

Statics, the study of non-moving bodies under known loads
Dynamics (or kinetics), the study of how forces affect moving bodies
Mechanics of materials, the study of how different materials deform under various types of stress
Fluid mechanics, the study of how fluids react to forces
Continuum mechanics, a method of applying mechanics that assumes that objects are continuous (rather than discrete)If the engineering project were to design a vehicle, statics might be employed to design the frame of the vehicle in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the manufacture of the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle or to design the intake system for the engine.


=== Kinematics ===

Kinematics is the study of the motion of bodies (objects) and systems (groups of objects), while ignoring the forces that cause the motion. The movement of a crane and the oscillations of a piston in an engine are both simple kinematic systems. The crane is a type of open kinematic chain, while the piston is part of a closed four-bar linkage.  Engineers typically use kinematics in the design and analysis of mechanisms. Kinematics can be used to find the possible range of motion for a given mechanism, or, working in reverse, can be used to design a mechanism that has a desired range of motion.


=== Drafting ===

Drafting or technical drawing is the means by which manufacturers create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information. A U.S engineer or skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions.
Instructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Optionally, an engineer may also manually manufacture a part using the technical drawings, but this is becoming an increasing rarity with the advent of computer numerically controlled (CNC) manufacturing. Engineers primarily manufacture parts manually in the areas of applied spray coatings, finishes, and other processes that cannot economically or practically be done by a machine.
Drafting is used in nearly every subdiscipline of mechanical and manufacturing engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).


=== Machine Tools and Metal Fabrication ===
Machine tools employ some sort of tool that does the cutting or shaping. All machine tools have some means of constraining the workpiece and provide a guided movement of the parts of the machine.  Metal fabrication is the building of metal structures by cutting, bending, and assembling processes.


=== Computer Integrated Manufacturing ===
Computer-integrated manufacturing (CIM) is the manufacturing approach of using computers to control the entire production process.  Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries.


=== Mechatronics ===

Mechatronics is an engineering discipline that deals with the convergence of electrical, mechanical and manufacturing systems. Such combined systems are known as electromechanical systems and are widespread. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various aircraft and automobile subsystems.
The term mechatronics is typically used to refer to macroscopic systems, but futurists have predicted the emergence of very small electromechanical devices. Already such small devices, known as Microelectromechanical systems  (MEMS), are used in automobiles to initiate the deployment of airbags, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high-definition printing. In future it is hoped that such devices will be used in tiny implantable medical devices and to improve optical communication.


=== Textile engineering ===
Textile engineering courses deal with the application of scientific and engineering principles to the design and control of all aspects of fiber, textile, and apparel processes, products, and machinery. These include natural and man-made materials, interaction of materials with machines, safety and health, energy conservation, and waste and pollution control. Additionally, students are given experience in plant design and layout, machine and wet process design and improvement, and designing and creating textile products. Throughout the textile engineering curriculum, students take classes from other engineering and disciplines including: mechanical, chemical, materials and industrial engineering.


=== Advanced composite materials ===
Advanced composite materials (engineering) (ACMs) are also known as advanced polymer matrix composites. These are generally characterized or determined by unusually high strength fibres with unusually high stiffness, or modulus of elasticity characteristics, compared to other materials, while bound together by weaker matrices.   Advanced composite materials have broad, proven applications, in the aircraft, aerospace, and sports equipment sectors. Even more specifically ACMs are very attractive for aircraft and aerospace structural parts.  Manufacturing ACMs is a multibillion-dollar industry worldwide. Composite products range from skateboards to components of the space shuttle. The industry can be generally divided into two basic segments, industrial composites and advanced composites.


== Employment ==
Manufacturing engineering is just one facet of the engineering manufacturing industry. Manufacturing engineers enjoy improving the production process from start to finish. They have the ability to keep the whole production process in mind as they focus on a particular portion of the process. Successful students in manufacturing engineering degree programs are inspired by the notion of starting with a natural resource, such as a block of wood, and ending with a usable, valuable product, such as a desk, produced efficiently and economically.
Manufacturing engineers are closely connected with engineering and industrial design efforts. Examples of major companies that employ manufacturing engineers in the United States include General Motors Corporation, Ford Motor Company, Chrysler, Boeing, Gates Corporation and Pfizer. Examples in Europe include Airbus, Daimler, BMW, Fiat, Navistar International, and Michelin Tyre.
Industries where manufacturing engineers are generally employed include:

Aerospace industry
Automotive industry
Chemical industry
Computer industry
Engineering management
Food processing industry
Garment industry
Industrial engineering
Management engineering
Mechanical engineering
Pharmaceutical industry
Process engineering
Pulp and paper industry
Systems engineering
Toy industry


== Frontiers of research ==


=== Flexible manufacturing systems ===

A flexible manufacturing system (FMS) is a manufacturing system in which there is some amount of flexibility that allows the system to react to changes, whether predicted or unpredicted. This flexibility is generally considered to fall into two categories, both of which have numerous subcategories.
The first category, machine flexibility, covers the system's ability to be changed to produce new product types and the ability to change the order of operations executed on a part. The second category, called routing flexibility, consists of the ability to use multiple machines to perform the same operation on a part, as well as the system's ability to absorb large-scale changes, such as in volume, capacity, or capability.
Most FMS systems comprise three main systems. The work machines, which are often automated CNC machines, are connected by a material handling system to optimize parts flow, and to a central control computer, which controls material movements and machine flow. The main advantages of an FMS is its high flexibility in managing manufacturing resources like time and effort in order to manufacture a new product. The best application of an FMS is found in the production of small sets of products from a mass production.


=== Computer integrated manufacturing ===

Computer-integrated manufacturing (CIM) in engineering is a method of manufacturing in which the entire production process is controlled by computer. Traditionally separated process methods are joined through a computer by CIM. This integration allows the processes to exchange information and to initiate actions. Through this integration, manufacturing can be faster and less error-prone, although the main advantage is the ability to create automated manufacturing processes. Typically CIM relies on closed-loop control processes based on real-time input from sensors. It is also known as flexible design and manufacturing.


=== Friction stir welding ===

Friction stir welding was discovered in 1991 by The Welding Institute (TWI). This innovative steady state (non-fusion) welding technique joins previously un-weldable materials, including several aluminum alloys. It may play an important role in the future construction of airplanes, potentially replacing rivets. Current uses of this technology to date include: welding the seams of the aluminum main space shuttle external tank, the Orion Crew Vehicle test article, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket; armor plating for amphibious assault ships; and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation, among an increasingly growing range of uses.
Other areas of research are Product Design, MEMS (Micro-Electro-Mechanical Systems), Lean Manufacturing, Intelligent Manufacturing Systems, Green Manufacturing, Precision Engineering, Smart Materials, etc.


== See also ==


== Notes ==


== External links ==
Institute of Manufacturing - UK
Georgia Tech Manufacturing Institute","pandas(index=157, _1=157, text='manufacturing engineering is a branch of professional engineering that shares many common concepts and ideas with other fields of engineering such as mechanical, chemical, electrical, and industrial engineering. manufacturing engineering requires the ability to plan the practices of manufacturing; to research and to develop tools, processes, machines and equipment; and to integrate the facilities and systems for producing quality products with the optimum expenditure of capital.the manufacturing or production engineer\'s primary focus is to turn raw material into an updated or new product in the most effective, efficient & economic way possible.   == overview == manufacturing engineering is based on core industrial engineering and mechanical engineering skills, adding important elements from mechatronics, commerce, economics and business management. this field also deals with the integration of different facilities and systems for producing quality products (with optimal expenditure) by applying the principles of physics and the results of manufacturing systems studies, such as the following:  manufacturing engineers develop and create physical artifacts, production processes, and technology. it is a very broad area which includes the design and development of products. manufacturing engineering is considered to be a subdiscipline of industrial engineering/systems engineering and has very strong overlaps with mechanical engineering. manufacturing engineers\' success or failure directly impacts the advancement of technology and the spread of innovation. this field of manufacturing engineering emerged from tool and die discipline in the early 20th century.  it expanded greatly from the 1960s when industrialized countries introduced factories with: 1. numerical control machine tools and automated systems of production. 2. advanced statistical methods of quality control: these factories were pioneered by the american electrical engineer william edwards deming, who was initially ignored by his home country. the same methods of quality control later turned japanese factories into world leaders in cost-effectiveness and production quality. 3. industrial robots on the factory floor, introduced in the late 1970s: these computer-controlled welding arms and grippers could perform simple tasks such as attaching a car door quickly and flawlessly 24 hours a day. this cut costs and improved production speed.   == history == the history of manufacturing engineering can be traced to factories in the mid 19th century usa and 18th century uk. although large home production sites and workshops were established in china, ancient rome and the middle east, the venice arsenal provides one of the first examples of a factory in the modern sense of the word. founded in 1104 in the republic of venice several hundred years before the industrial revolution, this factory mass-produced ships on assembly lines using manufactured parts. the venice arsenal apparently produced nearly one ship every day and, at its height, employed 16,000 people.  many historians regard matthew boulton\'s soho manufactory (established in 1761 in birmingham) as the first modern factory. similar claims can be made for john lombe\'s silk mill in derby (1721), or richard arkwright\'s cromford mill (1771). the cromford mill was purpose-built to accommodate the equipment it held and to take the material through the various manufacturing processes. one historian, jack weatherford, contends that the first factory was in potosí. the potosi factory took advantage of the abundant silver that was mined nearby and processed silver ingot slugs into coins. british colonies in the 19th century built factories simply as buildings where a large number of workers gathered to perform hand labor, usually in textile production. this proved more efficient for the administration and distribution of materials to individual workers than earlier methods of manufacturing, such as cottage industries or the putting-out system. cotton mills used inventions such as the steam engine and the power loom to pioneer the industrial factories of the 19th century, where precision machine tools and replaceable parts allowed greater efficiency and less waste. this experience formed the basis for the later studies of manufacturing engineering.  between 1820 and 1850, non-mechanized factories supplanted traditional artisan shops as the predominant form of manufacturing institution. henry ford further revolutionized the factory concept and thus manufacturing engineering in the early 20th century with the innovation of mass production. highly specialized workers situated alongside a series of rolling ramps would build up a product such as (in ford\'s case) an automobile. this concept dramatically decreased production costs for virtually all manufactured goods and brought about the age of consumerism. friction stir welding was discovered in 1991 by the welding institute (twi). this innovative steady state (non-fusion) welding technique joins previously un-weldable materials, including several aluminum alloys. it may play an important role in the future construction of airplanes, potentially replacing rivets. current uses of this technology to date include: welding the seams of the aluminum main space shuttle external tank, the orion crew vehicle test article, boeing delta ii and delta iv expendable launch vehicles and the spacex falcon 1 rocket; armor plating for amphibious assault ships; and welding the wings and fuselage panels of the new eclipse 500 aircraft from eclipse aviation, among an increasingly growing range of uses. other areas of research are product design, mems (micro-electro-mechanical systems), lean manufacturing, intelligent manufacturing systems, green manufacturing, precision engineering, smart materials, etc.   == see also ==   == notes ==   == external links == institute of manufacturing - uk georgia tech manufacturing institute')"
158,"Needs analysis is the formal process that sits alongside Requirements analysis and focuses on the human elements of the requirements.


== Introduction ==
User-centered design, according to Katz-Haas, is really about defining who the users are, defining their tasks and goals, their experience levels, what functions they want and need from a system, what information they want and need and understanding how the users think the system should work. User-centered design has also been linked to the identification of required job performance skills, the assessment of prospective trainees’ skills and the development of objectives.
The first step in any user centered design process is to understand the user's needs.
Put simply, whereas requirements analysis focuses on the elements needed to be represented in the system, needs analysis focuses on the requirements related to the goals, aspirations and needs of the users and/or the user community and feeds them into the system requirement analysis process. The main purpose of needs analysis is the user's satisfaction.
As it focuses on the needs of the human, needs analysis is not limited to addressing the requirements of just software, but can be applied to any domain, such as automotive, consumer product or services such as banking. Although it is not a business development tool, it can be used to help in the development of a business case.
We can identify the customers needs in three ways:

Client request,
Modification of an existing design,
Generation of new product.


== Underlying principles of needs analysis ==
The following list gives the principles of needs analysis as originally defined.

User's need-based requirements are complex and can conflict.
User's need-based requirements build a bridge from the business case to the design.
User's need-based requirements help to identify trade-offs that need to happen in the design process (i.e., where a design cannot resolve the user's need-based requirement conflicts).
User's need-based requirements are there to unify the multi-disciplinary design team; enabling them to meet their business case.
Formulate and ask questions to do with the business plan that provide an indication of the human aspects of the system, including the relative merit of functionality.
Always express these findings from the user's perspective.
Cross-relate these requirements to each other and to the impactors on the activity.
Allocate sufficient time during the development process to check and validate your user's need-based requirements.
Ensure that all user's need-based requirements are derived as low-level user requirements before being transposed into system requirements.
Word your requirements precisely and ensure that you cover all categories of human-related requirements.
Create test statements to validate the user's need-based requirements, the concept and the implementation.
Prior to freezing your design, validate your user's need-based requirements with users.
Accept that there still may be contradictory requirements.
Understand the nuances of the requirements and ensure that these are reflected in the precise wording of the requirements.
Keep asking your users until you have a true understanding of their requirements.
Elegant design can only be created from understanding the nuances of the requirements, sharing ideas and fact toward feasibility of work.


== Objectives of needs analysis ==
The objective of a user needs analysis is to define the audience, identify user goals, set usability objectives, identify design constraints and define functional specifications. Several methods of research inform a user needs analysis, including Task Analysis and Surveys.
User needs analysis is important because the interface design will be based on this information. Faulty assumptions or goals (about users) will lead to a faulty design that is difficult to fix once in place.


== See also ==
Usability
Human factors
Task analysis
Human factors integration
Training needs analysis
Needs assessment


== References ==

Harris, C., 2002, Hyperinnovation: Multidimensional Enterprise in the Connected Economy, Palgrave Macmillan","pandas(index=158, _1=158, text=""needs analysis is the formal process that sits alongside requirements analysis and focuses on the human elements of the requirements.   == introduction == user-centered design, according to katz-haas, is really about defining who the users are, defining their tasks and goals, their experience levels, what functions they want and need from a system, what information they want and need and understanding how the users think the system should work. user-centered design has also been linked to the identification of required job performance skills, the assessment of prospective trainees’ skills and the development of objectives. the first step in any user centered design process is to understand the user's needs. put simply, whereas requirements analysis focuses on the elements needed to be represented in the system, needs analysis focuses on the requirements related to the goals, aspirations and needs of the users and/or the user community and feeds them into the system requirement analysis process. the main purpose of needs analysis is the user's satisfaction. as it focuses on the needs of the human, needs analysis is not limited to addressing the requirements of just software, but can be applied to any domain, such as automotive, consumer product or services such as banking. although it is not a business development tool, it can be used to help in the development of a business case. we can identify the customers needs in three ways:  client request, modification of an existing design, generation of new product.   == underlying principles of needs analysis == the following list gives the principles of needs analysis as originally defined.  user's need-based requirements are complex and can conflict. user's need-based requirements build a bridge from the business case to the design. user's need-based requirements help to identify trade-offs that need to happen in the design process (i.e., where a design cannot resolve the user's need-based requirement conflicts). user's need-based requirements are there to unify the multi-disciplinary design team; enabling them to meet their business case. formulate and ask questions to do with the business plan that provide an indication of the human aspects of the system, including the relative merit of functionality. always express these findings from the user's perspective. cross-relate these requirements to each other and to the impactors on the activity. allocate sufficient time during the development process to check and validate your user's need-based requirements. ensure that all user's need-based requirements are derived as low-level user requirements before being transposed into system requirements. word your requirements precisely and ensure that you cover all categories of human-related requirements. create test statements to validate the user's need-based requirements, the concept and the implementation. prior to freezing your design, validate your user's need-based requirements with users. accept that there still may be contradictory requirements. understand the nuances of the requirements and ensure that these are reflected in the precise wording of the requirements. keep asking your users until you have a true understanding of their requirements. elegant design can only be created from understanding the nuances of the requirements, sharing ideas and fact toward feasibility of work.   == objectives of needs analysis == the objective of a user needs analysis is to define the audience, identify user goals, set usability objectives, identify design constraints and define functional specifications. several methods of research inform a user needs analysis, including task analysis and surveys. user needs analysis is important because the interface design will be based on this information. faulty assumptions or goals (about users) will lead to a faulty design that is difficult to fix once in place.   == see also == usability human factors task analysis human factors integration training needs analysis needs assessment   == references ==  harris, c., 2002, hyperinnovation: multidimensional enterprise in the connected economy, palgrave macmillan"")"
159,"Health Systems Engineering or Health Engineering (often known as ""Health Care Systems Engineering (HCSE)"") is an academic and a pragmatic discipline that approaches the health care industry, and other industries connected with health care delivery, as complex adaptive systems, and identifies and applies engineering design and analysis principles in such areas. This can overlap with biomedical engineering which focuses on design and development of various medical products; industrial engineering and operations management which involve improving organizational operations; and various health care practice fields like medicine, pharmacy, dentistry, nursing, etc.  Other fields participating in this interdisciplinary area include public health, information technology, management studies, and regulatory law.
People whose work implicates this field in some capacity can include members of all the above-noted fields, many of which have sub-fields targeted toward health care matters even if health or health care is not a principal focus of the overall field (e.g. management, law). Areas of biomedical engineering (BME) in this area often include clinical engineering (sometimes also called ""hospital engineering"") as well as those BMEs developing medical devices and pharmaceutical drugs.  The industrial engineering (IE) principles employed tend to include optimization, decision analysis, human factors engineering, quality engineering, and value engineering.


== See also ==
Biological engineering
Biomedical engineering
Clinical engineering
Industrial engineering
Systems engineering
Regulatory science
Operations management
Complex adaptive systems


== References ==","pandas(index=159, _1=159, text='health systems engineering or health engineering (often known as ""health care systems engineering (hcse)"") is an academic and a pragmatic discipline that approaches the health care industry, and other industries connected with health care delivery, as complex adaptive systems, and identifies and applies engineering design and analysis principles in such areas. this can overlap with biomedical engineering which focuses on design and development of various medical products; industrial engineering and operations management which involve improving organizational operations; and various health care practice fields like medicine, pharmacy, dentistry, nursing, etc.  other fields participating in this interdisciplinary area include public health, information technology, management studies, and regulatory law. people whose work implicates this field in some capacity can include members of all the above-noted fields, many of which have sub-fields targeted toward health care matters even if health or health care is not a principal focus of the overall field (e.g. management, law). areas of biomedical engineering (bme) in this area often include clinical engineering (sometimes also called ""hospital engineering"") as well as those bmes developing medical devices and pharmaceutical drugs.  the industrial engineering (ie) principles employed tend to include optimization, decision analysis, human factors engineering, quality engineering, and value engineering.   == see also == biological engineering biomedical engineering clinical engineering industrial engineering systems engineering regulatory science operations management complex adaptive systems   == references ==')"
160,"Richard (Dick) Muther (November 20, 1913 – October 15, 2014) was an American consulting engineer, faculty member at MIT, and author. He developed fundamental techniques used in plant layout, material handling, and other aspects of industrial engineering. He was also known as ""Mister Systematic"".


== Biography ==
Muther was born 1913 in Newton, Massachusetts to Lorenz Francis Muther and Josephine (Ashleman) Muther. After attending the University of Wisconsin, he obtained his B.S. and M.S. from the Massachusetts Institute of Technology.In his early career Muther worked for the Methods Engineering Council in Pittsburgh, the consulting firm of Harold Bright Maynard. In 1956 he founded his own consulting firm, Richard Muther and Associates, and worked as consulting engineer for organizations, such as Vendo in Kansas City, General Dynamics, Philips in the Netherlands, John Deere, and in the People's Republic of China for its Department of Energy.In World War II Muther served in the US Navy as expediter and facilities planning officer. In 1944 he published his first book, entitled Production Line Technique, on mass production methods, based on studies at over 75 industrial plants.Over the years Muther had taught at Massachusetts Institute of Technology, at the Naval Postgraduate School, and at Robert College in Turkey. He was visiting professor and instructor at the University of Missouri–Kansas City, and in Europe at the ETH Zurich in Switzerland, and at the Royal Institute of Technology in Sweden.Muther received an honorary doctorate ScD(hc) from Lund University in Sweden. He also was awarded the Gilbreth Medal (outstanding contributions to Industrial Engineering) in 1962; the Materials Handling Award (SAM); the Engineering Citation Award (SME); the Honor Award (IMMS); the Reed-Apple Award (MHIA); and the Donald Francis Award (for outstanding contributions to international material handling.)


== Contributions to engineering ==
Muther was the original developer of relationship chart (REL-CHART) and its companion space-relationship diagram. This tool is the basis for many other techniques which are used to optimize the proximity of related functions and minimize unnecessary transportation in industrial facilities.
Muther also created the Mag Count method of measuring the difficulty of handling (transporting) any solid material prior to knowing how it will be moved.   He developed the industry-standard color code used to classify industrial space and the related type-of-work symbols.  Corresponding black-and-white hatch patterns based on the heraldic tincture code are also part of his methodology. He is considered the ""Father of Systematic Planning"".


== Selected publications ==
Production Line Technique. New York: McGraw-Hill, 1944. ISBN 1114273023
Practical Plant Layout. New York: McGraw-Hill, 1955. ISBN 0-933684-03-7
Systematic Layout Planning (SLP). 1st edition. 1961; 2nd edition. Boston: Cahners Books, 1974, ISBN 9780843608144; Re-printed by Management & Industrial Research Publications, 1987, ISBN 978-0-933684-06-5.
Systematic Handling Analysis (SHA) (Kansas City, Missouri: Management and Industrial research Publications, 1969). ISBN 978-0-933684-03-4
Systematic Planning of Industrial Facilities (SPIF), 1979; 1980; 1999.
Planning By Design, 2010


== References ==


== External links ==
Richard Muther & Associates
RICHARD MUTHER '38, SM '41: Early Break Sparks Career in Production Management","pandas(index=160, _1=160, text='richard (dick) muther (november 20, 1913 – october 15, 2014) was an american consulting engineer, faculty member at mit, and author. he developed fundamental techniques used in plant layout, material handling, and other aspects of industrial engineering. he was also known as ""mister systematic"".   == biography == muther was born 1913 in newton, massachusetts to lorenz francis muther and josephine (ashleman) muther. after attending the university of wisconsin, he obtained his b.s. and m.s. from the massachusetts institute of technology.in his early career muther worked for the methods engineering council in pittsburgh, the consulting firm of harold bright maynard. in 1956 he founded his own consulting firm, richard muther and associates, and worked as consulting engineer for organizations, such as vendo in kansas city, general dynamics, philips in the netherlands, john deere, and in the people\'s republic of china for its department of energy.in world war ii muther served in the us navy as expediter and facilities planning officer. in 1944 he published his first book, entitled production line technique, on mass production methods, based on studies at over 75 industrial plants.over the years muther had taught at massachusetts institute of technology, at the naval postgraduate school, and at robert college in turkey. he was visiting professor and instructor at the university of missouri–kansas city, and in europe at the eth zurich in switzerland, and at the royal institute of technology in sweden.muther received an honorary doctorate scd(hc) from lund university in sweden. he also was awarded the gilbreth medal (outstanding contributions to industrial engineering) in 1962; the materials handling award (sam); the engineering citation award (sme); the honor award (imms); the reed-apple award (mhia); and the donald francis award (for outstanding contributions to international material handling.)   == contributions to engineering == muther was the original developer of relationship chart (rel-chart) and its companion space-relationship diagram. this tool is the basis for many other techniques which are used to optimize the proximity of related functions and minimize unnecessary transportation in industrial facilities. muther also created the mag count method of measuring the difficulty of handling (transporting) any solid material prior to knowing how it will be moved.   he developed the industry-standard color code used to classify industrial space and the related type-of-work symbols.  corresponding black-and-white hatch patterns based on the heraldic tincture code are also part of his methodology. he is considered the ""father of systematic planning"".   == selected publications == production line technique. new york: mcgraw-hill, 1944. isbn 1114273023 practical plant layout. new york: mcgraw-hill, 1955. isbn 0-933684-03-7 systematic layout planning (slp). 1st edition. 1961; 2nd edition. boston: cahners books, 1974, isbn 9780843608144; re-printed by management & industrial research publications, 1987, isbn 978-0-933684-06-5. systematic handling analysis (sha) (kansas city, missouri: management and industrial research publications, 1969). isbn 978-0-933684-03-4 systematic planning of industrial facilities (spif), 1979; 1980; 1999. planning by design, 2010   == references ==   == external links == richard muther & associates richard muther \'38, sm \'41: early break sparks career in production management')"
161,"Maintenance Engineering is the discipline and profession of applying engineering concepts for the optimization of equipment, procedures, and departmental budgets to achieve better maintainability, reliability, and availability of equipment.
Maintenance, and hence maintenance engineering, is increasing in importance due to rising amounts of equipment, systems, machineries and infrastructure.  Since the Industrial Revolution, devices, equipment, machinery and structures have grown increasingly complex, requiring a host of personnel, vocations and related systems needed to maintain them. Prior to 2006, the United States spent approximately US$300 billion annually on plant maintenance and operations alone. Maintenance is to ensure a unit is fit for purpose, with maximum availability at minimum costs. A person practicing maintenance engineering is known as a maintenance engineer.


== Maintenance engineer's description ==
A maintenance engineer should possess significant knowledge of statistics, probability and logistics, and additionally in the fundamentals of the operation of the equipment and machinery he or she is responsible for. A maintenance engineer should also possess high interpersonal, communication, and management skills, as well as the ability to make decisions quickly.
Typical responsibilities include:
Assure optimization of the Maintenance Organization structure
Analysis of repetitive equipment failures
Estimation of maintenance costs and evaluation of alternatives
Forecasting of spare parts
Assessing the needs for equipment replacements and establish replacement programs when due
Application of scheduling and project management principles to replacement programs
Assessing required maintenance tools and skills required for efficient maintenance of equipment
Assessing required skills for maintenance personnel
Reviewing personnel transfers to and from maintenance organizations
Assessing and reporting safety hazards associated with maintenance of equipment


== Maintenance engineering education ==
Institutions across the world have recognised the need for maintenance engineering. Maintenance engineers usually hold a degree in Mechanical Engineering, Industrial Engineering, or other engineering disciplines. In recent years specialised bachelor and master courses have developed. The Bachelor Degree program in Maintenance Engineering at the German-Jordanian University in Amman is addressing the need, as well as the Master Programme in Maintenance Engineering at Luleå University of Technology. With an increased demand for Chartered Engineers, The University of Central Lancashire in United Kingdom has developed a MSc in Maintenance Engineering currently under accreditation with the Institution of Engineering and Technology and a Top-up Bachelor of Engineering with honour degree for technicians holding a Higher National Diploma and seeking a progression in their professional career.


== See also ==
Aircraft Maintenance Engineering
Asset management
Auto mechanic
Civil engineer
Computerized Maintenance Management System
Computer repair technician
Electrician
Electrical Technologist
Industrial Engineering
Marine fuel management
Mechanic
Millwright (machinery maintenance)
Maintenance, repair and operations (MRO)
Reliability centered maintenance (RCM)
Reliability engineering
Preventive maintenance
Product lifecycle management
Stationary engineer
Total productive maintenance (TPM)
Six Sigma for maintenanceAssociationsINFORMS
Institute of Industrial Engineers


== References ==

https://web.archive.org/web/20110710024804/http://www.gju.edu.jo/page.aspx?id=36&type=s&lng=en&page=159","pandas(index=161, _1=161, text=""maintenance engineering is the discipline and profession of applying engineering concepts for the optimization of equipment, procedures, and departmental budgets to achieve better maintainability, reliability, and availability of equipment. maintenance, and hence maintenance engineering, is increasing in importance due to rising amounts of equipment, systems, machineries and infrastructure.  since the industrial revolution, devices, equipment, machinery and structures have grown increasingly complex, requiring a host of personnel, vocations and related systems needed to maintain them. prior to 2006, the united states spent approximately us$300 billion annually on plant maintenance and operations alone. maintenance is to ensure a unit is fit for purpose, with maximum availability at minimum costs. a person practicing maintenance engineering is known as a maintenance engineer.   == maintenance engineer's description == a maintenance engineer should possess significant knowledge of statistics, probability and logistics, and additionally in the fundamentals of the operation of the equipment and machinery he or she is responsible for. a maintenance engineer should also possess high interpersonal, communication, and management skills, as well as the ability to make decisions quickly. typical responsibilities include: assure optimization of the maintenance organization structure analysis of repetitive equipment failures estimation of maintenance costs and evaluation of alternatives forecasting of spare parts assessing the needs for equipment replacements and establish replacement programs when due application of scheduling and project management principles to replacement programs assessing required maintenance tools and skills required for efficient maintenance of equipment assessing required skills for maintenance personnel reviewing personnel transfers to and from maintenance organizations assessing and reporting safety hazards associated with maintenance of equipment   == maintenance engineering education == institutions across the world have recognised the need for maintenance engineering. maintenance engineers usually hold a degree in mechanical engineering, industrial engineering, or other engineering disciplines. in recent years specialised bachelor and master courses have developed. the bachelor degree program in maintenance engineering at the german-jordanian university in amman is addressing the need, as well as the master programme in maintenance engineering at luleå university of technology. with an increased demand for chartered engineers, the university of central lancashire in united kingdom has developed a msc in maintenance engineering currently under accreditation with the institution of engineering and technology and a top-up bachelor of engineering with honour degree for technicians holding a higher national diploma and seeking a progression in their professional career.   == see also == aircraft maintenance engineering asset management auto mechanic civil engineer computerized maintenance management system computer repair technician electrician electrical technologist industrial engineering marine fuel management mechanic millwright (machinery maintenance) maintenance, repair and operations (mro) reliability centered maintenance (rcm) reliability engineering preventive maintenance product lifecycle management stationary engineer total productive maintenance (tpm) six sigma for maintenanceassociationsinforms institute of industrial engineers   == references ==  https://web.archive.org/web/20110710024804/http://www.gju.edu.jo/page.aspx?id=36&type=s&lng=en&page=159"")"
162,"Work measurement is the application of techniques which is designed to establish the time for an average worker to carry out a specified manufacturing task at a defined level of performance. It is concerned with the duration of time it takes to complete a work task assigned to a specific job. It means the time taken to complete one unit of work or operation it also that the work should completely complete  in a complete basis under certain circumstances which take into account of accountants time


== Usage ==
Work measurement helps to uncover non-standardization that exist in the workplace and non-value adding activities and waste. A work has to be measured for the following reasons:

To discover and eliminate lost or ineffective time.
To establish standard times for performance measurement.
To measure performance against realistic expectations.
To set operating goals and objectives.


== Techniques ==
Time study
Predetermined motion time systems
Synthesis from elemental data
Work sampling+Analytical estimating


== Purpose ==
Work Measurement is a technique for establishing a Standard Time, which is the required time to perform a given task, based on time measurements of the work content of the prescribed method, with due consideration for fatigue and for personal and unavoidable delays. 
Method study is the principal technique for reducing the work involved, primarily by eliminating unnecessary movement on the part of material or operatives and by substituting good methods for poor ones. Work measurement is concerned with investigating, reducing and subsequently eliminating ineffective time, that is time during which no effective work is being performed, whatever the cause.
Work measurement, as the name suggests, provides management with a means of measuring the time taken in the performance of an operation or series of operations in such a way that ineffective time is shown up and can be separated from effective time. In this way its existence, nature and extent become known where previously they were concealed within the total. To see how much work has been done by the worker and how much salaries is given to him


== Uses ==
Revealing existing causes of ineffective time through study, important though it is, is perhaps less important in the long term than the setting of sound time standards, since these will continue to apply as long as the work to which they refer continues to be done. They will also show up any ineffective time or additional work which may occur once they have been established.
In the process of setting standards it may be necessary to use work measurement:
To compare the efficiency of alternative methods. Other conditions being equal, the method which takes the least time will be the best method.
To balance the work of members of teams, in association with multiple activity charts, so that, as nearly as possible, each member has a task taking an equal time to perform.
To determine, in association with man and machine multiple activity charts, the number of machines an operative can run.
The time standards, once set, may then be used:
To provide information on which the planning and scheduling of production can be based, including the plant and labour requirements for carrying out the programme of work and the utilisation of available capacity.
To provide information on which estimates for tenders, selling prices and delivery promises can be based.
To set standards of machine utilisation and labour performance which can be used for any of the above purposes and as a basis for incentive schemes.
To provide information for labour-cost control and to enable standard costs to be fixed and maintained.
It is thus clear that work measurement provides the basic information necessary for all the activities of organising and controlling the work of an enterprise in which the time element plays a part. Its uses in connection with these activities will be more clearly seen when we have shown how the standard time is obtained.


== Techniques of work measurement ==
The following are the principal techniques by which work measurement is carried out:
1. Time study
2. Activity sampling
3. Predetermined motion time systems
4. Synthesis from standard data
5. Estimating
6. Analytical estimating
7. Comparative estimating
Of these techniques we shall concern ourselves primarily with time study, since it is the basic technique of work measurement. Some of the other techniques either derive from it or are variants of it.
1. Time study
Time Study consists of recording times and rates of work for elements of a specified job carried out under specified conditions to obtain the time necessary to carry out a job at a defined level of performance.
In this technique the job to be studied is timed with a stopwatch, rated, and the Basic Time calculated.
1.1 Requirements for effective time study
The requirements for effective time study are:
a. Co-operation and goodwill
b. Defined job
c. Defined method
d. Correct normal equipment
e. Quality standard and checks
f. Experienced qualified motivated worker
g. Method of timing
h. Method of assessing relative performance
i. Elemental breakdown
j. Definition of break points
k. Recording media
One of the most critical requirements for time study is that of elemental breakdown. There are some general rules concerning the way in which a job should be broken down into elements. They include the following. Elements should be easily identifiable, with definite beginnings and endings so that, once established, they can be repeatedly recognised. These points are known as the break points and should be clearly described on the study sheet. Elements should be as short as can be conveniently timed by the observer. As far as possible, elements – particularly manual ones – should be chosen so that they represent naturally unified and distinct segments of the operation.
1.2  Performance rating
Time Study is based on a record of observed times for doing a job together with an assessment by the observer of the speed and effectiveness of the worker in relation to the observer's concept of Standard Rating.
This assessment is known as rating, the definition being given in BS 3138 (1979):
The numerical value or symbol used to denote a rate of working.
Standard rating is also defined (in this British Standard BS3138) as:
""The rating corresponding to the average rate at which qualified workers will naturally work, provided that they adhere to the specified method and that they are motivated to apply themselves to their work. If the standard rating is consistently maintained and the appropriate relaxation is taken, a qualified worker will achieve standard performance over the working day or shift.""
Industrial engineers use a variety of rating scales, and one which has achieved wide use is the British Standards Rating Scale which is a scale where 0 corresponds to no activity and 100 corresponds to standard rating. Rating should be expressed as 'X' BS.
Below is an illustration of the Standard Scale:
Rating walking pace
0 no activity
50 very slow
75 steady
100 brisk (standard rating)
125 very fast
150 exceptionally fast
The basic time for a task, or element, is the time for carrying out an element of work or an operation at standard rating.
Basic time = observed time x observed rating
The result is expressed in basic minutes – BMs.
The work content of a job or operation is defined as: basic time + relaxation allowance + any allowance for additional work – e.g. that part of contingency allowance which represents work.
1.3  Standard time
Standard time is the total time in which a job should be completed at standard performance i.e. work content, contingency allowance for delay, unoccupied time and interference allowance, where applicable.
Allowance for unoccupied time and for interference may be important for the measurement of machine-controlled operations, but they do not always appear in every computation of standard time. Relaxation allowance, on the other hand, has to be taken into account in every computation, whether the job is a simple manual one or a very complex operation requiring the simultaneous control of several machines. A contingency allowance will probably figure quite frequently in the compilation of standard times; it is therefore convenient to consider the contingency allowance and relaxation allowance, so that the sequence of calculation which started with the completion of observations at the workplace may be taken right through to the compilation of standard time.
Contingency allowance
A contingency allowance is a small allowance of time which may be included in a standard time to meet legitimate and expected items of work or delays, the precise measurement of which is uneconomical because of their infrequent or irregular occurrence.
Relaxation allowance
A relaxation allowance is an addition to the basic time to provide the worker with the opportunity to recover from physiological and psychological effects of carrying out specified work under specified conditions and to allow attention to personal needs. The amount of the allowance will depend on the nature of the job. Examples are:
Personal 5–7%
Energy output 0–10%
Noisy 0–5%
Conditions 0–100%
e.g. Electronics 5%
Other allowances
Other allowances include process allowance which is to cover when an operator is prevented from continuing with their work, although ready and waiting, by the process or machine requiring further time to complete its part of the job. A final allowance is that of Interference which is included whenever an operator has charge of more than one machine and the machines are subject to random stoppage. In normal circumstances the operator can only attend to one machine, and the others must wait for attention. This machine is then subject to interference which increased the machine cycle time.
It is now possible to obtain a complete picture of the standard time for a straightforward manual operation.
2. Activity Sampling
Activity sampling is a technique in which a large number of instantaneous observations are made over a period of time of a group of machines, processes or workers. Each observation records what is happening at that instant and the percentage of observations recorded for a particular activity or delay is a measure of the percentage of time during which the activity or delay occurs.
The advantages of this method are that
It is capable of measuring many activities that are impractical or too costly to be measured by time study.
One observer can collect data concerning the simultaneous activities of a group.
Activity sampling can be interrupted at any time without effect.
The disadvantages are that
It is quicker and cheaper to use time study on jobs of short duration.
It does not provide elemental detail.
The type of information provided by an activity sampling study is:

    a.  The proportion of the working day during which workers or machines are producing.
    b.  The proportion of the working day used up by delays. The reason for each delay must be recorded.
    c.  The relative activity of different workers and machines.

To determine the number of observations in a full study the following equation is used:
Where:
3. Predetermined motion time system
A predetermined motion time system is a work measurement technique whereby times established for basic human motions (classified according to the nature of the motion and the conditions under which it is made) are used to build up the time for a job at a defined level of performance.
The systems are based on the assumption that all manual tasks can be analysed into basic motions of the body or body members. They were compiled as a result of a very large number of studies of each movement, generally by a frame-by-frame analysis of films of a wide range of subjects, men and women, performing a wide variety of tasks.
The first generation of PMT systems, MTM1, were very finely detailed, involving much analysis and producing extremely accurate results. This attention to detail was both a strength and a weakness, and for many potential applications the quantity of detailed analysis was not necessary, and prohibitively time -consuming. In these cases ""second generation"" techniques, such as Simplified PMTS, Master Standard Data, Primary Standard Data and MTM2, could be used with advantage, and no great loss of accuracy. For even speedier application, where some detail could be sacrificed then a ""third generation"" technique such as Basic Work Data or MTM3 could be used.
4. Synthesis
Synthesis is a work measurement technique for building up the time for a job at a defined level of performance by totaling element times obtained previously from time studies on other jobs containing the elements concerned, or from synthetic data.
Synthetic data is the name given to tables and formulae derived from the analysis of accumulated work measurement data, arranged in a form suitable for building up standard times, machine process times, etc. by synthesis.
Synthetic times are increasingly being used as a substitute for individual time studies in the case of jobs made up of elements which have recurred a sufficient number of times in jobs previously studied to make it possible to compile accurate representative times for them.
5. Estimating
The technique of estimating is the least refined of all those available to the work measurement practitioner. It consists of an estimate of total job duration (or in common practice, the job price or cost). This estimate is made by a craftsman or person familiar with the craft. It normally embraces the total components of the job, including work content, preparation and disposal time, any contingencies etc., all estimated in one gross amount.
6. Analytical estimating
This technique introduces work measurement concepts into estimating. In analytical estimating the estimator is trained in elemental breakdown, and in the concept of standard performance. The estimate is prepared by first breaking the work content of the job into elements, and then utilising the experience of the estimator (normally a craftsman) the time for each element of work is estimated – at standard performance. These estimated basic minutes are totalled to give a total job time, in basic minutes. An allowance for relaxation and any necessary contingency is then made, as in conventional time study, to give the standard time.
7. Comparative estimating
This technique has been developed to permit speedy and reliable assessment of the duration of variable and infrequent jobs, by estimating them within chosen time bands. Limits are set within which the job under consideration will fall, rather than in terms of precise capital standard or capital allowed minute values. It is applied by comparing the job to be estimated with jobs of similar work content, and using these similar jobs as ""bench marks"" to locate the new job in its relevant time band – known as Work Group.


== Uses ==
To compare the efficiency of alternative methods. Other conditions being equal, the method which takes the least time will be the best method.
To balance the work of members of teams, in association with the multiple activity charts, so that, as far as possible, each member has tasks taking an equal time.
To determine, in association with man and machine multiple activity charts, the number of machines a worker can run.


== Balayla model – work measurement in the service sector ==
The work measurement concept has evolved from the manufacturing world but has not been fully adopted yet to the global shift to the service sector. Certain factors create inherent difficulties in determining standard times for labor allocation in service jobs: (a) wide variation in Time Between Arrivals and Service Performance Time; (b) the difficulty of assessing the damage done to the organization by long customer Waiting Times for service. This difficulty makes it hard to calculate the Break-Even Point between raising worker output, which minimizes labor costs but increases customer Waiting Times and reduces service quality. 
Dr. Isaac Balayla & Professor Yissachar Gilad from the Technion, Israel, developed the Balayla (Balaila) Model which overcomes most of the above-mentioned difficulties, by taking a multi-domain approach: 1) The model deploys a series of indicators for a correlation between output and Waiting Times. The indicator values are affected by service level of urgency. 2) the model determines the best Break-Even point by comparing the operational cost of an additional worker with the economical benefit caused by the decrease in WT. Thus, the model finds the best balance between worker output and service quality.


== References ==

  2. Balayla I.(2012) A manpower allocation model for service jobs (Balayla Model) , IJSMET International 
     Journal of Service Science, Management, Engineering, and Technology, 3(2), April-June 2012 , pp 13-34.","pandas(index=162, _1=162, text='work measurement is the application of techniques which is designed to establish the time for an average worker to carry out a specified manufacturing task at a defined level of performance. it is concerned with the duration of time it takes to complete a work task assigned to a specific job. it means the time taken to complete one unit of work or operation it also that the work should completely complete  in a complete basis under certain circumstances which take into account of accountants time   == usage == work measurement helps to uncover non-standardization that exist in the workplace and non-value adding activities and waste. a work has to be measured for the following reasons:  to discover and eliminate lost or ineffective time. to establish standard times for performance measurement. to measure performance against realistic expectations. to set operating goals and objectives.   == techniques == time study predetermined motion time systems synthesis from elemental data work samplinganalytical estimating   == purpose == work measurement is a technique for establishing a standard time, which is the required time to perform a given task, based on time measurements of the work content of the prescribed method, with due consideration for fatigue and for personal and unavoidable delays. method study is the principal technique for reducing the work involved, primarily by eliminating unnecessary movement on the part of material or operatives and by substituting good methods for poor ones. work measurement is concerned with investigating, reducing and subsequently eliminating ineffective time, that is time during which no effective work is being performed, whatever the cause. work measurement, as the name suggests, provides management with a means of measuring the time taken in the performance of an operation or series of operations in such a way that ineffective time is shown up and can be separated from effective time. in this way its existence, nature and extent become known where previously they were concealed within the total. to see how much work has been done by the worker and how much salaries is given to him   == uses == revealing existing causes of ineffective time through study, important though it is, is perhaps less important in the long term than the setting of sound time standards, since these will continue to apply as long as the work to which they refer continues to be done. they will also show up any ineffective time or additional work which may occur once they have been established. in the process of setting standards it may be necessary to use work measurement: to compare the efficiency of alternative methods. other conditions being equal, the method which takes the least time will be the best method. to balance the work of members of teams, in association with multiple activity charts, so that, as nearly as possible, each member has a task taking an equal time to perform. to determine, in association with man and machine multiple activity charts, the number of machines an operative can run. the time standards, once set, may then be used: to provide information on which the planning and scheduling of production can be based, including the plant and labour requirements for carrying out the programme of work and the utilisation of available capacity. to provide information on which estimates for tenders, selling prices and delivery promises can be based. to set standards of machine utilisation and labour performance which can be used for any of the above purposes and as a basis for incentive schemes. to provide information for labour-cost control and to enable standard costs to be fixed and maintained. it is thus clear that work measurement provides the basic information necessary for all the activities of organising and controlling the work of an enterprise in which the time element plays a part. its uses in connection with these activities will be more clearly seen when we have shown how the standard time is obtained.   == techniques of work measurement == the following are the principal techniques by which work measurement is carried out: 1. time study 2. activity sampling 3. predetermined motion time systems 4. synthesis from standard data 5. estimating 6. analytical estimating 7. comparative estimating of these techniques we shall concern ourselves primarily with time study, since it is the basic technique of work measurement. some of the other techniques either derive from it or are variants of it. 1. time study time study consists of recording times and rates of work for elements of a specified job carried out under specified conditions to obtain the time necessary to carry out a job at a defined level of performance. in this technique the job to be studied is timed with a stopwatch, rated, and the basic time calculated. 1.1 requirements for effective time study the requirements for effective time study are: a. co-operation and goodwill b. defined job c. defined method d. correct normal equipment e. quality standard and checks f. experienced qualified motivated worker g. method of timing h. method of assessing relative performance i. elemental breakdown j. definition of break points k. recording media one of the most critical requirements for time study is that of elemental breakdown. there are some general rules concerning the way in which a job should be broken down into elements. they include the following. elements should be easily identifiable, with definite beginnings and endings so that, once established, they can be repeatedly recognised. these points are known as the break points and should be clearly described on the study sheet. elements should be as short as can be conveniently timed by the observer. as far as possible, elements – particularly manual ones – should be chosen so that they represent naturally unified and distinct segments of the operation. 1.2  performance rating time study is based on a record of observed times for doing a job together with an assessment by the observer of the speed and effectiveness of the worker in relation to the observer\'s concept of standard rating. this assessment is known as rating, the definition being given in bs 3138 (1979): the numerical value or symbol used to denote a rate of working. standard rating is also defined (in this british standard bs3138) as: ""the rating corresponding to the average rate at which qualified workers will naturally work, provided that they adhere to the specified method and that they are motivated to apply themselves to their work. if the standard rating is consistently maintained and the appropriate relaxation is taken, a qualified worker will achieve standard performance over the working day or shift."" industrial engineers use a variety of rating scales, and one which has achieved wide use is the british standards rating scale which is a scale where 0 corresponds to no activity and 100 corresponds to standard rating. rating should be expressed as \'x\' bs. below is an illustration of the standard scale: rating walking pace 0 no activity 50 very slow 75 steady 100 brisk (standard rating) 125 very fast 150 exceptionally fast the basic time for a task, or element, is the time for carrying out an element of work or an operation at standard rating. basic time = observed time x observed rating the result is expressed in basic minutes – bms. the work content of a job or operation is defined as: basic timerelaxation allowanceany allowance for additional work – e.g. that part of contingency allowance which represents work. 1.3  standard time standard time is the total time in which a job should be completed at standard performance i.e. work content, contingency allowance for delay, unoccupied time and interference allowance, where applicable. allowance for unoccupied time and for interference may be important for the measurement of machine-controlled operations, but they do not always appear in every computation of standard time. relaxation allowance, on the other hand, has to be taken into account in every computation, whether the job is a simple manual one or a very complex operation requiring the simultaneous control of several machines. a contingency allowance will probably figure quite frequently in the compilation of standard times; it is therefore convenient to consider the contingency allowance and relaxation allowance, so that the sequence of calculation which started with the completion of observations at the workplace may be taken right through to the compilation of standard time. contingency allowance a contingency allowance is a small allowance of time which may be included in a standard time to meet legitimate and expected items of work or delays, the precise measurement of which is uneconomical because of their infrequent or irregular occurrence. relaxation allowance a relaxation allowance is an addition to the basic time to provide the worker with the opportunity to recover from physiological and psychological effects of carrying out specified work under specified conditions and to allow attention to personal needs. the amount of the allowance will depend on the nature of the job. examples are: personal 5–7% energy output 0–10% noisy 0–5% conditions 0–100% e.g. electronics 5% other allowances other allowances include process allowance which is to cover when an operator is prevented from continuing with their work, although ready and waiting, by the process or machine requiring further time to complete its part of the job. a final allowance is that of interference which is included whenever an operator has charge of more than one machine and the machines are subject to random stoppage. in normal circumstances the operator can only attend to one machine, and the others must wait for attention. this machine is then subject to interference which increased the machine cycle time. it is now possible to obtain a complete picture of the standard time for a straightforward manual operation. 2. activity sampling activity sampling is a technique in which a large number of instantaneous observations are made over a period of time of a group of machines, processes or workers. each observation records what is happening at that instant and the percentage of observations recorded for a particular activity or delay is a measure of the percentage of time during which the activity or delay occurs. the advantages of this method are that it is capable of measuring many activities that are impractical or too costly to be measured by time study. one observer can collect data concerning the simultaneous activities of a group. activity sampling can be interrupted at any time without effect. the disadvantages are that it is quicker and cheaper to use time study on jobs of short duration. it does not provide elemental detail. the type of information provided by an activity sampling study is:  a.  the proportion of the working day during which workers or machines are producing. b.  the proportion of the working day used up by delays. the reason for each delay must be recorded. c.  the relative activity of different workers and machines.  to determine the number of observations in a full study the following equation is used: where: 3. predetermined motion time system a predetermined motion time system is a work measurement technique whereby times established for basic human motions (classified according to the nature of the motion and the conditions under which it is made) are used to build up the time for a job at a defined level of performance. the systems are based on the assumption that all manual tasks can be analysed into basic motions of the body or body members. they were compiled as a result of a very large number of studies of each movement, generally by a frame-by-frame analysis of films of a wide range of subjects, men and women, performing a wide variety of tasks. the first generation of pmt systems, mtm1, were very finely detailed, involving much analysis and producing extremely accurate results. this attention to detail was both a strength and a weakness, and for many potential applications the quantity of detailed analysis was not necessary, and prohibitively time -consuming. in these cases ""second generation"" techniques, such as simplified pmts, master standard data, primary standard data and mtm2, could be used with advantage, and no great loss of accuracy. for even speedier application, where some detail could be sacrificed then a ""third generation"" technique such as basic work data or mtm3 could be used. 4. synthesis synthesis is a work measurement technique for building up the time for a job at a defined level of performance by totaling element times obtained previously from time studies on other jobs containing the elements concerned, or from synthetic data. synthetic data is the name given to tables and formulae derived from the analysis of accumulated work measurement data, arranged in a form suitable for building up standard times, machine process times, etc. by synthesis. synthetic times are increasingly being used as a substitute for individual time studies in the case of jobs made up of elements which have recurred a sufficient number of times in jobs previously studied to make it possible to compile accurate representative times for them. 5. estimating the technique of estimating is the least refined of all those available to the work measurement practitioner. it consists of an estimate of total job duration (or in common practice, the job price or cost). this estimate is made by a craftsman or person familiar with the craft. it normally embraces the total components of the job, including work content, preparation and disposal time, any contingencies etc., all estimated in one gross amount. 6. analytical estimating this technique introduces work measurement concepts into estimating. in analytical estimating the estimator is trained in elemental breakdown, and in the concept of standard performance. the estimate is prepared by first breaking the work content of the job into elements, and then utilising the experience of the estimator (normally a craftsman) the time for each element of work is estimated – at standard performance. these estimated basic minutes are totalled to give a total job time, in basic minutes. an allowance for relaxation and any necessary contingency is then made, as in conventional time study, to give the standard time. 7. comparative estimating this technique has been developed to permit speedy and reliable assessment of the duration of variable and infrequent jobs, by estimating them within chosen time bands. limits are set within which the job under consideration will fall, rather than in terms of precise capital standard or capital allowed minute values. it is applied by comparing the job to be estimated with jobs of similar work content, and using these similar jobs as ""bench marks"" to locate the new job in its relevant time band – known as work group.   == uses == to compare the efficiency of alternative methods. other conditions being equal, the method which takes the least time will be the best method. to balance the work of members of teams, in association with the multiple activity charts, so that, as far as possible, each member has tasks taking an equal time. to determine, in association with man and machine multiple activity charts, the number of machines a worker can run.   == balayla model – work measurement in the service sector == the work measurement concept has evolved from the manufacturing world but has not been fully adopted yet to the global shift to the service sector. certain factors create inherent difficulties in determining standard times for labor allocation in service jobs: (a) wide variation in time between arrivals and service performance time; (b) the difficulty of assessing the damage done to the organization by long customer waiting times for service. this difficulty makes it hard to calculate the break-even point between raising worker output, which minimizes labor costs but increases customer waiting times and reduces service quality. dr. isaac balayla & professor yissachar gilad from the technion, israel, developed the balayla (balaila) model which overcomes most of the above-mentioned difficulties, by taking a multi-domain approach: 1) the model deploys a series of indicators for a correlation between output and waiting times. the indicator values are affected by service level of urgency. 2) the model determines the best break-even point by comparing the operational cost of an additional worker with the economical benefit caused by the decrease in wt. thus, the model finds the best balance between worker output and service quality.   == references ==  2. balayla i.(2012) a manpower allocation model for service jobs (balayla model) , ijsmet international journal of service science, management, engineering, and technology, 3(2), april-june 2012 , pp 13-34.')"
163,"Follow the Sun (FTS), a sub-field of globally distributed software engineering (GDSE), is a type of global knowledge workflow designed in order to reduce the time to market, in which the knowledge product is owned and advanced by a production site in one time zone and handed off at the end of their work day to the next production site that is several time zones west to continue that work. Ideally, the work days in these time zones overlap such that when one site ends their day, the next one starts.
FTS has the potential to significantly increase the total development time per day (as viewed from the perspective of a single time zone): with two sites the development time can increase to up to 16 hours, or up to 24 hours if there are three sites, reducing the development duration by as much as 67%.
It is not commonly practiced in industry and has few documented cases where it is applied successfully. This is likely because of its uncommon requirements, leading to a lack of knowledge on how to successfully apply FTS in practice.


== History ==
Follow the Sun can be traced back to the mid-1990s where IBM had the first global software team which was specifically set up to take advantages of FTS. The team was spread out across five sites around the globe. Unfortunately, in this case FTS was unsuccessful because it was uncommon to hand off the software artifacts daily.
Two other cases of FTS at IBM have been documented by Treinen and Miller-Frost. The first team was spread out across a site in the United States and a site in Australia. FTS was successful for this team. The second team was spread out across a site in the United States and a site in India. In this case FTS was unsuccessful because of miscommunication, time zone issues and cultural differences.


== Principles ==
FTS is based on the following four principles:

The main objective is the reduction of development duration / time to market.
Production sites are many time zones apart.
There is always one and only one site that owns and works on the project.
Handoffs are conducted daily at the end of each shift. The next production site is several time zones west.


=== Common misconceptions ===
An important step in defining FTS is to disambiguate it from other globally distributed configurations to clearly state what FTS is not. The following four types of similar globally distributed configurations are not FTS:
Global knowledge work is defined as geographically dispersed knowledge workers working collaboratively from multiple locations. This is not FTS because there are no handoffs.
24/7 service. In this configuration work is distributed to workers who are available at that time. It is focused on availability and the workers have little dependency, whereas FTS is focused on duration reductions and requires dependencies between the different sites in order to perform the daily handoffs.
24-hour manufacturing.  This configuration focuses on making shifts fully optimize expensive resources that could not produce more by increasing the number of employees per shift. However, this driver of reducing the resource cost is not the driver of FTS.
Collocated multi shifts. In contrast to FTS this configuration chooses one location where labor is cheap and runs multiple eight-hour shifts concurrently.


== Difficulties ==
FTS's largest strength, spreading the development over multiple time zones, is simultaneously its largest weakness. Its distributed workflow is more complex to implement due to cultural and technical differences as well as the differences in time making coordination and communication challenging.
The main reason why FTS is difficult to implement is because the handoffs are an essential element that is hard to get right. The largest factor causing this difficulty is poor communication.There are few documented cases of companies successfully applying FTS. Some companies have claimed to successfully implement FTS but these companies did not practice the daily handoffs. However, a limited amount of successful applications of FTS that did include daily handoffs of artefacts, using a distributed-concurrent model, were found by Cameron.Recent studies on FTS have moved to mathematical modeling of FTS. The research is focused on the issue of speed and the issues around the handoffs.


== Methods ==
As FTS is a sub-field of GDSE, the same agile software development methodologies that are found to work well in GDSE work well with FTS. In particular, Carmel et al. (2009) argue that agile software development methodologies assist the FTS principles because they:
support daily handoffs. The continuous integration and automated integration of source code allows each site to work in their own code bases during their work day, while the integration maintains updated, testable code to be used by the next site.
deal with communication. Agile methodologies emphasize communication. They specifically emphasize face-to-face communication, which can be done within one site. Since FTS aims to reduce inter-site communication, the face-to-face aspect is not a large hindrance to the overall application of agile development methodologies.
elicit cooperation and collaboration. As FTS requires more collaboration and cooperation, this emphasis is especially useful.


=== Challenges ===
Kroll et al. (2013) have researched papers published between 1990 and 2012 and found 36 best practices and 17 challenges for FTS. The challenges were grouped in three categories: coordination, communication and culture. These challenges should be overcome to implement FTS successfully.


==== Coordination ====
Time zone differences reduce opportunities for real-time collaboration. Team members have to be flexible to achieve overlap with remote colleagues. The limited overlap and the delay in responses have a negative impact on the coordination.
Daily handoff cycles or handing off work-in-progress are a requirement of FTS because without it the time to market cannot be decreased.
Geographical dispersion
Cost estimation
Loss of teamness
Number of sites
Coordination breakdown
Managerial difficulties
Technical platforms


==== Communication ====
Loss of communication richness / face-to-face communication
Social cultural diversity difficulties
Synchronous communication
Language difference
Technical difficulties
Manage religious or national holidays.


==== Culture ====
Cultural differences
Different technical backgrounds


=== Best practices ===
It is of great importance to select and adapt a methodology for the daily handoffs e.g. using agile software development or the waterfall model. 
Identified best practices are the use of agile methods and using technologies to develop FTS activities. Agile supports daily handoffs which is a critical challenge in FTS. Management tools can be used to estimate and plan schedules, manage sprints and track progress. Additionally, technologies like conference video, emails and telephone calls are easy to implement and allow companies to perform synchronous and asynchronous communication between teams and works well in an agile environment.
Unfortunately, there is no solid best practice that works best since FTS can be applied in numerous ways.


== Follow the Moon ==
A related concept is follow-the-moon, which is scheduling work to be performed specifically during local night-time hours for reasons such as saving on datacenter costs by using cheaper night-time electricity or spare processing power.


== Other terms ==
24-hour development
round-the-clock-development


== See also ==
Time to market
Change-of-shift report


== Notes and references ==


== External links ==
Example of use in industry - IT Support","pandas(index=163, _1=163, text=""follow the sun (fts), a sub-field of globally distributed software engineering (gdse), is a type of global knowledge workflow designed in order to reduce the time to market, in which the knowledge product is owned and advanced by a production site in one time zone and handed off at the end of their work day to the next production site that is several time zones west to continue that work. ideally, the work days in these time zones overlap such that when one site ends their day, the next one starts. fts has the potential to significantly increase the total development time per day (as viewed from the perspective of a single time zone): with two sites the development time can increase to up to 16 hours, or up to 24 hours if there are three sites, reducing the development duration by as much as 67%. it is not commonly practiced in industry and has few documented cases where it is applied successfully. this is likely because of its uncommon requirements, leading to a lack of knowledge on how to successfully apply fts in practice.   == history == follow the sun can be traced back to the mid-1990s where ibm had the first global software team which was specifically set up to take advantages of fts. the team was spread out across five sites around the globe. unfortunately, in this case fts was unsuccessful because it was uncommon to hand off the software artifacts daily. two other cases of fts at ibm have been documented by treinen and miller-frost. the first team was spread out across a site in the united states and a site in australia. fts was successful for this team. the second team was spread out across a site in the united states and a site in india. in this case fts was unsuccessful because of miscommunication, time zone issues and cultural differences.   == principles == fts is based on the following four principles:  the main objective is the reduction of development duration / time to market. production sites are many time zones apart. there is always one and only one site that owns and works on the project. handoffs are conducted daily at the end of each shift. the next production site is several time zones west. it is of great importance to select and adapt a methodology for the daily handoffs e.g. using agile software development or the waterfall model. identified best practices are the use of agile methods and using technologies to develop fts activities. agile supports daily handoffs which is a critical challenge in fts. management tools can be used to estimate and plan schedules, manage sprints and track progress. additionally, technologies like conference video, emails and telephone calls are easy to implement and allow companies to perform synchronous and asynchronous communication between teams and works well in an agile environment. unfortunately, there is no solid best practice that works best since fts can be applied in numerous ways.   == follow the moon == a related concept is follow-the-moon, which is scheduling work to be performed specifically during local night-time hours for reasons such as saving on datacenter costs by using cheaper night-time electricity or spare processing power.   == other terms == 24-hour development round-the-clock-development   == see also == time to market change-of-shift report   == notes and references ==   == external links == example of use in industry - it support"")"
164,"Source reduction is activities designed to reduce the volume, mass, or toxicity of products throughout the life cycle.  It includes the design and manufacture, use, and disposal of products with minimum toxic content, minimum volume of material, and/or a longer useful life. 
An example of source reduction is use of a Reusable shopping bag at the grocery store;  although it uses more material than a single-use disposable bag, the material per use is less. 


== Synonyms ==
Pollution Prevention (or P2) and Toxics use reduction are also called source reduction because they address the use of hazardous substances at the source.


== Procedures ==
Source Reduction is achieved through improvements in design, production, use, reuse, recycling, and through Environmentally Preferable Purchasing (EPP).  A Life-cycle assessment is useful to help choose among several alternatives and options.


== Source reduction in the United States ==
In the United States, the Federal Trade Commission offers guidance for labelling claims: ""Source reduction"" refers to reducing or lowering the weight, volume or toxicity of a product or package. To avoid being misleading, source reduction claims must qualify the amount of the source reduction and give the basis for any comparison that is made. These principles apply regardless of whether a term like ""source reduced"" is used.
The Massachusetts Toxics Use Reduction Program (TURA) offers 6 strategies to achieve source reduction:
Toxic chemical substitution
Production process modification
Finished product reformulation
Production modernization
Improvements in operations and maintenance
In-process recycling of production material


== References ==


== See also ==
Bioplastic
Miniwaste
Pre-waste
Sustainable packaging


== External links ==
United States National Pollution Prevention Information Center
United States  Pollution Prevention Regional Information Center
NPPR Finds P2 Programs Effective
TURI
P2Gems Pollution prevention directory
Southwest Network for Zero Waste","pandas(index=164, _1=164, text='source reduction is activities designed to reduce the volume, mass, or toxicity of products throughout the life cycle.  it includes the design and manufacture, use, and disposal of products with minimum toxic content, minimum volume of material, and/or a longer useful life. an example of source reduction is use of a reusable shopping bag at the grocery store;  although it uses more material than a single-use disposable bag, the material per use is less.   == synonyms == pollution prevention (or p2) and toxics use reduction are also called source reduction because they address the use of hazardous substances at the source.   == procedures == source reduction is achieved through improvements in design, production, use, reuse, recycling, and through environmentally preferable purchasing (epp).  a life-cycle assessment is useful to help choose among several alternatives and options.   == source reduction in the united states == in the united states, the federal trade commission offers guidance for labelling claims: ""source reduction"" refers to reducing or lowering the weight, volume or toxicity of a product or package. to avoid being misleading, source reduction claims must qualify the amount of the source reduction and give the basis for any comparison that is made. these principles apply regardless of whether a term like ""source reduced"" is used. the massachusetts toxics use reduction program (tura) offers 6 strategies to achieve source reduction: toxic chemical substitution production process modification finished product reformulation production modernization improvements in operations and maintenance in-process recycling of production material   == references ==   == see also == bioplastic miniwaste pre-waste sustainable packaging   == external links == united states national pollution prevention information center united states  pollution prevention regional information center nppr finds p2 programs effective turi p2gems pollution prevention directory southwest network for zero waste')"
165,"Operations and Technology Management (OTM) is an interdisciplinary major which prepares students to gain knowledge and skills in the areas of operations management, IT management, and data analytics. This major is typically offered as part of business school and the curriculum is designed to develop the skills needed to manage and improve business operations through the integrated use of theories and methods from both operations management and information technology management (IT). Because of its inter-disciplinary nature, students graduating with OTM degrees tend to have more career options across a wide-range of industries. For instances, students with OTM degrees can pursue many roles across Operations, IT, and Analytics fields.Many universities offer this major. For instance, the University of Portland offers a BBA in OTM and MS in OTM (MSOTM) programs. Harvard University offers MBA and DBA in Technology and operations management (TOM). The University of Wisconsin-Madison offers BBA and MBA programs in OTM. Cal Poly-Pomona offers programs in Technology and Operations Management (TOM). UCLA Anderson School of Management of Management offers Decisions, operations and technology management (DTOM) programs.


== References ==


== Further reading ==
book series: Operations and Technology Management, ed. by Prof. Dr. Thorsten Blecker; Prof. Dr. George Q. Huang; Prof. Dr. Fabrizio Salvador, Buchreihe Operations and Technology Management","pandas(index=165, _1=165, text='operations and technology management (otm) is an interdisciplinary major which prepares students to gain knowledge and skills in the areas of operations management, it management, and data analytics. this major is typically offered as part of business school and the curriculum is designed to develop the skills needed to manage and improve business operations through the integrated use of theories and methods from both operations management and information technology management (it). because of its inter-disciplinary nature, students graduating with otm degrees tend to have more career options across a wide-range of industries. for instances, students with otm degrees can pursue many roles across operations, it, and analytics fields.many universities offer this major. for instance, the university of portland offers a bba in otm and ms in otm (msotm) programs. harvard university offers mba and dba in technology and operations management (tom). the university of wisconsin-madison offers bba and mba programs in otm. cal poly-pomona offers programs in technology and operations management (tom). ucla anderson school of management of management offers decisions, operations and technology management (dtom) programs.   == references ==   == further reading == book series: operations and technology management, ed. by prof. dr. thorsten blecker; prof. dr. george q. huang; prof. dr. fabrizio salvador, buchreihe operations and technology management')"
166,"Continuous emission monitoring systems (CEMS) are used as a tool to monitor the effluent gas streams resulting from combustion in industrial processes. CEMS can measure flue gas for oxygen, carbon monoxide and carbon dioxide to provide information for combustion control in industrial settings. They are also used as a means to comply with air emission standards such as the United States Environmental Protection Agency's (EPA) Acid Rain Program, other federal emission programs, or state permitted emission standards.
CEMS typically consist of analyzers to measure gas concentrations within the stream, equipment to direct a sample of that gas stream to the analyzers if they are remote, equipment to condition the sample gas by removing water and other components that could interfere with the reading, pneumatic plumbing with valves that can be controlled by a PLC to route the sample gas to and away from the analyzers, a calibration and maintenance system that allows for the injection of calibration gases into the sample line, and a Data Acquisition and Handling System (DAHS) that collects and stores each data point and can perform necessary calculations required to get total mass emissions. A CEMS operates at all times even if the process it measures is not on. They can continuously collect, record and report emissions data for process monitoring and/or for compliance purposes.
The standard CEM system consists of a sample probe, filter, sample line (umbilical), gas conditioning system, calibration gas system, and a series of gas analyzers which reflect the parameters being monitored.  Typical monitored emissions include:  sulfur dioxide, nitrogen oxides, carbon monoxide, carbon dioxide, hydrogen chloride, airborne particulate matter, mercury, volatile organic compounds, and oxygen. CEM systems can also measure air flow, flue gas opacity and moisture. A monitoring system that measures particulate matter is referred to as a PEMS.
In the U.S., the EPA requires a data acquisition and handling system to collect and report the data. Measurements of concentration can be converted to mass/hour by including flow rate measurements. The types of gases being measured and the calculations required are dependent upon the source type and each source type has its own subpart under 40 CFR part 60 and part 75. SO2 emissions are measured in pounds per hour using both an SO2 pollutant concentration monitor and a volumetric flow monitor.  For NOx, both a NOx pollutant concentration monitor and a diluent gas monitor are used to determine the emissions rate in weight per volume or weight per heat value (for example lbs/million Btu, lbs/ft3, kg/kWh or kg/m3). Opacity measurements are sometimes required, depending on the source type. CO2 measuring is sometimes a requirement, however if monitored, a CO2 or oxygen monitor plus a flow monitor should be used. The DAHS must be able to collect, record and store data, usually at 1-minute intervals. For compliance purposes, a DAHS must be in continuous operation 24/7/365 even when no process is on. For a valid measurement, the DAHS must record at least one reading every 15 minutes for 3 out of 4 quarters. The readings are then averaged hourly.


== Operation ==
A small sample of flue gas is extracted, by means of a pump, into the CEM system via a sample probe. Facilities that combust fossil fuels often use a dilution-extractive probe to dilute the sample with clean, dry air to a ratio typically between 50:1 to 200:1, but usually 100:1. Dilution is used because pure flue gas can be hot, wet and, with some pollutants, sticky. Once diluted to the appropriate ratio, the sample is transported through a sample line (typically referred to as an umbilical) to a manifold from which individual analyzers may extract a sample. Gas analyzers employ various techniques to accurately measure concentrations. Some commonly used techniques include: infrared and ultraviolet adsorption, chemiluminescence, fluorescence and beta ray absorption. After analysis, the gas exits the analyzer to a common manifold to all analyzers where it is vented out of doors. A Data Acquisition and Handling System (DAHS) receives the signal output from each analyzer in order to collect and record emissions data.Another sample extraction method used in industrial sources and utility sources with low emission rates, is commonly referred to as a ""dry extractive"", ""hot dry"" extractive, or ""direct"" CEMS. The sample is not diluted, but is carried along a heated sample line at high temperature into a sample conditioning unit. The sample is filtered to remove particulate matter and dried, usually with a chiller, to remove moisture. Once conditioned, the sample enters a sampling manifold and is measured by various gas analyzers, typically NOx and O2 (and sometimes CO) for combustion turbines and engines running natural gas or diesel. NOx analyzers typically work using chemiluminescence. O2 analyzers a magnetic field which attracts O2 to measure the concentration. The O2 causes movement of a suspended mirror within the analyzer which then changes the amount of light being reflected by that mirror onto a photocell. The amount of current required to move the mirror back to center is proportional to the O2 concentration. The ability to measure % oxygen in the sample is required to perform the required calculations.


== Quality assurance ==
Accuracy of the system is demonstrated in several ways. An internal quality assurance check is achieved by daily introduction of a certified concentration of gas to the sample probe. The CEMS measurement is then compared against the known concentration to arrive at a Calibration Error percentage. A zero gas reading is also taken and compared. If the calibration error % exceeds 2x the performance specification for 5 consecutive days or 4x the performance specification in 24 hours, the CEMS is considered out of control meaning the data can not be relied upon as accurate until it is brought back into control. Data substitution will be used for out of control periods. The data substitution method is generally not advantageous so it is critical to get the CEMS back into control as soon as possible.
The EPA also allows for the use of Continuous Emissions Monitoring Calibration Systems which dilute gases to generate calibration standards. The analyzer reading must be accurate to a certain percentage. The percent accuracy can vary, but most fall between 2.5% and 5%. In power stations affected by the Acid Rain Program, annual (or bi-annual) certification of the system must be performed by an independent firm. The firm will have an independent CEM system temporarily in place to collect emissions data in parallel with the plant CEMS. This testing is referred to as a Relative Accuracy Test Audit (RATA).
In the U.S., periodic evaluations of the equipment must be reported and recorded. This includes daily calibration error tests, daily interference tests for flow monitors, and semi-annual (or annual) RATA and bias tests. CEMS equipment is expensive and not always affordable for a facility. In such cases, a facility will install non-EPA compliant analysis equipment at the emissions point. Once yearly, for the equipment evaluation, a mobile CEMS company measures emissions with compliant equipment. The results are then compared to the non-compliant analyzer system.


== References ==","pandas(index=166, _1=166, text='continuous emission monitoring systems (cems) are used as a tool to monitor the effluent gas streams resulting from combustion in industrial processes. cems can measure flue gas for oxygen, carbon monoxide and carbon dioxide to provide information for combustion control in industrial settings. they are also used as a means to comply with air emission standards such as the united states environmental protection agency\'s (epa) acid rain program, other federal emission programs, or state permitted emission standards. cems typically consist of analyzers to measure gas concentrations within the stream, equipment to direct a sample of that gas stream to the analyzers if they are remote, equipment to condition the sample gas by removing water and other components that could interfere with the reading, pneumatic plumbing with valves that can be controlled by a plc to route the sample gas to and away from the analyzers, a calibration and maintenance system that allows for the injection of calibration gases into the sample line, and a data acquisition and handling system (dahs) that collects and stores each data point and can perform necessary calculations required to get total mass emissions. a cems operates at all times even if the process it measures is not on. they can continuously collect, record and report emissions data for process monitoring and/or for compliance purposes. the standard cem system consists of a sample probe, filter, sample line (umbilical), gas conditioning system, calibration gas system, and a series of gas analyzers which reflect the parameters being monitored.  typical monitored emissions include:  sulfur dioxide, nitrogen oxides, carbon monoxide, carbon dioxide, hydrogen chloride, airborne particulate matter, mercury, volatile organic compounds, and oxygen. cem systems can also measure air flow, flue gas opacity and moisture. a monitoring system that measures particulate matter is referred to as a pems. in the u.s., the epa requires a data acquisition and handling system to collect and report the data. measurements of concentration can be converted to mass/hour by including flow rate measurements. the types of gases being measured and the calculations required are dependent upon the source type and each source type has its own subpart under 40 cfr part 60 and part 75. so2 emissions are measured in pounds per hour using both an so2 pollutant concentration monitor and a volumetric flow monitor.  for nox, both a nox pollutant concentration monitor and a diluent gas monitor are used to determine the emissions rate in weight per volume or weight per heat value (for example lbs/million btu, lbs/ft3, kg/kwh or kg/m3). opacity measurements are sometimes required, depending on the source type. co2 measuring is sometimes a requirement, however if monitored, a co2 or oxygen monitor plus a flow monitor should be used. the dahs must be able to collect, record and store data, usually at 1-minute intervals. for compliance purposes, a dahs must be in continuous operation 24/7/365 even when no process is on. for a valid measurement, the dahs must record at least one reading every 15 minutes for 3 out of 4 quarters. the readings are then averaged hourly.   == operation == a small sample of flue gas is extracted, by means of a pump, into the cem system via a sample probe. facilities that combust fossil fuels often use a dilution-extractive probe to dilute the sample with clean, dry air to a ratio typically between 50:1 to 200:1, but usually 100:1. dilution is used because pure flue gas can be hot, wet and, with some pollutants, sticky. once diluted to the appropriate ratio, the sample is transported through a sample line (typically referred to as an umbilical) to a manifold from which individual analyzers may extract a sample. gas analyzers employ various techniques to accurately measure concentrations. some commonly used techniques include: infrared and ultraviolet adsorption, chemiluminescence, fluorescence and beta ray absorption. after analysis, the gas exits the analyzer to a common manifold to all analyzers where it is vented out of doors. a data acquisition and handling system (dahs) receives the signal output from each analyzer in order to collect and record emissions data.another sample extraction method used in industrial sources and utility sources with low emission rates, is commonly referred to as a ""dry extractive"", ""hot dry"" extractive, or ""direct"" cems. the sample is not diluted, but is carried along a heated sample line at high temperature into a sample conditioning unit. the sample is filtered to remove particulate matter and dried, usually with a chiller, to remove moisture. once conditioned, the sample enters a sampling manifold and is measured by various gas analyzers, typically nox and o2 (and sometimes co) for combustion turbines and engines running natural gas or diesel. nox analyzers typically work using chemiluminescence. o2 analyzers a magnetic field which attracts o2 to measure the concentration. the o2 causes movement of a suspended mirror within the analyzer which then changes the amount of light being reflected by that mirror onto a photocell. the amount of current required to move the mirror back to center is proportional to the o2 concentration. the ability to measure % oxygen in the sample is required to perform the required calculations.   == quality assurance == accuracy of the system is demonstrated in several ways. an internal quality assurance check is achieved by daily introduction of a certified concentration of gas to the sample probe. the cems measurement is then compared against the known concentration to arrive at a calibration error percentage. a zero gas reading is also taken and compared. if the calibration error % exceeds 2x the performance specification for 5 consecutive days or 4x the performance specification in 24 hours, the cems is considered out of control meaning the data can not be relied upon as accurate until it is brought back into control. data substitution will be used for out of control periods. the data substitution method is generally not advantageous so it is critical to get the cems back into control as soon as possible. the epa also allows for the use of continuous emissions monitoring calibration systems which dilute gases to generate calibration standards. the analyzer reading must be accurate to a certain percentage. the percent accuracy can vary, but most fall between 2.5% and 5%. in power stations affected by the acid rain program, annual (or bi-annual) certification of the system must be performed by an independent firm. the firm will have an independent cem system temporarily in place to collect emissions data in parallel with the plant cems. this testing is referred to as a relative accuracy test audit (rata). in the u.s., periodic evaluations of the equipment must be reported and recorded. this includes daily calibration error tests, daily interference tests for flow monitors, and semi-annual (or annual) rata and bias tests. cems equipment is expensive and not always affordable for a facility. in such cases, a facility will install non-epa compliant analysis equipment at the emissions point. once yearly, for the equipment evaluation, a mobile cems company measures emissions with compliant equipment. the results are then compared to the non-compliant analyzer system.   == references ==')"
167,"Reverse engineering (also known as backwards engineering or back engineering) is a process or method through the application of which one attempts to understand through deductive reasoning how a device, process, system, or piece of software accomplishes a task with very little (if any) insight into exactly how it does so.
Reverse engineering is applicable in the fields of computer engineering, mechanical engineering, electronic engineering, software engineering, chemical engineering, and systems biology.


== Overview ==
There are many reasons for performing reverse engineering in various fields. Reverse engineering has its origins in the analysis of hardware for commercial or military advantage. However, the reverse engineering process, as such, is not concerned with creating a copy or changing the artifact in some way. It is only an analysis to deduce design features from products with little or no additional knowledge about the procedures involved in their original production.In some cases, the goal of the reverse engineering process can simply be a redocumentation of legacy systems. Even when the reverse-engineered product is that of a competitor, the goal may not be to copy it but to perform competitor analysis. Reverse engineering may also be used to create interoperable products and despite some narrowly-tailored United States and European Union legislation, the legality of using specific reverse engineering techniques for that purpose has been hotly contested in courts worldwide for more than two decades.Software reverse engineering can help to improve the understanding of the underlying source code for the maintenance and improvement of the software, relevant information can be extracted to make a decision for software development and graphical representations of the code can provide alternate views regarding the source code, which can help to detect and fix a software bug or vulnerability. Frequently, as some software develops, its design information and improvements are often lost over time, but that lost information can usually be recovered with reverse engineering. The process can also help to cut down the time required to understand the source code, thus reducing the overall cost of the software development. Reverse engineering can also help to detect and to eliminate a malicious code written to the software with better code detectors. Reversing a source code can be used to find alternate uses of the source code, such as detecting the unauthorized replication of the source code where it was not intended to be used, or revealing how a competitor's product was built. That process is commonly used for ""cracking"" software and media to remove their copy protection, or to create a possibly-improved copy or even a knockoff, which is usually the goal of a competitor or a hacker.Malware developers often use reverse engineering techniques to find vulnerabilities in an operating system to build a computer virus that can exploit the system vulnerabilities. Reverse engineering is also being used in cryptanalysis to find vulnerabilities in substitution cipher, symmetric-key algorithm or public-key cryptography.There are other uses to reverse engineering:

Interfacing. Reverse engineering can be used when a system is required to interface to another system and how both systems would negotiate is to be established. Such requirements typically exist for interoperability.
Military or commercial espionage. Learning about an enemy's or competitor's latest research by stealing or capturing a prototype and dismantling it may result in the development of a similar product or a better countermeasure against it.
Obsolescence. Integrated circuits are often designed on proprietary systems and built on production lines, which become obsolete in only a few years. When systems using those parts can no longer be maintained since the parts are no longer made, the only way to incorporate the functionality into new technology is to reverse-engineer the existing chip and then to redesign it using newer tools by using the understanding gained as a guide. Another obsolescence originated problem that can be solved by reverse engineering is the need to support (maintenance and supply for continuous operation) existing legacy devices that are no longer supported by their original equipment manufacturer. The problem is particularly critical in military operations.
Product security analysis. That examines how a product works by determining the specifications of its components and estimate costs and identifies potential patent infringement. Also part of product security analysis is acquiring sensitive data by disassembling and analyzing the design of a system component. Another intent may be to remove copy protection or to circumvent access restrictions.
Competitive technical intelligence. That is to understand what one's competitor is actually doing, rather than what it says that it is doing.
Saving money. Finding out what a piece of electronics can do may spare a user from purchasing a separate product.
Repurposing. Obsolete objects are then reused in a different-but-useful manner.


== Common situations ==


=== Machines ===
As computer-aided design (CAD) has become more popular, reverse engineering has become a viable method to create a 3D virtual model of an existing physical part for use in 3D CAD, CAM, CAE, or other software. The reverse-engineering process involves measuring an object and then reconstructing it as a 3D model. The physical object can be measured using 3D scanning technologies like CMMs, laser scanners, structured light digitizers, or industrial CT scanning (computed tomography). The measured data alone, usually represented as a point cloud, lacks topological information and design intent. The former may be recovered by converting the point cloud to a triangular-faced mesh. Reverse engineering aims to go beyond producing such a mesh and to recover the design intent in terms of simple analytical surfaces where appropriate (planes, cylinders, etc.) as well as possibly NURBS surfaces to produce a boundary-representation CAD model. Recovery of such a model allows a design to be modified to meet new requirements, a manufacturing plan to be generated, etc.
Hybrid modeling is a commonly used term when NURBS and parametric modeling are implemented together. Using a combination of geometric and freeform surfaces can provide a powerful method of 3D modeling. Areas of freeform data can be combined with exact geometric surfaces to create a hybrid model. A typical example of this would be the reverse engineering of a cylinder head, which includes freeform cast features, such as water jackets and high-tolerance machined areas.Reverse engineering is also used by businesses to bring existing physical geometry into digital product development environments, to make a digital 3D record of their own products, or to assess competitors' products. It is used to analyze how a product works, what it does, what components it has; estimate costs; identify potential patent infringement; etc.
Value engineering, a related activity that is also used by businesses, involves deconstructing and analyzing products. However, the objective is to find opportunities for cost-cutting.


=== Software ===
In 1990, the Institute of Electrical and Electronics Engineers (IEEE) defined (software) reverse engineering (SRE) as ""the process of analyzing a
subject system to identify the system's components and their interrelationships and to create representations of the system in another form or at a higher
level of abstraction"" in which the ""subject system"" is the end product of software development. Reverse engineering is a process of examination only, and the software system under consideration is not modified, which would otherwise be re-engineering or restructuring. Reverse engineering can be performed from any stage of the product cycle, not necessarily from the functional end product.There are two components in reverse engineering: redocumentation and design recovery. Redocumentation is the creation of new representation of the computer code so that it is easier to understand. Meanwhile, design recovery is the use of deduction or reasoning from general knowledge or personal experience of the product to understand the product's functionality fully. It can also be seen as ""going backwards through the development cycle."" In this model, the output of the implementation phase (in source code form) is reverse-engineered back to the analysis phase, in an inversion of the traditional waterfall model. Another term for this technique is program comprehension. The Working Conference on Reverse Engineering (WCRE) has been held yearly to explore and expand the techniques of reverse engineering. Computer-aided software engineering (CASE) and automated code generation have contributed greatly in the field of reverse engineering.Software anti-tamper technology like obfuscation is used to deter both reverse engineering and re-engineering of proprietary software and software-powered systems. In practice, two main types of reverse engineering emerge. In the first case, source code is already available for the software, but higher-level aspects of the program, which are perhaps poorly documented or documented but no longer valid, are discovered. In the second case, there is no source code available for the software, and any efforts towards discovering one possible source code for the software are regarded as reverse engineering. The second usage of the term is more familiar to most people. Reverse engineering of software can make use of the clean room design technique to avoid copyright infringement.
On a related note, black box testing in software engineering has a lot in common with reverse engineering. The tester usually has the API but has the goals to find bugs and undocumented features by bashing the product from outside.Other purposes of reverse engineering include security auditing, removal of copy protection (""cracking""), circumvention of access restrictions often present in consumer electronics, customization of embedded systems (such as engine management systems), in-house repairs or retrofits, enabling of additional features on low-cost ""crippled"" hardware (such as some graphics card chip-sets), or even mere satisfaction of curiosity.


==== Binary software ====
Binary reverse engineering is performed if source code for a software is unavailable. This process is sometimes termed reverse code engineering, or RCE. For example, decompilation of binaries for the Java platform can be accomplished by using Jad. One famous case of reverse engineering was the first non-IBM implementation of the PC BIOS, which launched the historic IBM PC compatible industry that has been the overwhelmingly-dominant computer hardware platform for many years. Reverse engineering of software is protected in the US by the fair use exception in copyright law. The Samba software, which allows systems that do not run Microsoft Windows systems to share files with systems that run it, is a classic example of software reverse engineering since the Samba project had to reverse-engineer unpublished information about how Windows file sharing worked so that non-Windows computers could emulate it. The Wine project does the same thing for the Windows API, and OpenOffice.org is one party doing that for the Microsoft Office file formats. The ReactOS project is even more ambitious in its goals by striving to provide binary (ABI and API) compatibility with the current Windows operating systems of the NT branch, which allows software and drivers written for Windows to run on a clean-room reverse-engineered free software (GPL) counterpart. WindowsSCOPE allows for reverse-engineering the full contents of a Windows system's live memory including a binary-level, graphical reverse engineering of all running processes.
Another classic, if not well-known, example is that in 1987 Bell Laboratories reverse-engineered the Mac OS System 4.1, originally running on the Apple Macintosh SE, so that it could run it on RISC machines of their own.


===== Binary software techniques =====
Reverse engineering of software can be accomplished by various methods.
The three main groups of software reverse engineering are

Analysis through observation of information exchange, most prevalent in protocol reverse engineering, which involves using bus analyzers and packet sniffers, such as for accessing a computer bus or computer network connection and revealing the traffic data thereon. Bus or network behavior can then be analyzed to produce a standalone implementation that mimics that behavior. That is especially useful for reverse engineering device drivers. Sometimes, reverse engineering on embedded systems is greatly assisted by tools deliberately introduced by the manufacturer, such as JTAG ports or other debugging means. In Microsoft Windows, low-level debuggers such as SoftICE are popular.
Disassembly using a disassembler, meaning the raw machine language of the program is read and understood in its own terms, only with the aid of machine-language mnemonics. It works on any computer program but can take quite some time, especially for those who are not used to machine code. The Interactive Disassembler is a particularly popular tool.
Decompilation using a decompiler, a process that tries, with varying results, to recreate the source code in some high-level language for a program only available in machine code or bytecode.


==== Software classification ====
Software classification is the process of identifying similarities between different software binaries (such as two different versions of the same binary) used to detect code relations between software samples. The task was traditionally done manually for several reasons (such as patch analysis for vulnerability detection and copyright infringement), but it can now be done somewhat automatically for large numbers of samples.
This method is being used mostly for long and thorough reverse engineering tasks (complete analysis of a complex algorithm or big piece of software). In general, statistical classification is considered to be a hard problem, which is also true for software classification, and so few solutions/tools that handle this task well.


=== Source code ===
A number of UML tools refer to the process of importing and analysing source code to generate UML diagrams as ""reverse engineering."" See List of UML tools.
Although UML is one approach to providing ""reverse engineering"" more recent advances in international standards activities have resulted in the development of the Knowledge Discovery Metamodel (KDM). The standard delivers an ontology for the intermediate (or abstracted) representation of programming language constructs and their interrelationships. An Object Management Group standard (on its way to becoming an ISO standard as well), KDM has started to take hold in industry with the development of tools and analysis environments that can deliver the extraction and analysis of source, binary, and byte code. For source code analysis, KDM's granular standards' architecture enables the extraction of software system flows (data, control, and call maps), architectures, and business layer knowledge (rules, terms, and process). The standard enables the use of a common data format (XMI) enabling the correlation of the various layers of system knowledge for either detailed analysis (such as root cause, impact) or derived analysis (such as business process extraction). Although efforts to represent language constructs can be never-ending because of the number of languages, the continuous evolution of software languages, and the development of new languages, the standard does allow for the use of extensions to support the broad language set as well as evolution. KDM is compatible with UML, BPMN, RDF, and other standards enabling migration into other environments and thus leverage system knowledge for efforts such as software system transformation and enterprise business layer analysis.


=== Protocols ===
Protocols are sets of rules that describe message formats and how messages are exchanged: the protocol state machine. Accordingly, the problem of protocol reverse-engineering can be partitioned into two subproblems: message format and state-machine reverse-engineering.
The message formats have traditionally been reverse-engineered by a tedious manual process, which involved analysis of how protocol implementations process messages, but recent research proposed a number of automatic solutions. Typically, the automatic approaches group observe messages into clusters by using various clustering analyses, or they emulate the protocol implementation tracing the message processing.
There has been less work on reverse-engineering of state-machines of protocols. In general, the protocol state-machines can be learned either through a process of offline learning, which passively observes communication and attempts to build the most general state-machine accepting all observed sequences of messages, and online learning, which allows interactive generation of probing sequences of messages and listening to responses to those probing sequences. In general, offline learning of small state-machines is known to be NP-complete, but online learning can be done in polynomial time. An automatic offline approach has been demonstrated by Comparetti et al. and an online approach by Cho et al.Other components of typical protocols, like encryption and hash functions, can be reverse-engineered automatically as well. Typically, the automatic approaches trace the execution of protocol implementations and try to detect buffers in memory holding unencrypted packets.


=== Integrated circuits/smart cards ===
Reverse engineering is an invasive and destructive form of analyzing a smart card. The attacker uses chemicals to etch away layer after layer of the smart card and takes pictures with a scanning electron microscope (SEM). That technique can reveal the complete hardware and software part of the smart card. The major problem for the attacker is to bring everything into the right order to find out how everything works. The makers of the card try to hide keys and operations by mixing up memory positions, such as by bus scrambling.In some cases, it is even possible to attach a probe to measure voltages while the smart card is still operational. The makers of the card employ sensors to detect and prevent that attack. That attack is not very common because it requires both a large investment in effort and special equipment that is generally available only to large chip manufacturers. Furthermore, the payoff from this attack is low since other security techniques are often used such as shadow accounts. It is still uncertain whether attacks against chip-and-PIN cards to replicate encryption data and then to crack PINs would provide a cost-effective attack on multifactor authentication.
Full reverse engineering proceeds in several major steps.
The first step after images have been taken with a SEM is stitching the images together, which is necessary because each layer cannot be captured by a single shot. A SEM needs to sweep across the area of the circuit and take several hundred images to cover the entire layer. Image stitching takes as input several hundred pictures and outputs a single properly-overlapped picture of the complete layer.
Next, the stitched layers need to be aligned because the sample, after etching, cannot be put into the exact same position relative to the SEM each time. Therefore, the stitched versions will not overlap in the correct fashion, as on the real circuit. Usually, three corresponding points are selected, and a transformation applied on the basis of that.
To extract the circuit structure, the aligned, stitched images need to be segmented, which highlights the important circuitry and separates it from the uninteresting background and insulating materials.
Finally, the wires can be traced from one layer to the next, and the netlist of the circuit, which contains all of the circuit's information, can be reconstructed.


=== Military applications ===
Reverse engineering is often used by people to copy other nations' technologies, devices, or information that have been obtained by regular troops in the fields or by intelligence operations. It was often used during the Second World War and the Cold War. Here are well-known examples from the Second World War and later:

Jerry can: British and American forces noticed that the Germans had gasoline cans with an excellent design. They reverse-engineered copies of those cans, which cans were popularly known as ""Jerry cans.""
Panzerschreck: The Germans captured an American bazooka during the Second World War and reverse engineered it to create the larger Panzerschreck.
Tupolev Tu-4: In 1944, three American B-29 bombers on missions over Japan were forced to land in the Soviet Union. The Soviets, who did not have a similar strategic bomber, decided to copy the B-29. Within three years, they had developed the Tu-4, a nearly-perfect copy.
SCR-584 radar: copied by the Soviet Union after the Second World War, it is known for a few modifications - СЦР-584, Бинокль-Д.
V-2 rocket: Technical documents for the V-2 and related technologies were captured by the Western Allies at the end of the war. The Americans focused their reverse engineering efforts via Operation Paperclip, which led to the development of the PGM-11 Redstone rocket. The Soviets used captured German engineers to reproduce technical documents and plans and worked from captured hardware to make their clone of the rocket, the R-1. Thus began the postwar Soviet rocket program, which led to the R-7 and the beginning of the space race.
K-13/R-3S missile (NATO reporting name AA-2 Atoll), a Soviet reverse-engineered copy of the AIM-9 Sidewinder, was made possible after a Taiwanese AIM-9B hit a Chinese MiG-17 without exploding in September 1958. The missile became lodged within the airframe, and the pilot returned to base with what Soviet scientists would describe as a university course in missile development.
BGM-71 TOW missile: In May 1975, negotiations between Iran and Hughes Missile Systems on co-production of the TOW and Maverick missiles stalled over disagreements in the pricing structure, the subsequent 1979 revolution ending all plans for such co-production. Iran was later successful in reverse-engineering the missile and now produces its own copy, the Toophan.
China has reversed engineered many examples of Western and Russian hardware, from fighter aircraft to missiles and HMMWV cars, such as the MiG-15 (which became the J-7) and the Su-33 (which became the J-15). More recent analyses of China's military growth have pointed to the inherent limitations of reverse engineering for advanced weapon systems.
During the Second World War, Polish and British cryptographers studied captured German """"Enigma"" message encryption machines for weaknesses. Their operation was then simulated on electromechanical devices, ""bombes, which tried all the possible scrambler settings of the ""Enigma"" machines that helped the breaking of coded messages that had been sent by the Germans.
Also during the Second World War, British scientists analyzed and defeated a series of increasingly-sophisticated radio navigation systems used by the Luftwaffe to perform guided bombing missions at night. The British countermeasures to the system were so effective that in some cases, German aircraft were led by signals to land at RAF bases since they believed that they had returned to German territory.


=== Gene networks ===
Reverse engineering concepts have been applied to biology as well, specifically to the task of understanding the structure and function of gene regulatory networks. They regulate almost every aspect of biological behavior and allow cells to carry out physiological processes and responses to perturbations. Understanding the structure and the dynamic behavior of gene networks is therefore one of the paramount challenges of systems biology, with immediate practical repercussions in several applications that are beyond basic research.
There are several methods for reverse engineering gene regulatory networks by using molecular biology and data science methods. They have been generally divided into six classes:

Coexpression methods are based on the notion that if two genes exhibit a similar expression profile, they may be related although no causation can be simply inferred from coexpression.
Sequence motif methods analyze gene promoters to find specific transcription factor binding domains. If a transcription factor is predicted to bind a promoter of a specific gene, a regulatory connection can be hypothesized.
Chromatin ImmunoPrecipitation (ChIP) methods investigate the genome-wide profile of DNA binding of chosen transcription factors to infer their downstream gene networks.
Orthology methods transfer gene network knowledge from one species to another.
Literature methods implement text mining and manual research to identify putative or experimentally-proven gene network connections.
Transcriptional complexes methods leverage information on protein-protein interactions between transcription factors, thus extending the concept of gene networks to include transcriptional regulatory complexes.Often, gene network reliability is tested by genetic perturbation experiments followed by dynamic modelling, based on the principle that removing one network node has predictable effects on the functioning of the remaining nodes of the network.
Applications of the reverse engineering of gene networks range from understanding mechanisms of plant physiology to the highlighting of new targets for anticancer therapy.


=== Overlap with patent law ===
Reverse engineering applies primarily to gaining understanding of a process or artifact in which the manner of its construction, use, or internal processes has not been made clear by its creator.
Patented items do not of themselves have to be reverse-engineered to be studied, for the essence of a patent is that inventors provide a detailed public disclosure themselves, and in return receive legal protection of the invention that is involved. However, an item produced under one or more patents could also include other technology that is not patented and not disclosed. Indeed, one common motivation of reverse engineering is to determine whether a competitor's product contains patent infringement or copyright infringement.


== Legality ==


=== United States ===
In the United States, even if an artifact or process is protected by trade secrets, reverse-engineering the artifact or process is often lawful if it has been legitimately obtained.Reverse engineering of computer software often falls under both contract law as a breach of contract as well as any other relevant laws. That is because most end user license agreements specifically prohibit it, and US courts have ruled that if such terms are present, they override the copyright law that expressly permits it (see Bowers v. Baystate Technologies). According to Section 103(f) of the Digital Millennium Copyright Act (17 U.S.C. § 1201 (f)), a person in legal possession of a program may reverse-engineer and circumvent its protection if that is necessary to achieve ""interoperability,"" a term that broadly covers other devices and programs that can interact with it, make use of it, and to use and transfer data to and from it in useful ways. A limited exemption exists that allows the knowledge thus gained to be shared and used for interoperability purposes.


=== European Union ===
EU Directive 2009/24 on the legal protection of computer programs, which superseded an earlier (1991) directive, governs reverse engineering in the European Union.


== See also ==


== References ==


== Sources ==","pandas(index=167, _1=167, text='reverse engineering (also known as backwards engineering or back engineering) is a process or method through the application of which one attempts to understand through deductive reasoning how a device, process, system, or piece of software accomplishes a task with very little (if any) insight into exactly how it does so. reverse engineering is applicable in the fields of computer engineering, mechanical engineering, electronic engineering, software engineering, chemical engineering, and systems biology.   == overview == there are many reasons for performing reverse engineering in various fields. reverse engineering has its origins in the analysis of hardware for commercial or military advantage. however, the reverse engineering process, as such, is not concerned with creating a copy or changing the artifact in some way. it is only an analysis to deduce design features from products with little or no additional knowledge about the procedures involved in their original production.in some cases, the goal of the reverse engineering process can simply be a redocumentation of legacy systems. even when the reverse-engineered product is that of a competitor, the goal may not be to copy it but to perform competitor analysis. reverse engineering may also be used to create interoperable products and despite some narrowly-tailored united states and european union legislation, the legality of using specific reverse engineering techniques for that purpose has been hotly contested in courts worldwide for more than two decades.software reverse engineering can help to improve the understanding of the underlying source code for the maintenance and improvement of the software, relevant information can be extracted to make a decision for software development and graphical representations of the code can provide alternate views regarding the source code, which can help to detect and fix a software bug or vulnerability. frequently, as some software develops, its design information and improvements are often lost over time, but that lost information can usually be recovered with reverse engineering. the process can also help to cut down the time required to understand the source code, thus reducing the overall cost of the software development. reverse engineering can also help to detect and to eliminate a malicious code written to the software with better code detectors. reversing a source code can be used to find alternate uses of the source code, such as detecting the unauthorized replication of the source code where it was not intended to be used, or revealing how a competitor\'s product was built. that process is commonly used for ""cracking"" software and media to remove their copy protection, or to create a possibly-improved copy or even a knockoff, which is usually the goal of a competitor or a hacker.malware developers often use reverse engineering techniques to find vulnerabilities in an operating system to build a computer virus that can exploit the system vulnerabilities. reverse engineering is also being used in cryptanalysis to find vulnerabilities in substitution cipher, symmetric-key algorithm or public-key cryptography.there are other uses to reverse engineering:  interfacing. reverse engineering can be used when a system is required to interface to another system and how both systems would negotiate is to be established. such requirements typically exist for interoperability. military or commercial espionage. learning about an enemy\'s or competitor\'s latest research by stealing or capturing a prototype and dismantling it may result in the development of a similar product or a better countermeasure against it. obsolescence. integrated circuits are often designed on proprietary systems and built on production lines, which become obsolete in only a few years. when systems using those parts can no longer be maintained since the parts are no longer made, the only way to incorporate the functionality into new technology is to reverse-engineer the existing chip and then to redesign it using newer tools by using the understanding gained as a guide. another obsolescence originated problem that can be solved by reverse engineering is the need to support (maintenance and supply for continuous operation) existing legacy devices that are no longer supported by their original equipment manufacturer. the problem is particularly critical in military operations. product security analysis. that examines how a product works by determining the specifications of its components and estimate costs and identifies potential patent infringement. also part of product security analysis is acquiring sensitive data by disassembling and analyzing the design of a system component. another intent may be to remove copy protection or to circumvent access restrictions. competitive technical intelligence. that is to understand what one\'s competitor is actually doing, rather than what it says that it is doing. saving money. finding out what a piece of electronics can do may spare a user from purchasing a separate product. repurposing. obsolete objects are then reused in a different-but-useful manner.   == common situations == eu directive 2009/24 on the legal protection of computer programs, which superseded an earlier (1991) directive, governs reverse engineering in the european union.   == see also ==   == references ==   == sources ==')"
168,"The Industrial & Systems Engineering Program offers a Bachelor of Science degree in industrial engineering. With a total of 133 credit hours, the program covers the major areas of industrial engineering, such as operations research, production planning, inventory control, methods engineering, quality control, facility location, manufacturing, and facility layout.


== History ==
The Industrial & Systems Engineering (ISE) program in the Systems Engineering Department was first introduced in 1984 and has been revised in 1996 based on the Accreditation Board for Engineering and Technology (ABET) recommendation after their first visit in 1993. The revision made in 1996 came after when the number of credit hours of the Bachelor of Science (B.Sc) was reduced from 141 to 133 credit hours. The program has received ABET accreditation extension in 2010.


== Program Courses ==
The ISE program has a total of 50 credit hours on required ISE courses, with the following descriptions:

Introduction to I&SE
Probability & Statistics
Regression for Industrial Engineering
Linear Control Systems
Numerical Methods
Operations Research I
Statistical Quality Control
Principles of Industrial Costing
Engineering Economics
Manufacturing Technology
Work and Process Improvement
Fundamental of Database Systems
Seminar
Industrial Engineering Design
Production Systems
Stochastic Systems Simulation
Operations Research II
Facility Layout and Location
Senior Design


== External links ==
Codes of courses and description
Department website","pandas(index=168, _1=168, text='the industrial & systems engineering program offers a bachelor of science degree in industrial engineering. with a total of 133 credit hours, the program covers the major areas of industrial engineering, such as operations research, production planning, inventory control, methods engineering, quality control, facility location, manufacturing, and facility layout.   == history == the industrial & systems engineering (ise) program in the systems engineering department was first introduced in 1984 and has been revised in 1996 based on the accreditation board for engineering and technology (abet) recommendation after their first visit in 1993. the revision made in 1996 came after when the number of credit hours of the bachelor of science (b.sc) was reduced from 141 to 133 credit hours. the program has received abet accreditation extension in 2010.   == program courses == the ise program has a total of 50 credit hours on required ise courses, with the following descriptions:  introduction to i&se probability & statistics regression for industrial engineering linear control systems numerical methods operations research i statistical quality control principles of industrial costing engineering economics manufacturing technology work and process improvement fundamental of database systems seminar industrial engineering design production systems stochastic systems simulation operations research ii facility layout and location senior design   == external links == codes of courses and description department website')"
169,"Component engineering is an engineering discipline primarily used to ensure the availability of suitable components required to manufacture a larger product.
The term combines two ideas:

A component—a smaller, self-contained part of a larger entity
Engineering—the discipline and profession of applying science to implement some functional designThose who practice this discipline are called component engineers. Component engineers typically select, qualify, approve, document, and manage purchased components and direct material required to produce an end product. Component engineers typically analyze and qualify interchangeable parts from sources (vendors) outside their organization. Because of the high number of components used in electronic assemblies, component engineering is closely associated with design and manufacture.
Component engineering can also refer to the manufacturer of selected equipment used in theatrical motion picture projection. This equipment falls into two categories: units that automatically control the presentation and those that comprise part of the sound system.
Component engineering also involves product lifecycle management, that is to know when a component is going to be obsolete or to analyse the form–fit–functionality changes in the component.
It involves finding alternate components for discontinued/Obsolete Components


== See also ==
Engineering
Just in time (business)
List of engineering branches
Manufacturing engineering


== References ==


== External links ==
Component Engineering Society of Orange County, California (CESOC) definition
Design Chain Associates definition","pandas(index=169, _1=169, text='component engineering is an engineering discipline primarily used to ensure the availability of suitable components required to manufacture a larger product. the term combines two ideas:  a component—a smaller, self-contained part of a larger entity engineering—the discipline and profession of applying science to implement some functional designthose who practice this discipline are called component engineers. component engineers typically select, qualify, approve, document, and manage purchased components and direct material required to produce an end product. component engineers typically analyze and qualify interchangeable parts from sources (vendors) outside their organization. because of the high number of components used in electronic assemblies, component engineering is closely associated with design and manufacture. component engineering can also refer to the manufacturer of selected equipment used in theatrical motion picture projection. this equipment falls into two categories: units that automatically control the presentation and those that comprise part of the sound system. component engineering also involves product lifecycle management, that is to know when a component is going to be obsolete or to analyse the form–fit–functionality changes in the component. it involves finding alternate components for discontinued/obsolete components   == see also == engineering just in time (business) list of engineering branches manufacturing engineering   == references ==   == external links == component engineering society of orange county, california (cesoc) definition design chain associates definition')"
170,"The H. Milton Stewart School of Industrial and Systems Engineering is a department in the Georgia Institute of Technology's College of Engineering dedicated to education and research in industrial engineering. The school is named after H. Milton Stewart, a local philanthropist and successful businessman who formerly graduated from the BSIE undergraduate program. 
Unlike similar programs at other schools, the School of Industrial and Systems Engineering at Georgia Tech focuses on core disciplines for both Industrial Engineering (such as manufacturing and quality control) and Systems Engineering (such as global logistics and system optimization). US News & World Report consistently ranks the program at number 1.


== External links ==
Official website","pandas(index=170, _1=170, text=""the h. milton stewart school of industrial and systems engineering is a department in the georgia institute of technology's college of engineering dedicated to education and research in industrial engineering. the school is named after h. milton stewart, a local philanthropist and successful businessman who formerly graduated from the bsie undergraduate program. unlike similar programs at other schools, the school of industrial and systems engineering at georgia tech focuses on core disciplines for both industrial engineering (such as manufacturing and quality control) and systems engineering (such as global logistics and system optimization). us news & world report consistently ranks the program at number 1.   == external links == official website"")"
171,"Material flow is the description of the transportation of raw materials, pre-fabricates, parts, components, integrated objects and final products as a flow of entities. The term applies mainly to advanced modeling of supply chain management.
As industrial material flow can easily become very complex, several different specialized simulation tools have been developed for complex systems. Typical tools are:

AnyLogic
AutoMod for logistics systems
Plant Simulation for production system


== References ==","pandas(index=171, _1=171, text='material flow is the description of the transportation of raw materials, pre-fabricates, parts, components, integrated objects and final products as a flow of entities. the term applies mainly to advanced modeling of supply chain management. as industrial material flow can easily become very complex, several different specialized simulation tools have been developed for complex systems. typical tools are:  anylogic automod for logistics systems plant simulation for production system   == references ==')"
172,"Industrial engineering is an engineering profession that is concerned with the optimization of complex processes, systems, or organizations by developing, improving and implementing integrated systems of people, money, knowledge, information and equipment.
Industrial engineers use specialized knowledge and skills in the mathematical, physical and social sciences, together with the principles and methods of engineering analysis and design, to specify, predict, and evaluate the results obtained from systems and processes.  From these results, they are able to create new systems, processes or situations for the useful coordination of labour, materials and machines and also improve the quality and productivity of systems, physical or social. Depending on the sub-specialties involved, industrial engineering may also overlap with, operations research, systems engineering, manufacturing engineering, production engineering, supply chain engineering, management science, management engineering, financial engineering, ergonomics or human factors engineering, safety engineering, logistics engineering or others, depending on the viewpoint or motives of the user.


== History ==


=== Origins ===


==== Industrial Engineering ====
There is a general consensus among historians that the roots of the industrial engineering profession date back to the Industrial Revolution.  The technologies that helped mechanize traditional manual operations in the textile industry including the flying shuttle, the spinning jenny, and perhaps most importantly the steam engine generated economies of scale that made mass production in centralized locations attractive for the first time.  The concept of the production system had its genesis in the factories created by these innovations.


==== Specialization of labor ====

Adam Smith's concepts of Division of Labour and the ""Invisible Hand"" of capitalism introduced in his treatise ""The Wealth of Nations"" motivated many of the technological innovators of the Industrial revolution to establish and implement factory systems.  The efforts of James Watt and Matthew Boulton led to the first integrated machine manufacturing facility in the world, including the application of concepts such as cost control systems to reduce waste and increase productivity and the institution of skills training for craftsmen.Charles Babbage became associated with Industrial engineering because of the concepts he introduced in his book ""On the Economy of Machinery and Manufacturers"" which he wrote as a result of his visits to factories in England and the United States in the early 1800s.  The book includes subjects such as the time required to perform a specific task, the effects of subdividing tasks into smaller and less detailed elements, and the advantages to be gained from repetitive tasks.


==== Interchangeable parts ====
Eli Whitney and Simeon North proved the feasibility of the notion of Interchangeable parts in the manufacture of muskets and pistols for the US Government.  Under this system, individual parts were mass-produced to tolerances to enable their use in any finished product.  The result was a significant reduction in the need for skill from specialized workers, which eventually led to the industrial environment to be studied later.


=== Pioneers ===
Frederick Taylor (1856 – 1915) is generally credited as being the father of the Industrial Engineering discipline.  He earned a degree in mechanical engineering from Steven's University and earned several patents from his inventions.  His books, Shop Management and The Principles of Scientific Management which were published in the early 1900s, were the beginning of Industrial Engineering.  Improvements in work efficiency under his methods was based on improving work methods, developing of work standards, and reduction in time required to carry out the work.  With an abiding faith in the scientific method, Taylor's contribution to ""Time Study"" sought a high level of precision and predictability for manual tasks.The husband-and-wife team of Frank Gilbreth (1868 – 1924) and Lillian Gilbreth (1878 – 1972) was the other cornerstone of the Industrial Engineering movement whose work is housed at Purdue University School of Industrial Engineering.  They categorized the elements of human motion into 18 basic elements called  therbligs.  This development permitted analysts to design jobs without knowledge of the time required to do a job. These developments were the beginning of a much broader field known as human factors or ergonomics.In 1908, the first course on Industrial Engineering was offered as an elective at Pennsylvania State University, which became a separate program in 1909 through the efforts of Hugo Diemer. The first doctoral degree in industrial engineering was awarded in 1933 by Cornell University.
In 1912 Henry Laurence Gantt developed the Gantt chart which outlines actions the organization along with their relationships. This chart opens later form familiar to us today by Wallace Clark.
With the development of assembly lines, the factory of Henry Ford (1913) accounted for a significant leap forward in the field. Ford reduced the assembly time of a car more than 700 hours to 1.5 hours. In addition, he was a pioneer of the economy of the capitalist welfare (""welfare capitalism"") and the flag of providing financial incentives for employees to increase productivity.
In 1927, the then Technische Hochschule Berlin was the first German university to introduce the degree.  The course of studies developed by Willi Prion was then still called ""Business and Technology"" and was intended to provide descendants of industrialists with an adequate education. 
Comprehensive quality management system (Total quality management or TQM) developed in the forties was gaining momentum after World War II and was part of the recovery of Japan after the war.
The American Institute of Industrial Engineering was formed in 1948. The early work by F. W. Taylor and the Gilbreths was documented in papers presented to the American Society of Mechanical Engineers as interest grew from merely improving machine performance to the performance of the overall manufacturing process; most notably starting with the presentation by Henry R. Towne (1844 - 1924) of his paper The Engineer as An Economist (1186).


=== Modern practice ===
In the 1960 to 1975, with the development of decision support systems in supply such as the Material requirements planning (MRP), one can emphasize the timing issue (inventory, production, compounding, transportation, etc.) of industrial organization. Israeli scientist Dr. Jacob Rubinovitz installed the CMMS program developed in IAI and Control-Data (Israel) in 1976 in South Africa and worldwide.
In the seventies, with the penetration of Japanese management theories such as Kaizen and Kanban, Japan realized very high levels of quality and productivity.  These theories improved issues of quality, delivery time, and flexibility.   Companies in the west realized the great impact of Kaizen and started implementing their own Continuous improvement programs.
In the nineties, following the global industry globalization process, the emphasis was on supply chain management and customer-oriented business process design. Theory of constraints developed by an Israeli scientist Eliyahu M. Goldratt (1985) is also a significant milestone in the field.


=== Compared to other engineering disciplines ===
Engineering is traditionally decompositional. To understand the whole of something, it is first broken down into its parts. One masters the parts, then puts them back together to create a better understanding of how to master the whole. The approach of Industrial and systems engineering(ISE) is opposite; any one part cannot be understood without the context of the whole system. Changes in one part of the system affect the entire system, and the role of a single part is to better serve the whole system.
Also, Industrial engineering considers the human factor and its relation to the technical aspect of the situation and the all of the other factors that influence the entire situation, while other engineering disciplines focus on the design of inanimate objects.
""Industrial Engineers integrate combinations of people, information, materials, and equipment that produce innovative and efficient organizations. In addition to manufacturing, Industrial Engineers work and consult in every industry, including hospitals, communications, e-commerce, entertainment, government, finance, food, pharmaceuticals, semiconductors, sports, insurance, sales, accounting, banking, travel, and transportation.""""Industrial Engineering is the branch of Engineering most closely related to human resources in that we apply social skills to work with all types of employees, from engineers to salespeople to top management. One of the main focuses of an Industrial Engineer is to improve the working environments of people – not to change the worker, but to change the workplace.""""All engineers, including Industrial Engineers, take mathematics through calculus and differential equations. Industrial Engineering is different in that it is based on discrete variable math, whereas all other engineering is based on continuous variable math. We emphasize the use of linear algebra and difference equations, as opposed to the use of differential equations which are so prevalent in other engineering disciplines. This emphasis becomes evident in optimization of production systems in which we are sequencing orders, scheduling batches, determining the number of materials handling units, arranging factory layouts, finding sequences of motions, etc. As, Industrial Engineers, we deal almost exclusively with systems of discrete components.""


== Etymology ==


=== Etymology ===
While originally applied to manufacturing, the use of ""industrial"" in ""industrial engineering"" can be somewhat misleading, since it has grown to encompass any methodical or quantitative approach to optimizing how a process, system, or organization operates. In fact, the ""Industrial"" in Industrial engineering means the ""industry"" in its broadest sense. People have changed the term ""industrial"" to broader terms such as Industrial and Manufacturing Engineering, Industrial and Systems Engineering, Industrial Engineering & Operations Research, Industrial Engineering & Management.


== Sub-disciplines ==
Industrial engineering has many sub-disciplines, the most common of which are listed below. Although there are industrial engineers who focus exclusively on one of these sub-disciplines, many deal with a combination of them such as Supply Chain and Logistics, and Facilities and Energy Management.Methods Engineering

Facilities Engineering & Energy Management
Financial Engineering
Energy Engineering
Human Factors & Safety Engineering
Information Systems Engineering & Management
Manufacturing Engineering

Operations Engineering & ManagementOperations Research & Optimization
Policy Planning

Production Engineering
Quality & Reliability Engineering

Supply Chain Management & LogisticsSystem Analysis
Systems Engineering
Systems Simulation
Related Disciplines
Organization Development & Change Management
Behavioral Economics


== Education ==
Industrial engineers study the interaction of human beings with machines, materials, information, procedures and environments in such developments and in designing a technological system.Universities offer degrees at the bachelor, masters, and doctoral level.


=== Undergraduate curriculum ===
In the United States, the undergraduate degree earned is the bachelor of science (B.S.) or bachelor of science and engineering (B.S.E.) in industrial engineering (IE). Variations of the title include Industrial & Operations Engineering (IOE), and Industrial & Systems Engineering (ISE or ISyE). The typical curriculum includes a broad math and science foundation spanning chemistry, physics, mechanics (i.e., statics, kinematics, and dynamics), materials science, computer science, electronics/circuits, engineering design, and the standard range of engineering mathematics (i.e., calculus, linear algebra, differential equations, statistics). For any engineering undergraduate program to be accredited, regardless of concentration, it must cover a largely similar span of such foundational work - which also overlaps heavily with the content tested on one or more engineering licensure exams in most jurisdictions.
The coursework specific to IE entails specialized courses in areas such as optimization, applied probability, stochastic modeling, design of experiments, statistical process control, simulation, manufacturing engineering, ergonomics/safety engineering, and engineering economics. Industrial engineering elective courses typically cover more specialized topics in areas such as manufacturing, supply chains and logistics, analytics and machine learning, production systems, human factors and industrial design, and service systems.Certain business schools may offer programs with some overlapping relevance to IE, but the engineering programs are distinguished by a much more intensely quantitative focus, required engineering science electives, and the core math and science courses required of all engineering programs.


=== Graduate curriculum ===
The usual graduate degree earned is the master of science (MS) or master of science and engineering (MSE) in industrial engineering or various alternative related concentration titles.
Typical MS curricula may cover:


=== Differences in teaching ===
While industrial engineering as a formal degree has been around for years, consensus on what topics should be taught and studied differs across countries. For example, Turkey focuses on a very technical degree while Denmark, Finland and the United Kingdom have a management focus degree, thus making it less technical. The United States, meanwhile, focuses on case-studies, group problem solving and maintains
a balance between the technical and non technical side.


== Practicing engineers ==
Traditionally, a major aspect of industrial engineering was planning the layouts of factories and designing assembly lines and other manufacturing paradigms. And now, in lean manufacturing systems, industrial engineers work to eliminate wastes of time, money, materials, energy, and other resources.
Examples of where industrial engineering might be used include flow process charting, process mapping, designing an assembly workstation, strategizing for various operational logistics, consulting as an efficiency expert, developing a new financial algorithm or loan system for a bank, streamlining operation and emergency room location or usage in a hospital, planning complex distribution schemes for materials or products (referred to as supply-chain management), and shortening lines (or queues) at a bank, hospital, or a theme park.
Modern industrial engineers typically use predetermined motion time system, computer simulation (especially discrete event simulation), along with extensive mathematical tools for modeling, such as mathematical optimization and queueing theory, and computational methods for system analysis, evaluation, and optimization. Industrial engineers also use the tools of data science and machine learning in their work owing to the strong relatedness of these disciplines with the field and the similar technical background required of industrial engineers (including a strong foundation in probability theory, linear algebra, and statistics, as well as having coding skills).


== See also ==


=== Related topics ===


=== Associations ===

Industrial Revolution


== Notes ==


== Further reading ==
Badiru, A. (Ed.) (2005).  Handbook of industrial and systems engineering. CRC Press. ISBN 0-8493-2719-9.
B. S. Blanchard and Fabrycky, W. (2005).  Systems Engineering and Analysis (4th Edition).  Prentice-Hall. ISBN 0-13-186977-9.
Salvendy, G. (Ed.) (2001).  Handbook of industrial engineering: Technology and operations management. Wiley-Interscience. ISBN 0-471-33057-4.
Turner, W. et al. (1992). Introduction to industrial and systems engineering (Third edition). Prentice Hall.  ISBN 0-13-481789-3.
Eliyahu M. Goldratt, Jeff Cox (1984). The Goal  North River Press; 2nd Rev edition (1992). ISBN 0-88427-061-0; 20th Anniversary edition (2004) ISBN 0-88427-178-1
Miller, Doug, Towards Sustainable Labour Costing in UK Fashion Retail (February 5, 2013). doi:10.2139/ssrn.2212100
Malakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley & Sons.ISBN 978-1-118-58537-5
Systems Engineering Body of Knowledge (SEBoK)
Traditional Engineering
Master of Engineering Administration (MEA)


== External links ==
 Media related to Industrial engineering at Wikimedia Commons","pandas(index=172, _1=172, text='industrial engineering is an engineering profession that is concerned with the optimization of complex processes, systems, or organizations by developing, improving and implementing integrated systems of people, money, knowledge, information and equipment. industrial engineers use specialized knowledge and skills in the mathematical, physical and social sciences, together with the principles and methods of engineering analysis and design, to specify, predict, and evaluate the results obtained from systems and processes.  from these results, they are able to create new systems, processes or situations for the useful coordination of labour, materials and machines and also improve the quality and productivity of systems, physical or social. depending on the sub-specialties involved, industrial engineering may also overlap with, operations research, systems engineering, manufacturing engineering, production engineering, supply chain engineering, management science, management engineering, financial engineering, ergonomics or human factors engineering, safety engineering, logistics engineering or others, depending on the viewpoint or motives of the user.   == history == industrial revolution   == notes ==   == further reading == badiru, a. (ed.) (2005).  handbook of industrial and systems engineering. crc press. isbn 0-8493-2719-9. b. s. blanchard and fabrycky, w. (2005).  systems engineering and analysis (4th edition).  prentice-hall. isbn 0-13-186977-9. salvendy, g. (ed.) (2001).  handbook of industrial engineering: technology and operations management. wiley-interscience. isbn 0-471-33057-4. turner, w. et al. (1992). introduction to industrial and systems engineering (third edition). prentice hall.  isbn 0-13-481789-3. eliyahu m. goldratt, jeff cox (1984). the goal  north river press; 2nd rev edition (1992). isbn 0-88427-061-0; 20th anniversary edition (2004) isbn 0-88427-178-1 miller, doug, towards sustainable labour costing in uk fashion retail (february 5, 2013). doi:10.2139/ssrn.2212100 malakooti, b. (2013). operations and production systems with multiple objectives. john wiley & sons.isbn 978-1-118-58537-5 systems engineering body of knowledge (sebok) traditional engineering master of engineering administration (mea)   == external links == media related to industrial engineering at wikimedia commons')"
173,"Blend time, sometimes termed mixing time, is the time to achieve a predefined level of homogeneity of a tracer in a mixing vessel. Blend time is an important parameter to evaluate the mixing efficiency of mixing devices.  In order to make this definition valid, the tracer should be in the same physical phase (e.g. liquid) as the bulk material.   
Blend time can be determined either with experiments or numerical modeling, such as computational fluid dynamics (CFD). The experimental methods to determine the blend time in liquid include conductivity method and discoloration method. The conductivity method requires a conductivity probe to present in the target system, which make it an intrusive method because the existence of the probe might change the mixing efficiency of the mixing device. Discoloration method does not require any probe which makes it a non-intrusive method. However, the color detection device (sometimes the human eye) needs to be calibrated against the conductivity method.  Both methods are usually applied to monitor the  concentration of the tracer in the most difficult to mix locations such as the area adjacent to the impeller shaft.
The benefit of numerical modeling is that once the modeling is completed, the blend time of any predetermined level of homogeneity of any location within the mixing system can be predicted, which is impossible to accomplish by experimental methods. However, numerical modeling needs to be validated by experimental methods.


== References ==","pandas(index=173, _1=173, text='blend time, sometimes termed mixing time, is the time to achieve a predefined level of homogeneity of a tracer in a mixing vessel. blend time is an important parameter to evaluate the mixing efficiency of mixing devices.  in order to make this definition valid, the tracer should be in the same physical phase (e.g. liquid) as the bulk material. blend time can be determined either with experiments or numerical modeling, such as computational fluid dynamics (cfd). the experimental methods to determine the blend time in liquid include conductivity method and discoloration method. the conductivity method requires a conductivity probe to present in the target system, which make it an intrusive method because the existence of the probe might change the mixing efficiency of the mixing device. discoloration method does not require any probe which makes it a non-intrusive method. however, the color detection device (sometimes the human eye) needs to be calibrated against the conductivity method.  both methods are usually applied to monitor the  concentration of the tracer in the most difficult to mix locations such as the area adjacent to the impeller shaft. the benefit of numerical modeling is that once the modeling is completed, the blend time of any predetermined level of homogeneity of any location within the mixing system can be predicted, which is impossible to accomplish by experimental methods. however, numerical modeling needs to be validated by experimental methods.   == references ==')"
174,"The Society for Health Systems (SHS) is a professional society within the Institute of Industrial and Systems Engineers to the support the industrial engineering profession and individuals involved with improving quality and productivity within healthcare.


== External links ==
Official website","pandas(index=174, _1=174, text='the society for health systems (shs) is a professional society within the institute of industrial and systems engineers to the support the industrial engineering profession and individuals involved with improving quality and productivity within healthcare.   == external links == official website')"
175,"Package testing or packaging testing involves the measurement of a characteristic or property involved with packaging. This  includes packaging materials, packaging components, primary packages, shipping containers, and unit loads, as well as the associated processes.
Testing measures the effects and interactions of the levels of packaging, the package contents, external forces, and end-use.
It can involve controlled laboratory experiments, subjective evaluations by people, or field testing.  Documentation is important:  formal test method, test report, photographs, video, etc.
Testing can be a qualitative or quantitative procedure.  Package testing is often a physical test. With some types of packaging such as food and pharmaceuticals, chemical tests are conducted to determine suitability of food contact materials.  Testing programs range from simple tests with little replication to more thorough experimental designs.
Package testing can extend for the full life cycle.  Packages can be tested for their ability to be recycled and their ability to degrade as surface litter, in a sealed landfill or under composting conditions.


== Purposes ==
Packaging testing might have a variety of purposes, such as:

Determine if, or verify that, the requirements of a specification, regulation, or contract are met
Decide if a new product development program is on track: Demonstrate proof of concept
Provide standard data for other scientific, engineering, and quality assurance functions
Validate suitability for end-use
Provide a basis for  technical communication
Provide a technical means of comparison of several options
Provide evidence in legal proceedings: product liability, patents, product claims, etc.
Help solve problems with current packaging
Help identify potential cost savings in packagingPackaging tests can be used for:

Subjecting packages (and contents) to stresses and dynamics found in the field
Reproducing the types of damage to packages and contents found in actual shipments
Controlling the uniformity of production of packages or components


== Importance of testing ==

For some types of products, package testing is mandated by regulations: food. pharmaceuticals, medical devices, dangerous goods, etc.  This may cover both the design qualification, periodic retesting, and control of the packaging processes.  Processes may be controlled by a variety of quality management systems such as HACCP, statistical process control, validation protocols, ISO 9000, etc.
For unregulated products, testing can be required by a contract or governing specification.  The degree of package testing can often be a business decision.  Risk management may involve factors such as

costs of packaging
costs of package testing
value of contents being shipped
value of customer's good will
product liability exposure
other potential costs of inadequate packaging
etc.With distribution packaging, one vital packaging development consideration is to determine if a packaged-product is likely to be damaged in the process of getting to the final customer. A primary purpose of a package is to ensure the safety of a product during transportation and storage. If a product is damaged during this process, then the package has failed to accomplish a primary objective and the customer will either return the product or be unlikely to purchase the product altogether.Package testing is often a formal part of Project management programs.  Packages are usually tested when there is a new packaging design, a revision to a current design, a change in packaging material, and various other reasons. Testing a new packaging design before full scale manufacturing can save  time and money.


== Laboratory affiliation ==
Many suppliers or vendors offer limited material and package testing as a free service to customers.  It is common for packagers to partner with reputable suppliers: Many suppliers have certified quality management systems such as ISO 9000 or allow customers to conduct  technical and quality audits.  Data from testing is commonly shared.  There is sometimes a risk that supplier testing may tend to be self-serving and not completely impartial.
Large companies often have their own packaging staff and a package testing and development laboratory.  Corporate engineers know their products, manufacturing capabilities, logistics system, and their customers best.  Cost reduction of existing products and cost avoidance for new products have been documented.Another option is to use a paid consultant, Independent contractor, and third-party independent testing laboratory.  They are commonly chosen for specialized expertise, for access to certain test equipment, for surge projects, or where independent testing is otherwise required.  Many have certifications and accreditations:  ISO 9000, ISO/IEC 17025, and various governing agencies.


== Procedures ==
Several standards organizations publish test methods for package testing.  Included are:

International Organization for Standardization, ISO
ASTM International
European Committee for Standardization. CEN
TAPPI
Military Standards
ISTA (International Safe Transit Association)
etc.Governments and regulators publish some packaging test methods.  There are also many corporate test standards in use.  A review of technical literature and patents provides good options to consider for test procedures.
Researchers are not restricted to the use of published standards but can modify existing test methods or develop procedures specific to their particular needs.  If a test is conducted with a deviation from a published test method or if a new method is employed, the test report must fully disclose the procedure.


== Materials testing ==

The basis of packaging design and performance is the component materials.  The physical properties, and sometimes chemical properties, of the materials need to be communicated to packaging engineers to aid in the design process.  Suppliers publish data sheets and other technical communications that include the typical or average relevant physical properties and the test method these are based upon.  Sometimes these are adequate.  Other times, additional material and component testing is required by the packager or supplier to better define certain characteristics.
When a final package design is complete, the specifications for the component materials needs to be communicated to suppliers.  Packaging materials testing is often needed to identify the critical material characteristics and engineering tolerances. These are used to prepare and enforce specifications.
For example, shrink film data might include: tensile strength (MD and CD), elongation,  Elastic modulus, surface energy,  thickness, Moisture vapor transmission rate, Oxygen transmission rate, heat seal strength, heat sealing conditions, heat shrinking conditions, etc.  Average and process capability are often provided. The chemical properties related for use as  Food contact materials may be necessary.


== Testing with people ==
Some types of package testing do not use scientific instruments but use people for the evaluation.
The regulations for child-resistant packaging require a test protocol that involves children. Samples of the test packages are given to a prescribed population of children. With specified 50-child panels, a high percentage must be unable to open a test package within 5 minutes.
Adults are also tested for their ability to open a child-resistant package.
Consumer packages are often evaluated by focus groups. People evaluate the package features in a room monitored by video cameras.  The consumer responses are treated qualitatively for feedback into the new packaging process.
Some food packagers use organoleptic evaluations.  People use their senses (taste, smell, etc.) to determine if a package component has tainted the food in the package.
A new package may be evaluated in a test market that uses people to try the packages at home.  Consumers have the opportunity to buy a product, perhaps with a coupon or discount.  Return postcards or Internet sites provide feedback to package developers.  Perhaps the most critical feedback is repeated sales items in the new package. Packaging evaluations are an important part of marketing research.
Legibility of text on packaging and labels is always subjective due to the inherent variations of people. Efforts have been made to help better quantify this by people in a laboratory: still using people for the evaluation but also employing a test apparatus to help reduce variability.Some laboratory tests are conducted but still result in an observation by people.  Some test procedures call for a judgment by test engineers whether or not pre-established acceptance criteria have been met.


=== Relevant standards ===
ASTM D7298  Test Method for Measurement of Comparative Legibility by Means of Polarizing Filter Instrumentation.
ASTM E460  Practice for Determining Effect of Packaging on Food and Beverage Products During Storage
ASTM E619  Practice for Evaluating Foreign Odors in Paper Packaging
ASTM E1870 Test Method for Odor and Taste Transfer from Polymeric Packaging Film
ASTM 2609  Test Method for Odor and Flavor Transfer from Rigid Polymeric Packaging
ISO 16820  Sensory Analysis – Methodology – Sequential Analysis
ISO 5495   Sensory Analysis – Methodology – Paired Comparisons
ISO 13302  Sensory Analysis – Methods for assessing modifications to the flavour of foodstuffs due to packaging


== Conditioning, testing atmosphere ==

The environmental conditions of testing are critical.  The measured performance of many packages is affected by the conditioning and testing atmospheres.  For example, paper based products are strongly affected by their moisture content:  Relative humidity needs to be controlled.  Plastic products are often strongly affected by temperature.Conditions of 23 °C (73.4 °F) and 50% relative humidity are common but other standard testing conditions are also published in material and package test standards. Engineering tolerances for the conditions are also specified.  Often the package is conditioned to the specified environment and tested under those conditions.  This can be in a conditioned room or in a chamber enclosing the test.  With some testing, the package is conditioned to a specified environment, then is removed to ambient conditions and quickly tested.  The test report needs to state the actual conditions used.
Engineers have found it important to know the effects of the full range of expected conditions on package performance.  This can be through investigating published technical literature, obtaining supplier documentation, or by conducting controlled tests at diverse conditions.


=== Relevant Standards ===
ASTM D4332- Standard Practice for Conditioning Containers, Packages, or Packaging Components for Testing
ASTM E171- Standard Specification for Standard Atmospheres for Conditioning and Testing Flexible Barrier Materials
ASTM F2825 Standard Practice for Climate Stressing of Packaging Systems for Single Parcel Delivery


== Degradation of product ==

Laboratory tests can help determine the shelf life of a package and its contents under a variety of conditions. This is particularly important for foods, pharmaceuticals, some chemicals, and a variety of products. The testing is usually product specific: the mechanisms of degradation are often different. Exposures to expected and elevated temperatures and humidities are commonly used for shelf life testing.  The ability of packaging to control product degradation is frequently a subject of laboratory and field evaluations.


=== Relevant tests ===
ASTM E2454 Standard Guide for Sensory Evaluation Methods to Determine the Sensory Shelf -life of Consumer Products
DoD 4140.27M Shelf Life Management Manual, 2000
ISO 11987 Ophthalmic Optics, Contact Lenses, Determination of Shelf Life


=== Barrier Properties ===

Many products degrade with exposure to the atmosphere: foods, pharmaceuticals, chemicals, etc.   The ability of a package to control the permeation and penetration of gasses is vital for many types of products.  Tests are often conducted on the packaging materials but also on the completed packages, sometimes after being subjected to flexing, handling, vibration, or temperature.


== Degradation of Packages ==
Packages can degrade with exposure to temperature, humidity, time, sterilization (steam, radiation, gas, etc.), sunlight, and other environmental factors. For some types of packaging, it is common to test for possible corrosion of metals, polymer degradation, and weather testing of polymers.  Several types of accelerated aging  of packaging and materials can be accomplished in a laboratory.
Exposure to elevated temperatures accelerates some degradation mechanisms.  An Arrhenius equation is often used to correlate certain chemical reactions at different temperatures, based on the proper choice of  Q10 coefficients.
As with any laboratory testing, validating field trials are important.


=== Relevant tests ===
ASTM D3045 Standard Practice for Heat Aging of Plastics without Load
ASTM F1640 Standard Guide for Packaging Materials for Foods to be Irradiated
ASTM F1980– Standard Guide for Accelerated Aging of Sterile Medical Device Packages
ASTM G151 Standard Practice for Exposing Non-metallic Materials in Accelerated Test Devices that are Laboratory Light Sources


== Vacuum testing ==

Vacuum chambers are used to test the ability of a package to withstand low pressures.  This can be to:

Determine the ability of packages to withstand low pressures that might be encountered.  this could be in an air shipment or high altitude truck shipment.
A laboratory vacuum places controlled stress on a sealed package to test the strength of seals, the tendency for leakage, and the ability to retain sterility.


=== Relevant tests ===
ASTM D3078- Standard Test Method for Determination of Leaks in Flexible Packaging by Bubble Emission
ASTM D4991- Standard Test Method for Leakage Testing of Empty Rigid Containers by Vacuum Method
ASTM D6653- Standard Test Methods for Determining the Effects of High Altitude on Packaging Systems by Vacuum Method
ASTM D6834- Standard Test Method for Determining Product Leakage from a Package with a Mechanical Pump Dispenser
ASTM E493- Standard Test Methods for Leaks Using the Mass Spectrometer Leak Detector in the Inside-Out Testing Mode
ASTM F2338- Standard Test Method for Nondestructive Detection of Leaks in Packages by Vacuum Decay Method
ASTM F2391- Standard Test Method for Measuring Package and Seal Integrity Using Helium as the Tracer Gas


== Shock and impact ==

Both primary (consumer) packages and shipping containers have a risk of being dropped or being impacted by other items.  Package integrity and product protection are important packaging functions. Tests are conducted to measure the resistance of packages and products to controlled laboratory shock and impact.
Testing also determines the effectiveness of package cushioning to isolate fragile products from shock. Instrumentation is used to measure the shock transmitted to a cushioned product.


=== Relevant tests ===
ASTM D880- Standard Test Method for Impact Testing for Shipping Containers and Systems
ASTM D1596- Standard Test Method for Dynamic Shock Cushioning Characteristics of Packaging Materials
ASTM D3332- Standard Test Methods for Mechanical-Shock Fragility of Products, Using Shock Machines
ASTM D4003- Standard Test Methods for Programmable Horizontal Impact Test for Shipping Containers and Systems
ASTM D5265- Standard Test Method for Bridge Impact Testing
ASTM D5276- Standard Test Method for Drop Test of Loaded Containers by Free Fall
ASTM D5277- Standard Test Method for Performing Programmed Horizontal Impacts Using an Inclined Impact Tester
ASTM D5487- Standard Test Method for Simulated Drop of Loaded Containers by Shock Machines
ASTM D6344- Standard Test Method for Concentrated Impacts to Transport Packages
ASTM D6537- Standard Practice for Instrumented Package Shock Testing For Determination of Package Performance
MIL-STD-810G


== Package Insulation ==
Many packages are used for products that are sensitive to temperature.  The ability of insulated shipping containers to protect their contents from exposure to temperature fluctuations can be measured in a laboratory. The testing can be of empty containers or of full containers with appropriate jell or ice packs, contents, etc.  Ovens, freezers, and environmental chambers are commonly used for this and other types of packaging.
Digital temperature data loggers are used to measure temperatures experienced in different distribution systems.  This data is sometimes used to develop unique laboratory  test methods for that distribution system.


=== Relevant tests ===
ASTM D3103-Standard Test Method for Thermal Insulation Performance of Distribution Packages
ISTA 7E – Testing Standard for Thermal Transport Packaging Used in Parcel Delivery System Shipment


== Thermal shock ==
Some packages, particularly glass, can be sensitive to sudden changes in temperature: Thermal shock.   One method of testing involves rapid movement from cold to hot water baths, and back.


=== Relevant tests ===
ASTM C149  -Standard Test Method for Thermal Shock Resistance of Glass Containers
MIL-STD-810G METHOD 503.5


== Handles ==
Package handles (and hand holes in packages) assist carrying and handling packages. Objective laboratory  procedures are frequently used to help determine performance.  Fixtured ‘’hands’’ of various designs are used to hold a handle (sometimes two handles for a box).  Most common are “jerk testing’’ by modified drop test procedures or use of  the constant pull rates of a universal testing machine.  Other procedures use a static force by hanging a heavily loaded package for an extended time or even using a centrifuge.


=== Relevant tests ===
ASTM D6804, Standard Guide for Hand Hole Design in Corrugated Boxes, Appendix
ASTM F852 Specification for Portable Gasoline, Kerosene, and Diesel Containers for Consumer Use, section 7.2
Centrifugal test of beverage carrier handle


== Vibration ==

Vibration is encountered during shipping (vehicle vibration, rough roads, etc.) and movement on conveyors.  Potential vibration damage may include:

fractures and fatigue damage
loose wires, screw caps, etc.
bruises on soft products (fruit, etc.)
surface abrasion
etc.The ability of a package to withstand these vibrations and to protect the contents can be measured by several laboratory test procedures.  Some allow searching for the particular frequencies of vibration that have potential for damage. Modal testing methodologies are sometimes employed. Others use specified bands of random vibration to better represent complex vibrations measured in field studies of distribution environments.


=== Relevant tests ===
ASTM D999- Standard Test Methods for Vibration Testing of Shipping Containers
ASTM D3580-Standard Test Methods for Vibration (Vertical Linear Motion) Test of Products
ASTM D4728- Standard Test Method for Random Vibration Testing of Shipping Containers
ASTM D5112- Standard Test Method for Vibration (Horizontal Linear Sinusoidal Motion) Test of Products
ASTM D7387- Standard Test Method for Vibration Testing of Intermediate Bulk Containers (IBCs) Used for Shipping Liquid Hazardous Materials (Dangerous Goods)


== Compression ==

Compression testing relates to stacking or crushing of packages, particularly shipping containers.  It usually measures of the force required to crush a package, stack of packages, or a unit load.  Packages can be empty or filled as for shipment.   A force-deflection curve used to obtain the peak load or other desired points.  Other tests use a constant load and measure the time to failure or to a critical deflection.
Dynamic compression is sometimes tested by shock or impact testing with an additional load to crush the test package.  Dynamic compression also takes place in stacked vibration testing.


=== Relevant Tests ===
ASTM Standard D642 Test Method for Determining Compressive Resistance of Shipping Containers, Components, and Unit Loads.
ASTM Standard D4577 Test Method for Compression Resistance of a Container Under Constant Load
ASTM Standard D7030 Test Method for Short Term Creep Performance of Corrugated Fiberboard Containers Under Constant Load Using a Compression Test Machine
German Standard DIN 55440-1 Packaging Test; compression test; test with a constant conveyance-speed
ISO 12048  Packaging—Complete, filled transport packages—Compression and stacking tests using a compression tester


== Large loads ==

Large pallet loads, bulk boxes,  wooden boxes, and crates can be evaluated by many of the other test procedures previously listed.  In addition, some special test methods are available for these larger loads.


=== Relevant tests ===
ASTM D5331- Standard Test Method for Evaluation of Mechanical Handling of Unitized Loads Secured with Stretch Wrap Films
ASTM D5414- Standard Test Method for Evaluation of Horizontal Impact Performance of Load Unitizing Stretch Wrap Films
ASTM D5415- Standard Test Method for Evaluating Load Containment Performance of Stretch Wrap Films by Vibration Testing
ASTM D5416- Standard Test Method for Evaluating Abrasion Resistance of Stretch Wrap Films by Vibration Testing
ASTM D6055- Standard Test Methods for Mechanical Handling of Unitized Loads and Large Shipping Cases and Crates
ASTM D6179- Standard Test Methods for Rough Handling of Unitized Loads and Large Shipping Cases and Crates
ISO 10531- Stability testing of unit loads


== Bar codes ==
Package bar codes are evaluated for several aspects of legibility by bar code verifiers as part of a continuing  quality program.  More thorough validation may include evaluations after use (and abuse) testing such as sunlight, abrasion, impact, moisture, etc.


=== Relevant tests ===
ISO/IEC 15426    Information technology - Automatic identification and data capture techniques - Bar code verifier conformance specification - Part 1: Linear symbols, Part 2: Two-dimensional symbols


== Test Protocols for Shipping Containers ==
Shipping containers are often subjected to sequential tests involving a combination of individual test methods.  A variety of standard  test schedules or protocols are available for evaluating transport packaging.  They are used to help determine the ability of complete and filled shipping containers to various types of logistics systems.  Some test the general ruggedness of the shipping container while others have been shown to reproduce the types of damage encountered in distribution.  Some base the type and severity of testing on formal studies of the distribution environment: instrumentation, data loggers, and observation. Test cycles with these documented elements better simulate parts of certain logistics shipping environments.

ASTM International
ASTM D4169- Standard Practice for Performance Testing of Shipping Containers and Systems
ASTM D7386- Standard Practice for Performance Testing of Packages for Single Parcel Delivery Systems.
ISO
ISO 4180:2009   Packaging – Complete filled transport packages – General rules for the compilation of performance test schedulesInternational Safe Transit Association
Procedure 1A: Packaged-Products weighing 150 lb (68 kg) or Less
Procedure 1B: Packaged-Products weighing Over 150 lb (68 kg)
Procedure 1C: Extended Testing for Individual Packaged-Products weighing 150 lb (68 kg) or Less
Procedure 1D: Extended Testing for Individual Packaged-Products weighing Over 150 lb (68 kg)
Procedure 1E: Unitized Loads
Procedure 1G: Packaged-Products weighing 150 lb (68 kg) or Less (Random Vibration)
Procedure 1H: Packaged-Products weighing Over 150 lb (68 kg) (Random Vibration)
Procedure 2A: Packaged-Products weighing 150 lb (68 kg) or Less
Procedure 2B: Packaged-Products weighing over 150 lb (68 kg)
Procedure 2C: Furniture Packages
Procedure 3A: Packaged-Products for Parcel Delivery System Shipments 70 kg (150 lb) or Less (standard, small, flat or elongated)
Procedure 3B: Packaged-Products for Less-Than-Truckload (LTL) Shipment
Procedure 3E: Unitized Loads of Same Product
Procedure 3F: Packaged Products for Distribution Center to Retail Outlet Shipment 100 lb (45 kg)
Procedure 3H: Performance Test for Products or Packaged-Products in Mechanically Handled Bulk Transport Containers
Project 3K: Fast Moving Consumer Goods for the European Retail Supply Chain
Project 4AB: Enhanced Simulation Performance Tests (online test planner)
6-FEDEX-A: FedEx Procedures for Testing Packaged Products Weighing Up to 150 lbs
6-FEDEX-B: FedEx Procedures for Testing Packaged Products Weighing Over 150 lbs
6-SAMSCLUB, Packaged-Products for Sam’s Club® Distribution System Shipment
Procedure 7D: Thermal Controlled Transport Packaging for Parcel Delivery System Shipment
ISTA 7E: Testing Standard for Thermal Transport Packaging Used in Parcel Delivery System


=== Field trials ===
Laboratory testing can often help identify shipping container constructions that, in general, should perform well in the field.  Of course, laboratory tests cannot fully reproduce the full range of field hazards, their magnitudes, nor their frequency.  Field experiments are often conducted to help validate the laboratory testing.The advantage of laboratory testing is that it subjects replicate packages to identical sets of test sequences: a relatively small number of samples often can suffice.  Field hazards, by their nature, are highly variable: thus repeated shipments do not receive the same types or magnitudes of drops, vibrations, kicks, impacts, abrasion, etc.  Because of this uncontrolled variability, more replicate sample shipments are often necessary.Larger scale test markets are used to give additional assurance of performance and acceptability for a new or revised packaged-product.  Feedback is carefully obtained and evaluated.   Feedback on package performance continues when full production and distribution have been achieved.


== Product requirements ==
In addition, package testing often relates to the specific product inside the package.  Some broad categories of products and special package testing considerations follow:


=== Food packaging ===
Foods categories such as fresh produce, frozen foods, irradiated foods, fresh fish, canned foods, etc. have regulatory requirements and special packaging needs.  Package testing often relates to:

Food safety
Compatibility of the package with the food
Migration of material from the packaging to the food
Shelf life
Barrier properties, porosity, package atmosphere, etc
Special quality assurance needs, good manufacturing practices, HACCP, validation protocols, etc


=== Pharmaceutical packaging ===
Packaging for drugs and pharmaceuticals is highly regulated.  Special testing needs include:

Safety of drugs and pharmaceuticals
Barrier properties
Shelf life
Compatibility of package with the drugs
Sterility
Tamper resistance, child resistance, etc
Special quality assurance needs, good manufacturing practices, validation protocols, etc


=== Medical Packaging ===
Packaging for medical materials, medical devices, health care supplies, etc., have special user requirements and is highly regulated. Barrier properties, durability, visibility, sterility and strength need to be controlled; usually with documented test results for initial designs and for production.
Assurance of sterility and suitability for use are critical.  For example, medical devices and products are often sterilized in the package.  The sterility must be maintained throughout distribution to allow immediate use by physicians.  A series of special packaging tests is used to measure the ability of the package to maintain sterility.  Verification and validation protocols are rigidly maintained.

Relevant standards
ASTM F1585 – Guide for Integrity Testing of Porous Medical Packages
ASTM D3078 – Standard Test Method for Detection of Leaks in Flexible Packaging (Bubble)
ASTM F1140 – Standard Test Methods for Internal Pressurization Failure Resistance of Unrestrained Packages
ASTM F1608 – Standard Test Method for Microbial Ranking of Packaging Materials
ASTM F1929 – Standard Test Method for Detecting Seal Strength in Porous Medical Packaging by Dye Penetration
ASTM F2054 – Standard Test Method for Burst Testing of Flexible Package Seals Using Internal Air Pressurization Within Restraining Plates
ASTM F2095 – Standard Test Methods for Pressure Decay Leak Test for Flexible Packages With and Without Restraining Plates
ASTM F2096 – Standard Test Method for Detecting Gross Leaks in Medical Packaging by Internal Pressurization
ASTM F2097 – Standard Guide for Design and Evaluation of Primary Flexible Packaging for Medical Products
ASTM F2228 – Standard Test Method for Non-Destructive Detection of Leaks in Medical Packaging Which Incorporates Pourous Barrier Material by CO2 Tracer Gas
ASTM F2391 – Standard Test Method for Measuring Package and Seal Integrity using Helium as the Tracer Gas
ASTM F3039 - Standard Test Method for Detecting Leaks in Nonporous Packaging or Flexible Barrier Materials by Dye Penetration
EN 868-1 – Packaging materials and systems for medical devices which are to be sterilized. General requirements and test methods (superseded by ISO 11607-1)
ISO 11607-1 – Packaging for terminally sterilized medical devices -- Part 1: Requirements for materials, sterile barrier systems and packaging systems
ISO 11607-2 – Packaging for terminally sterilized medical devices -- Part 2: Validation for Forming, Sealing, and Assembly Processes


=== Dangerous Goods ===
Packaging of hazardous materials, or dangerous goods, are highly regulated.  There are some material and construction requirements but also performance testing is required. The testing is based on the packing group (hazard level) of the contents, the quantity of material, and the type of container.
Research into improvements is continuing.
Relevant standards
ASTM D4919- Standard Specification for Testing of Hazardous Materials Packaging
ASTM D7387- Standard Test Method for Vibration Testing of Intermediate Bulk Containers (IBCs) Used for Shipping Liquid Hazardous Materials (Dangerous Goods)
ASTM D7760 Standard Guide for Conducting Internal Pressure Tests on United Nations (UN) Packagings
ASTM D7887 Standard Guide for Selection of Substitute, Non-hazardous, Liquid Filling Substances for Packagings Subjected to the United Nations Performance Tests
ASTM D7790: Standard Guide for Preparation of Plastic Packagings Containing Liquids for United Nations (UN) Drop TestingUN Recommendations on the Transport of Dangerous Goods
ISO 16104 – 2003  Packaging – Transport packaging for dangerous goods – Test methods


== See also ==
Data analysis
Nondestructive testing
Verification and validation


== References ==


=== Books, General References ===
ASTM STP 1294  Durability Testing of Nonmetallic Materials, 1996
Lockhart, H., and Paine, F.A.,  ""Packaging of Pharmaceuticals and Healthcare Products"", 2006, Blackie, ISBN 0-7514-0167-6
Meisner, ""Transport Packaging"", Third Edition, IoPP, 2016
Pilchik, R., ""Validating Medical Packaging"" 2002, ISBN 1-56676-807-1
Robertson, G. L., ""Food Packaging"", 2005, ISBN 0-8493-3775-5
Russel, P G,  and Daum, M P, ""Product Protection Test Book"", IoPP
Soroka, W, ""Fundamentals of Packaging Technology"", IoPP, 2002, ISBN 1-930268-25-4
Yam, K. L., ""Encyclopedia of Packaging Technology"", John Wiley & Sons, 2009, ISBN 978-0-470-08704-6
Guidelines for Selecting and Using ISTA Procedures and Projects, ISTA, 2013


== External links ==
Institute of Packaging Professionals
International Safe Transit Association (ISTA)
American Society for Testing and Materials
Safe Load Testing Technologies, 'All you need to know about Amazon ISTA Test', 2018","pandas(index=175, _1=175, text='package testing or packaging testing involves the measurement of a characteristic or property involved with packaging. this  includes packaging materials, packaging components, primary packages, shipping containers, and unit loads, as well as the associated processes. testing measures the effects and interactions of the levels of packaging, the package contents, external forces, and end-use. it can involve controlled laboratory experiments, subjective evaluations by people, or field testing.  documentation is important:  formal test method, test report, photographs, video, etc. testing can be a qualitative or quantitative procedure.  package testing is often a physical test. with some types of packaging such as food and pharmaceuticals, chemical tests are conducted to determine suitability of food contact materials.  testing programs range from simple tests with little replication to more thorough experimental designs. package testing can extend for the full life cycle.  packages can be tested for their ability to be recycled and their ability to degrade as surface litter, in a sealed landfill or under composting conditions.   == purposes == packaging testing might have a variety of purposes, such as:  determine if, or verify that, the requirements of a specification, regulation, or contract are met decide if a new product development program is on track: demonstrate proof of concept provide standard data for other scientific, engineering, and quality assurance functions validate suitability for end-use provide a basis for  technical communication provide a technical means of comparison of several options provide evidence in legal proceedings: product liability, patents, product claims, etc. help solve problems with current packaging help identify potential cost savings in packagingpackaging tests can be used for:  subjecting packages (and contents) to stresses and dynamics found in the field reproducing the types of damage to packages and contents found in actual shipments controlling the uniformity of production of packages or components   == importance of testing ==  for some types of products, package testing is mandated by regulations: food. pharmaceuticals, medical devices, dangerous goods, etc.  this may cover both the design qualification, periodic retesting, and control of the packaging processes.  processes may be controlled by a variety of quality management systems such as haccp, statistical process control, validation protocols, iso 9000, etc. for unregulated products, testing can be required by a contract or governing specification.  the degree of package testing can often be a business decision.  risk management may involve factors such as  costs of packaging costs of package testing value of contents being shipped value of customer\'s good will product liability exposure other potential costs of inadequate packaging etc.with distribution packaging, one vital packaging development consideration is to determine if a packaged-product is likely to be damaged in the process of getting to the final customer. a primary purpose of a package is to ensure the safety of a product during transportation and storage. if a product is damaged during this process, then the package has failed to accomplish a primary objective and the customer will either return the product or be unlikely to purchase the product altogether.package testing is often a formal part of project management programs.  packages are usually tested when there is a new packaging design, a revision to a current design, a change in packaging material, and various other reasons. testing a new packaging design before full scale manufacturing can save  time and money.   == laboratory affiliation == many suppliers or vendors offer limited material and package testing as a free service to customers.  it is common for packagers to partner with reputable suppliers: many suppliers have certified quality management systems such as iso 9000 or allow customers to conduct  technical and quality audits.  data from testing is commonly shared.  there is sometimes a risk that supplier testing may tend to be self-serving and not completely impartial. large companies often have their own packaging staff and a package testing and development laboratory.  corporate engineers know their products, manufacturing capabilities, logistics system, and their customers best.  cost reduction of existing products and cost avoidance for new products have been documented.another option is to use a paid consultant, independent contractor, and third-party independent testing laboratory.  they are commonly chosen for specialized expertise, for access to certain test equipment, for surge projects, or where independent testing is otherwise required.  many have certifications and accreditations:  iso 9000, iso/iec 17025, and various governing agencies.   == procedures == several standards organizations publish test methods for package testing.  included are:  international organization for standardization, iso astm international european committee for standardization. cen tappi military standards ista (international safe transit association) etc.governments and regulators publish some packaging test methods.  there are also many corporate test standards in use.  a review of technical literature and patents provides good options to consider for test procedures. researchers are not restricted to the use of published standards but can modify existing test methods or develop procedures specific to their particular needs.  if a test is conducted with a deviation from a published test method or if a new method is employed, the test report must fully disclose the procedure.   == materials testing ==  the basis of packaging design and performance is the component materials.  the physical properties, and sometimes chemical properties, of the materials need to be communicated to packaging engineers to aid in the design process.  suppliers publish data sheets and other technical communications that include the typical or average relevant physical properties and the test method these are based upon.  sometimes these are adequate.  other times, additional material and component testing is required by the packager or supplier to better define certain characteristics. when a final package design is complete, the specifications for the component materials needs to be communicated to suppliers.  packaging materials testing is often needed to identify the critical material characteristics and engineering tolerances. these are used to prepare and enforce specifications. for example, shrink film data might include: tensile strength (md and cd), elongation,  elastic modulus, surface energy,  thickness, moisture vapor transmission rate, oxygen transmission rate, heat seal strength, heat sealing conditions, heat shrinking conditions, etc.  average and process capability are often provided. the chemical properties related for use as  food contact materials may be necessary.   == testing with people == some types of package testing do not use scientific instruments but use people for the evaluation. the regulations for child-resistant packaging require a test protocol that involves children. samples of the test packages are given to a prescribed population of children. with specified 50-child panels, a high percentage must be unable to open a test package within 5 minutes. adults are also tested for their ability to open a child-resistant package. consumer packages are often evaluated by focus groups. people evaluate the package features in a room monitored by video cameras.  the consumer responses are treated qualitatively for feedback into the new packaging process. some food packagers use organoleptic evaluations.  people use their senses (taste, smell, etc.) to determine if a package component has tainted the food in the package. a new package may be evaluated in a test market that uses people to try the packages at home.  consumers have the opportunity to buy a product, perhaps with a coupon or discount.  return postcards or internet sites provide feedback to package developers.  perhaps the most critical feedback is repeated sales items in the new package. packaging evaluations are an important part of marketing research. legibility of text on packaging and labels is always subjective due to the inherent variations of people. efforts have been made to help better quantify this by people in a laboratory: still using people for the evaluation but also employing a test apparatus to help reduce variability.some laboratory tests are conducted but still result in an observation by people.  some test procedures call for a judgment by test engineers whether or not pre-established acceptance criteria have been met. astm stp 1294  durability testing of nonmetallic materials, 1996 lockhart, h., and paine, f.a.,  ""packaging of pharmaceuticals and healthcare products"", 2006, blackie, isbn 0-7514-0167-6 meisner, ""transport packaging"", third edition, iopp, 2016 pilchik, r., ""validating medical packaging"" 2002, isbn 1-56676-807-1 robertson, g. l., ""food packaging"", 2005, isbn 0-8493-3775-5 russel, p g,  and daum, m p, ""product protection test book"", iopp soroka, w, ""fundamentals of packaging technology"", iopp, 2002, isbn 1-930268-25-4 yam, k. l., ""encyclopedia of packaging technology"", john wiley & sons, 2009, isbn 978-0-470-08704-6 guidelines for selecting and using ista procedures and projects, ista, 2013   == external links == institute of packaging professionals international safe transit association (ista) american society for testing and materials safe load testing technologies, \'all you need to know about amazon ista test\', 2018')"
176,"The Indian Institution of Industrial Engineering (IIIE) is a non-profit organization and registered society for propagating the profession of Industrial Engineering in India. It was founded in 1957 and is a Registered Public Trust under the Bombay Public Trust Act, 1950.The headquarters is at Navi Mumbai. IIIE is a member organization of Engineering Council of India.The IIIE has instituted many honors and awards for various achievements and outstanding contribution to the industrial engineering profession for individuals and Performance Excellence Awards for Organisations.


== National Council ==

A National Council consisting of twelve elected representative from among Corporate
Members and six representatives from the Chapters is the Executive body of the Institution
which is located in Navi Mumbai. The office bearers - A Chairman, two Vice-Chairmen, an
Hon. Secretary, two Hon. Jt. Secretaries and an Hon. Treasurer for each year are elected
by the National Council from among its members.
President of the Institution is nominated by the National Council each year.


== Membership ==
There are three classes of Corporate Membership, viz. Fellow, Member and Associate
Member. Other classes of membership are : (i) Honorary Membership conferred by the
National Council in recognition of outstanding services in the fields of Industrial Engineering
and Management Sciences (ii) Affiliate (iii) Graduate (iv) Student and (v) Institutional
Mernbership.
A member can join seminars, workshops, training programs, special lectures, industry visits and other professional activities of Institution.
Fellows, Members, Associate Members and Graduate Members of the Institution shall be permitted to affix the appropriate symbols as given below to their names.
Fellow - (F.I.I.E), Member - (M.I.I.E), Associate Member - (A.M.I.I.E), Graduate Member - (Grad. I.I.E)
After Graduateship the student is entitled to use below title
ASSOCIATE MEMBER OF INSTITUTE OF INDUSTRIAL ENGINEERING (A.M.I.I.E)
This is equal to B.E (Bachelor's of Engineering), B.tech (Bachelor's of Technology) and A.M.I.E (Associate member of Institute of Engineering)


== Chapters ==
The Institution has twenty nine chapters, distributed all over India, at Aurangabad, Allahabad, Bangalore, Baroda, Bhilai, Bhopal, Kolkata, Kozhikode, Chennai, Cochin, Coimbatore, Orissa, Durgapur, Hyderabad, Jamshedpur, Kanpur, Lucknow, Mumbai, Nagpur, New Delhi, Pune, Ranchi, Rourkela, Tiruchirappalli, Surat, Trivandrum, Visakhapatnam, Udaipur and Goa.


== Student Chapters ==
The Institution has two recognized active students chapters in India: ITER and PSG College of Technology. Both are recognized and function under their respective home chapter.


== Journal ==
The Institution brings out a monthly Journal entitled ""Industrial Engineering Journal"".It publishes papers and articles relating to application of industrial engineering and
management techniques, including research work.


== Examination ==
The Institution conducts Graduateship Examination as external Examination for enabling candidates to qualify for GRADUATE MEMBERSHIP of the institution. The graduateship examination conducted by the institution (AMIIE) is recognized by the Government of India as equivalent to bachelor's degree in industrial engineering from any recognized university.IIIE Graduates are eligible for appearing the  Graduate Aptitude Test in Engineering (GATE) conducted by IITs and IISc.The Examinations are open only for the student Members of the Institution and whose membership continues to remain Valid.
The requirements for eligibility to student Membership of the Institution:
Person should not be less than 18 years of age and should possess one of the following qualifications:
1) A pass in the Higher Secondary (XII Standard) Examination under the 10 + 2 + 3 Scheme of Education, of a Statutory Board of Higher Secondary Education, the Intermediate Examination, recognized as equivalent thereto and has at least two years working experience in an organisation.
2) Completed and passed first two years of regular degree course in Engineering / Technology.
3) A Graduate in Science stream only and has at least one year working experience in an organisation.
4) A Diploma (3 yrs course) in Engineering/technology recognized by the concerned Director of Technical Education and at least one year working experience in an organisation. In case, a candidate has undergone 1 year service training as a part of the course recognised by Board of Apprenticeship Training, Govt. of India. This will be counted as working experience.
5) A Degree in Engineering / Technology of a University in India or equivalent qualification recognised by the All India Council for Technical Education (AICTE) as equivalent.


== Scheme,Subjects and Recognition of the Examination ==
The Examinations consist of three parts of written papers viz. the Preliminary, the Section A and the Section B. In addition, a project work has to be undertaken and Report submitted for acceptance. A student can appear for Section A papers only after completely passing the preliminary (or exempted from the preliminary). For appearance in a section, a student must have passed the previous section fully.
The Student will have to carry out an approved project in an organisation under a project guide. A Project Report has to be submitted to the Board of Examinations after the completion of the Project, for its acceptance.
The student is considered to have completed the Graduateship Examination only after his/her Project Report has been accepted by the Board of Examinations. Graduateship examination of Indian Institution of Industrial Engineering is recognised by Association of Indian Universities, Ministry of education & Social Welfare India at par with Bachelor Degree in Industrial Engineering of an Indian university. Indian Institution of Industrial Engineering is also a member of Engineering Council of India.
http://www.iiie-india.com/IIIE/images/GOVT-OF-INDIA.pdf
http://www.iiie-india.com/IIIE/images/AIU.pdf
http://www.iiie-india.com/IIIE/images/Mumbai-University1.pdf
http://www.ecindia.org/
http://www.iiie-india.com/IIIE/images/IGNOU1.pdf
http://www.iiie-india.com/IIIE/images/The-Institution-of-Engineers.pdf


== Awards and honors ==
The IIIE has instituted many Honors and Awards for various achievements and outstanding contributions to the Industrial Engineering profession for members, non-members, chapters and other bodies. All Honors and Awards are conferred by the IIIE National Council at its National Convention.
1 - Lillian Gilbreth Award - Awarded annually to the person making outstanding contribution to the profession of Industrial Engineering.
2 - Honorary Membership - Awarded to individual's professional achievements recognized nationally or internationally.
3 -Ramaswamy Cup - Donated by Mr. S. S. Rangnekar - The Cup is awarded annually to the person making outstanding contribution to the institution.
4 - H. K. Firodia Award - Donated by Mr. H. K. Firodia, Past President - Cash Award, a Medal and Certificate.
5 Cdr Dr. Bhaskar Bhandarkar Award  for Visionary leadership ( Donated by Late H. N Thadani , Founder President
6 - Krish Pennathur Shield - Donated by Dr. K. Pennathur - The shield is awarded annually to the BEST CHAPTER of the year of its outstanding overall performance.
7 - Fellowship - Awarded to the Members in recognition of their outstanding contribution to the profession and the Institution.


== See also ==


== References ==


== External links ==
Official Site - IIIE
IIIE Exam Papers & References
IIIE Hyderabad Chapter
IIIE Cuttack Chapter
IIIE Pune Chapter","pandas(index=176, _1=176, text='the indian institution of industrial engineering (iiie) is a non-profit organization and registered society for propagating the profession of industrial engineering in india. it was founded in 1957 and is a registered public trust under the bombay public trust act, 1950.the headquarters is at navi mumbai. iiie is a member organization of engineering council of india.the iiie has instituted many honors and awards for various achievements and outstanding contribution to the industrial engineering profession for individuals and performance excellence awards for organisations.   == national council ==  a national council consisting of twelve elected representative from among corporate members and six representatives from the chapters is the executive body of the institution which is located in navi mumbai. the office bearers - a chairman, two vice-chairmen, an hon. secretary, two hon. jt. secretaries and an hon. treasurer for each year are elected by the national council from among its members. president of the institution is nominated by the national council each year.   == membership == there are three classes of corporate membership, viz. fellow, member and associate member. other classes of membership are : (i) honorary membership conferred by the national council in recognition of outstanding services in the fields of industrial engineering and management sciences (ii) affiliate (iii) graduate (iv) student and (v) institutional mernbership. a member can join seminars, workshops, training programs, special lectures, industry visits and other professional activities of institution. fellows, members, associate members and graduate members of the institution shall be permitted to affix the appropriate symbols as given below to their names. fellow - (f.i.i.e), member - (m.i.i.e), associate member - (a.m.i.i.e), graduate member - (grad. i.i.e) after graduateship the student is entitled to use below title associate member of institute of industrial engineering (a.m.i.i.e) this is equal to b.e (bachelor\'s of engineering), b.tech (bachelor\'s of technology) and a.m.i.e (associate member of institute of engineering)   == chapters == the institution has twenty nine chapters, distributed all over india, at aurangabad, allahabad, bangalore, baroda, bhilai, bhopal, kolkata, kozhikode, chennai, cochin, coimbatore, orissa, durgapur, hyderabad, jamshedpur, kanpur, lucknow, mumbai, nagpur, new delhi, pune, ranchi, rourkela, tiruchirappalli, surat, trivandrum, visakhapatnam, udaipur and goa.   == student chapters == the institution has two recognized active students chapters in india: iter and psg college of technology. both are recognized and function under their respective home chapter.   == journal == the institution brings out a monthly journal entitled ""industrial engineering journal"".it publishes papers and articles relating to application of industrial engineering and management techniques, including research work.   == examination == the institution conducts graduateship examination as external examination for enabling candidates to qualify for graduate membership of the institution. the graduateship examination conducted by the institution (amiie) is recognized by the government of india as equivalent to bachelor\'s degree in industrial engineering from any recognized university.iiie graduates are eligible for appearing the  graduate aptitude test in engineering (gate) conducted by iits and iisc.the examinations are open only for the student members of the institution and whose membership continues to remain valid. the requirements for eligibility to student membership of the institution: person should not be less than 18 years of age and should possess one of the following qualifications: 1) a pass in the higher secondary (xii standard) examination under the 1023 scheme of education, of a statutory board of higher secondary education, the intermediate examination, recognized as equivalent thereto and has at least two years working experience in an organisation. 2) completed and passed first two years of regular degree course in engineering / technology. 3) a graduate in science stream only and has at least one year working experience in an organisation. 4) a diploma (3 yrs course) in engineering/technology recognized by the concerned director of technical education and at least one year working experience in an organisation. in case, a candidate has undergone 1 year service training as a part of the course recognised by board of apprenticeship training, govt. of india. this will be counted as working experience. 5) a degree in engineering / technology of a university in india or equivalent qualification recognised by the all india council for technical education (aicte) as equivalent.   == scheme,subjects and recognition of the examination == the examinations consist of three parts of written papers viz. the preliminary, the section a and the section b. in addition, a project work has to be undertaken and report submitted for acceptance. a student can appear for section a papers only after completely passing the preliminary (or exempted from the preliminary). for appearance in a section, a student must have passed the previous section fully. the student will have to carry out an approved project in an organisation under a project guide. a project report has to be submitted to the board of examinations after the completion of the project, for its acceptance. the student is considered to have completed the graduateship examination only after his/her project report has been accepted by the board of examinations. graduateship examination of indian institution of industrial engineering is recognised by association of indian universities, ministry of education & social welfare india at par with bachelor degree in industrial engineering of an indian university. indian institution of industrial engineering is also a member of engineering council of india. http://www.iiie-india.com/iiie/images/govt-of-india.pdf http://www.iiie-india.com/iiie/images/aiu.pdf http://www.iiie-india.com/iiie/images/mumbai-university1.pdf http://www.ecindia.org/ http://www.iiie-india.com/iiie/images/ignou1.pdf http://www.iiie-india.com/iiie/images/the-institution-of-engineers.pdf   == awards and honors == the iiie has instituted many honors and awards for various achievements and outstanding contributions to the industrial engineering profession for members, non-members, chapters and other bodies. all honors and awards are conferred by the iiie national council at its national convention. 1 - lillian gilbreth award - awarded annually to the person making outstanding contribution to the profession of industrial engineering. 2 - honorary membership - awarded to individual\'s professional achievements recognized nationally or internationally. 3 -ramaswamy cup - donated by mr. s. s. rangnekar - the cup is awarded annually to the person making outstanding contribution to the institution. 4 - h. k. firodia award - donated by mr. h. k. firodia, past president - cash award, a medal and certificate. 5 cdr dr. bhaskar bhandarkar award  for visionary leadership ( donated by late h. n thadani , founder president 6 - krish pennathur shield - donated by dr. k. pennathur - the shield is awarded annually to the best chapter of the year of its outstanding overall performance. 7 - fellowship - awarded to the members in recognition of their outstanding contribution to the profession and the institution.   == see also ==   == references ==   == external links == official site - iiie iiie exam papers & references iiie hyderabad chapter iiie cuttack chapter iiie pune chapter')"
177,"In manufacturing engineering, process layout is a design for the floor plan of a plant which aims to improve efficiency by arranging equipment according to its function. The production line should ideally be designed to eliminate waste in material flows, inventory handling and management. 
In process layout, the work stations and machinery are not arranged according to a particular production sequence. Instead, there is an assembly of similar operations or similar machinery in each department (for example, a drill department, a paint department, etc.)
It is also known as function layout. In this layout machining operation are performed in group together and not arranged according to any sequence. 


== Main advantages ==
Provide visual control of activities
Use space efficiently
Use labour efficiently
Eliminate bottlenecks
Facilitate communication and interaction  between workers and supervisors


== Criticism ==
A common criticism of this layout is that the work can be monotonous for staff, especially if they are involved only in one stage of the process.
This criticism can however be eliminated if the staff are rotated to different departments (involving different processes) thus developing a multi-skilled body of staff.


== See also ==
Product layout


== References ==


== Further reading ==
S.N. Chary (2006). Production and Operations Management. McGraw Hill. ISBN 978-0-07-058355-9
S. Moran (2016). Process Plant Layout. Elsevier. ISBN 978-0128033555","pandas(index=177, _1=177, text='in manufacturing engineering, process layout is a design for the floor plan of a plant which aims to improve efficiency by arranging equipment according to its function. the production line should ideally be designed to eliminate waste in material flows, inventory handling and management. in process layout, the work stations and machinery are not arranged according to a particular production sequence. instead, there is an assembly of similar operations or similar machinery in each department (for example, a drill department, a paint department, etc.) it is also known as function layout. in this layout machining operation are performed in group together and not arranged according to any sequence.   == main advantages == provide visual control of activities use space efficiently use labour efficiently eliminate bottlenecks facilitate communication and interaction  between workers and supervisors   == criticism == a common criticism of this layout is that the work can be monotonous for staff, especially if they are involved only in one stage of the process. this criticism can however be eliminated if the staff are rotated to different departments (involving different processes) thus developing a multi-skilled body of staff.   == see also == product layout   == references ==   == further reading == s.n. chary (2006). production and operations management. mcgraw hill. isbn 978-0-07-058355-9 s. moran (2016). process plant layout. elsevier. isbn 978-0128033555')"
178,"Lawler's algorithm is a powerful technique for solving a variety of constrained scheduling problems. The algorithm handles any precedence constraints. It schedules a set of simultaneously arriving tasks on one processor with precedence constraints to minimize maximum tardiness or lateness. Precedence constraints occur when certain jobs must be completed before other jobs can be started.


== Objective functions ==
The objective function is assumed to be in the form 
  
    
      
        m
        i
        n
        
        m
        a
        
          x
          
            0
            ≤
            i
            ≤
            n
          
        
        
        
          g
          
            i
          
        
        (
        
          F
          
            i
          
        
        )
      
    
    {\displaystyle min\,max_{0\leq i\leq n}\,g_{i}(F_{i})}
  , where 
  
    
      
        
          g
          
            i
          
        
      
    
    {\displaystyle g_{i}}
   is any nondecreasing function and 
  
    
      
        
          F
          
            i
          
        
      
    
    {\displaystyle F_{i}}
   is the flow time. When 
  
    
      
        
          g
          
            i
          
        
        (
        
          F
          
            i
          
        
        )
        =
        
          F
          
            i
          
        
        −
        
          d
          
            i
          
        
        =
        
          L
          
            i
          
        
      
    
    {\displaystyle g_{i}(F_{i})=F_{i}-d_{i}=L_{i}}
  , the objective function corresponds to minimizing the maximum lateness, where 
  
    
      
        
          d
          
            i
          
        
      
    
    {\displaystyle d_{i}}
   is due time for job 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        
          L
          
            i
          
        
      
    
    {\displaystyle L_{i}}
   lateness of job 
  
    
      
        i
      
    
    {\displaystyle i}
  . Another expression is 
  
    
      
        
          g
          
            i
          
        
        (
        
          F
          
            i
          
        
        )
        =
        m
        a
        x
        
          (
          
            F
            
              i
            
          
          −
          
            d
            
              i
            
          
          ,
          0
          )
        
      
    
    {\displaystyle g_{i}(F_{i})=max{(F_{i}-d_{i},0)}}
  , which corresponds to minimizing the maximum tardiness.


== Algorithm ==
The algorithm builds the schedule back to front. For each scheduling step, it looks only at the tasks that no other tasks depend on, and puts the one with the latest due date at the end of the schedule queue. Then it repeats this processs until all jobs are scheduled. 
The algorithm works by planning the job with the least impact as late as possible. Starting at 
  
    
      
        t
        =
        ∑
        
          p
          
            j
          
        
      
    
    {\displaystyle t=\sum p_{j}}
  .

  
    
      
        S
      
    
    {\displaystyle S}
   set of already scheduled jobs (at start: S = 
  
    
      
        ∅
      
    
    {\displaystyle \emptyset }
  )

  
    
      
        J
      
    
    {\displaystyle J}
   set of jobs whose successors have been scheduled (at start: all jobs without successors)

  
    
      
        t
      
    
    {\displaystyle t}
   time when the next job will be completed (at start: 
  
    
      
        t
        =
        ∑
        
          p
          
            j
          
        
      
    
    {\displaystyle t=\sum p_{j}}
  )
while
  
    
      
        J
        ≠
        ∅
      
    
    {\displaystyle J\neq \emptyset }
   do
    select 
  
    
      
        j
        ∈
        J
      
    
    {\displaystyle j\in J}
   such that 
  
    
      
        
          f
          
            j
          
        
        (
        t
        )
        =
        m
        i
        
          n
          
            k
            ∈
            J
          
        
        
          f
          
            k
          
        
        (
        t
        )
      
    
    {\displaystyle f_{j}(t)=min_{k\in J}f_{k}(t)}
  
    schedule 
  
    
      
        j
      
    
    {\displaystyle j}
   such that it completes at time 
  
    
      
        t
      
    
    {\displaystyle t}
  
    add 
  
    
      
        j
      
    
    {\displaystyle j}
   to 
  
    
      
        S
      
    
    {\displaystyle S}
  , delete 
  
    
      
        j
      
    
    {\displaystyle j}
   from 
  
    
      
        J
      
    
    {\displaystyle J}
   and update 
  
    
      
        J
      
    
    {\displaystyle J}
  .
    
  
    
      
        t
        =
        t
        −
        
          p
          
            j
          
        
      
    
    {\displaystyle t=t-p_{j}}
  
end while


== Example 1 ==
Assuming there are three jobs: t1, t2, and t3, with the following precedence constraints:

t1-> t2, t1 must finish before t2
t1-> t3, t1 must finish before t3And the following deadlines (due date in a month)

t1: 2nd day
t2: 5th day
t3: 8th dayNow we construct the required set of jobs:

S = {empty},  initially empty set of scheduled jobs
J = {t2, t3}, the set of jobs whose successors have been scheduled or jobs without successors. t2 and t3 have no successors.Repeat the following steps until J is empty:

select a job j in J, so its due date is the latests, in this example, it is t3 with a due date 8th.
move j from J to S's front, now J = {t2}, S={t3}.
update J to add any new job whose successors have been scheduled. There is none this time.Do the next round: 

select a job j in J, so its due date is the latests. It is t2 with due date 5th this time.
move j from J to S's front, now J = {empty}, S={t2, t3}
update J to add any new job whose successors have been scheduled, now J= {t1} since both t2 and t3 have been scheduled.Do the next round: 

select a job j in J={t1}, so its due date is the latests. This example, it is t1.
move j from J to S's front, now J = {empty}, S={t1, t2, t3}
update J to add any new job whose successors have been scheduled. Nothing to add.J is now empty. The end.
So the final schedule is t1 -> t2-> t3 as S = {t1, t2, t3}


== Example 2 ==
A more complex example, with simplified steps:
The jobs and precedence constraints are shown below: a parent node --> child node in the tree.

      j1 
     (2) 
    /   \
   j2    j3 
  (2)    (4)
  / \     |
 j4 j5   j6
(3) (5)  (6)  

The due dates of jobs are shown underneath of each node of the tree in parentheses. 

j1: 2
j2: 5
j3: 4
j4: 3
j5: 5
j6: 6Now look at the set of jobs without any successors, find the one with latest due date, put it into the front of S: 

The S set would be { j1, j2, j4, j3, j5, j6}


== References ==


== Further reading ==
Michael Pinedo. Scheduling: theory, algorithms, and systems. 2008. ISBN 978-0-387-78934-7
Conway, Maxwell, Miller. Theory of Scheduling. 1967. ISBN 0-486-42817-6","pandas(index=178, _1=178, text=""lawler's algorithm is a powerful technique for solving a variety of constrained scheduling problems. the algorithm handles any precedence constraints. it schedules a set of simultaneously arriving tasks on one processor with precedence constraints to minimize maximum tardiness or lateness. precedence constraints occur when certain jobs must be completed before other jobs can be started.   == objective functions == the objective function is assumed to be in the form    m i n  m a  x  0 ≤ i ≤ n     g  i   (  f  i   )      == references ==   == further reading == michael pinedo. scheduling: theory, algorithms, and systems. 2008. isbn 978-0-387-78934-7 conway, maxwell, miller. theory of scheduling. 1967. isbn 0-486-42817-6"")"
179,"Junior Philippine Institute of Industrial Engineers or JPIIE (former name: Society of Industrial Engineering) is an academic organization based in University of Perpetual Help System DALTA - Calamba Campus exclusive for BS Industrial Engineering students, faculty and alumni of the said university. It is one of the seven student organizations under the College of Engineering of the university which includes MES (Mechanical Engineering Society) for BS Mechanical Engineering students and ACES (Association of Civil Engineering Students) for BS Civil Engineering among others. The organization is also a member of PIIE (Philippine Institute of Industrial Engineers), a premier organization of BS Industrial Engineering graduates and students, and IE professionals in the Philippines. Engr. Philip Ermita, PIE, ASEAN Engr. is the organization's adviser aside from being the Dean of College of Engineering in the university.


== History ==
The society was first recognized by the University and its former school director Mr. Rey Dalde in August 2002 with the efforts of its first student officers for AY 2002-2003 being headed by the former president, Ms. Maria Fe dela Cruz.
The society's by-laws was written by Ms. Cherryl C. Marudo. According to the by-laws, the name of the organization is Society of Industrial Engineering Students (or SIES). But, for some reasons, it became locally known as SIE (without the second S which means Students) probably because its name structure has been compared to the university's College of Engineering.
By the year 2016, the SIE renamed as the Junior Philippine Institute of Industrial Engineers or JPIIE.


== List of Presidents of SIE ==


== References ==


== External links ==
http://piie.org/
https://web.archive.org/web/20110831233320/http://uphsdsie.webs.com/
http://www.perpetualdalta.edu.ph","pandas(index=179, _1=179, text=""junior philippine institute of industrial engineers or jpiie (former name: society of industrial engineering) is an academic organization based in university of perpetual help system dalta - calamba campus exclusive for bs industrial engineering students, faculty and alumni of the said university. it is one of the seven student organizations under the college of engineering of the university which includes mes (mechanical engineering society) for bs mechanical engineering students and aces (association of civil engineering students) for bs civil engineering among others. the organization is also a member of piie (philippine institute of industrial engineers), a premier organization of bs industrial engineering graduates and students, and ie professionals in the philippines. engr. philip ermita, pie, asean engr. is the organization's adviser aside from being the dean of college of engineering in the university.   == history == the society was first recognized by the university and its former school director mr. rey dalde in august 2002 with the efforts of its first student officers for ay 2002-2003 being headed by the former president, ms. maria fe dela cruz. the society's by-laws was written by ms. cherryl c. marudo. according to the by-laws, the name of the organization is society of industrial engineering students (or sies). but, for some reasons, it became locally known as sie (without the second s which means students) probably because its name structure has been compared to the university's college of engineering. by the year 2016, the sie renamed as the junior philippine institute of industrial engineers or jpiie.   == list of presidents of sie ==   == references ==   == external links == http://piie.org/ https://web.archive.org/web/20110831233320/http://uphsdsie.webs.com/ http://www.perpetualdalta.edu.ph"")"
180,"Service quality (SQ), in its contemporary conceptualisation, is a comparison of perceived expectations (E) of a service with perceived performance (P), giving rise to the equation SQ=P-E. This conceptualistion of service quality has its origins in the expectancy-disconfirmation paradigm.A business with high service quality will meet or exceed customer expectations whilst remaining economically competitive. Evidence from empirical studies suggests that improved service quality increases profitability and long term economic competitiveness. Improvements to service quality may be achieved by improving operational processes; identifying problems quickly and systematically; establishing valid and reliable service performance measures and measuring customer satisfaction and other performance outcomes.


== Definition ==
From the viewpoint of business administration, service quality is an achievement in customer service.  It reflects at each service encounter. Customers form service expectations from past experiences, word of mouth and marketing communications. In general, customers compare perceived service with expected service, and if the former falls short of the latter the customers are disappointed.
For example, in the case of Taj Hotels Resorts and Palaces, wherein TAJ remaining the old world, luxury brand in the five-star category, the umbrella branding was diluting the image of the TAJ brand because although the different hotels such as Vivanta by Taj- the four star category, Gateway in the three star category and Ginger the two star economy brand, were positioned and categorised differently, customers still expected high quality of Taj.
The measurement of subjective aspects of customer service depends on the conformity of the expected benefit with the perceived result. This in turns depends upon the customer's expectation in terms of service, they might receive and the service provider's ability and talent to  present this expected service. Successful companies add benefits to their offering that not only satisfy the customers but also surprise and delight them. Delighting customers is a matter of exceeding their expectations. 
Pre-defined objective criteria may be unattainable in practice, in which case, the best possible achievable result becomes the ideal. The objective ideal may still be poor, in subjective terms.
Service quality can be related to service potential (for example, worker's qualifications); service process (for example, the quickness of service) and  service result (customer satisfaction).
Individual service quality states the service quality of employees as distinct from the quality that the customers perceived


== Evolution of service quality concept ==
Historically, scholars have treated service quality as very difficult to define and measure, due to the inherent intangible nature of services, which are often experienced subjectively.One of the earliest attempts to grapple with the service quality concept came from the so-called Nordic School. In this approach, service quality was seen as having two basic dimensions:
Technical quality: What the customer receives as a result of interactions with the service firm (e.g. a meal in a restaurant, a bed in a hotel)
Functional quality: How the customer receives the service; the expressive nature of the service delivery (e.g. courtesy, attentiveness, promptness)The technical quality is relatively objective and therefore easy to measure. However, difficulties arise when trying to evaluate functional quality.


== Dimensions of service quality ==

A customer's expectation of a particular service is determined by factors such as recommendations, personal needs and past experiences. The expected service and the perceived service sometimes may not be equal, thus leaving a gap. The service quality model or the ‘GAP model’ developed in 1985, highlights the main requirements for delivering high service quality. It identifies five ‘gaps’ that cause unsuccessful delivery. Customers generally have a tendency to compare the service they 'experience' with the service they 'expect'. If the experience does not match the expectation, there arises a gap. Given the emphasis on expectations, this approach to measuring service quality is known as the expectancy-disconfirmation paradigm and is the dominant model in the consumer behaviour and marketing literature. A model of service quality, based on the expectancy-disconformation paradigm, and developed by A. Parasuraman, Valarie A. Zeithaml and Len Berry, identifies the principal dimensions (or components) of service quality and proposes a scale for measuring service quality, known as SERVQUAL. The model's developers originally identified ten dimensions of service quality that influence customer's perceptions of service quality. However, after extensive testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness. These five dimensions are thought to represent the dimensions of service quality across a range of industries and settings.  Among students of marketing, the mnemonic,  RATER, an acronym formed from the first letter of each of the five dimensions, is often used as an aid to recall.
In spite of the dominance of the expectancy-disconfirmation paradigm, scholars have questioned its validity. In particular scholars have pointed out the expectancy-disconfirmation approach had its roots in consumer research and was fundamentally concerned with measuring customer satisfaction rather than service quality. In other words, questions surround the face validity of the model and whether service quality can be conceptualised as a gap.


== Measuring service quality ==
Measuring service quality may involve both subjective and objective processes. In both cases, it is often some aspect of customer satisfaction which is being assessed. However, customer satisfaction is an indirect measure of service quality. Research has also indicated that the presence of service quality leads to several outcomes including changes in perceived value, customer satisfaction and loyalty intentions with consumers


== E-service quality: The next frontier ==
Given the widespread use of internet and e-commerce, researchers have also sought to define and measure e-service quality. Parasuraman, Zeithaml, and Malhotra (2005, p. 5) define e-service quality as the “extent to which a website facilitates efficient and effective shopping, purchasing, and delivery.” Wolfinbarger and Gilly (2003, p. 183) define e-service quality as “the beginning to the end of the transaction including information search, website navigation, order, customer service interactions, delivery, and satisfaction with the ordered product.”.A recent paper examined research on e-service quality. The author identified four dimensions of e-service quality: website design, fulfillment, customer service, and security and privacy.


=== Measuring subjective elements of service quality ===
Subjective processes can be assessed in characteristics (assessed be the SERVQUAL method); in incidents (assessed in Critical Incident Theory) and in problems (assessed by Frequenz Relevanz Analyse a German term. The most important and most used method with which to measure subjective elements of service quality is the Servqual method.


=== Measuring objective elements of service quality ===
Objective processes may be subdivided into primary processes and secondary processes. During primary processes, silent customers create test episodes of service or the service episodes of normal customers are observed. In secondary processes, quantifiable factors such as numbers of customer complaints or numbers of returned goods are analysed in order to make inferences about service quality.


== Approaches to the improvement of service quality ==
In general, an improvement in service design and delivery helps achieve higher levels of service quality. For example, in service design, changes can be brought about in the design of service products and facilities. On the other hand, in service delivery, changes can be brought about in the service delivery processes, the environment in which the service delivery takes place and improvements in the interaction processes between customers and service providers.
Various techniques can be used to make changes such as: Quality function deployment (QFD); failsafing; moving the line of visibility and the line of accessibility; and blueprinting.


== Approaches to improve the conformity of service quality ==
In order to ensure and increase the 'conformance quality' of services, that is,  service delivery happening as designed, various methods are available. Some of these include Guaranteeing; Mystery Shopping; Recovering; Setting standards and measuring; Statistical process control and Customer involvement.


== Service quality and customer satisfaction ==
The relationship between service quality and customer satisfaction has received considerable attention in academic literature. The results of most research studies have indicated that the service quality and customer satisfaction are indeed independent but are closely related that and a rise in one is likely to result in an increase in another construct.


== See also ==
ISO 9001
Quality Management
Customer focus
Services marketing
Mystery shopping
Work quality


== References ==","pandas(index=180, _1=180, text=""service quality (sq), in its contemporary conceptualisation, is a comparison of perceived expectations (e) of a service with perceived performance (p), giving rise to the equation sq=p-e. this conceptualistion of service quality has its origins in the expectancy-disconfirmation paradigm.a business with high service quality will meet or exceed customer expectations whilst remaining economically competitive. evidence from empirical studies suggests that improved service quality increases profitability and long term economic competitiveness. improvements to service quality may be achieved by improving operational processes; identifying problems quickly and systematically; establishing valid and reliable service performance measures and measuring customer satisfaction and other performance outcomes.   == definition == from the viewpoint of business administration, service quality is an achievement in customer service.  it reflects at each service encounter. customers form service expectations from past experiences, word of mouth and marketing communications. in general, customers compare perceived service with expected service, and if the former falls short of the latter the customers are disappointed. for example, in the case of taj hotels resorts and palaces, wherein taj remaining the old world, luxury brand in the five-star category, the umbrella branding was diluting the image of the taj brand because although the different hotels such as vivanta by taj- the four star category, gateway in the three star category and ginger the two star economy brand, were positioned and categorised differently, customers still expected high quality of taj. the measurement of subjective aspects of customer service depends on the conformity of the expected benefit with the perceived result. this in turns depends upon the customer's expectation in terms of service, they might receive and the service provider's ability and talent to  present this expected service. successful companies add benefits to their offering that not only satisfy the customers but also surprise and delight them. delighting customers is a matter of exceeding their expectations. pre-defined objective criteria may be unattainable in practice, in which case, the best possible achievable result becomes the ideal. the objective ideal may still be poor, in subjective terms. service quality can be related to service potential (for example, worker's qualifications); service process (for example, the quickness of service) and  service result (customer satisfaction). individual service quality states the service quality of employees as distinct from the quality that the customers perceived   == evolution of service quality concept == historically, scholars have treated service quality as very difficult to define and measure, due to the inherent intangible nature of services, which are often experienced subjectively.one of the earliest attempts to grapple with the service quality concept came from the so-called nordic school. in this approach, service quality was seen as having two basic dimensions: technical quality: what the customer receives as a result of interactions with the service firm (e.g. a meal in a restaurant, a bed in a hotel) functional quality: how the customer receives the service; the expressive nature of the service delivery (e.g. courtesy, attentiveness, promptness)the technical quality is relatively objective and therefore easy to measure. however, difficulties arise when trying to evaluate functional quality.   == dimensions of service quality ==  a customer's expectation of a particular service is determined by factors such as recommendations, personal needs and past experiences. the expected service and the perceived service sometimes may not be equal, thus leaving a gap. the service quality model or the ‘gap model’ developed in 1985, highlights the main requirements for delivering high service quality. it identifies five ‘gaps’ that cause unsuccessful delivery. customers generally have a tendency to compare the service they 'experience' with the service they 'expect'. if the experience does not match the expectation, there arises a gap. given the emphasis on expectations, this approach to measuring service quality is known as the expectancy-disconfirmation paradigm and is the dominant model in the consumer behaviour and marketing literature. a model of service quality, based on the expectancy-disconformation paradigm, and developed by a. parasuraman, valarie a. zeithaml and len berry, identifies the principal dimensions (or components) of service quality and proposes a scale for measuring service quality, known as servqual. the model's developers originally identified ten dimensions of service quality that influence customer's perceptions of service quality. however, after extensive testing and retesting, some of the dimensions were found to be autocorrelated and the total number of dimensions was reduced to five, namely - reliability, assurance, tangibles, empathy and responsiveness. these five dimensions are thought to represent the dimensions of service quality across a range of industries and settings.  among students of marketing, the mnemonic,  rater, an acronym formed from the first letter of each of the five dimensions, is often used as an aid to recall. in spite of the dominance of the expectancy-disconfirmation paradigm, scholars have questioned its validity. in particular scholars have pointed out the expectancy-disconfirmation approach had its roots in consumer research and was fundamentally concerned with measuring customer satisfaction rather than service quality. in other words, questions surround the face validity of the model and whether service quality can be conceptualised as a gap.   == measuring service quality == measuring service quality may involve both subjective and objective processes. in both cases, it is often some aspect of customer satisfaction which is being assessed. however, customer satisfaction is an indirect measure of service quality. research has also indicated that the presence of service quality leads to several outcomes including changes in perceived value, customer satisfaction and loyalty intentions with consumers   == e-service quality: the next frontier == given the widespread use of internet and e-commerce, researchers have also sought to define and measure e-service quality. parasuraman, zeithaml, and malhotra (2005, p. 5) define e-service quality as the “extent to which a website facilitates efficient and effective shopping, purchasing, and delivery.” wolfinbarger and gilly (2003, p. 183) define e-service quality as “the beginning to the end of the transaction including information search, website navigation, order, customer service interactions, delivery, and satisfaction with the ordered product.”.a recent paper examined research on e-service quality. the author identified four dimensions of e-service quality: website design, fulfillment, customer service, and security and privacy. objective processes may be subdivided into primary processes and secondary processes. during primary processes, silent customers create test episodes of service or the service episodes of normal customers are observed. in secondary processes, quantifiable factors such as numbers of customer complaints or numbers of returned goods are analysed in order to make inferences about service quality.   == approaches to the improvement of service quality == in general, an improvement in service design and delivery helps achieve higher levels of service quality. for example, in service design, changes can be brought about in the design of service products and facilities. on the other hand, in service delivery, changes can be brought about in the service delivery processes, the environment in which the service delivery takes place and improvements in the interaction processes between customers and service providers. various techniques can be used to make changes such as: quality function deployment (qfd); failsafing; moving the line of visibility and the line of accessibility; and blueprinting.   == approaches to improve the conformity of service quality == in order to ensure and increase the 'conformance quality' of services, that is,  service delivery happening as designed, various methods are available. some of these include guaranteeing; mystery shopping; recovering; setting standards and measuring; statistical process control and customer involvement.   == service quality and customer satisfaction == the relationship between service quality and customer satisfaction has received considerable attention in academic literature. the results of most research studies have indicated that the service quality and customer satisfaction are indeed independent but are closely related that and a rise in one is likely to result in an increase in another construct.   == see also == iso 9001 quality management customer focus services marketing mystery shopping work quality   == references =="")"
181,"Work sampling is the statistical technique used for determining the proportion of time spent by workers in various defined categories of activity (e.g. setting up a machine, assembling two parts, idle…etc.).  It is as important as all other statistical techniques because it permits quick analysis, recognition, and enhancement of job responsibilities, tasks, performance competencies, and organizational work flows. Other names used for it are 'activity sampling', 'occurrence sampling', and 'ratio delay study'.In a work sampling study, a large number of observations are made of the workers over an extended period of time. For statistical accuracy, the observations must be taken at random times during the period of study, and the period must be representative of the types of activities performed by the subjects.
One important usage of the work sampling technique is the determination of the standard time for a manual manufacturing task. Similar techniques for calculating the standard time are time study, standard data, and predetermined motion time systems.........


== Characteristics of work sampling study ==
The study of work sampling has some general characteristics related to the work condition:

One of them is the sufficient time available to perform the study. A work sampling study usually requires a substantial period of time to complete. There must be enough time available (several weeks or more) to conduct the study.
Another characteristic is multiple workers. Work sampling is commonly used to study the activities of multiple workers rather than one worker.
The third characteristic is long cycle time. The job covered in the study has relatively a long cycle time.
The last condition is the non-repetitive work cycles. The work is not highly repetitive. The jobs consist of various tasks rather than a single repetitive task. However, it must be possible to classify the work activities into a distinct number of categories.


== Steps in conducting a work sampling study ==
There are several recommended steps when starting to prepare a work sampling study:
Define the manufacturing tasks for which the standard time is to be determined.
Define the task elements. These are the defined broken-down steps of the task that will be observed during the study. Since a worker is going to be observed, additional categories will likely be included as well, such as ""idle"", ""waiting for work"", and ""absent"".
Design the study. This includes designing the forms that will be used to record the observations, determining how many observations will be required, deciding on the number of days or shifts to be included in the study, scheduling the observations, and finally determining the number of observers needed.
Identify the observers who will do the sampling.
Start the study. All those who are affected by the study should be informed about it.
Make random visits to the plant and collect the observations.
After completing the study, analyze and present the results. This is done by preparing a report that summarizes and analyzes all data and making recommendations when required.


== Determining the number of observations needed in work sampling ==
After the work elements are defined, the number of observations for the desired accuracy at the desired confidence level must be determined.
The formula used in this method is:

  
    
      
        
          σ
          
            P
          
        
        =
        
          
            
              
                p
                q
              
              n
            
          
        
      
    
    {\displaystyle \sigma _{P}={\sqrt {\frac {pq}{n}}}}
  

  
    
      
        n
        =
        
          
            
              p
              q
            
            
              
                
                  σ
                  
                    P
                  
                
              
              
                2
              
            
          
        
      
    
    {\displaystyle n={\frac {pq}{{\sigma _{P}}^{2}}}}
  

  
    
      
        
          σ
          
            P
          
        
        =
      
    
    {\displaystyle \sigma _{P}=}
   standard error of proportion

  
    
      
        p
        =
      
    
    {\displaystyle p=}
   percentage of idle time

  
    
      
        q
        =
      
    
    {\displaystyle q=}
   percentage of working time

  
    
      
        n
        =
      
    
    {\displaystyle n=}
   number of observations


== Additional applications of work sampling ==
Work sampling was initially developed for determining time allocation among workers' tasks in manufacturing environments. However, the technique has also been applied more broadly to examine work in a number of different environments, such as healthcare and construction. More recently, in the academic fields of organizational psychology and organizational behaviour, the basic technique has been developed into a detailed job analysis method for examining a range of different research questions.


== See also ==
Sampling (statistics)
Profiling (computer programming) can be done by work sampling a computer program.
Staffing models


== References ==


== External links ==
""Work sampling: Methodological advances and new applications"". Human Factors and Ergonomics in Manufacturing. doi:10.1002/hfm.20186.","pandas(index=181, _1=181, text='work sampling is the statistical technique used for determining the proportion of time spent by workers in various defined categories of activity (e.g. setting up a machine, assembling two parts, idle…etc.).  it is as important as all other statistical techniques because it permits quick analysis, recognition, and enhancement of job responsibilities, tasks, performance competencies, and organizational work flows. other names used for it are \'activity sampling\', \'occurrence sampling\', and \'ratio delay study\'.in a work sampling study, a large number of observations are made of the workers over an extended period of time. for statistical accuracy, the observations must be taken at random times during the period of study, and the period must be representative of the types of activities performed by the subjects. one important usage of the work sampling technique is the determination of the standard time for a manual manufacturing task. similar techniques for calculating the standard time are time study, standard data, and predetermined motion time systems.........   == characteristics of work sampling study == the study of work sampling has some general characteristics related to the work condition:  one of them is the sufficient time available to perform the study. a work sampling study usually requires a substantial period of time to complete. there must be enough time available (several weeks or more) to conduct the study. another characteristic is multiple workers. work sampling is commonly used to study the activities of multiple workers rather than one worker. the third characteristic is long cycle time. the job covered in the study has relatively a long cycle time. the last condition is the non-repetitive work cycles. the work is not highly repetitive. the jobs consist of various tasks rather than a single repetitive task. however, it must be possible to classify the work activities into a distinct number of categories.   == steps in conducting a work sampling study == there are several recommended steps when starting to prepare a work sampling study: define the manufacturing tasks for which the standard time is to be determined. define the task elements. these are the defined broken-down steps of the task that will be observed during the study. since a worker is going to be observed, additional categories will likely be included as well, such as ""idle"", ""waiting for work"", and ""absent"". design the study. this includes designing the forms that will be used to record the observations, determining how many observations will be required, deciding on the number of days or shifts to be included in the study, scheduling the observations, and finally determining the number of observers needed. identify the observers who will do the sampling. start the study. all those who are affected by the study should be informed about it. make random visits to the plant and collect the observations. after completing the study, analyze and present the results. this is done by preparing a report that summarizes and analyzes all data and making recommendations when required.   == determining the number of observations needed in work sampling == after the work elements are defined, the number of observations for the desired accuracy at the desired confidence level must be determined. the formula used in this method is:      σ  p   =     p q  n       number of observations   == additional applications of work sampling == work sampling was initially developed for determining time allocation among workers\' tasks in manufacturing environments. however, the technique has also been applied more broadly to examine work in a number of different environments, such as healthcare and construction. more recently, in the academic fields of organizational psychology and organizational behaviour, the basic technique has been developed into a detailed job analysis method for examining a range of different research questions.   == see also == sampling (statistics) profiling (computer programming) can be done by work sampling a computer program. staffing models   == references ==   == external links == ""work sampling: methodological advances and new applications"". human factors and ergonomics in manufacturing. doi:10.1002/hfm.20186.')"
182,"A worker–machine activity chart is a chart used to describe or plan the interactions between workers and machines over time.As the name indicates, the chart deals with the criteria of work elements and their time for both the worker and the machine. This chart is useful to describe any repetitive worker-machine system.


== Formats ==
A typical worker-machine activity chart consists of two main columns, one for the worker and the other the machine; in some chart formats, there is a third column showing the cumulative time. The chart can also be color-coded to convey information; for example, The time column is used to specify the activity of the worker and the machine, if the column is shaded with black color, it indicates that the worker or the machine is performing an operation, while if it is shaded with gray color, it refers to inspection. For moving, it is customary to refer to it with diagonal lines, whereas horizontal lines indicate a holding activity. If the column is blank then the worker or the machine is idle.  For some other uses, there is a same version to accommodate enormous worker-machine interactions, called the multiple worker-multiple machine activity chart.


== Uses ==
The chart can be used to investigate potential process improvements. It can be used to illustrate delays and redundancy, so process improvement efforts can be made to eliminate inefficiencies and identify the activities that can be combined.


== References ==","pandas(index=182, _1=182, text='a worker–machine activity chart is a chart used to describe or plan the interactions between workers and machines over time.as the name indicates, the chart deals with the criteria of work elements and their time for both the worker and the machine. this chart is useful to describe any repetitive worker-machine system.   == formats == a typical worker-machine activity chart consists of two main columns, one for the worker and the other the machine; in some chart formats, there is a third column showing the cumulative time. the chart can also be color-coded to convey information; for example, the time column is used to specify the activity of the worker and the machine, if the column is shaded with black color, it indicates that the worker or the machine is performing an operation, while if it is shaded with gray color, it refers to inspection. for moving, it is customary to refer to it with diagonal lines, whereas horizontal lines indicate a holding activity. if the column is blank then the worker or the machine is idle.  for some other uses, there is a same version to accommodate enormous worker-machine interactions, called the multiple worker-multiple machine activity chart.   == uses == the chart can be used to investigate potential process improvements. it can be used to illustrate delays and redundancy, so process improvement efforts can be made to eliminate inefficiencies and identify the activities that can be combined.   == references ==')"
183,"The defect concentration diagram (also problem concentration diagram) is a graphical tool that is useful in analyzing the causes of the product or part defects. It is a drawing of the product (or other item of interest), with all relevant views displayed, onto which the locations and frequencies of various defects are shown.


== Usage ==
Defect concentration diagram is used effectively in the following situations:

During data collection phase of problem identification.
Analyzing a part or assembly for possible defects.
Analyzing a product (or a part of a product) being manufactured with several defects.


== Steps ==
There are a number of steps that are needed to be follow when constructing the defect concentration diagram:

Define the fault or faults (or whatever) being investigated.
Make a map, drawing, or picture.
Mark on the diagram each time a fault (or whatever) occurs and where it occurs.
After a sufficient period of time, analyze it to identify where the faults occur.


== References ==","pandas(index=183, _1=183, text='the defect concentration diagram (also problem concentration diagram) is a graphical tool that is useful in analyzing the causes of the product or part defects. it is a drawing of the product (or other item of interest), with all relevant views displayed, onto which the locations and frequencies of various defects are shown.   == usage == defect concentration diagram is used effectively in the following situations:  during data collection phase of problem identification. analyzing a part or assembly for possible defects. analyzing a product (or a part of a product) being manufactured with several defects.   == steps == there are a number of steps that are needed to be follow when constructing the defect concentration diagram:  define the fault or faults (or whatever) being investigated. make a map, drawing, or picture. mark on the diagram each time a fault (or whatever) occurs and where it occurs. after a sufficient period of time, analyze it to identify where the faults occur.   == references ==')"
184,"The systematic layout planning (SLP) - also referred to as site layout planning - is a tool used to arrange a workplace in a plant by locating areas with high frequency and logical relationships close to each other. The process permits the quickest material flow in processing the product at the lowest cost and least amount of handling. It is used in construction projects to optimize the location of temporary facilities (such as engineers' caravans, material storage, generators, etc.) during construction to minimize transportation, minimize cost, minimize travel time, and enhance safety.


== Levels of plant layout design ==
There are four levels of detail in plant layout design,

Site layout: shows how the building should be located in a proper way.
Block layout: shows the sizes of departments in the buildings.
Detailed layout: shows the arrangements of equipment and workstations in the departments.
Workstation layout: shows the locations of every part of the workstation.


== References ==","pandas(index=184, _1=184, text=""the systematic layout planning (slp) - also referred to as site layout planning - is a tool used to arrange a workplace in a plant by locating areas with high frequency and logical relationships close to each other. the process permits the quickest material flow in processing the product at the lowest cost and least amount of handling. it is used in construction projects to optimize the location of temporary facilities (such as engineers' caravans, material storage, generators, etc.) during construction to minimize transportation, minimize cost, minimize travel time, and enhance safety.   == levels of plant layout design == there are four levels of detail in plant layout design,  site layout: shows how the building should be located in a proper way. block layout: shows the sizes of departments in the buildings. detailed layout: shows the arrangements of equipment and workstations in the departments. workstation layout: shows the locations of every part of the workstation.   == references =="")"
185,"In manufacturing engineering, a product layout refers to a production system where the work stations and equipment are located along the line of production, as with assembly lines.
Usually, work units are moved along  line (not necessarily a geometric line, but a set of interconnected work stations) by a conveyor. Work is done in small amounts at each of the work stations on the line. To use the product layout, the total work to be performed must be dividable into small tasks that can be assigned to each of the workstations.
Because the work stations each do small amounts of work, the stations utilize specific techniques and equipment tailored to the individual job they are assigned.  This can lead to a higher rate of production.


== See also ==
Process layout


== References ==
Mikell P. Groover (2007). Work Systems: The Methods, Measurement & Management of Work. Prentice Hall. ISBN 978-0-13-140650-6","pandas(index=185, _1=185, text='in manufacturing engineering, a product layout refers to a production system where the work stations and equipment are located along the line of production, as with assembly lines. usually, work units are moved along  line (not necessarily a geometric line, but a set of interconnected work stations) by a conveyor. work is done in small amounts at each of the work stations on the line. to use the product layout, the total work to be performed must be dividable into small tasks that can be assigned to each of the workstations. because the work stations each do small amounts of work, the stations utilize specific techniques and equipment tailored to the individual job they are assigned.  this can lead to a higher rate of production.   == see also == process layout   == references == mikell p. groover (2007). work systems: the methods, measurement & management of work. prentice hall. isbn 978-0-13-140650-6')"
186,"The flow process chart is a graphical and symbolic representation of the activities performed on the work piece during the operation in industrial engineering. 


== History ==
The first structured method for documenting process flow, e.g., in flow shop scheduling, the flow process chart, was introduced by Frank and Lillian Gilbreth to members of ASME in 1921 as the presentation ""Process Charts, First Steps in Finding the One Best Way to Do Work"". The Gilbreths' tools quickly found their way into industrial engineering curricula.In the early 1930s, an industrial engineer, Allan H. Mogensen, began training business people in the use of some of the tools of industrial engineering at his Work Simplification Conferences in Lake Placid, New York. A 1944 graduate of Mogensen's class, Art Spinanger, took the tools back to Procter and Gamble, where he developed their Deliberate Methods Change Program. Another 1944 graduate, Ben S. Graham, Director of Formcraft Engineering at Standard Register Corporation, adapted the flow process chart to information processing with his development of the multi-flow process chart to display multiple documents and their relationships.


== Symbols ==
In 1947, ASME adopted the following symbol set derived from Gilbreth's original work as the ASME Standard for Process Charts.
Operation: to change the physical or chemical characteristics of the material.
Inspection: to check the quality or the quantity of the material.
Move: transporting the material from one place to another.
Delay: when material cannot go to the next activity.
Storage: when the material is kept in a safe location.


== When to use it ==
It is used when observing a physical process, to record actions as they happen, and thus get an accurate description of the process.
It is used when analyzing the steps in a process, to help identify and eliminate waste—thus, it is a tool for efficiency planning.
It is used when the process is mostly sequential, containing few decisions.


== See also ==
Business process mapping
Control flow diagram
Data flow diagram
Flowchart
Functional flow block diagram
Workflow


== References ==


== External links ==","pandas(index=186, _1=186, text='the flow process chart is a graphical and symbolic representation of the activities performed on the work piece during the operation in industrial engineering.   == history == the first structured method for documenting process flow, e.g., in flow shop scheduling, the flow process chart, was introduced by frank and lillian gilbreth to members of asme in 1921 as the presentation ""process charts, first steps in finding the one best way to do work"". the gilbreths\' tools quickly found their way into industrial engineering curricula.in the early 1930s, an industrial engineer, allan h. mogensen, began training business people in the use of some of the tools of industrial engineering at his work simplification conferences in lake placid, new york. a 1944 graduate of mogensen\'s class, art spinanger, took the tools back to procter and gamble, where he developed their deliberate methods change program. another 1944 graduate, ben s. graham, director of formcraft engineering at standard register corporation, adapted the flow process chart to information processing with his development of the multi-flow process chart to display multiple documents and their relationships.   == symbols == in 1947, asme adopted the following symbol set derived from gilbreth\'s original work as the asme standard for process charts. operation: to change the physical or chemical characteristics of the material. inspection: to check the quality or the quantity of the material. move: transporting the material from one place to another. delay: when material cannot go to the next activity. storage: when the material is kept in a safe location.   == when to use it == it is used when observing a physical process, to record actions as they happen, and thus get an accurate description of the process. it is used when analyzing the steps in a process, to help identify and eliminate waste—thus, it is a tool for efficiency planning. it is used when the process is mostly sequential, containing few decisions.   == see also == business process mapping control flow diagram data flow diagram flowchart functional flow block diagram workflow   == references ==   == external links ==')"
187,"In work measurement, a standard data system (SDS) is a database of normal time values, usually organized by work elements that can be used to establish time standards for tasks composed of work elements similar to those in the database.


== Steps in using an SDS ==
Analyze the new task and divide into work elements.
Access database to determine normal times for work elements.
Add element normal times to obtain task normal time.
Compute standard times.


== References ==
Aft, Lawrence (2000). Work Measurement and Methods Improvement. New York: John Wiley & Sons, Inc. ISBN 978-0-471-37089-5.
Groover, Mikell (2007). Work Systems and the Methods, Measurement, and Management of Work. Upper Saddle River: Pearson Prentice Hall. ISBN 0-13-140650-7.","pandas(index=187, _1=187, text='in work measurement, a standard data system (sds) is a database of normal time values, usually organized by work elements that can be used to establish time standards for tasks composed of work elements similar to those in the database.   == steps in using an sds == analyze the new task and divide into work elements. access database to determine normal times for work elements. add element normal times to obtain task normal time. compute standard times.   == references == aft, lawrence (2000). work measurement and methods improvement. new york: john wiley & sons, inc. isbn 978-0-471-37089-5. groover, mikell (2007). work systems and the methods, measurement, and management of work. upper saddle river: pearson prentice hall. isbn 0-13-140650-7.')"
188,"In industrial engineering, the standard time is the time required by an average skilled operator, working at
a normal pace, to perform a specified task using a prescribed method. It includes appropriate allowances to allow the person to recover from fatigue and, where necessary, an additional allowance to cover contingent elements which may occur but have not been observed.
Standard time =normal time +allowance 
Where; normal time =avg time *rating factor. 
(take rating factor between 1.1 and 1.2)


== Usage of the standard time ==
Time times for all operations are known. 

Staffing (or workforce planning): the number of workers required cannot accurately be determined unless the time required to process the existing work is known.
Line balancing (or production leveling): the correct number of workstations for optimum work flow depends on the processing time, or standard, at each workstation.
Materials requirement planning (MRP): MRP systems cannot operate properly without accurate work standards.
System simulation: simulation models cannot accurately simulate operation unless times for all operations are known.
Wage payment: comparing expected performance with actual performance requires the use of work standards.
Cost accounting: work standards are necessary for determining not only the labor component of costs, but also the correct allocation of production costs to specific products.
Employee evaluation: in order to assess whether individual employees are performing as well as they should, a performance standard is necessary against which to measure the level of performance.


== Techniques to establish a standard time ==
The standard time can be determined using the following techniques:
Time study,
Predetermined motion time system aka PMTS or PTS,
Standard data system,
Work sampling.


== Method of calculation ==
The Standard Time is the product of three factors:

Observed time: The time measured to complete the task.
Performance rating factor: The pace the person is working at. 90% is working slower than normal, 110% is working faster than normal, 100% is normal.  This factor is calculated by an experienced worker who is trained to observe and determine the rating.
Personal, fatigue, and delay (PFD) allowance.The standard time can then be calculated by using:

  
    
      
        
          Standard Time
        
        =
        (
        
          Observed Time
        
        )
        (
        
          Rating Factor
        
        )
        (
        1
        +
        
          PFD Allowance
        
        )
      
    
    {\displaystyle {\text{Standard Time}}=({\text{Observed Time}})({\text{Rating Factor}})(1+{\text{PFD Allowance}})}
  


== References ==

Groover, M. P. (2007). Work systems: the methods, measurement and management of work, Prentice Hall, ISBN 978-0-13-140650-6
Salvendy, G. (Ed.) (2001). Handbook of Industrial Engineering: Technology and Operations Management, third edition, John Wiley & Sons, Hoboken, NJ.
Zandin, K. (Ed.) (2001). Maynard's Industrial Engineering Handbook, fifth edition, McGraw-Hill, New York, NY.
Standard Performance","pandas(index=188, _1=188, text=""in industrial engineering, the standard time is the time required by an average skilled operator, working at a normal pace, to perform a specified task using a prescribed method. it includes appropriate allowances to allow the person to recover from fatigue and, where necessary, an additional allowance to cover contingent elements which may occur but have not been observed. standard time =normal timeallowance where; normal time =avg time *rating factor. (take rating factor between 1.1 and 1.2)   == usage of the standard time == time times for all operations are known.  staffing (or workforce planning): the number of workers required cannot accurately be determined unless the time required to process the existing work is known. line balancing (or production leveling): the correct number of workstations for optimum work flow depends on the processing time, or standard, at each workstation. materials requirement planning (mrp): mrp systems cannot operate properly without accurate work standards. system simulation: simulation models cannot accurately simulate operation unless times for all operations are known. wage payment: comparing expected performance with actual performance requires the use of work standards. cost accounting: work standards are necessary for determining not only the labor component of costs, but also the correct allocation of production costs to specific products. employee evaluation: in order to assess whether individual employees are performing as well as they should, a performance standard is necessary against which to measure the level of performance.   == techniques to establish a standard time == the standard time can be determined using the following techniques: time study, predetermined motion time system aka pmts or pts, standard data system, work sampling.   == method of calculation == the standard time is the product of three factors:  observed time: the time measured to complete the task. performance rating factor: the pace the person is working at. 90% is working slower than normal, 110% is working faster than normal, 100% is normal.  this factor is calculated by an experienced worker who is trained to observe and determine the rating. personal, fatigue, and delay (pfd) allowance.the standard time can then be calculated by using:      standard time  = (  observed time  ) (  rating factor  ) ( 1pfd allowance  )       == references ==  groover, m. p. (2007). work systems: the methods, measurement and management of work, prentice hall, isbn 978-0-13-140650-6 salvendy, g. (ed.) (2001). handbook of industrial engineering: technology and operations management, third edition, john wiley & sons, hoboken, nj. zandin, k. (ed.) (2001). maynard's industrial engineering handbook, fifth edition, mcgraw-hill, new york, ny. standard performance"")"
189,"Performance rating is the step in the work measurement in which the analyst observes the worker's performance and records a value representing that performance relative to the analyst's concept of standard performance.Performance rating helps people do their jobs better, identifies training and education needs, assigns people to work they can excel in, and maintains fairness in salaries, benefits, promotion, hiring, and firing. Most workers want to know how they are doing on the job. Workers need performance feedback to work effectively. Accessing an employee timely, accurate, constructive feedback is key to effective performance. Motivational strategies such as goal setting depend upon regular performance updates. While there are many sources of error with performance ratings, error can be reduced through rater training  and through the use of behaviorally anchored rating scales. In industrial and organizational psychology such scales are used to clearly define the behaviors that constitute poor, average, and superior performance.
There are several methods of performance rating. The simplest and most common method is based on speed or pace. Dexterity and effectiveness are also important considerations when assessing performance.  Standard performance is denoted as 100. A performance rating greater than 100 means the worker's performance is more than standard, and less than 100 means the worker's performance is less than standard.  It is important to note that standard performance is not necessarily the performance level expected of workers, the term standard can be misleading.  For example, a standard performance rating of a worker walking is 4.5 miles/hour.  The ratings is used in conjunction with a timing study to level out actual time (observed time) taken by the worker under observation. This leads to a basic minute value (observed time/100*rating).  This balances out fast and slow workers to get to a standard/average time.  Standard at a 100 is not a percentage, it simply makes the calculations easier.  Most companies that set targets using work study methods will set it at a level of around 85, not 100.


== Attributions to work performance ==
Performance rating has become a continuous process by which an employer and employees attempt to understand company goals and how his or her progress toward contributing to them are measured. Performance measurement is an ongoing activity for all managers and their subordinates.  A performance measurement uses the following indicators:

Quantity: addresses how much work is produced. A quantity measure can be expressed as an error rate, such as number one percentage of errors allowable per unit of work, or as a general result to be achieved.
Quality: address how well the work is performed and/or how accurate or how effective the final product is.
Timeliness: addresses how quickly, when or by what date the work is produced. The most common error made in setting timeliness standards is to allow no margin for error. As with other standards, timeliness standards should be set realistically in view of other performance requirements and needs of the organization.
Cost-effectiveness: addresses dollar savings to the organization or working within a budget. Standards that address cost-effectiveness should be based on specific resource levels (money, personnel, or time) that generally can be documented and measured in agencies' annual fiscal year budgets. Cost-effectiveness standards may include such aspects of performance as maintaining or reducing unit costs, reducing the time it takes to produce a product or service, or reducing waste.
Absenteeism/tardiness:  addresses the ability for employee to show up at work and on time. How it is affecting their work performance and other employees.
Adherence to policy: addresses deviation from policy and performance goals.
Professional appearance: addresses how well employees conduct themselves in the work place and comply with dress code/working environment.


== Effectiveness of performance rating ==
The purpose of performance rating is to provide systematic evaluation of the employees’ contribution to the organization. Globally, the combination of indicators and performance management, combined with intensifying work, transforms the work of employees and of the managers. On the managerial level, the will of hierarchy to fulfill performance indicators is dependent on task prioritizing, which is not shared amongst everyone. 
Performance Rating intensifies the environment of the organization but provides structure for production. Performance satisfaction is found to be directly related to both affective commitment and intention of employee. If motivated more likely to meet goals.


== See also ==
Performance appraisal


== References ==","pandas(index=189, _1=189, text=""performance rating is the step in the work measurement in which the analyst observes the worker's performance and records a value representing that performance relative to the analyst's concept of standard performance.performance rating helps people do their jobs better, identifies training and education needs, assigns people to work they can excel in, and maintains fairness in salaries, benefits, promotion, hiring, and firing. most workers want to know how they are doing on the job. workers need performance feedback to work effectively. accessing an employee timely, accurate, constructive feedback is key to effective performance. motivational strategies such as goal setting depend upon regular performance updates. while there are many sources of error with performance ratings, error can be reduced through rater training  and through the use of behaviorally anchored rating scales. in industrial and organizational psychology such scales are used to clearly define the behaviors that constitute poor, average, and superior performance. there are several methods of performance rating. the simplest and most common method is based on speed or pace. dexterity and effectiveness are also important considerations when assessing performance.  standard performance is denoted as 100. a performance rating greater than 100 means the worker's performance is more than standard, and less than 100 means the worker's performance is less than standard.  it is important to note that standard performance is not necessarily the performance level expected of workers, the term standard can be misleading.  for example, a standard performance rating of a worker walking is 4.5 miles/hour.  the ratings is used in conjunction with a timing study to level out actual time (observed time) taken by the worker under observation. this leads to a basic minute value (observed time/100*rating).  this balances out fast and slow workers to get to a standard/average time.  standard at a 100 is not a percentage, it simply makes the calculations easier.  most companies that set targets using work study methods will set it at a level of around 85, not 100.   == attributions to work performance == performance rating has become a continuous process by which an employer and employees attempt to understand company goals and how his or her progress toward contributing to them are measured. performance measurement is an ongoing activity for all managers and their subordinates.  a performance measurement uses the following indicators:  quantity: addresses how much work is produced. a quantity measure can be expressed as an error rate, such as number one percentage of errors allowable per unit of work, or as a general result to be achieved. quality: address how well the work is performed and/or how accurate or how effective the final product is. timeliness: addresses how quickly, when or by what date the work is produced. the most common error made in setting timeliness standards is to allow no margin for error. as with other standards, timeliness standards should be set realistically in view of other performance requirements and needs of the organization. cost-effectiveness: addresses dollar savings to the organization or working within a budget. standards that address cost-effectiveness should be based on specific resource levels (money, personnel, or time) that generally can be documented and measured in agencies' annual fiscal year budgets. cost-effectiveness standards may include such aspects of performance as maintaining or reducing unit costs, reducing the time it takes to produce a product or service, or reducing waste. absenteeism/tardiness:  addresses the ability for employee to show up at work and on time. how it is affecting their work performance and other employees. adherence to policy: addresses deviation from policy and performance goals. professional appearance: addresses how well employees conduct themselves in the work place and comply with dress code/working environment.   == effectiveness of performance rating == the purpose of performance rating is to provide systematic evaluation of the employees’ contribution to the organization. globally, the combination of indicators and performance management, combined with intensifying work, transforms the work of employees and of the managers. on the managerial level, the will of hierarchy to fulfill performance indicators is dependent on task prioritizing, which is not shared amongst everyone. performance rating intensifies the environment of the organization but provides structure for production. performance satisfaction is found to be directly related to both affective commitment and intention of employee. if motivated more likely to meet goals.   == see also == performance appraisal   == references =="")"
190,"The operation chart is a graphical and symbolic representation of the manufacturing operations used to produce a product. The operation chart illustrates only the value-adding activities in the manufacturing process; therefore, material handling and storage are not illustrated in this chart. operation chart records the overall picture of process and sequencewise steps of operations.


== Operations and their symbols in the operation chart ==
The operations described in the operation chart are:

Processing and assembly operations: Processing operation such as changing in shape and properties. On the other hand, joining two or more parts is an assembly operation. Furthermore, these two operations are represented by this symbols, circle (○) and (O) letter.
Inspection operations: Inspection operations are represented by square symbol (□) and (I) as letter. It's done by an inspector checks the material, work part and assembly for quality and quantity.


=== References ===","pandas(index=190, _1=190, text=""the operation chart is a graphical and symbolic representation of the manufacturing operations used to produce a product. the operation chart illustrates only the value-adding activities in the manufacturing process; therefore, material handling and storage are not illustrated in this chart. operation chart records the overall picture of process and sequencewise steps of operations.   == operations and their symbols in the operation chart == the operations described in the operation chart are:  processing and assembly operations: processing operation such as changing in shape and properties. on the other hand, joining two or more parts is an assembly operation. furthermore, these two operations are represented by this symbols, circle (○) and (o) letter. inspection operations: inspection operations are represented by square symbol (□) and (i) as letter. it's done by an inspector checks the material, work part and assembly for quality and quantity. "")"
191,"PFD allowance in work systems is the adjustment done to the normal time to obtain the standard time for the purpose of recovering the lost time due to personal needs, fatigue, and unavoidable delays. By providing a small increase to the normal time in each cycle, a worker can still be able to cover lost time and complete the work assigned to him or her.


== Allowance technique ==
There are two types of interruption: (1) interruption related to work (2) interruption not related to work. For example, a machine breakdown, rest break to overcome fatigue, and receiving instruction from the manager are the interruption related to work, but personal needs, lunch breaks, and personal telephone calls are interruptions not related to work. However, the two types of interruption are both essential for the worker because it is almost impossible to work continually during a regular shift.


=== Personal needs, fatigue, and unavoidable delays allowance ===
The standard time is calculated by multiplying the normal time by 1 plus the PFD allowance:

  
    
      
        
          T
          
            std
          
        
        =
        
          T
          
            n
          
        
        ∗
        (
        1
        +
        
          A
          
            pfd
          
        
        )
      
    
    {\displaystyle T_{\text{std}}=T_{n}*(1+A_{\text{pfd}})}
  

  
    
      
        
          T
          
            std
          
        
        =
      
    
    {\displaystyle T_{\text{std}}=}
  standard time.

  
    
      
        
          T
          
            n
          
        
        =
      
    
    {\displaystyle T_{n}=}
   normal time.

  
    
      
        
          A
          
            pfd
          
        
        =
      
    
    {\displaystyle A_{\text{pfd}}=}
   PFD allowance.


==== Personal needs ====
The personal needs allowance is the time that is associated with workers’ daily personal needs which include going to the restroom, phone calls, going to the water fountain, and similar interruptions of a personal nature. However, it is categorized as 5%, but it also depends on the work environment, e.g. in terms of discomfort and temperature.


==== Fatigue ====
The fatigue allowance is intended to cover the time that the worker should be given to overcome fatigue due to work related stress and conditions. There are three factors that cause fatigue: (1) physical factors like standing and use of force, (2) mental and cognitive factors like mental strain and eye strain, and (3) environmental and work factors like poor lighting, noise, and heat.


==== Unavoidable Delays ====
Unavoidable delays are categorized under unavoidable interruption that occurs at random times during the day in the workplace. They usually refer to work-related events like cleaning up at the end of the shift, and machine breakdowns or malfunctions. Unavoidable delays occur because of many random events at work stations.


== References ==","pandas(index=191, _1=191, text='pfd allowance in work systems is the adjustment done to the normal time to obtain the standard time for the purpose of recovering the lost time due to personal needs, fatigue, and unavoidable delays. by providing a small increase to the normal time in each cycle, a worker can still be able to cover lost time and complete the work assigned to him or her.   == allowance technique == there are two types of interruption: (1) interruption related to work (2) interruption not related to work. for example, a machine breakdown, rest break to overcome fatigue, and receiving instruction from the manager are the interruption related to work, but personal needs, lunch breaks, and personal telephone calls are interruptions not related to work. however, the two types of interruption are both essential for the worker because it is almost impossible to work continually during a regular shift. unavoidable delays are categorized under unavoidable interruption that occurs at random times during the day in the workplace. they usually refer to work-related events like cleaning up at the end of the shift, and machine breakdowns or malfunctions. unavoidable delays occur because of many random events at work stations.   == references ==')"
192,"An inspection is, most generally, an organized examination or formal evaluation exercise. In engineering activities inspection involves the measurements, tests, and gauges applied to certain characteristics in regard to  an object or activity. The results are usually compared to specified requirements and standards for determining whether the item or activity is in line with these targets, often with a Standard Inspection Procedure in place to ensure consistent checking. Inspections are usually non-destructive.
Inspections may be a visual inspection or involve sensing technologies such as ultrasonic testing, accomplished with a direct physical presence or remotely such as a remote visual inspection, and manually or automatically such as an automated optical inspection. Non-contact optical measurement and photogrammetry have become common NDT methods for inspection of manufactured components and design optimisation.
A 2007 Scottish Government review of scrutiny of public services (the Crerar Review) defined inspection of public services as ""... periodic, targeted scrutiny of specific services, to check whether they are meeting national and local performance standards, legislative and professional requirements, and the needs of service users.""A surprise inspection tends to have different results than an announced inspection. Leaders wanting to know how others in their organization perform can drop in without warning, to see directly what  happens. If an inspection is made known in advance, it can give people a chance to cover up or to fix mistakes. This could lead to distorted and inaccurate findings. A surprise inspection, therefore, gives inspectors a better picture of the typical state of the inspected object or process than an announced inspection. It also enhances external confidence in the inspection process.


== Specific inspection ==


=== Manufacturing ===

Quality related in-process inspection/verification is an essential part of quality control in manufacturing. characteristics of a product or process and comparing the results with specified requirements to determine whether is the requirements are met for each characteristic.  Common examples of inspection by measurement or gauging include using a caliper or micrometer to determine if a dimension of a manufactured part is within the dimensional tolerance specified in a drawing for that part, and is thus acceptable for use.
Design for Inspection (DFI) is a concept that should complement and work in collaboration with Design for Manufacturability (DFM) and Design for Assembly (DMA) to reduce product manufacturing cost and increase manufacturing practicality.
Photogrammetry is a modern way of visual inspection, delivering high accuracy and traceability for various industries. The portable 3D system is a versatile optical coordinate measuring machine (CMM) with a wide range of capabilities.  Highly accurate point measurements can be taken with inspection carried out directly to CAD models, geometry or drawings.(DFI)


=== Fire equipment ===
Most fire equipment needs to be inspected to make sure in the event of a fire, every effort has been taken to make sure it doesn't get out of control. Extinguishers are to be inspected every month by law and inspected by a servicing company at least once a year. Fire extinguishers can be heavy, so it's a good idea to practice picking up and holding an extinguisher to get an idea of the weight and feel.


=== Business ===
In international trade several destination countries require pre-shipment inspection. The importer instructs the shipper which inspection company should be used. The inspector makes pictures and a report to certify that the goods that are being shipped and produced are in accordance with the accompanying documents.
Commodity inspection is other term that is used between buyers and sellers. The scope of work for commodity inspection depends to the buyers. Some buyers hire the inspection agencies only for pre-shipment inspections i.e. visual quality, quantity, packing, marking and loading inspections and some others request for higher level inspections and ask inspection agencies to attend in the vendor shops and inspect commodities during manufacturing processes. Normally inspection is done based on an agreed inspection and test plan (ITP).


=== Government ===
In government and politics, an inspection is the act of a monitoring authority administering an official review of various criteria (such as documents, facilities, records, and any other assets) that are deemed by the authority to be related to the inspection. Inspections are used for the purpose of determining if a body is complying with regulations. The inspector examines the criteria and talks with involved individuals. A report and evaluation follows such visits.
In the United States, the Food Safety Inspection Service is charged with ensuring that all meat and egg products are safe to consume and accurately labeled. The Meat Inspection Act of 1906 authorized the Secretary of Agriculture to order meat inspections and condemn any found unfit for human consumption.
The United Nations Monitoring, Verification and Inspection Commission is a regulatory body that inspects for weapons of mass destruction.
The Scottish Commission for the Regulation of Care regulates and inspects care services in Scotland.
A labour inspectorate is a government body that executes checks on compliance to the labour law. It performs inspections on the workplace or building site.


=== Road vehicles ===
A vehicle inspection, e.g., an annual inspection, is a necessary inspection required on vehicles to conform with laws regarding safety, emissions, or both.  It consists of an examination of a vehicle's components, usually done by a certified mechanic. Vehicles pass a pre-warranty inspection, if, and only if, a mechanic provide evidence for the proper working condition of the vehicle systems specified in the type of inspection.


=== Engineering, mechanics ===

A mechanical inspection is usually undertaken to ensure the safety or reliability of structures or machinery.In Europe bodies involved in engineering inspection may be assessed by accreditation bodies according to ISO 17020 ""General criteria for the operation of various types of bodies performing inspection"". This standard defines inspection as ""examination of a product, process, service, or installation or their design and determination of its conformity with specific requirements or, on the basis of professional judgment, with general requirements"".Non-destructive examination (NDE) or nondestructive testing (NDT) is a family of technologies used during inspection to analyze materials, components and products for either inherent defects (such as fractures or cracks), or service induced defects (damage from use). Some common methods are visual, industrial computed tomography scanning, microscopy,  dye penetrant inspection, magnetic-particle inspection, X-ray or radiographic testing, ultrasonic testing, eddy-current testing, acoustic emission testing, and thermographic inspection. In addition, many non-destructive inspections can be performed by a precision scale, or when in motion, a checkweigher. Stereo microscopes are often used for examining small products like circuit boards for product defects.
Inspection and technical assistance during turnarounds helps to decrease costly downtime as well as ensures restart of operations quickly and safely.


=== Medical ===
A medical inspection is the thorough and unhurried visualization of a patient, this requires the use of the naked eye.


=== Military ===
An examination vessel is a craft used to inspect ships entering or leaving a port during wartime.


=== Railroad ===
The railroad's inspection locomotive were special types of steam locomotives designed to carry railroad officials on inspection tours of the railroad property.


=== Real estate ===
A property condition assessment is the examination for purposes of evaluating a commercial or business property's condition often as a part of a due diligence investigation for a company to know what it is buying. Building code officials do a building inspection to determine code compliance in new or altered buildings before issuing a certificate of occupancy. Residential inspections not for code compliance are called a home inspection. There are numerous types of more specific real estate and infrastructure inspections such as windstorm inspection, energy audit, and pipeline video inspection.


=== Software inspection ===
Software inspection, in software programming, refers to peer review of any work product by skilled individuals who look for bugs using a defined test protocol.


== See also ==
Digital Product Passport
Issue tracking systems in government
Maintenance (technical)
Pre-purchase inspection
Review
Site survey
Third-party inspection company
United States Postal Inspection Service
Workers and Peasants Inspection


== References ==","pandas(index=192, _1=192, text='an inspection is, most generally, an organized examination or formal evaluation exercise. in engineering activities inspection involves the measurements, tests, and gauges applied to certain characteristics in regard to  an object or activity. the results are usually compared to specified requirements and standards for determining whether the item or activity is in line with these targets, often with a standard inspection procedure in place to ensure consistent checking. inspections are usually non-destructive. inspections may be a visual inspection or involve sensing technologies such as ultrasonic testing, accomplished with a direct physical presence or remotely such as a remote visual inspection, and manually or automatically such as an automated optical inspection. non-contact optical measurement and photogrammetry have become common ndt methods for inspection of manufactured components and design optimisation. a 2007 scottish government review of scrutiny of public services (the crerar review) defined inspection of public services as ""... periodic, targeted scrutiny of specific services, to check whether they are meeting national and local performance standards, legislative and professional requirements, and the needs of service users.""a surprise inspection tends to have different results than an announced inspection. leaders wanting to know how others in their organization perform can drop in without warning, to see directly what  happens. if an inspection is made known in advance, it can give people a chance to cover up or to fix mistakes. this could lead to distorted and inaccurate findings. a surprise inspection, therefore, gives inspectors a better picture of the typical state of the inspected object or process than an announced inspection. it also enhances external confidence in the inspection process.   == specific inspection == software inspection, in software programming, refers to peer review of any work product by skilled individuals who look for bugs using a defined test protocol.   == see also == digital product passport issue tracking systems in government maintenance (technical) pre-purchase inspection review site survey third-party inspection company united states postal inspection service workers and peasants inspection   == references ==')"
193,"An activity relationship chart (ARC) is a tabular means of displaying the closeness rating among all pairs of activities or departments. In an ARC there are six closeness ratings which may be assigned to each pair of departments, as well as nine reasons for those ratings (each is assigned by a reason code).


== Rating symbols ==
A: Absolutely necessary
E: Especially important
I: Important and core
O: Ordinary
U: Unimportant
X: Prohibited or Undesirable 


== Reason codes ==
Same table
Flow of material
Service
Convenience
Inventory control
Communication
Same personnel
Cleanliness
Flow of partsA rule of thumb is used to restrict the choice of rating letters:

Very few A and X relationships (no more than five percent) should be assigned
No more than 10 percent should be E
No more than 15 percent should be I
No more than 20 percent should be O
About 50 percent of the relationships should be U


== Developing an ARC ==
List all the departments within the facility, and draw a rectangle around each one.
Draw a rhombus between each department, until you fully construct the rhombus as a tree.
Divide each rhombus into two halves; the upper half will contain the rating letter, while the lower half will contain the rating-reason code.


== See also ==
distance matrix
correlation matrix


== References ==","pandas(index=193, _1=193, text='an activity relationship chart (arc) is a tabular means of displaying the closeness rating among all pairs of activities or departments. in an arc there are six closeness ratings which may be assigned to each pair of departments, as well as nine reasons for those ratings (each is assigned by a reason code).   == rating symbols == a: absolutely necessary e: especially important i: important and core o: ordinary u: unimportant x: prohibited or undesirable   == reason codes == same table flow of material service convenience inventory control communication same personnel cleanliness flow of partsa rule of thumb is used to restrict the choice of rating letters:  very few a and x relationships (no more than five percent) should be assigned no more than 10 percent should be e no more than 15 percent should be i no more than 20 percent should be o about 50 percent of the relationships should be u   == developing an arc == list all the departments within the facility, and draw a rectangle around each one. draw a rhombus between each department, until you fully construct the rhombus as a tree. divide each rhombus into two halves; the upper half will contain the rating letter, while the lower half will contain the rating-reason code.   == see also == distance matrix correlation matrix   == references ==')"
194,"Left-hand–right-hand activity chart is an illustration that shows the contributions of the right and left hands of a worker and the balance of the workload between the right and left hands. The chart is a very effective method to analyze the work done by a single worker and it is very helpful in improving the work done by the worker.


== References ==


== Further reading ==
Aft, L. S. (2000). Work measurements and methods improvement, Wiley, ISBN 978-0-471-37089-5.","pandas(index=194, _1=194, text='left-hand–right-hand activity chart is an illustration that shows the contributions of the right and left hands of a worker and the balance of the workload between the right and left hands. the chart is a very effective method to analyze the work done by a single worker and it is very helpful in improving the work done by the worker.   == references ==   == further reading == aft, l. s. (2000). work measurements and methods improvement, wiley, isbn 978-0-471-37089-5.')"
195,"Human factors and ergonomics (commonly referred to as human factors) is the application of psychological and physiological principles to the engineering and design of products, processes, and systems. The goal of human factors is to reduce human error, increase productivity, and enhance safety and comfort with a specific focus on the interaction between the human and the thing of interest.The field is a combination of numerous disciplines, such as psychology, sociology, engineering, biomechanics, industrial design, physiology, anthropometry, interaction design, visual design, user experience, and user interface design. In research, human factors employs the scientific method to study human behavior so that the resultant data may be applied to the four primary goals. In essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. The two terms ""human factors"" and ""ergonomics"" are essentially synonymous.The International Ergonomics Association defines ergonomics or human factors as follows:
Ergonomics (or human factors) is the scientific discipline concerned with the understanding of interactions among humans and other elements of a system, and the profession that applies theory, principles, data and methods to design to optimize human well-being and overall system performance.
Human factors is employed to fulfill the goals of occupational health and safety and productivity. It is relevant in the design of such things as safe furniture and easy-to-use interfaces to machines and equipment. Proper ergonomic design is necessary to prevent repetitive strain injuries and other musculoskeletal disorders, which can develop over time and can lead to long-term disability. Human factors and ergonomics are concerned with the ""fit"" between the user, equipment, and environment or ""fitting a job to a person"". It accounts for the user's capabilities and limitations in seeking to ensure that tasks, functions, information, and the environment suit that user.
To assess the fit between a person and the used technology, human factors specialists or ergonomists consider the job (activity) being done and the demands on the user; the equipment used (its size, shape, and how appropriate it is for the task), and the information used (how it is presented, accessed, and changed). Ergonomics draws on many disciplines in its study of humans and their environments, including anthropometry, biomechanics, mechanical engineering, industrial engineering, industrial design, information design, kinesiology, physiology, cognitive psychology, industrial and organizational psychology, and space psychology.


== Etymology ==
The term ergonomics (from the Greek ἔργον, meaning ""work"", and νόμος, meaning ""natural law"") first entered the modern lexicon when Polish scientist Wojciech Jastrzębowski used the word in his 1857 article Rys ergonomji czyli nauki o pracy, opartej na prawdach poczerpniętych z Nauki Przyrody (The Outline of Ergonomics; i.e. Science of Work, Based on the Truths Taken from the Natural Science). The French scholar Jean-Gustave Courcelle-Seneuil, apparently without knowledge of Jastrzębowski's article, used the word with a slightly different meaning in 1858. The introduction of the term to the English lexicon is widely attributed to British psychologist Hywel Murrell, at the 1949 meeting at the UK's Admiralty, which led to the foundation of The Ergonomics Society. He used it to encompass the studies in which he had been engaged during and after World War II.The expression human factors is a predominantly North American term which has been adopted to emphasize the application of the same methods to non-work-related situations. A ""human factor"" is a physical or cognitive property of an individual or social behavior specific to humans that may influence the functioning of technological systems. The terms ""human factors"" and ""ergonomics"" are essentially synonymous.


== Domains of specialization ==
Ergonomics comprise three main fields of research: physical, cognitive and organizational ergonomics.
There are many specializations within these broad categories. Specializations in the field of physical ergonomics may include visual ergonomics. Specializations within the field of cognitive ergonomics may include usability, human–computer interaction, and user experience engineering.
Some specializations may cut across these domains: Environmental ergonomics is concerned with human interaction with the environment as characterized by climate, temperature, pressure, vibration, light. The emerging field of human factors in highway safety uses human factor principles to understand the actions and capabilities of road users – car and truck drivers, pedestrians, cyclists, etc. – and use this knowledge to design roads and streets to reduce traffic collisions. Driver error is listed as a contributing factor in 44% of fatal collisions in the United States, so a topic of particular interest is how road users gather and process information about the road and its environment, and how to assist them to make the appropriate decision.New terms are being generated all the time. For instance, ""user trial engineer"" may refer to a human factors professional who specializes in user trials. Although the names change, human factors professionals apply an understanding of human factors to the design of equipment, systems and working methods to improve comfort, health, safety, and productivity.
According to the International Ergonomics Association, within the discipline of ergonomics there exist domains of specialization.


=== Physical ergonomics ===

Physical ergonomics is concerned with human anatomy, and some of the anthropometric, physiological and bio mechanical characteristics as they relate to physical activity. Physical ergonomic principles have been widely used in the design of both consumer and industrial products for optimizing performance and to preventing / treating work-related disorders by reducing the mechanisms behind mechanically induced acute and chronic musculoskeletal injuries / disorders.  Risk factors such as localized mechanical pressures, force and posture in a sedentary office environment lead to injuries attributed to an occupational environment. Physical ergonomics is important to those diagnosed with physiological ailments or disorders such as arthritis (both chronic and temporary) or carpal tunnel syndrome. Pressure that is insignificant or imperceptible to those unaffected by these disorders may be very painful, or render a device unusable, for those who are. Many ergonomically designed products are also used or recommended to treat or prevent such disorders, and to treat pressure-related chronic pain.One of the most prevalent types of work-related injuries is musculoskeletal disorder. Work-related musculoskeletal disorders (WRMDs) result in persistent pain, loss of functional capacity and work disability, but their initial diagnosis is difficult because they are mainly based on complaints of pain and other symptoms. Every year, 1.8 million U.S. workers experience WRMDs and nearly 600,000 of the injuries are serious enough to cause workers to miss work. Certain jobs or work conditions cause a higher rate of worker complaints of undue strain, localized fatigue, discomfort, or pain that does not go away after overnight rest. These types of jobs are often those involving activities such as repetitive and forceful exertions; frequent, heavy, or overhead lifts; awkward work positions; or use of vibrating equipment. The Occupational Safety and Health Administration (OSHA) has found substantial evidence that ergonomics programs can cut workers' compensation costs, increase productivity and decrease employee turnover. Mitigation solutions can include both short term and long-term solutions. Short and long-term solutions involve awareness training, positioning of the body, furniture and equipment and ergonomic exercises. Sit-stand stations and computer accessories that provide soft surfaces for resting the palm as well as split keyboards are recommended. Additionally, resources within the HR department can be allocated to provide assessments to employees to ensure the above criteria are met. Therefore, it is important to gather data to identify jobs or work conditions that are most problematic, using sources such as injury and illness logs, medical records, and job analyses.

Innovative workstations that are being tested include sit-stand desks, treadmill desks, pedal devices and cycle ergometers. In multiple studies these new workstations resulted in decreased waist circumference and improved psychological well being. However a significant number of additional studies have seen no marked improvement in health outcomes.


=== Cognitive ergonomics ===

Cognitive ergonomics is concerned with mental processes, such as perception, memory, reasoning, and motor response, as they affect interactions among humans and other elements of a system. (Relevant topics include mental workload, decision-making, skilled performance, human reliability, work stress and training as these may relate to human-system and Human-Computer Interaction design.) Epidemiological studies show a correlation between the time one spends sedentary and their cognitive function such as lowered mood and depression.


=== Organizational ergonomics ===
Organizational ergonomics is concerned with the optimization of socio-technical systems, including their organizational structures, policies, and processes. (Relevant topics include communication, crew resource management, work design, work systems, design of working times, teamwork, participatory design, community ergonomics, cooperative work, new work programs, virtual organizations, telework, and quality management.)


== History ==


=== Ancient societies ===
Some have stated that human ergonomics began with Australopithecus prometheus (also known as “little foot”), a primate who created handheld tools out of different types of stone, clearly distinguishing between tools based on their ability to perform designated tasks. The foundations of the science of ergonomics appear to have been laid within the context of the culture of Ancient Greece. A good deal of evidence indicates that Greek civilization in the 5th century BC used ergonomic principles in the design of their tools, jobs, and workplaces. One outstanding example of this can be found in the description Hippocrates gave of how a surgeon's workplace should be designed and how the tools he uses should be arranged. The archaeological record also shows that the early Egyptian dynasties made tools and household equipment that illustrated ergonomic principles.


=== Industrial societies ===
Bernardino Ramazzini was one of the first people to systematically study the illness that resulted from work earning himself the nickname “father of occupational medicine”. In the late 1600s and early 1700s Ramazzini visited many worksites where he documented the movements of laborers and spoke to them about their ailments. He then published “De Morbis Artificum Diatriba” (Latin for Diseases of Workers) which detailed occupations, common illnesses, remedies. In the 19th century, Frederick Winslow Taylor pioneered the ""scientific management"" method, which proposed a way to find the optimum method of carrying out a given task. Taylor found that he could, for example, triple the amount of coal that workers were shoveling by incrementally reducing the size and weight of coal shovels until the fastest shoveling rate was reached. Frank and Lillian Gilbreth expanded Taylor's methods in the early 1900s to develop the ""time and motion study"". They aimed to improve efficiency by eliminating unnecessary steps and actions. By applying this approach, the Gilbreths reduced the number of motions in bricklaying from 18 to 4.5, allowing bricklayers to increase their productivity from 120 to 350 bricks per hour.However, this approach was rejected by Russian researchers who focused on the well-being of the worker. At the First Conference on Scientific Organization of Labour (1921) Vladimir Bekhterev and Vladimir Nikolayevich Myasishchev criticised Taylorism. Bekhterev argued that ""The ultimate ideal of the labour problem is not in it [Taylorism], but is in such organisation of the labour process that would yield a maximum of efficiency coupled with a minimum of health hazards, absence of fatigue and a guarantee of the sound health and all round personal development of the working people."" Myasishchev rejected Frederick Taylor's proposal to turn man into a machine. Dull monotonous work was a temporary necessity until a corresponding machine can be developed. He also went on to suggest a new discipline of ""ergology"" to study work as an integral part of the re-organisation of work. The concept was taken up by Myasishchev's mentor, Bekhterev, in his final report on the conference, merely changing the name to ""ergonology""


=== Aviation ===
Prior to World War I, the focus of aviation psychology was on the aviator himself, but the war shifted the focus onto the aircraft, in particular, the design of controls and displays, and the effects of altitude and environmental factors on the pilot. The war saw the emergence of aeromedical research and the need for testing and measurement methods. Studies on driver behavior started gaining momentum during this period, as Henry Ford started providing millions of Americans with automobiles. Another major development during this period was the performance of aeromedical research. By the end of World War I, two aeronautical labs were established, one at Brooks Air Force Base, Texas and the other at Wright-Patterson Air Force Base outside of Dayton, Ohio. Many tests were conducted to determine which characteristic differentiated the successful pilots from the unsuccessful ones. During the early 1930s, Edwin Link developed the first flight simulator. The trend continued and more sophisticated simulators and test equipment were developed. Another significant development was in the civilian sector, where the effects of illumination on worker productivity were examined. This led to the identification of the Hawthorne Effect, which suggested that motivational factors could significantly influence human performance.World War II marked the development of new and complex machines and weaponry, and these made new demands on operators' cognition. It was no longer possible to adopt the Tayloristic principle of matching individuals to preexisting jobs. Now the design of equipment had to take into account human limitations and take advantage of human capabilities. The decision-making, attention, situational awareness and hand-eye coordination of the machine's operator became key in the success or failure of a task. There was substantial research conducted to determine the human capabilities and limitations that had to be accomplished. A lot of this research took off where the aeromedical research between the wars had left off. An example of this is the study done by Fitts and Jones (1947), who studied the most effective configuration of control knobs to be used in aircraft cockpits.
Much of this research transcended into other equipment with the aim of making the controls and displays easier for the operators to use. The entry of the terms ""human factors"" and ""ergonomics"" into the modern lexicon date from this period. It was observed that fully functional aircraft flown by the best-trained pilots, still crashed. In 1943 Alphonse Chapanis, a lieutenant in the U.S. Army, showed that this so-called ""pilot error"" could be greatly reduced when more logical and differentiable controls replaced confusing designs in airplane cockpits. After the war, the Army Air Force published 19 volumes summarizing what had been established from research during the war.In the decades since World War II, human factors has continued to flourish and diversify. Work by Elias Porter and others within the RAND Corporation after WWII extended the conception of human factors. ""As the thinking progressed, a new concept developed—that it was possible to view an organization such as an air-defense, man-machine system as a single organism and that it was possible to study the behavior of such an organism.  It was the climate for a breakthrough."" In the initial 20 years after the World War II, most activities were done by the ""founding fathers"": Alphonse Chapanis, Paul Fitts, and Small.


=== Cold War ===
The beginning of the Cold War led to a major expansion of Defense supported research laboratories. Also, many labs established during WWII started expanding. Most of the research following the war was military-sponsored. Large sums of money were granted to universities to conduct research. The scope of the research also broadened from small equipments to entire workstations and systems. Concurrently, a lot of opportunities started opening up in the civilian industry. The focus shifted from research to participation through advice to engineers in the design of equipment. After 1965, the period saw a maturation of the discipline. The field has expanded with the development of the computer and computer applications.The Space Age created new human factors issues such as weightlessness and extreme g-forces. Tolerance of the harsh environment of space and its effects on the mind and body were widely studied.


=== Information age ===
The dawn of the Information Age has resulted in the related field of human–computer interaction (HCI). Likewise, the growing demand for and competition among consumer goods and electronics has resulted in more companies and industries including human factors in their product design. Using advanced technologies in human kinetics, body-mapping, movement patterns and heat zones, companies are able to manufacture purpose-specific garments, including full body suits, jerseys, shorts, shoes, and even underwear.


== Organizations ==
Formed in 1946 in the UK, the oldest professional body for human factors specialists and ergonomists is The Chartered Institute of Ergonomics and Human Factors, formally known as the Institute of Ergonomics and Human Factors and before that, The Ergonomics Society.
The Human Factors and Ergonomics Society (HFES) was founded in 1957. The Society's mission is to promote the discovery and exchange of knowledge concerning the characteristics of human beings that are applicable to the design of systems and devices of all kinds.
The Association of Canadian Ergonomists - l'Association canadienne d'ergonomie (ACE) was founded in 1968. It was originally named the Human Factors Association of Canada (HFAC), with ACE (in French) added in 1984, and the consistent, bilingual title adopted in 1999.  According to it 2017 mission statement, ACE unites and advances the knowledge and skills of ergonomics and human factors practitioners to optimise human and organisational well-being.The International Ergonomics Association (IEA) is a federation of ergonomics and human factors societies from around the world. The mission of the IEA is to elaborate and advance ergonomics science and practice, and to improve the quality of life by expanding its scope of application and contribution to society. As of September 2008, the International Ergonomics Association has 46 federated societies and 2 affiliated societies.
The Human Factors Transforming Healthcare (HFTH) is an international network of HF practitioners who are embedded within hospitals and health systems. The goal of the network is to provide resources for human factors practitioners and healthcare organizations looking to successfully apply HF principles to improve patient care and provider performance. The network also serves as collaborative platform for human factors practitioners, students, faculty, industry partners, and those curious about human factors in healthcare. 


=== Related organizations ===
The Institute of Occupational Medicine (IOM) was founded by the coal industry in 1969. From the outset the IOM employed an ergonomics staff to apply ergonomics principles to the design of mining machinery and environments. To this day, the IOM continues ergonomics activities, especially in the fields of musculoskeletal disorders; heat stress and the ergonomics of personal protective equipment (PPE). Like many in occupational ergonomics, the demands and requirements of an ageing UK workforce are a growing concern and interest to IOM ergonomists.
The International Society of Automotive Engineers (SAE) is a professional organization for mobility engineering professionals in the aerospace, automotive, and commercial vehicle industries. The Society is a standards development organization for the engineering of powered vehicles of all kinds, including cars, trucks, boats, aircraft, and others. The Society of Automotive Engineers has established a number of standards used in the automotive industry and elsewhere. It encourages the design of vehicles in accordance with established human factors principles. It is one of the most influential organizations with respect to ergonomics work in automotive design. This society regularly holds conferences which address topics spanning all aspects of human factors and ergonomics.


== Practitioners ==
Human factors practitioners come from a variety of backgrounds, though predominantly they are psychologists (from the various subfields of industrial and organizational psychology, engineering psychology, cognitive psychology, perceptual psychology, applied psychology, and experimental psychology) and physiologists. Designers (industrial, interaction, and graphic), anthropologists, technical communication scholars and computer scientists also contribute. Typically, an ergonomist will have an undergraduate degree in psychology, engineering, design or health sciences, and usually a master's degree or doctoral degree in a related discipline. Though some practitioners enter the field of human factors from other disciplines, both M.S. and PhD degrees in Human Factors Engineering are available from several universities worldwide.


=== Sedentary workplace ===
Contemporary offices did not exist until the 1830s, with Wojciech Jastrzębowsk's seminal book on MSDergonomics following in 1857  and the first published study of posture appearing in 1955.As the American workforce began to shift towards sedentary employment, the prevalence of [WMSD/cognitive issues/ etc..] began to rise. In 1900, 41% of the US workforce was employed in agriculture but by 2000 that had dropped to 1.9%  This coincides with an increase in growth in desk-based employment (25% of all employment in 2000)  and the surveillance of non-fatal workplace injuries by OSHA and Bureau of Labor Statistics in 1971. 0–1.5 and occurs in a sitting or reclining position. Adults older than 50 years report spending more time sedentary and for adults older than 65 years this is often 80% of their awake time. Multiple studies show a dose-response relationship between sedentary time and all-cause mortality with an increase of 3% mortality per additional sedentary hour each day. High quantities of sedentary time without breaks is correlated to higher risk of chronic disease, obesity, cardiovascular disease, type 2 diabetes and cancer.Currently, there is a large proportion of the overall workforce who is employed in low physical activity occupations. Sedentary behavior, such as spending long periods of time in seated positions poses a serious threat for injuries and additional health risks. Unfortunately, even though some workplaces make an effort to provide a well designed environment for sedentary employees, any employee who is performing large amounts of sitting will likely suffer discomfort. 
There are existing conditions that would predispose both individuals and populations to an increase in prevalence of living sedentary lifestyles, including: socioeconomic determinants, education levels, occupation, living environment, age (as mentioned above) and more. A study published by the Iranian Journal of Public Health examined socioeconomic factors and sedentary lifestyle effects for individuals in a working community. The study concluded that individuals who reported living in low income environments were more inclined to living sedentary behavior compared to those who reported being of high socioeconomic status. Individuals who achieve less education are also considered to be a high risk group to partake in sedentary lifestyles, however, each community is different and has different resources available that may vary this risk.  Oftentimes, larger worksites are associated with increased occupational sitting. Those who work in environments that are classified as business and office jobs are typically more exposed to sitting and sedentary behavior while in the workplace. Additionally, occupations that are full-time, have schedule flexibility, are also included in that demographic, and are more likely to sit often throughout their workday.


=== Policy implementation ===
Obstacles surrounding better ergonomic features to sedentary employees include cost, time, effort and for both companies and employees. The evidence above helps establish the importance of ergonomics in a sedentary workplace, yet missing information from this problem is enforcement and policy implementation. As a modernized workplace becomes more and more technology based more jobs are becoming primarily seated, therefore leading to a need to prevent chronic injuries and pain.  This is becoming easier with the amount of research around ergonomic tools saving money companies by limiting the number of days missed from work and workers comp cases. The way to ensure that corporations prioritize these health outcomes for their employees is through policy and implementation.Nationwide there are no policies that are currently in place, however a handful of big companies and states have taken on cultural policies to insure the safety of all workers. For example, the state of Nevada risk management department has established a set of ground rules for both agencies responsibilities and employees responsibilities.  The agency responsibilities include evaluating workstations, using risk management resources when necessary and keeping OSHA records. To see specific workstation ergonomic policies and responsibilities click here.


== Methods ==
Until recently, methods used to evaluate human factors and ergonomics ranged from simple questionnaires to more complex and expensive usability labs. Some of the more common human factors methods are listed below:

Ethnographic analysis: Using methods derived from ethnography, this process focuses on observing the uses of technology in a practical environment. It is a qualitative and observational method that focuses on ""real-world"" experience and pressures, and the usage of technology or environments in the workplace. The process is best used early in the design process.
Focus Groups are another form of qualitative research in which one individual will facilitate discussion and elicit opinions about the technology or process under investigation. This can be on a one-to-one interview basis, or in a group session. Can be used to gain a large quantity of deep qualitative data, though due to the small sample size, can be subject to a higher degree of individual bias. Can be used at any point in the design process, as it is largely dependent on the exact questions to be pursued, and the structure of the group. Can be extremely costly.
Iterative design: Also known as prototyping, the iterative design process seeks to involve users at several stages of design, to correct problems as they emerge. As prototypes emerge from the design process, these are subjected to other forms of analysis as outlined in this article, and the results are then taken and incorporated into the new design. Trends among users are analyzed, and products redesigned. This can become a costly process, and needs to be done as soon as possible in the design process before designs become too concrete.
Meta-analysis: A supplementary technique used to examine a wide body of already existing data or literature to derive trends or form hypotheses to aid design decisions. As part of a literature survey, a meta-analysis can be performed to discern a collective trend from individual variables.
Subjects-in-tandem: Two subjects are asked to work concurrently on a series of tasks while vocalizing their analytical observations. The technique is also known as ""Co-Discovery"" as participants tend to feed off of each other's comments to generate a richer set of observations than is often possible with the participants separately. This is observed by the researcher, and can be used to discover usability difficulties. This process is usually recorded.
Surveys and questionnaires: A commonly used technique outside of human factors as well, surveys and questionnaires have an advantage in that they can be administered to a large group of people for relatively low cost, enabling the researcher to gain a large amount of data. The validity of the data obtained is, however, always in question, as the questions must be written and interpreted correctly, and are, by definition, subjective. Those who actually respond are in effect self-selecting as well, widening the gap between the sample and the population further.
Task analysis: A process with roots in activity theory, task analysis is a way of systematically describing human interaction with a system or process to understand how to match the demands of the system or process to human capabilities. The complexity of this process is generally proportional to the complexity of the task being analyzed, and so can vary in cost and time involvement. It is a qualitative and observational process. Best used early in the design process.
Human performance modeling: A method of quantifying human behavior, cognition, and processes; a tool used by human factors researchers and practitioners for both the analysis of human function and for the development of systems designed for optimal user experience and interaction.
Think aloud protocol: Also known as ""concurrent verbal protocol"", this is the process of asking a user to execute a series of tasks or use technology, while continuously verbalizing their thoughts so that a researcher can gain insights as to the users' analytical process. Can be useful for finding design flaws that do not affect task performance, but may have a negative cognitive effect on the user. Also useful for utilizing experts to better understand procedural knowledge of the task in question. Less expensive than focus groups, but tends to be more specific and subjective.
User analysis: This process is based around designing for the attributes of the intended user or operator, establishing the characteristics that define them, creating a persona for the user. Best done at the outset of the design process, a user analysis will attempt to predict the most common users, and the characteristics that they would be assumed to have in common. This can be problematic if the design concept does not match the actual user, or if the identified are too vague to make clear design decisions from. This process is, however, usually quite inexpensive, and commonly used.
""Wizard of Oz"": This is a comparatively uncommon technique but has seen some use in mobile devices. Based upon the Wizard of Oz experiment, this technique involves an operator who remotely controls the operation of a device to imitate the response of an actual computer program. It has the advantage of producing a highly changeable set of reactions, but can be quite costly and difficult to undertake.
Methods analysis is the process of studying the tasks a worker completes using a step-by-step investigation. Each task in broken down into smaller steps until each motion the worker performs is described. Doing so enables you to see exactly where repetitive or straining tasks occur.
Time studies determine the time required for a worker to complete each task. Time studies are often used to analyze cyclical jobs. They are considered ""event based"" studies because time measurements are triggered by the occurrence of predetermined events.
Work sampling is a method in which the job is sampled at random intervals to determine the proportion of total time spent on a particular task. It provides insight into how often workers are performing tasks which might cause strain on their bodies.
Predetermined time systems are methods for analyzing the time spent by workers on a particular task. One of the most widely used predetermined time system is called Methods-Time-Measurement. Other common work measurement systems include MODAPTS and MOST. Industry specific applications based on PTS are Seweasy, MODAPTS and GSD as seen in paper:  Miller, Doug (2013). ""Towards Sustainable Labour Costing in UK Fashion Retail"". SSRN Electronic Journal. doi:10.2139/ssrn.2212100. S2CID 166733679. .
Cognitive walkthrough: This method is a usability inspection method in which the evaluators can apply user perspective to task scenarios to identify design problems. As applied to macroergonomics, evaluators are able to analyze the usability of work system designs to identify how well a work system is organized and how well the workflow is integrated.
Kansei method: This is a method that transforms consumer's responses to new products into design specifications. As applied to macroergonomics, this method can translate employee's responses to changes to a work system into design specifications.
High Integration of Technology, Organization, and People: This is a manual procedure done step-by-step to apply technological change to the workplace. It allows managers to be more aware of the human and organizational aspects of their technology plans, allowing them to efficiently integrate technology in these contexts.
Top modeler: This model helps manufacturing companies identify the organizational changes needed when new technologies are being considered for their process.
Computer-integrated Manufacturing, Organization, and People System Design: This model allows for evaluating computer-integrated manufacturing, organization, and people system design based on knowledge of the system.
Anthropotechnology: This method considers analysis and design modification of systems for the efficient transfer of technology from one culture to another.
Systems analysis tool: This is a method to conduct systematic trade-off evaluations of work-system intervention alternatives.
Macroergonomic analysis of structure: This method analyzes the structure of work systems according to their compatibility with unique sociotechnical aspects.
Macroergonomic analysis and design: This method assesses work-system processes by using a ten-step process.
Virtual manufacturing and response surface methodology: This method uses computerized tools and statistical analysis for workstation design.


=== Weaknesses ===
Problems related to measures of usability include the fact that measures of learning and retention of how to use an interface are rarely employed and some studies treat measures of how users interact with interfaces as synonymous with quality-in-use, despite an unclear relation.Although field methods can be extremely useful because they are conducted in the users' natural environment, they have some major limitations to consider. The limitations include:

Usually take more time and resources than other methods
Very high effort in planning, recruiting, and executing compared with other methods
Much longer study periods and therefore requires much goodwill among the participants
Studies are longitudinal in nature, therefore, attrition can become a problem.


== See also ==


== References ==


== Further reading ==
Books

Thomas J. Armstrong (2008), Chapter 10: Allowances, Localized Fatigue, Musculoskeletal Disorders, and Biomechanics (not yet published)
Berlin C. & Adams C. & 2017. Production Ergonomics: Designing Work Systems to Support Optimal Human Performance. London: Ubiquity Press. DOI: https://doi.org/10.5334/bbe
Jan Dul and Bernard Weedmaster, Ergonomics for Beginners. A classic introduction on ergonomics – Original title: Vademecum Ergonomie (Dutch)—published and updated since the 1960s.
Valerie J Gawron (2000), Human Performance Measures Handbook Lawrence Erlbaum Associates – A useful summary of human performance measures.
Lee, J.D.; Wickens, C.D.; Liu Y.; Boyle, L.N (2017). Designing for People: An introduction to human factors engineering, 3nd Edition. Charleston, SC: CreateSpace. ISBN 9781539808008.
Liu, Y (2007). IOE 333. Course pack. Industrial and Operations Engineering 333 (Introduction to Ergonomics), University of Michigan, Ann Arbor, MI. Winter 2007
Meister, D. (1999). The History of Human Factors and Ergonomics. Mahwah, N.J.: Lawrence Erlbaum Associates. ISBN 978-0-8058-2769-9.
Donald Norman, The Design of Everyday Things—An entertaining user-centered critique of nearly every gadget out there (at the time it was published)
Peter Opsvik (2009), ""Re-Thinking Sitting"" Interesting insights on the history of the chair and how we sit from an ergonomic pioneer
Oviatt, S. L.; Cohen, P. R. (March 2000). ""Multimodal systems that process what comes naturally"". Communications of the ACM. 43 (3): 45–53. doi:10.1145/330534.330538. S2CID 1940810.
Computer Ergonomics & Work Related Upper Limb Disorder Prevention- Making The Business Case For Pro-active Ergonomics (Rooney et al., 2008)
Stephen Pheasant, Bodyspace—A classic exploration of ergonomics
Sarter, N. B.; Cohen, P. R. (2002). Multimodal information presentation in support of human-automation communication and coordination. Advances in Human Performance and Cognitive Engineering Research. 2. pp. 13–36. doi:10.1016/S1479-3601(02)02004-0. ISBN 978-0-7623-0748-7.
Smith, Thomas J.;  et al. (2015). Variability in Human performance. CRC Press. ISBN 978-1-4665-7972-9.
Alvin R. Tilley & Henry Dreyfuss Associates (1993, 2002), The Measure of Man & Woman: Human Factors in Design  A human factors design manual.
Kim Vicente, The Human Factor Full of examples and statistics illustrating the gap between existing technology and the human mind, with suggestions to narrow it
Wickens, C.D.; Lee J.D.; Liu Y.; Gorden Becker S.E. (2003). An Introduction to Human Factors Engineering, 2nd Edition. Prentice Hall. ISBN 978-0-321-01229-6.
Wickens, C. D.; Sandy, D. L.; Vidulich, M. (1983). ""Compatibility and resource competition between modalities of input, central processing, and output"". Human Factors. 25 (2): 227–248. doi:10.1177/001872088302500209. ISSN 0018-7208. PMID 6862451. S2CID 1291342.Wu, S. (2011). ""Warranty claims analysis considering human factors"" (PDF). Reliability Engineering & System Safety. 96: 131–138. doi:10.1016/j.ress.2010.07.010.
Wickens and Hollands (2000). Engineering Psychology and Human Performance. Discusses memory, attention, decision making, stress and human error, among other topics
Wilson & Corlett, Evaluation of Human Work A practical ergonomics methodology. Warning: very technical and not a suitable 'intro' to ergonomics
Zamprotta, Luigi, La qualité comme philosophie de la production.Interaction avec l'ergonomie et perspectives futures, thèse de Maîtrise ès Sciences Appliquées – Informatique, Institut d'Etudes Supérieures L'Avenir, Bruxelles, année universitaire 1992–93, TIU [1] Press, Independence, Missouri (USA), 1994, ISBN 0-89697-452-9Peer-reviewed Journals (numbers between brackets are the ISI impact factor, followed by the date)

Behavior & Information Technology (0.915, 2008)
Ergonomics (0.747, 2001–2003)
Ergonomics in Design (-)
Applied Ergonomics (1.713, 2015)
Human Factors (1.37, 2015)
International Journal of Industrial Ergonomics (0.395, 2001–2003)
Human Factors and Ergonomics in Manufacturing (0.311, 2001–2003)
Travail Humain (0.260, 2001–2003)
Theoretical Issues in Ergonomics Science (-)
International Journal of Human Factors and Ergonomics (-)
International Journal of Occupational Safety and Ergonomics (-)


== External links ==
Directory of Design Support Methods Directory of Design Support Methods
Engineering Data Compendium of Human Perception and Performance
Index of Non-Government Standards on Human Engineering...
Index of Government Standards on Human Engineering...
NIOSH Topic Page on Ergonomics and Musculoskeletal Disorders
Office Ergonomics Information from European Agency for Safety and Health at Work
Human Factors Standards & Handbooks from the University of Maryland Department of Mechanical Engineering
Human Factors and Ergonomics Resources
Human Factors Engineering Collection, The University of Alabama in Huntsville Archives and Special Collections","pandas(index=195, _1=195, text='human factors and ergonomics (commonly referred to as human factors) is the application of psychological and physiological principles to the engineering and design of products, processes, and systems. the goal of human factors is to reduce human error, increase productivity, and enhance safety and comfort with a specific focus on the interaction between the human and the thing of interest.the field is a combination of numerous disciplines, such as psychology, sociology, engineering, biomechanics, industrial design, physiology, anthropometry, interaction design, visual design, user experience, and user interface design. in research, human factors employs the scientific method to study human behavior so that the resultant data may be applied to the four primary goals. in essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. the two terms ""human factors"" and ""ergonomics"" are essentially synonymous.the international ergonomics association defines ergonomics or human factors as follows: ergonomics (or human factors) is the scientific discipline concerned with the understanding of interactions among humans and other elements of a system, and the profession that applies theory, principles, data and methods to design to optimize human well-being and overall system performance. human factors is employed to fulfill the goals of occupational health and safety and productivity. it is relevant in the design of such things as safe furniture and easy-to-use interfaces to machines and equipment. proper ergonomic design is necessary to prevent repetitive strain injuries and other musculoskeletal disorders, which can develop over time and can lead to long-term disability. human factors and ergonomics are concerned with the ""fit"" between the user, equipment, and environment or ""fitting a job to a person"". it accounts for the user\'s capabilities and limitations in seeking to ensure that tasks, functions, information, and the environment suit that user. to assess the fit between a person and the used technology, human factors specialists or ergonomists consider the job (activity) being done and the demands on the user; the equipment used (its size, shape, and how appropriate it is for the task), and the information used (how it is presented, accessed, and changed). ergonomics draws on many disciplines in its study of humans and their environments, including anthropometry, biomechanics, mechanical engineering, industrial engineering, industrial design, information design, kinesiology, physiology, cognitive psychology, industrial and organizational psychology, and space psychology.   == etymology == the term ergonomics (from the greek ἔργον, meaning ""work"", and νόμος, meaning ""natural law"") first entered the modern lexicon when polish scientist wojciech jastrzębowski used the word in his 1857 article rys ergonomji czyli nauki o pracy, opartej na prawdach poczerpniętych z nauki przyrody (the outline of ergonomics; i.e. science of work, based on the truths taken from the natural science). the french scholar jean-gustave courcelle-seneuil, apparently without knowledge of jastrzębowski\'s article, used the word with a slightly different meaning in 1858. the introduction of the term to the english lexicon is widely attributed to british psychologist hywel murrell, at the 1949 meeting at the uk\'s admiralty, which led to the foundation of the ergonomics society. he used it to encompass the studies in which he had been engaged during and after world war ii.the expression human factors is a predominantly north american term which has been adopted to emphasize the application of the same methods to non-work-related situations. a ""human factor"" is a physical or cognitive property of an individual or social behavior specific to humans that may influence the functioning of technological systems. the terms ""human factors"" and ""ergonomics"" are essentially synonymous.   == domains of specialization == ergonomics comprise three main fields of research: physical, cognitive and organizational ergonomics. there are many specializations within these broad categories. specializations in the field of physical ergonomics may include visual ergonomics. specializations within the field of cognitive ergonomics may include usability, human–computer interaction, and user experience engineering. some specializations may cut across these domains: environmental ergonomics is concerned with human interaction with the environment as characterized by climate, temperature, pressure, vibration, light. the emerging field of human factors in highway safety uses human factor principles to understand the actions and capabilities of road users – car and truck drivers, pedestrians, cyclists, etc. – and use this knowledge to design roads and streets to reduce traffic collisions. driver error is listed as a contributing factor in 44% of fatal collisions in the united states, so a topic of particular interest is how road users gather and process information about the road and its environment, and how to assist them to make the appropriate decision.new terms are being generated all the time. for instance, ""user trial engineer"" may refer to a human factors professional who specializes in user trials. although the names change, human factors professionals apply an understanding of human factors to the design of equipment, systems and working methods to improve comfort, health, safety, and productivity. according to the international ergonomics association, within the discipline of ergonomics there exist domains of specialization. problems related to measures of usability include the fact that measures of learning and retention of how to use an interface are rarely employed and some studies treat measures of how users interact with interfaces as synonymous with quality-in-use, despite an unclear relation.although field methods can be extremely useful because they are conducted in the users\' natural environment, they have some major limitations to consider. the limitations include:  usually take more time and resources than other methods very high effort in planning, recruiting, and executing compared with other methods much longer study periods and therefore requires much goodwill among the participants studies are longitudinal in nature, therefore, attrition can become a problem.   == see also ==   == references ==   == further reading == books  thomas j. armstrong (2008), chapter 10: allowances, localized fatigue, musculoskeletal disorders, and biomechanics (not yet published) berlin c. & adams c. & 2017. production ergonomics: designing work systems to support optimal human performance. london: ubiquity press. doi: https://doi.org/10.5334/bbe jan dul and bernard weedmaster, ergonomics for beginners. a classic introduction on ergonomics – original title: vademecum ergonomie (dutch)—published and updated since the 1960s. valerie j gawron (2000), human performance measures handbook lawrence erlbaum associates – a useful summary of human performance measures. lee, j.d.; wickens, c.d.; liu y.; boyle, l.n (2017). designing for people: an introduction to human factors engineering, 3nd edition. charleston, sc: createspace. isbn 9781539808008. liu, y (2007). ioe 333. course pack. industrial and operations engineering 333 (introduction to ergonomics), university of michigan, ann arbor, mi. winter 2007 meister, d. (1999). the history of human factors and ergonomics. mahwah, n.j.: lawrence erlbaum associates. isbn 978-0-8058-2769-9. donald norman, the design of everyday things—an entertaining user-centered critique of nearly every gadget out there (at the time it was published) peter opsvik (2009), ""re-thinking sitting"" interesting insights on the history of the chair and how we sit from an ergonomic pioneer oviatt, s. l.; cohen, p. r. (march 2000). ""multimodal systems that process what comes naturally"". communications of the acm. 43 (3): 45–53. doi:10.1145/330534.330538. s2cid 1940810. computer ergonomics & work related upper limb disorder prevention- making the business case for pro-active ergonomics (rooney et al., 2008) stephen pheasant, bodyspace—a classic exploration of ergonomics sarter, n. b.; cohen, p. r. (2002). multimodal information presentation in support of human-automation communication and coordination. advances in human performance and cognitive engineering research. 2. pp. 13–36. doi:10.1016/s1479-3601(02)02004-0. isbn 978-0-7623-0748-7. smith, thomas j.;  et al. (2015). variability in human performance. crc press. isbn 978-1-4665-7972-9. alvin r. tilley & henry dreyfuss associates (1993, 2002), the measure of man & woman: human factors in design  a human factors design manual. kim vicente, the human factor full of examples and statistics illustrating the gap between existing technology and the human mind, with suggestions to narrow it wickens, c.d.; lee j.d.; liu y.; gorden becker s.e. (2003). an introduction to human factors engineering, 2nd edition. prentice hall. isbn 978-0-321-01229-6. wickens, c. d.; sandy, d. l.; vidulich, m. (1983). ""compatibility and resource competition between modalities of input, central processing, and output"". human factors. 25 (2): 227–248. doi:10.1177/001872088302500209. issn 0018-7208. pmid 6862451. s2cid 1291342.wu, s. (2011). ""warranty claims analysis considering human factors"" (pdf). reliability engineering & system safety. 96: 131–138. doi:10.1016/j.ress.2010.07.010. wickens and hollands (2000). engineering psychology and human performance. discusses memory, attention, decision making, stress and human error, among other topics wilson & corlett, evaluation of human work a practical ergonomics methodology. warning: very technical and not a suitable \'intro\' to ergonomics zamprotta, luigi, la qualité comme philosophie de la production.interaction avec l\'ergonomie et perspectives futures, thèse de maîtrise ès sciences appliquées – informatique, institut d\'etudes supérieures l\'avenir, bruxelles, année universitaire 1992–93, tiu [1] press, independence, missouri (usa), 1994, isbn 0-89697-452-9peer-reviewed journals (numbers between brackets are the isi impact factor, followed by the date)  behavior & information technology (0.915, 2008) ergonomics (0.747, 2001–2003) ergonomics in design (-) applied ergonomics (1.713, 2015) human factors (1.37, 2015) international journal of industrial ergonomics (0.395, 2001–2003) human factors and ergonomics in manufacturing (0.311, 2001–2003) travail humain (0.260, 2001–2003) theoretical issues in ergonomics science (-) international journal of human factors and ergonomics (-) international journal of occupational safety and ergonomics (-)   == external links == directory of design support methods directory of design support methods engineering data compendium of human perception and performance index of non-government standards on human engineering... index of government standards on human engineering... niosh topic page on ergonomics and musculoskeletal disorders office ergonomics information from european agency for safety and health at work human factors standards & handbooks from the university of maryland department of mechanical engineering human factors and ergonomics resources human factors engineering collection, the university of alabama in huntsville archives and special collections')"
196,"In operations management and industrial engineering, production flow analysis refers to methods which share the following characteristics:

Classification of machines
Technological cycles information control
Generating a binary product-machines matrix (1 if a given product requires processing in a given machine, 0 otherwise)Methods differ on how they group together machines with products. These play an important role in designing manufacturing cells.


== Rank Order Clustering ==
Given a binary product-machines n-by-m matrix 
  
    
      
        
          b
          
            i
            p
          
        
      
    
    {\displaystyle b_{ip}}
  , Rank Order Clustering is an algorithm characterized by the following steps:

For each row i compute the number 
  
    
      
        
          ∑
          
            p
            =
            1
          
          
            m
          
        
        
          b
          
            i
            p
          
        
        ∗
        
          2
          
            m
            −
            p
          
        
      
    
    {\displaystyle \sum _{p=1}^{m}b_{ip}*2^{m-p}}
  
Order rows according to descending numbers previously computed
For each column p compute the number 
  
    
      
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          b
          
            i
            p
          
        
        ∗
        
          2
          
            n
            −
            i
          
        
      
    
    {\displaystyle \sum _{i=1}^{n}b_{ip}*2^{n-i}}
  
Order columns according to descending numbers previously computed
If on steps 2 and 4 no reordering happened go to step 6, otherwise go to step 1
Stop


== Similarity coefficients ==
Given a binary product-machines n-by-m matrix, the algorithm proceeds by the following steps:

Compute the similarity coefficient 
  
    
      
        
          s
          
            i
            j
          
        
        =
        
          n
          
            i
            j
          
        
        
          /
        
        (
        
          n
          
            i
            j
          
        
        +
        u
        )
      
    
    {\displaystyle s_{ij}=n_{ij}/(n_{ij}+u)}
   for all  with 
  
    
      
        
          n
          
            i
            j
          
        
      
    
    {\displaystyle n_{ij}}
   being the number of products that need to be processed on both machine i and machine j, u comprises the number of components which visit machine j but not k and vice versa.
Group together in cell k the tuple (i*,j*) with higher similarity coefficient, with k being the algorithm iteration index
Remove row i* and column j* from the original binary matrix and substitute for the row and column of the cell k, 
  
    
      
        
          s
          
            r
            k
          
        
        =
        m
        a
        x
        (
        
          s
          
            r
            i
            ∗
          
        
        ,
        
          s
          
            r
            j
            ∗
          
        
        )
      
    
    {\displaystyle s_{rk}=max(s_{ri*},s_{rj*})}
  
Go to step 2, iteration index k raised by oneUnless this procedure is stopped the algorithm eventually will put all machines in one single group.


== References ==","pandas(index=196, _1=196, text='in operations management and industrial engineering, production flow analysis refers to methods which share the following characteristics:  classification of machines technological cycles information control generating a binary product-machines matrix (1 if a given product requires processing in a given machine, 0 otherwise)methods differ on how they group together machines with products. these play an important role in designing manufacturing cells.   == rank order clustering == given a binary product-machines n-by-m matrix     b  i p       go to step 2, iteration index k raised by oneunless this procedure is stopped the algorithm eventually will put all machines in one single group.   == references ==')"
197,"Industrial and production engineering (IPE) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. It is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. Industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by Sir Adam Smith, Henry Ford, Eli Whitney, Frank Gilbreth and Lilian Gilbreth, Henry Gantt, F.W. Taylor, etc. After the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. Industrial and production engineering includes three areas: Mechanical engineering (where the production engineering comes from), industrial engineering, and management science. 
The objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable. Industrial engineering is concerned with the development, improvement, and implementation of integrated systems of people, money, knowledge, information, equipment, energy, materials, as well as analysis and synthesis. The principles of IPE include mathematical, physical and social sciences and methods of engineering design to specify, predict, and evaluate the results to be obtained from the systems or processes currently in place or being developed. The target of production engineering is to complete the production process in the smoothest, most-judicious and most-economic way. Production engineering also overlaps substantially with manufacturing engineering and industrial engineering. The concept of production engineering is interchangeable with manufacturing engineering.
As for education, undergraduates normally start off by taking courses such as physics, mathematics (calculus, linear analysis, differential equations), computer science, and chemistry. Undergraduates will take more major specific courses like production and inventory scheduling, process management, CAD/CAM manufacturing, ergonomics, etc., towards the later years of their undergraduate careers. In some parts of the world, universities will offer Bachelor's in Industrial and Production Engineering. However, most universities in the U.S. will offer them separately. Various career paths that may follow for industrial and production engineers include: Plant Engineers, Manufacturing Engineers, Quality Engineers, Process Engineers and industrial managers, project management, manufacturing, production and distribution, From the various career paths people can take as an industrial and production engineer, most average a starting salary of at least $50,000.


== History ==


=== Industrial Revolution ===
The roots of the Industrial Engineering Profession date back to the Industrial Revolution.  The technologies that helped mechanize traditional manual operations in the textile industry including the Flying shuttle, the Spinning jenny, and perhaps most importantly the Steam engine generated Economies of scale that made Mass production in centralized locations attractive for the first time.  The concept of the production system had its genesis in the factories created by these innovations.


=== Specialization of labor ===
Adam Smith's concepts of Division of Labour and the ""Invisible Hand"" of capitalism introduced in his treatise ""The Wealth of Nations"" motivated many of the technological innovators of the Industrial revolution to establish and implement factory systems.  The efforts of James Watt and Matthew Boulton led to the first integrated machine manufacturing facility in the world, including the implementation of concepts such as cost control systems to reduce waste and increase productivity and the institution of skills training for craftsmen.Charles Babbage became associated with Industrial engineering because of the concepts he introduced in his book ""On the Economy of Machinery and Manufacturers"" which he wrote as a result of his visits to factories in England and the United States in the early 1800s.  The book includes subjects such as the time required to perform a specific task, the effects of subdividing tasks into smaller and less detailed elements, and the advantages to be gained from repetitive tasks.


=== Interchangeable parts ===
Eli Whitney and Simeon North proved the feasibility of the notion of Interchangeable parts in the manufacture of muskets and pistols for the US Government.  Under this system, individual parts were mass-produced to tolerances to enable their use in any finished product.  The result was a significant reduction in the need for skill from specialized workers, which eventually led to the industrial environment to be studied later.


=== Modern development ===


==== Industrial engineering ====
In 1960 to 1975, with the development of decision support systems in supply such as the Material requirements planning (MRP), people can emphasize the timing issue (inventory, production, compounding, transportation, etc.) of industrial organization. Israeli scientist Dr. Jacob Rubinovitz installed the CMMS program developed in IAI and Control-Data (Israel) in 1976 in South Africa and worldwide.In the seventies, with the penetration of Japanese management theories such as Kaizen and Kanban, Japan realized very high levels of quality and productivity. These theories improved issues of quality, delivery time, and flexibility. Companies in the west realized the great impact of Kaizen and started implementing their own Continuous improvement programs.In the nineties, following the global industry globalization process, the emphasis was on supply chain management, and customer-oriented business process design. Theory of constraints developed by an Israeli scientist Eliyahu M. Goldratt (1985) is also a significant milestone in the field.


==== Manufacturing (production) engineering ====
Modern manufacturing engineering studies include all intermediate processes required for the production and integration of a product's components.Some industries, such as semiconductor and steel manufacturers use the term ""fabrication"" for these processes.

Automation is used in different processes of manufacturing such as machining and welding. Automated manufacturing refers to the application of automation to produce goods in a factory. The main advantages of automated manufacturing for the manufacturing process are realized with effective implementation of automation and include: higher consistency and quality, reduction of lead times, simplification of production, reduced handling, improved work flow, and improved worker morale.

Robotics is the application of mechatronics and automation to create robots, which are often used in manufacturing to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot). Robots are used extensively in manufacturing engineering.Robots allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform economically, and to ensure better quality. Many companies employ assembly lines of robots, and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications.


== Overview ==


=== Industrial engineering ===
Industrial engineering is the branch of engineering that involves figuring out how to make or do things better. Industrial engineers are concerned with reducing production costs, increasing efficiency, improving the quality of products and services, ensuring worker health and safety, protecting the environment and complying with government regulations.The various fields and topics that industrial engineers are involved with include:

Manufacturing Engineering
Engineering management
Process engineering: design, operation, control, and optimization of chemical, physical, and biological processes.
Systems engineering: an interdisciplinary field of engineering that focuses on how to design and manage complex engineering systems over their life cycles.
Software engineering: an interdisciplinary field of engineering that focusing on design, development, maintenance, testing, and evaluation of the software that make computers or other devices containing software work
Safety engineering: an engineering discipline which assures that engineered systems provide acceptable levels of safety.
Data science: the science of exploring, manipulating, analyzing, and visualizing data to derive useful insights and conclusions
Machine learning: the automation of learning from data using models and algorithms
Analytics and data mining: the discovery, interpretation, and extraction of patterns and insights from large quantities of data
Cost engineering: practice devoted to the management of project cost, involving such activities as cost- and control- estimating, which is cost control and cost forecasting, investment appraisal, and risk analysis.
Value engineering: a systematic method to improve the ""value"" of goods or products and services by using an examination of function.
Predetermined motion time system: a technique to quantify time required for repetitive tasks.
Quality engineering: a way of preventing mistakes or defects in manufactured products and avoiding problems when delivering solutions or services to customers.
Project management: is the process and activity of planning, organizing, motivating, and controlling resources, procedures and protocols to achieve specific goals in scientific or daily problems.
Supply chain management: the management of the flow of goods. It includes the movement and storage of raw materials, work-in-process inventory, and finished goods from point of origin to point of consumption.
Ergonomics: the practice of designing products, systems or processes to take proper account of the interaction between them and the people that use them.
Operations research, also known as management science: discipline that deals with the application of advanced analytical methods to help make better decisions
Operations management: an area of management concerned with overseeing, designing, and controlling the process of production and redesigning business operations in the production of goods or services.
Job design: the specification of contents, methods and relationship of jobs in order to satisfy technological and organizational requirements as well as the social and personal requirements of the job holder.
Financial engineering: the application of technical methods, especially from mathematical finance and computational finance, in the practice of finance
Industrial plant configuration: sizing of necessary infrastructure used in support and maintenance of a given facility.
Facility management: an interdisciplinary field devoted to the coordination of space, infrastructure, people and organization
Engineering design process: formulation of a plan to help an engineer build a product with a specified performance goal.
Logistics: the management of the flow of goods between the point of origin and the point of consumption in order to meet some requirements, of customers or corporations.
Accounting: the measurement, processing and communication of financial information about economic entities
Capital projects: the management of activities in capital projects involves the flow of resources, or inputs, as they are transformed into outputs. Many of the tools and principles of industrial engineering can be applied to the configuration of work activities within a project. The application of industrial engineering and operations management concepts and techniques to the execution of projects has been thus referred to as Project Production Management. Traditionally, a major aspect of industrial engineering was planning the layouts of factories and designing assembly lines and other manufacturing paradigms. And now, in lean manufacturing systems, industrial engineers work to eliminate wastes of time, money, materials, energy, and other resources.Examples of where industrial engineering might be used include flow process charting, process mapping, designing an assembly workstation, strategizing for various operational logistics, consulting as an efficiency expert, developing a new financial algorithm or loan system for a bank, streamlining operation and emergency room location or usage in a hospital, planning complex distribution schemes for materials or products (referred to as supply-chain management), and shortening lines (or queues) at a bank, hospital, or a theme park.Modern industrial engineers typically use predetermined motion time system, computer simulation (especially discrete event simulation), along with extensive mathematical tools for modeling, such as mathematical optimization and queueing theory, and computational methods for system analysis, evaluation, and optimization. Industrial engineers also use the tools of data science and machine learning in their work owing to the strong relatedness of these disciplines with the field and the similar technical background required of industrial engineers (including a strong foundation in probability theory, linear algebra, and statistics, as well as having coding skills).


=== Manufacturing (production) engineering ===
Manufacturing Engineering is based on core industrial engineering and mechanical engineering skills, adding important elements from mechatronics, commerce, economics and business management. This field also deals with the integration of different facilities and systems for producing quality products (with optimal expenditure) by applying the principles of physics and the results of manufacturing systems studies, such as the following:

Manufacturing engineers develop and create physical artifacts, production processes, and technology. It is a very broad area which includes the design and development of products. Manufacturing engineering is considered to be a sub-discipline of industrial engineering/systems engineering and has very strong overlaps with mechanical engineering. Manufacturing engineers' success or failure directly impacts the advancement of technology and the spread of innovation. This field of manufacturing engineering emerged from tool and die discipline in the early 20th century. It expanded greatly from the 1960s when industrialized countries introduced factories with:
1. Numerical control machine tools and automated systems of production.2. Advanced statistical methods of quality control: These factories were pioneered by the American electrical engineer William Edwards Deming, who was initially ignored by his home country. The same methods of quality control later turned Japanese factories into world leaders in cost-effectiveness and production quality.
3. Industrial robots on the factory floor, introduced in the late 1970s: These computer-controlled welding arms and grippers could perform simple tasks such as attaching a car door quickly and flawlessly 24 hours a day. This cut costs and improved production speed.


== Education ==


=== Industrial engineering ===


==== Undergraduate curriculum ====
In the United States the undergraduate degree earned is the Bachelor of Science (B.S.) or Bachelor of Science and Engineering (B.S.E.) in Industrial Engineering (IE).  Variations of the title include Industrial & Operations Engineering (IOE), and Industrial & Systems Engineering (ISE).  The typical curriculum includes a broad math and science foundation spanning chemistry, physics, mechanics (i.e., statics, kinematics, and dynamics), materials science, computer science, electronics/circuits, engineering design, and the standard range of engineering mathematics (i.e. calculus, linear algebra, differential equations, statistics). For any engineering undergraduate program to be accredited, regardless of concentration, it must cover a largely similar span of such foundational work – which also overlaps heavily with the content tested on one or more engineering licensure exams in most jurisdictions.
The coursework specific to IE entails specialized courses in aeas such as optimization, applied probability, stochastic modeling, design of experiments, statistical process control, simulation, manufacturing engineering, ergonomics/safety engineering, and engineering economics. Industrial engineering elective courses typically cover more specialized topics in areas such as manufacturing, supply chains and logistics, analytics and machine learning, production systems, human factors and industrial design, and service systems.Certain business schools may offer programs with some overlapping relevance to IE, but the engineering programs are distinguished by a much more intensely quantitative focus, required engineering science electives, and the core math and science courses required of all engineering programs.


==== Graduate curriculum ====
The usual graduate degree earned is the Master of Science (MS) or Master of Science and Engineering (MSE) in Industrial Engineering or various alternative related concentration titles. Typical MS curricula may cover:


=== Manufacturing (production) engineering ===


==== Degree certification programs ====
Manufacturing engineers possess an associate's or bachelor's degree in engineering with a major in manufacturing engineering. The length of study for such a degree is usually two to five years followed by five more years of professional practice to qualify as a professional engineer. Working as a manufacturing engineering technologist involves a more applications-oriented qualification path.
Academic degrees for manufacturing engineers are usually the Associate or Bachelor of Engineering, [BE] or [BEng], and the Associate or Bachelor of Science, [BS] or [BSc]. For manufacturing technologists the required degrees are Associate or Bachelor of Technology [B.TECH] or Associate or Bachelor of Applied Science [BASc] in Manufacturing, depending upon the university. Master's degrees in engineering manufacturing include Master of Engineering [ME] or [MEng] in Manufacturing, Master of Science [M.Sc] in Manufacturing Management, Master of Science [M.Sc] in Industrial and Production Management, and Master of Science [M.Sc] as well as Master of Engineering [ME] in Design, which is a subdiscipline of manufacturing. Doctoral [PhD] or [DEng] level courses in manufacturing are also available depending on the university.
The undergraduate degree curriculum generally includes courses in physics, mathematics, computer science, project management, and specific topics in mechanical and manufacturing engineering. Initially such topics cover most, if not all, of the subdisciplines of manufacturing engineering. Students then choose to specialize in one or more sub disciplines towards the end of their degree work.
Specific to Industrial Engineers, people will see courses covering ergonomics, scheduling, inventory management, forecasting, product development, and in general courses that focus on optimization. Most colleges breakdown the large sections of industrial engineering into Healthcare, Ergonomics, Product Development, or Consulting sectors. This allows for the student to get a good grasp on each of the varying sub-sectors so they know what area they are most interested about pursuing a career in.


==== Undergraduate curriculum ====
The Foundational Curriculum for a bachelor's degree of Manufacturing Engineering or Production Engineering includes below mentioned Syllabus. This Syllabus is closely related to Industrial Engineering and Mechanical Engineering. But it Differs by Placing more Emphasis on Manufacturing Science or Production Science. It includes following:

A degree in Manufacturing Engineering versus Mechanical Engineering will typically differ only by a few specialized classes. Mechanical Engineering degree focuses more on the Product Design Process and on Complex Products which requires more Mathematics Expertise.


== Manufacturing engineering certification ==


=== Professional engineering license ===
A Professional Engineer, PE,  is a licensed engineer who is permitted to offer professional services to the public. Professional Engineers may prepare, sign, seal, and submit engineering plans to the public. Before a candidate can become a professional engineer, they will need to receive a bachelor's degree from an ABET recognized university in the USA, take and pass the Fundamentals of Engineering exam to become an ""engineer-in-training"", and work four years under the supervision of a professional engineer. After those tasks are complete the candidate will be able to take the PE exam. Upon receiving a passing score on the test, the candidate will receive their PE License .


=== Society of Manufacturing Engineers (SME) certifications (USA) ===
The SME (society) administers qualifications specifically for the manufacturing industry. These are not degree level qualifications and are not recognized at the professional engineering level. The SME offers two certifications for Manufacturing engineers: Certified Manufacturing Technologist Certificate (CMfgT) and Certified Manufacturing Engineer (CMfgE).


==== Certified manufacturing technologist ====
Qualified candidates for the Certified Manufacturing Technologist Certificate (CMfgT) must pass a three-hour, 130-question multiple-choice exam. The exam covers math, manufacturing processes, manufacturing management, automation, and related subjects. A score of 60% or higher must be achieved to pass the exam. Additionally, a candidate must have at least four years of combined education and manufacturing-related work experience. The CMfgT certification must be renewed every three years in order to stay certified.


==== Certified manufacturing engineer ====
Certified Manufacturing Engineer (CMfgE) is an engineering qualification administered by the Society of Manufacturing Engineers, Dearborn, Michigan, USA. Candidates qualifying for a Certified Manufacturing Engineer credential must pass a four-hour, 180 question multiple-choice exam which covers more in-depth topics than does the CMfgT exam. A score of 60% or higher must be achieved to pass the exam. CMfgE candidates must also have eight years of combined education and manufacturing-related work experience, with a minimum of four years of work experience. The CMfgT certification must be renewed every three years in order to stay certified.


== Research ==


=== Industrial engineering ===


==== Human factors ====
The Human Factors area specializes in exploring how systems fit the people who must operate them, determining the roles of people with the systems, and selecting those people who can best fit particular roles within these systems. Students who focus on Human Factors will be able to work with a multidisciplinary team of faculty with strengths in understanding cognitive behavior as it relates to automation, air and ground transportation, medical studies, and space exploration.


==== Production systems ====
The Production Systems area develops new solutions in areas such as engineering design, supply chain management (e.g. supply chain system design, error recovery, large scale systems), manufacturing (e.g. system design, planning and scheduling), and medicine (e.g. disease diagnosis, discovery of medical knowledge). Students who focus on production systems will be able to work on topics related to computational intelligence theories for applications in industry, healthcare, and service organizations.
Biomanufacturing is our most recent research addition.


==== Reliability systems ====
The objective of the Reliability Systems area is to provide students with advanced data analysis and decision making techniques that will improve quality and reliability of complex systems. Students who focus on system reliability and uncertainty will be able to work on areas related to contemporary reliability systems including integration of quality and reliability, simultaneous life cycle design for manufacturing systems, decision theory in quality and reliability engineering, condition-based maintenance and degradation modeling, discrete event simulation and decision analysis.


==== Wind power management ====
The Wind Power Management Program aims at meeting the emerging needs for graduating professionals involved in design, operations, and management of wind farms deployed in massive numbers all over the country. The graduates will be able to fully understand the system and management issues of wind farms and their interactions with alternative and conventional power generation systems.


=== Production (manufacturing) engineering ===


==== Flexible manufacturing systems ====
A flexible manufacturing system (FMS) is a manufacturing system in which there is some amount of flexibility that allows the system to react to changes, whether predicted or unpredicted. This flexibility is generally considered to fall into two categories, both of which have numerous subcategories. The first category, machine flexibility, covers the system's ability to be changed to produce new product types and the ability to change the order of operations executed on a part. The second category, called routing flexibility, consists of the ability to use multiple machines to perform the same operation on a part, as well as the system's ability to absorb large-scale changes, such as in volume, capacity, or capability.
Most FMS systems comprise three main systems. The work machines, which are often automated CNC machines, are connected by a material handling system to optimize parts flow, and to a central control computer, which controls material movements and machine flow. The main advantages of an FMS is its high flexibility in managing manufacturing resources like time and effort in order to manufacture a new product. The best application of an FMS is found in the production of small sets of products from a mass production.


==== Computer integrated manufacturing ====

Computer-integrated manufacturing (CIM) in engineering is a method of manufacturing in which the entire production process is controlled by computer. Traditionally separated process methods are joined through a computer by CIM. This integration allows the processes to exchange information and to initiate actions. Through this integration, manufacturing can be faster and less error-prone, although the main advantage is the ability to create automated manufacturing processes. Typically CIM relies on closed-loop control processes based on real-time input from sensors. It is also known as flexible design and manufacturing.


==== Friction stir welding ====
Friction stir welding was discovered in 1991 by The Welding Institute (TWI). This innovative steady state (non-fusion) welding technique joins previously un-weldable materials, including several aluminum alloys. It may play an important role in the future construction of airplanes, potentially replacing rivets. Current uses of this technology to date include: welding the seams of the aluminum main space shuttle external tank, the Orion Crew Vehicle test article, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket; armor plating for amphibious assault ships; and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation, among an increasingly growing range of uses.


== Employment ==


=== Industrial engineering ===
The total number of engineers employed in the US in 2015 was roughly 1.6 million. Of these, 272,470 were industrial engineers (16.92%), the third most popular engineering specialty. The median salaries by experience level are $62,000 with 0–5 years experience, $75,000 with 5–10 years experience, and $81,000 with 10–20 years experience. The average starting salaries were $55,067 with a bachelor's degree, $77,364 with a master's degree, and $100,759 with a doctorate degree. This places industrial engineering at 7th of 15 among engineering bachelor's degrees, 3rd of 10 among master's degrees, and 2nd of 7 among doctorate degrees in average annual salary.  The median annual income of industrial engineers in the U.S. workforce is $83,470.


=== Production (manufacturing) engineering ===
Manufacturing engineering is just one facet of the engineering industry. Manufacturing engineers enjoy improving the production process from start to finish. They have the ability to keep the whole production process in mind as they focus on a particular portion of the process. Successful students in manufacturing engineering degree programs are inspired by the notion of starting with a natural resource, such as a block of wood, and ending with a usable, valuable product, such as a desk, produced efficiently and economically.
Manufacturing engineers are closely connected with engineering and industrial design efforts. Examples of major companies that employ manufacturing engineers in the United States include General Motors Corporation, Ford Motor Company, Chrysler, Boeing, Gates Corporation and Pfizer. Examples in Europe include Airbus, Daimler, BMW, Fiat, Navistar International, and Michelin Tyre.


=== Related industries ===
Industries where industrial and production engineers are generally employed include:


== Modern tools ==

Many manufacturing companies, especially those in industrialized nations, have begun to incorporate computer-aided engineering (CAE) programs, such as SolidWorks and AutoCAD, into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and ease of use in designing mating interfaces and tolerances.


=== SolidWorks ===
SolidWorks is an example of a CAD modeling computer program developed by Dassault Systèmes. SolidWorks is an industry standard for drafting designs and specifications for physical objects and has been used by more than 165,000 companies as of 2013.


=== AutoCAD ===
AutoCAD is an example of a CAD modeling computer program developed by Autodesk. AutoCad is also widely used for CAD modeling and CAE.Other CAE programs commonly used by product manufacturers include product life cycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM). Using CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. There is no need to create a physical prototype until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of relatively few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.
Just as manufacturing engineering is linked with other disciplines, such as mechatronics, multidisciplinary design optimization (MDO) is also being used with other CAE programs to automate and improve the iterative design process. MDO tools wrap around existing CAE processes by automating the process of trial and error method used by classical engineers. MDO uses a computer based algorithm that will iteratively seek better alternatives from an initial guess within given constants. MDO uses this procedure to determine the best design outcome and lists various options as well.


== Sub-disciplines ==


=== Mechanics ===

Classical Mechanics, attempts to use Newtons basic laws of motion to describe how a body will react when that body undergoes a force.  However modern mechanics includes the rather recent quantum theory. Sub disciplines of mechanics include:
Classical Mechanics:

Statics, the study of non-moving bodies at equilibrium.
Kinematics, is the study of the motion of bodies (objects) and systems (groups of objects), while ignoring the forces that cause the motion.
Dynamics (or kinetics), the study of how forces affect moving bodies.
Mechanics of materials, the study of how different materials deform under various types of stress.
Fluid mechanics, the study of how the principles of classical mechanics are observed with liquids and gases.
Continuum mechanics, a method of applying mechanics that assumes that objects are continuous (rather than discrete)Quantum:

Quantum mechanics, the study of atoms, molecules, electrons, protons, and neutrons on a sub atomic scale. This type of mechanics attempts to explain their motion and physical properties within an atom.If the engineering project were to design a vehicle, statics might be employed to design the frame of the vehicle in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the manufacture of the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle or to design the intake system for the engine.


=== Drafting ===

Drafting or technical drawing is the means by which manufacturers create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information. A skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions. Instructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Programs such as SolidWorks and AutoCAD  are examples of programs used to draft new parts and products under development.
Optionally, an engineer may also manually manufacture a part using the technical drawings, but this is becoming an increasing rarity with the advent of computer numerically controlled (CNC) manufacturing. Engineers primarily manufacture parts manually in the areas of applied spray coatings, finishes, and other processes that cannot economically or practically be done by a machine.
Drafting is used in nearly every sub discipline of mechanical and manufacturing engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).


=== Metal fabrication and machine tools ===
Metal fabrication is the building of metal structures by cutting, bending, and assembling processes. Technologies such as electron beam melting, laser engineered net shape, and direct metal laser sintering has allowed for the production of metal structures to become much less difficult when compared to other conventional metal fabrication methods. These help to alleviate various issues when the idealized CAD structures do not align with the actual fabricated structure.
Machine tools employ many types of tools that do the cutting or shaping of materials. Machine tools usually include many components consisting of motors, levers, arms, pulleys, and other basic simple systems to create a complex system that can build various things. All of these components must work correctly in order to stay on schedule and remain on task. Machine tools aim to efficiently and effectively produce good parts at a quick pace with a small amount of error.


=== Computer integrated manufacturing ===
Computer-integrated manufacturing (CIM) is the manufacturing approach of using computers to control the entire production process.  Computer-integrated manufacturing is used in automotive, aviation, space, and ship building industries. Computer-integrated manufacturing allows for data, through various sensing mechanisms to be observed during manufacturing. This type of manufacturing has computers controlling and observing every part of the process. This gives CIM a unique advantage over other manufacturing processes.


=== Mechatronics ===

Mechatronics is an engineering discipline that deals with the convergence of electrical, mechanical and manufacturing systems. Examples include automated manufacturing systems, heating, ventilation and air-conditioning systems, and various aircraft and automobile subsystems. A mechatronic system typically includes a mechanical skeleton, motors, controllers, sensors, actuators, and digital hardware. Mechatronics is greatly used in various applications of industrial processes and in automation.
The term mechatronics is typically used to refer to macroscopic systems, but futurists have predicted the emergence of very small electromechanical devices. Already such small devices, known as Microelectromechanical systems  (MEMS), are used in automobiles to initiate the deployment of airbags, in digital projectors to create sharper images, and in inkjet printers to create nozzles for high-definition printing. In future it is hoped that such devices will be used in tiny implantable medical devices and to improve optical communication.


=== Textile engineering ===
Textile engineering courses deal with the application of scientific and engineering principles to the design and control of all aspects of fiber, textile, and apparel processes, products, and machinery. These include natural and man-made materials, interaction of materials with machines, safety and health, energy conservation, and waste and pollution control. Additionally, students are given experience in plant design and layout, machine and wet process design and improvement, and designing and creating textile products. Throughout the textile engineering curriculum, students take classes from other engineering and disciplines including: mechanical, chemical, materials and industrial engineering.


=== Advanced composite materials ===
Advanced composite materials (engineering) (ACMs) are also known as advanced polymer matrix composites. These are generally characterized or determined by unusually high strength fibres with unusually high stiffness, or modulus of elasticity characteristics, compared to other materials, while bound together by weaker matrices. Advanced composite materials have broad, proven applications, in the aircraft, aerospace, and sports equipment sectors. Even more specifically ACMs are very attractive for aircraft and aerospace structural parts.  Manufacturing ACMs is a multibillion-dollar industry worldwide. Composite products range from skateboards to components of the space shuttle. The industry can be generally divided into two basic segments, industrial composites and advanced composites.


== See also ==

Associations

American Society for Engineering Education
American Society for Quality
European Students of Industrial Engineering and Management (ESTIEM)
Indian Institution of Industrial Engineering
Institute for Operations Research and the Management Sciences (INFORMS)
Institute of Industrial Engineers
Institution of Electrical Engineers
Society of Manufacturing Engineers


== References ==","pandas(index=197, _1=197, text='industrial and production engineering (ipe) is an interdisciplinary engineering discipline that includes manufacturing technology, engineering sciences, management science, and optimization of complex processes, systems, or organizations. it is concerned with the understanding and application of engineering procedures in manufacturing processes and production methods. industrial engineering dates back all the way to the industrial revolution, initiated in 1700s by sir adam smith, henry ford, eli whitney, frank gilbreth and lilian gilbreth, henry gantt, f.w. taylor, etc. after the 1970s, industrial and production engineering developed worldwide and started to widely use automation and robotics. industrial and production engineering includes three areas: mechanical engineering (where the production engineering comes from), industrial engineering, and management science. the objective is to improve efficiency, drive up effectiveness of manufacturing, quality control, and to reduce cost while making their products more attractive and marketable. industrial engineering is concerned with the development, improvement, and implementation of integrated systems of people, money, knowledge, information, equipment, energy, materials, as well as analysis and synthesis. the principles of ipe include mathematical, physical and social sciences and methods of engineering design to specify, predict, and evaluate the results to be obtained from the systems or processes currently in place or being developed. the target of production engineering is to complete the production process in the smoothest, most-judicious and most-economic way. production engineering also overlaps substantially with manufacturing engineering and industrial engineering. the concept of production engineering is interchangeable with manufacturing engineering. as for education, undergraduates normally start off by taking courses such as physics, mathematics (calculus, linear analysis, differential equations), computer science, and chemistry. undergraduates will take more major specific courses like production and inventory scheduling, process management, cad/cam manufacturing, ergonomics, etc., towards the later years of their undergraduate careers. in some parts of the world, universities will offer bachelor\'s in industrial and production engineering. however, most universities in the u.s. will offer them separately. various career paths that may follow for industrial and production engineers include: plant engineers, manufacturing engineers, quality engineers, process engineers and industrial managers, project management, manufacturing, production and distribution, from the various career paths people can take as an industrial and production engineer, most average a starting salary of at least $50,000.   == history == advanced composite materials (engineering) (acms) are also known as advanced polymer matrix composites. these are generally characterized or determined by unusually high strength fibres with unusually high stiffness, or modulus of elasticity characteristics, compared to other materials, while bound together by weaker matrices. advanced composite materials have broad, proven applications, in the aircraft, aerospace, and sports equipment sectors. even more specifically acms are very attractive for aircraft and aerospace structural parts.  manufacturing acms is a multibillion-dollar industry worldwide. composite products range from skateboards to components of the space shuttle. the industry can be generally divided into two basic segments, industrial composites and advanced composites.   == see also ==  associations  american society for engineering education american society for quality european students of industrial engineering and management (estiem) indian institution of industrial engineering institute for operations research and the management sciences (informs) institute of industrial engineers institution of electrical engineers society of manufacturing engineers   == references ==')"
198,"Asset health management or (AHM) is the field of study which looks at how to manage the ""health"" of an asset or assets.  This often includes methods to establish asset health and effort to decide the appropriate actions to be taken to manage the assets' health. This also includes the discussion of health at end of life to ensure the asset's full life is used efficiently.
Asset health management includes many different methods which can sometime overlap in their intended scope and methods.  Asset health management has become a difficult field to discuss due to the use of the same acronym to describe multiple different approaches and the use of the same approach with different names.  
Asset health management can be considered a subset of Asset management


== Asset health management taxonomy ==
Asset health management consists of multiple strategies and parts but a rough description is given as follows.
Asset health diagnosticsMethods used to establish the current health of the assetAsset health prognosticsMethods used to predict how the health of the asset will change in the futureAsset health maintenancesee Category:Maintenance
including maintenance, repair, and operations (better referred to as maintenance, repair, and overhaul). These methods are used to keep an asset health, restore health to an asset or through major work restore usable life to an asset.Asset end of life decisions
Asset disposal


== Management of multiple assets ==
There is often also a consideration of additional work done to manage the health of multiple assets within the same framework.
Sometimes referred to as Fleet health management and falling within the study of Fleet management. Although it is common to need to manage the health of multiple assets they are not always vehicles and frequently of mixed type.When resources are constrained it is a fascinating management problem to consider how best to manage the health of assets.  It is rare that assets can be managed in an unconstrained way  as resources are always limited by a need to make efficient use of them.


== Asset health management relevant standards ==
A collection of some standards which are often use to manage the health of assets.
This is not intended to be an exhaustive list and will organically improve.

Open O&M
MSG-3


== Asset health management examples ==
A short list is provided to illustrate the many methods that are some sort of asset health management method or philosophy.

IVHM
Built-in_self-test
Built-in_test_equipment
HUMS


== References ==","pandas(index=198, _1=198, text='asset health management or (ahm) is the field of study which looks at how to manage the ""health"" of an asset or assets.  this often includes methods to establish asset health and effort to decide the appropriate actions to be taken to manage the assets\' health. this also includes the discussion of health at end of life to ensure the asset\'s full life is used efficiently. asset health management includes many different methods which can sometime overlap in their intended scope and methods.  asset health management has become a difficult field to discuss due to the use of the same acronym to describe multiple different approaches and the use of the same approach with different names. asset health management can be considered a subset of asset management   == asset health management taxonomy == asset health management consists of multiple strategies and parts but a rough description is given as follows. asset health diagnosticsmethods used to establish the current health of the assetasset health prognosticsmethods used to predict how the health of the asset will change in the futureasset health maintenancesee category:maintenance including maintenance, repair, and operations (better referred to as maintenance, repair, and overhaul). these methods are used to keep an asset health, restore health to an asset or through major work restore usable life to an asset.asset end of life decisions asset disposal   == management of multiple assets == there is often also a consideration of additional work done to manage the health of multiple assets within the same framework. sometimes referred to as fleet health management and falling within the study of fleet management. although it is common to need to manage the health of multiple assets they are not always vehicles and frequently of mixed type.when resources are constrained it is a fascinating management problem to consider how best to manage the health of assets.  it is rare that assets can be managed in an unconstrained way  as resources are always limited by a need to make efficient use of them.   == asset health management relevant standards == a collection of some standards which are often use to manage the health of assets. this is not intended to be an exhaustive list and will organically improve.  open o&m msg-3   == asset health management examples == a short list is provided to illustrate the many methods that are some sort of asset health management method or philosophy.  ivhm built-in_self-test built-in_test_equipment hums   == references ==')"
199,"Human factors and ergonomics (commonly referred to as human factors) is the application of psychological and physiological principles to the engineering and design of products, processes, and systems. The goal of human factors is to reduce human error, increase productivity, and enhance safety and comfort with a specific focus on the interaction between the human and the thing of interest.The field is a combination of numerous disciplines, such as psychology, sociology, engineering, biomechanics, industrial design, physiology, anthropometry, interaction design, visual design, user experience, and user interface design. In research, human factors employs the scientific method to study human behavior so that the resultant data may be applied to the four primary goals. In essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. The two terms ""human factors"" and ""ergonomics"" are essentially synonymous.The International Ergonomics Association defines ergonomics or human factors as follows:
Ergonomics (or human factors) is the scientific discipline concerned with the understanding of interactions among humans and other elements of a system, and the profession that applies theory, principles, data and methods to design to optimize human well-being and overall system performance.
Human factors is employed to fulfill the goals of occupational health and safety and productivity. It is relevant in the design of such things as safe furniture and easy-to-use interfaces to machines and equipment. Proper ergonomic design is necessary to prevent repetitive strain injuries and other musculoskeletal disorders, which can develop over time and can lead to long-term disability. Human factors and ergonomics are concerned with the ""fit"" between the user, equipment, and environment or ""fitting a job to a person"". It accounts for the user's capabilities and limitations in seeking to ensure that tasks, functions, information, and the environment suit that user.
To assess the fit between a person and the used technology, human factors specialists or ergonomists consider the job (activity) being done and the demands on the user; the equipment used (its size, shape, and how appropriate it is for the task), and the information used (how it is presented, accessed, and changed). Ergonomics draws on many disciplines in its study of humans and their environments, including anthropometry, biomechanics, mechanical engineering, industrial engineering, industrial design, information design, kinesiology, physiology, cognitive psychology, industrial and organizational psychology, and space psychology.


== Etymology ==
The term ergonomics (from the Greek ἔργον, meaning ""work"", and νόμος, meaning ""natural law"") first entered the modern lexicon when Polish scientist Wojciech Jastrzębowski used the word in his 1857 article Rys ergonomji czyli nauki o pracy, opartej na prawdach poczerpniętych z Nauki Przyrody (The Outline of Ergonomics; i.e. Science of Work, Based on the Truths Taken from the Natural Science). The French scholar Jean-Gustave Courcelle-Seneuil, apparently without knowledge of Jastrzębowski's article, used the word with a slightly different meaning in 1858. The introduction of the term to the English lexicon is widely attributed to British psychologist Hywel Murrell, at the 1949 meeting at the UK's Admiralty, which led to the foundation of The Ergonomics Society. He used it to encompass the studies in which he had been engaged during and after World War II.The expression human factors is a predominantly North American term which has been adopted to emphasize the application of the same methods to non-work-related situations. A ""human factor"" is a physical or cognitive property of an individual or social behavior specific to humans that may influence the functioning of technological systems. The terms ""human factors"" and ""ergonomics"" are essentially synonymous.


== Domains of specialization ==
Ergonomics comprise three main fields of research: physical, cognitive and organizational ergonomics.
There are many specializations within these broad categories. Specializations in the field of physical ergonomics may include visual ergonomics. Specializations within the field of cognitive ergonomics may include usability, human–computer interaction, and user experience engineering.
Some specializations may cut across these domains: Environmental ergonomics is concerned with human interaction with the environment as characterized by climate, temperature, pressure, vibration, light. The emerging field of human factors in highway safety uses human factor principles to understand the actions and capabilities of road users – car and truck drivers, pedestrians, cyclists, etc. – and use this knowledge to design roads and streets to reduce traffic collisions. Driver error is listed as a contributing factor in 44% of fatal collisions in the United States, so a topic of particular interest is how road users gather and process information about the road and its environment, and how to assist them to make the appropriate decision.New terms are being generated all the time. For instance, ""user trial engineer"" may refer to a human factors professional who specializes in user trials. Although the names change, human factors professionals apply an understanding of human factors to the design of equipment, systems and working methods to improve comfort, health, safety, and productivity.
According to the International Ergonomics Association, within the discipline of ergonomics there exist domains of specialization.


=== Physical ergonomics ===

Physical ergonomics is concerned with human anatomy, and some of the anthropometric, physiological and bio mechanical characteristics as they relate to physical activity. Physical ergonomic principles have been widely used in the design of both consumer and industrial products for optimizing performance and to preventing / treating work-related disorders by reducing the mechanisms behind mechanically induced acute and chronic musculoskeletal injuries / disorders.  Risk factors such as localized mechanical pressures, force and posture in a sedentary office environment lead to injuries attributed to an occupational environment. Physical ergonomics is important to those diagnosed with physiological ailments or disorders such as arthritis (both chronic and temporary) or carpal tunnel syndrome. Pressure that is insignificant or imperceptible to those unaffected by these disorders may be very painful, or render a device unusable, for those who are. Many ergonomically designed products are also used or recommended to treat or prevent such disorders, and to treat pressure-related chronic pain.One of the most prevalent types of work-related injuries is musculoskeletal disorder. Work-related musculoskeletal disorders (WRMDs) result in persistent pain, loss of functional capacity and work disability, but their initial diagnosis is difficult because they are mainly based on complaints of pain and other symptoms. Every year, 1.8 million U.S. workers experience WRMDs and nearly 600,000 of the injuries are serious enough to cause workers to miss work. Certain jobs or work conditions cause a higher rate of worker complaints of undue strain, localized fatigue, discomfort, or pain that does not go away after overnight rest. These types of jobs are often those involving activities such as repetitive and forceful exertions; frequent, heavy, or overhead lifts; awkward work positions; or use of vibrating equipment. The Occupational Safety and Health Administration (OSHA) has found substantial evidence that ergonomics programs can cut workers' compensation costs, increase productivity and decrease employee turnover. Mitigation solutions can include both short term and long-term solutions. Short and long-term solutions involve awareness training, positioning of the body, furniture and equipment and ergonomic exercises. Sit-stand stations and computer accessories that provide soft surfaces for resting the palm as well as split keyboards are recommended. Additionally, resources within the HR department can be allocated to provide assessments to employees to ensure the above criteria are met. Therefore, it is important to gather data to identify jobs or work conditions that are most problematic, using sources such as injury and illness logs, medical records, and job analyses.

Innovative workstations that are being tested include sit-stand desks, treadmill desks, pedal devices and cycle ergometers. In multiple studies these new workstations resulted in decreased waist circumference and improved psychological well being. However a significant number of additional studies have seen no marked improvement in health outcomes.


=== Cognitive ergonomics ===

Cognitive ergonomics is concerned with mental processes, such as perception, memory, reasoning, and motor response, as they affect interactions among humans and other elements of a system. (Relevant topics include mental workload, decision-making, skilled performance, human reliability, work stress and training as these may relate to human-system and Human-Computer Interaction design.) Epidemiological studies show a correlation between the time one spends sedentary and their cognitive function such as lowered mood and depression.


=== Organizational ergonomics ===
Organizational ergonomics is concerned with the optimization of socio-technical systems, including their organizational structures, policies, and processes. (Relevant topics include communication, crew resource management, work design, work systems, design of working times, teamwork, participatory design, community ergonomics, cooperative work, new work programs, virtual organizations, telework, and quality management.)


== History ==


=== Ancient societies ===
Some have stated that human ergonomics began with Australopithecus prometheus (also known as “little foot”), a primate who created handheld tools out of different types of stone, clearly distinguishing between tools based on their ability to perform designated tasks. The foundations of the science of ergonomics appear to have been laid within the context of the culture of Ancient Greece. A good deal of evidence indicates that Greek civilization in the 5th century BC used ergonomic principles in the design of their tools, jobs, and workplaces. One outstanding example of this can be found in the description Hippocrates gave of how a surgeon's workplace should be designed and how the tools he uses should be arranged. The archaeological record also shows that the early Egyptian dynasties made tools and household equipment that illustrated ergonomic principles.


=== Industrial societies ===
Bernardino Ramazzini was one of the first people to systematically study the illness that resulted from work earning himself the nickname “father of occupational medicine”. In the late 1600s and early 1700s Ramazzini visited many worksites where he documented the movements of laborers and spoke to them about their ailments. He then published “De Morbis Artificum Diatriba” (Latin for Diseases of Workers) which detailed occupations, common illnesses, remedies. In the 19th century, Frederick Winslow Taylor pioneered the ""scientific management"" method, which proposed a way to find the optimum method of carrying out a given task. Taylor found that he could, for example, triple the amount of coal that workers were shoveling by incrementally reducing the size and weight of coal shovels until the fastest shoveling rate was reached. Frank and Lillian Gilbreth expanded Taylor's methods in the early 1900s to develop the ""time and motion study"". They aimed to improve efficiency by eliminating unnecessary steps and actions. By applying this approach, the Gilbreths reduced the number of motions in bricklaying from 18 to 4.5, allowing bricklayers to increase their productivity from 120 to 350 bricks per hour.However, this approach was rejected by Russian researchers who focused on the well-being of the worker. At the First Conference on Scientific Organization of Labour (1921) Vladimir Bekhterev and Vladimir Nikolayevich Myasishchev criticised Taylorism. Bekhterev argued that ""The ultimate ideal of the labour problem is not in it [Taylorism], but is in such organisation of the labour process that would yield a maximum of efficiency coupled with a minimum of health hazards, absence of fatigue and a guarantee of the sound health and all round personal development of the working people."" Myasishchev rejected Frederick Taylor's proposal to turn man into a machine. Dull monotonous work was a temporary necessity until a corresponding machine can be developed. He also went on to suggest a new discipline of ""ergology"" to study work as an integral part of the re-organisation of work. The concept was taken up by Myasishchev's mentor, Bekhterev, in his final report on the conference, merely changing the name to ""ergonology""


=== Aviation ===
Prior to World War I, the focus of aviation psychology was on the aviator himself, but the war shifted the focus onto the aircraft, in particular, the design of controls and displays, and the effects of altitude and environmental factors on the pilot. The war saw the emergence of aeromedical research and the need for testing and measurement methods. Studies on driver behavior started gaining momentum during this period, as Henry Ford started providing millions of Americans with automobiles. Another major development during this period was the performance of aeromedical research. By the end of World War I, two aeronautical labs were established, one at Brooks Air Force Base, Texas and the other at Wright-Patterson Air Force Base outside of Dayton, Ohio. Many tests were conducted to determine which characteristic differentiated the successful pilots from the unsuccessful ones. During the early 1930s, Edwin Link developed the first flight simulator. The trend continued and more sophisticated simulators and test equipment were developed. Another significant development was in the civilian sector, where the effects of illumination on worker productivity were examined. This led to the identification of the Hawthorne Effect, which suggested that motivational factors could significantly influence human performance.World War II marked the development of new and complex machines and weaponry, and these made new demands on operators' cognition. It was no longer possible to adopt the Tayloristic principle of matching individuals to preexisting jobs. Now the design of equipment had to take into account human limitations and take advantage of human capabilities. The decision-making, attention, situational awareness and hand-eye coordination of the machine's operator became key in the success or failure of a task. There was substantial research conducted to determine the human capabilities and limitations that had to be accomplished. A lot of this research took off where the aeromedical research between the wars had left off. An example of this is the study done by Fitts and Jones (1947), who studied the most effective configuration of control knobs to be used in aircraft cockpits.
Much of this research transcended into other equipment with the aim of making the controls and displays easier for the operators to use. The entry of the terms ""human factors"" and ""ergonomics"" into the modern lexicon date from this period. It was observed that fully functional aircraft flown by the best-trained pilots, still crashed. In 1943 Alphonse Chapanis, a lieutenant in the U.S. Army, showed that this so-called ""pilot error"" could be greatly reduced when more logical and differentiable controls replaced confusing designs in airplane cockpits. After the war, the Army Air Force published 19 volumes summarizing what had been established from research during the war.In the decades since World War II, human factors has continued to flourish and diversify. Work by Elias Porter and others within the RAND Corporation after WWII extended the conception of human factors. ""As the thinking progressed, a new concept developed—that it was possible to view an organization such as an air-defense, man-machine system as a single organism and that it was possible to study the behavior of such an organism.  It was the climate for a breakthrough."" In the initial 20 years after the World War II, most activities were done by the ""founding fathers"": Alphonse Chapanis, Paul Fitts, and Small.


=== Cold War ===
The beginning of the Cold War led to a major expansion of Defense supported research laboratories. Also, many labs established during WWII started expanding. Most of the research following the war was military-sponsored. Large sums of money were granted to universities to conduct research. The scope of the research also broadened from small equipments to entire workstations and systems. Concurrently, a lot of opportunities started opening up in the civilian industry. The focus shifted from research to participation through advice to engineers in the design of equipment. After 1965, the period saw a maturation of the discipline. The field has expanded with the development of the computer and computer applications.The Space Age created new human factors issues such as weightlessness and extreme g-forces. Tolerance of the harsh environment of space and its effects on the mind and body were widely studied.


=== Information age ===
The dawn of the Information Age has resulted in the related field of human–computer interaction (HCI). Likewise, the growing demand for and competition among consumer goods and electronics has resulted in more companies and industries including human factors in their product design. Using advanced technologies in human kinetics, body-mapping, movement patterns and heat zones, companies are able to manufacture purpose-specific garments, including full body suits, jerseys, shorts, shoes, and even underwear.


== Organizations ==
Formed in 1946 in the UK, the oldest professional body for human factors specialists and ergonomists is The Chartered Institute of Ergonomics and Human Factors, formally known as the Institute of Ergonomics and Human Factors and before that, The Ergonomics Society.
The Human Factors and Ergonomics Society (HFES) was founded in 1957. The Society's mission is to promote the discovery and exchange of knowledge concerning the characteristics of human beings that are applicable to the design of systems and devices of all kinds.
The Association of Canadian Ergonomists - l'Association canadienne d'ergonomie (ACE) was founded in 1968. It was originally named the Human Factors Association of Canada (HFAC), with ACE (in French) added in 1984, and the consistent, bilingual title adopted in 1999.  According to it 2017 mission statement, ACE unites and advances the knowledge and skills of ergonomics and human factors practitioners to optimise human and organisational well-being.The International Ergonomics Association (IEA) is a federation of ergonomics and human factors societies from around the world. The mission of the IEA is to elaborate and advance ergonomics science and practice, and to improve the quality of life by expanding its scope of application and contribution to society. As of September 2008, the International Ergonomics Association has 46 federated societies and 2 affiliated societies.
The Human Factors Transforming Healthcare (HFTH) is an international network of HF practitioners who are embedded within hospitals and health systems. The goal of the network is to provide resources for human factors practitioners and healthcare organizations looking to successfully apply HF principles to improve patient care and provider performance. The network also serves as collaborative platform for human factors practitioners, students, faculty, industry partners, and those curious about human factors in healthcare. 


=== Related organizations ===
The Institute of Occupational Medicine (IOM) was founded by the coal industry in 1969. From the outset the IOM employed an ergonomics staff to apply ergonomics principles to the design of mining machinery and environments. To this day, the IOM continues ergonomics activities, especially in the fields of musculoskeletal disorders; heat stress and the ergonomics of personal protective equipment (PPE). Like many in occupational ergonomics, the demands and requirements of an ageing UK workforce are a growing concern and interest to IOM ergonomists.
The International Society of Automotive Engineers (SAE) is a professional organization for mobility engineering professionals in the aerospace, automotive, and commercial vehicle industries. The Society is a standards development organization for the engineering of powered vehicles of all kinds, including cars, trucks, boats, aircraft, and others. The Society of Automotive Engineers has established a number of standards used in the automotive industry and elsewhere. It encourages the design of vehicles in accordance with established human factors principles. It is one of the most influential organizations with respect to ergonomics work in automotive design. This society regularly holds conferences which address topics spanning all aspects of human factors and ergonomics.


== Practitioners ==
Human factors practitioners come from a variety of backgrounds, though predominantly they are psychologists (from the various subfields of industrial and organizational psychology, engineering psychology, cognitive psychology, perceptual psychology, applied psychology, and experimental psychology) and physiologists. Designers (industrial, interaction, and graphic), anthropologists, technical communication scholars and computer scientists also contribute. Typically, an ergonomist will have an undergraduate degree in psychology, engineering, design or health sciences, and usually a master's degree or doctoral degree in a related discipline. Though some practitioners enter the field of human factors from other disciplines, both M.S. and PhD degrees in Human Factors Engineering are available from several universities worldwide.


=== Sedentary workplace ===
Contemporary offices did not exist until the 1830s, with Wojciech Jastrzębowsk's seminal book on MSDergonomics following in 1857  and the first published study of posture appearing in 1955.As the American workforce began to shift towards sedentary employment, the prevalence of [WMSD/cognitive issues/ etc..] began to rise. In 1900, 41% of the US workforce was employed in agriculture but by 2000 that had dropped to 1.9%  This coincides with an increase in growth in desk-based employment (25% of all employment in 2000)  and the surveillance of non-fatal workplace injuries by OSHA and Bureau of Labor Statistics in 1971. 0–1.5 and occurs in a sitting or reclining position. Adults older than 50 years report spending more time sedentary and for adults older than 65 years this is often 80% of their awake time. Multiple studies show a dose-response relationship between sedentary time and all-cause mortality with an increase of 3% mortality per additional sedentary hour each day. High quantities of sedentary time without breaks is correlated to higher risk of chronic disease, obesity, cardiovascular disease, type 2 diabetes and cancer.Currently, there is a large proportion of the overall workforce who is employed in low physical activity occupations. Sedentary behavior, such as spending long periods of time in seated positions poses a serious threat for injuries and additional health risks. Unfortunately, even though some workplaces make an effort to provide a well designed environment for sedentary employees, any employee who is performing large amounts of sitting will likely suffer discomfort. 
There are existing conditions that would predispose both individuals and populations to an increase in prevalence of living sedentary lifestyles, including: socioeconomic determinants, education levels, occupation, living environment, age (as mentioned above) and more. A study published by the Iranian Journal of Public Health examined socioeconomic factors and sedentary lifestyle effects for individuals in a working community. The study concluded that individuals who reported living in low income environments were more inclined to living sedentary behavior compared to those who reported being of high socioeconomic status. Individuals who achieve less education are also considered to be a high risk group to partake in sedentary lifestyles, however, each community is different and has different resources available that may vary this risk.  Oftentimes, larger worksites are associated with increased occupational sitting. Those who work in environments that are classified as business and office jobs are typically more exposed to sitting and sedentary behavior while in the workplace. Additionally, occupations that are full-time, have schedule flexibility, are also included in that demographic, and are more likely to sit often throughout their workday.


=== Policy implementation ===
Obstacles surrounding better ergonomic features to sedentary employees include cost, time, effort and for both companies and employees. The evidence above helps establish the importance of ergonomics in a sedentary workplace, yet missing information from this problem is enforcement and policy implementation. As a modernized workplace becomes more and more technology based more jobs are becoming primarily seated, therefore leading to a need to prevent chronic injuries and pain.  This is becoming easier with the amount of research around ergonomic tools saving money companies by limiting the number of days missed from work and workers comp cases. The way to ensure that corporations prioritize these health outcomes for their employees is through policy and implementation.Nationwide there are no policies that are currently in place, however a handful of big companies and states have taken on cultural policies to insure the safety of all workers. For example, the state of Nevada risk management department has established a set of ground rules for both agencies responsibilities and employees responsibilities.  The agency responsibilities include evaluating workstations, using risk management resources when necessary and keeping OSHA records. To see specific workstation ergonomic policies and responsibilities click here.


== Methods ==
Until recently, methods used to evaluate human factors and ergonomics ranged from simple questionnaires to more complex and expensive usability labs. Some of the more common human factors methods are listed below:

Ethnographic analysis: Using methods derived from ethnography, this process focuses on observing the uses of technology in a practical environment. It is a qualitative and observational method that focuses on ""real-world"" experience and pressures, and the usage of technology or environments in the workplace. The process is best used early in the design process.
Focus Groups are another form of qualitative research in which one individual will facilitate discussion and elicit opinions about the technology or process under investigation. This can be on a one-to-one interview basis, or in a group session. Can be used to gain a large quantity of deep qualitative data, though due to the small sample size, can be subject to a higher degree of individual bias. Can be used at any point in the design process, as it is largely dependent on the exact questions to be pursued, and the structure of the group. Can be extremely costly.
Iterative design: Also known as prototyping, the iterative design process seeks to involve users at several stages of design, to correct problems as they emerge. As prototypes emerge from the design process, these are subjected to other forms of analysis as outlined in this article, and the results are then taken and incorporated into the new design. Trends among users are analyzed, and products redesigned. This can become a costly process, and needs to be done as soon as possible in the design process before designs become too concrete.
Meta-analysis: A supplementary technique used to examine a wide body of already existing data or literature to derive trends or form hypotheses to aid design decisions. As part of a literature survey, a meta-analysis can be performed to discern a collective trend from individual variables.
Subjects-in-tandem: Two subjects are asked to work concurrently on a series of tasks while vocalizing their analytical observations. The technique is also known as ""Co-Discovery"" as participants tend to feed off of each other's comments to generate a richer set of observations than is often possible with the participants separately. This is observed by the researcher, and can be used to discover usability difficulties. This process is usually recorded.
Surveys and questionnaires: A commonly used technique outside of human factors as well, surveys and questionnaires have an advantage in that they can be administered to a large group of people for relatively low cost, enabling the researcher to gain a large amount of data. The validity of the data obtained is, however, always in question, as the questions must be written and interpreted correctly, and are, by definition, subjective. Those who actually respond are in effect self-selecting as well, widening the gap between the sample and the population further.
Task analysis: A process with roots in activity theory, task analysis is a way of systematically describing human interaction with a system or process to understand how to match the demands of the system or process to human capabilities. The complexity of this process is generally proportional to the complexity of the task being analyzed, and so can vary in cost and time involvement. It is a qualitative and observational process. Best used early in the design process.
Human performance modeling: A method of quantifying human behavior, cognition, and processes; a tool used by human factors researchers and practitioners for both the analysis of human function and for the development of systems designed for optimal user experience and interaction.
Think aloud protocol: Also known as ""concurrent verbal protocol"", this is the process of asking a user to execute a series of tasks or use technology, while continuously verbalizing their thoughts so that a researcher can gain insights as to the users' analytical process. Can be useful for finding design flaws that do not affect task performance, but may have a negative cognitive effect on the user. Also useful for utilizing experts to better understand procedural knowledge of the task in question. Less expensive than focus groups, but tends to be more specific and subjective.
User analysis: This process is based around designing for the attributes of the intended user or operator, establishing the characteristics that define them, creating a persona for the user. Best done at the outset of the design process, a user analysis will attempt to predict the most common users, and the characteristics that they would be assumed to have in common. This can be problematic if the design concept does not match the actual user, or if the identified are too vague to make clear design decisions from. This process is, however, usually quite inexpensive, and commonly used.
""Wizard of Oz"": This is a comparatively uncommon technique but has seen some use in mobile devices. Based upon the Wizard of Oz experiment, this technique involves an operator who remotely controls the operation of a device to imitate the response of an actual computer program. It has the advantage of producing a highly changeable set of reactions, but can be quite costly and difficult to undertake.
Methods analysis is the process of studying the tasks a worker completes using a step-by-step investigation. Each task in broken down into smaller steps until each motion the worker performs is described. Doing so enables you to see exactly where repetitive or straining tasks occur.
Time studies determine the time required for a worker to complete each task. Time studies are often used to analyze cyclical jobs. They are considered ""event based"" studies because time measurements are triggered by the occurrence of predetermined events.
Work sampling is a method in which the job is sampled at random intervals to determine the proportion of total time spent on a particular task. It provides insight into how often workers are performing tasks which might cause strain on their bodies.
Predetermined time systems are methods for analyzing the time spent by workers on a particular task. One of the most widely used predetermined time system is called Methods-Time-Measurement. Other common work measurement systems include MODAPTS and MOST. Industry specific applications based on PTS are Seweasy, MODAPTS and GSD as seen in paper:  Miller, Doug (2013). ""Towards Sustainable Labour Costing in UK Fashion Retail"". SSRN Electronic Journal. doi:10.2139/ssrn.2212100. S2CID 166733679. .
Cognitive walkthrough: This method is a usability inspection method in which the evaluators can apply user perspective to task scenarios to identify design problems. As applied to macroergonomics, evaluators are able to analyze the usability of work system designs to identify how well a work system is organized and how well the workflow is integrated.
Kansei method: This is a method that transforms consumer's responses to new products into design specifications. As applied to macroergonomics, this method can translate employee's responses to changes to a work system into design specifications.
High Integration of Technology, Organization, and People: This is a manual procedure done step-by-step to apply technological change to the workplace. It allows managers to be more aware of the human and organizational aspects of their technology plans, allowing them to efficiently integrate technology in these contexts.
Top modeler: This model helps manufacturing companies identify the organizational changes needed when new technologies are being considered for their process.
Computer-integrated Manufacturing, Organization, and People System Design: This model allows for evaluating computer-integrated manufacturing, organization, and people system design based on knowledge of the system.
Anthropotechnology: This method considers analysis and design modification of systems for the efficient transfer of technology from one culture to another.
Systems analysis tool: This is a method to conduct systematic trade-off evaluations of work-system intervention alternatives.
Macroergonomic analysis of structure: This method analyzes the structure of work systems according to their compatibility with unique sociotechnical aspects.
Macroergonomic analysis and design: This method assesses work-system processes by using a ten-step process.
Virtual manufacturing and response surface methodology: This method uses computerized tools and statistical analysis for workstation design.


=== Weaknesses ===
Problems related to measures of usability include the fact that measures of learning and retention of how to use an interface are rarely employed and some studies treat measures of how users interact with interfaces as synonymous with quality-in-use, despite an unclear relation.Although field methods can be extremely useful because they are conducted in the users' natural environment, they have some major limitations to consider. The limitations include:

Usually take more time and resources than other methods
Very high effort in planning, recruiting, and executing compared with other methods
Much longer study periods and therefore requires much goodwill among the participants
Studies are longitudinal in nature, therefore, attrition can become a problem.


== See also ==


== References ==


== Further reading ==
Books

Thomas J. Armstrong (2008), Chapter 10: Allowances, Localized Fatigue, Musculoskeletal Disorders, and Biomechanics (not yet published)
Berlin C. & Adams C. & 2017. Production Ergonomics: Designing Work Systems to Support Optimal Human Performance. London: Ubiquity Press. DOI: https://doi.org/10.5334/bbe
Jan Dul and Bernard Weedmaster, Ergonomics for Beginners. A classic introduction on ergonomics – Original title: Vademecum Ergonomie (Dutch)—published and updated since the 1960s.
Valerie J Gawron (2000), Human Performance Measures Handbook Lawrence Erlbaum Associates – A useful summary of human performance measures.
Lee, J.D.; Wickens, C.D.; Liu Y.; Boyle, L.N (2017). Designing for People: An introduction to human factors engineering, 3nd Edition. Charleston, SC: CreateSpace. ISBN 9781539808008.
Liu, Y (2007). IOE 333. Course pack. Industrial and Operations Engineering 333 (Introduction to Ergonomics), University of Michigan, Ann Arbor, MI. Winter 2007
Meister, D. (1999). The History of Human Factors and Ergonomics. Mahwah, N.J.: Lawrence Erlbaum Associates. ISBN 978-0-8058-2769-9.
Donald Norman, The Design of Everyday Things—An entertaining user-centered critique of nearly every gadget out there (at the time it was published)
Peter Opsvik (2009), ""Re-Thinking Sitting"" Interesting insights on the history of the chair and how we sit from an ergonomic pioneer
Oviatt, S. L.; Cohen, P. R. (March 2000). ""Multimodal systems that process what comes naturally"". Communications of the ACM. 43 (3): 45–53. doi:10.1145/330534.330538. S2CID 1940810.
Computer Ergonomics & Work Related Upper Limb Disorder Prevention- Making The Business Case For Pro-active Ergonomics (Rooney et al., 2008)
Stephen Pheasant, Bodyspace—A classic exploration of ergonomics
Sarter, N. B.; Cohen, P. R. (2002). Multimodal information presentation in support of human-automation communication and coordination. Advances in Human Performance and Cognitive Engineering Research. 2. pp. 13–36. doi:10.1016/S1479-3601(02)02004-0. ISBN 978-0-7623-0748-7.
Smith, Thomas J.;  et al. (2015). Variability in Human performance. CRC Press. ISBN 978-1-4665-7972-9.
Alvin R. Tilley & Henry Dreyfuss Associates (1993, 2002), The Measure of Man & Woman: Human Factors in Design  A human factors design manual.
Kim Vicente, The Human Factor Full of examples and statistics illustrating the gap between existing technology and the human mind, with suggestions to narrow it
Wickens, C.D.; Lee J.D.; Liu Y.; Gorden Becker S.E. (2003). An Introduction to Human Factors Engineering, 2nd Edition. Prentice Hall. ISBN 978-0-321-01229-6.
Wickens, C. D.; Sandy, D. L.; Vidulich, M. (1983). ""Compatibility and resource competition between modalities of input, central processing, and output"". Human Factors. 25 (2): 227–248. doi:10.1177/001872088302500209. ISSN 0018-7208. PMID 6862451. S2CID 1291342.Wu, S. (2011). ""Warranty claims analysis considering human factors"" (PDF). Reliability Engineering & System Safety. 96: 131–138. doi:10.1016/j.ress.2010.07.010.
Wickens and Hollands (2000). Engineering Psychology and Human Performance. Discusses memory, attention, decision making, stress and human error, among other topics
Wilson & Corlett, Evaluation of Human Work A practical ergonomics methodology. Warning: very technical and not a suitable 'intro' to ergonomics
Zamprotta, Luigi, La qualité comme philosophie de la production.Interaction avec l'ergonomie et perspectives futures, thèse de Maîtrise ès Sciences Appliquées – Informatique, Institut d'Etudes Supérieures L'Avenir, Bruxelles, année universitaire 1992–93, TIU [1] Press, Independence, Missouri (USA), 1994, ISBN 0-89697-452-9Peer-reviewed Journals (numbers between brackets are the ISI impact factor, followed by the date)

Behavior & Information Technology (0.915, 2008)
Ergonomics (0.747, 2001–2003)
Ergonomics in Design (-)
Applied Ergonomics (1.713, 2015)
Human Factors (1.37, 2015)
International Journal of Industrial Ergonomics (0.395, 2001–2003)
Human Factors and Ergonomics in Manufacturing (0.311, 2001–2003)
Travail Humain (0.260, 2001–2003)
Theoretical Issues in Ergonomics Science (-)
International Journal of Human Factors and Ergonomics (-)
International Journal of Occupational Safety and Ergonomics (-)


== External links ==
Directory of Design Support Methods Directory of Design Support Methods
Engineering Data Compendium of Human Perception and Performance
Index of Non-Government Standards on Human Engineering...
Index of Government Standards on Human Engineering...
NIOSH Topic Page on Ergonomics and Musculoskeletal Disorders
Office Ergonomics Information from European Agency for Safety and Health at Work
Human Factors Standards & Handbooks from the University of Maryland Department of Mechanical Engineering
Human Factors and Ergonomics Resources
Human Factors Engineering Collection, The University of Alabama in Huntsville Archives and Special Collections","pandas(index=199, _1=199, text='human factors and ergonomics (commonly referred to as human factors) is the application of psychological and physiological principles to the engineering and design of products, processes, and systems. the goal of human factors is to reduce human error, increase productivity, and enhance safety and comfort with a specific focus on the interaction between the human and the thing of interest.the field is a combination of numerous disciplines, such as psychology, sociology, engineering, biomechanics, industrial design, physiology, anthropometry, interaction design, visual design, user experience, and user interface design. in research, human factors employs the scientific method to study human behavior so that the resultant data may be applied to the four primary goals. in essence, it is the study of designing equipment, devices and processes that fit the human body and its cognitive abilities. the two terms ""human factors"" and ""ergonomics"" are essentially synonymous.the international ergonomics association defines ergonomics or human factors as follows: ergonomics (or human factors) is the scientific discipline concerned with the understanding of interactions among humans and other elements of a system, and the profession that applies theory, principles, data and methods to design to optimize human well-being and overall system performance. human factors is employed to fulfill the goals of occupational health and safety and productivity. it is relevant in the design of such things as safe furniture and easy-to-use interfaces to machines and equipment. proper ergonomic design is necessary to prevent repetitive strain injuries and other musculoskeletal disorders, which can develop over time and can lead to long-term disability. human factors and ergonomics are concerned with the ""fit"" between the user, equipment, and environment or ""fitting a job to a person"". it accounts for the user\'s capabilities and limitations in seeking to ensure that tasks, functions, information, and the environment suit that user. to assess the fit between a person and the used technology, human factors specialists or ergonomists consider the job (activity) being done and the demands on the user; the equipment used (its size, shape, and how appropriate it is for the task), and the information used (how it is presented, accessed, and changed). ergonomics draws on many disciplines in its study of humans and their environments, including anthropometry, biomechanics, mechanical engineering, industrial engineering, industrial design, information design, kinesiology, physiology, cognitive psychology, industrial and organizational psychology, and space psychology.   == etymology == the term ergonomics (from the greek ἔργον, meaning ""work"", and νόμος, meaning ""natural law"") first entered the modern lexicon when polish scientist wojciech jastrzębowski used the word in his 1857 article rys ergonomji czyli nauki o pracy, opartej na prawdach poczerpniętych z nauki przyrody (the outline of ergonomics; i.e. science of work, based on the truths taken from the natural science). the french scholar jean-gustave courcelle-seneuil, apparently without knowledge of jastrzębowski\'s article, used the word with a slightly different meaning in 1858. the introduction of the term to the english lexicon is widely attributed to british psychologist hywel murrell, at the 1949 meeting at the uk\'s admiralty, which led to the foundation of the ergonomics society. he used it to encompass the studies in which he had been engaged during and after world war ii.the expression human factors is a predominantly north american term which has been adopted to emphasize the application of the same methods to non-work-related situations. a ""human factor"" is a physical or cognitive property of an individual or social behavior specific to humans that may influence the functioning of technological systems. the terms ""human factors"" and ""ergonomics"" are essentially synonymous.   == domains of specialization == ergonomics comprise three main fields of research: physical, cognitive and organizational ergonomics. there are many specializations within these broad categories. specializations in the field of physical ergonomics may include visual ergonomics. specializations within the field of cognitive ergonomics may include usability, human–computer interaction, and user experience engineering. some specializations may cut across these domains: environmental ergonomics is concerned with human interaction with the environment as characterized by climate, temperature, pressure, vibration, light. the emerging field of human factors in highway safety uses human factor principles to understand the actions and capabilities of road users – car and truck drivers, pedestrians, cyclists, etc. – and use this knowledge to design roads and streets to reduce traffic collisions. driver error is listed as a contributing factor in 44% of fatal collisions in the united states, so a topic of particular interest is how road users gather and process information about the road and its environment, and how to assist them to make the appropriate decision.new terms are being generated all the time. for instance, ""user trial engineer"" may refer to a human factors professional who specializes in user trials. although the names change, human factors professionals apply an understanding of human factors to the design of equipment, systems and working methods to improve comfort, health, safety, and productivity. according to the international ergonomics association, within the discipline of ergonomics there exist domains of specialization. problems related to measures of usability include the fact that measures of learning and retention of how to use an interface are rarely employed and some studies treat measures of how users interact with interfaces as synonymous with quality-in-use, despite an unclear relation.although field methods can be extremely useful because they are conducted in the users\' natural environment, they have some major limitations to consider. the limitations include:  usually take more time and resources than other methods very high effort in planning, recruiting, and executing compared with other methods much longer study periods and therefore requires much goodwill among the participants studies are longitudinal in nature, therefore, attrition can become a problem.   == see also ==   == references ==   == further reading == books  thomas j. armstrong (2008), chapter 10: allowances, localized fatigue, musculoskeletal disorders, and biomechanics (not yet published) berlin c. & adams c. & 2017. production ergonomics: designing work systems to support optimal human performance. london: ubiquity press. doi: https://doi.org/10.5334/bbe jan dul and bernard weedmaster, ergonomics for beginners. a classic introduction on ergonomics – original title: vademecum ergonomie (dutch)—published and updated since the 1960s. valerie j gawron (2000), human performance measures handbook lawrence erlbaum associates – a useful summary of human performance measures. lee, j.d.; wickens, c.d.; liu y.; boyle, l.n (2017). designing for people: an introduction to human factors engineering, 3nd edition. charleston, sc: createspace. isbn 9781539808008. liu, y (2007). ioe 333. course pack. industrial and operations engineering 333 (introduction to ergonomics), university of michigan, ann arbor, mi. winter 2007 meister, d. (1999). the history of human factors and ergonomics. mahwah, n.j.: lawrence erlbaum associates. isbn 978-0-8058-2769-9. donald norman, the design of everyday things—an entertaining user-centered critique of nearly every gadget out there (at the time it was published) peter opsvik (2009), ""re-thinking sitting"" interesting insights on the history of the chair and how we sit from an ergonomic pioneer oviatt, s. l.; cohen, p. r. (march 2000). ""multimodal systems that process what comes naturally"". communications of the acm. 43 (3): 45–53. doi:10.1145/330534.330538. s2cid 1940810. computer ergonomics & work related upper limb disorder prevention- making the business case for pro-active ergonomics (rooney et al., 2008) stephen pheasant, bodyspace—a classic exploration of ergonomics sarter, n. b.; cohen, p. r. (2002). multimodal information presentation in support of human-automation communication and coordination. advances in human performance and cognitive engineering research. 2. pp. 13–36. doi:10.1016/s1479-3601(02)02004-0. isbn 978-0-7623-0748-7. smith, thomas j.;  et al. (2015). variability in human performance. crc press. isbn 978-1-4665-7972-9. alvin r. tilley & henry dreyfuss associates (1993, 2002), the measure of man & woman: human factors in design  a human factors design manual. kim vicente, the human factor full of examples and statistics illustrating the gap between existing technology and the human mind, with suggestions to narrow it wickens, c.d.; lee j.d.; liu y.; gorden becker s.e. (2003). an introduction to human factors engineering, 2nd edition. prentice hall. isbn 978-0-321-01229-6. wickens, c. d.; sandy, d. l.; vidulich, m. (1983). ""compatibility and resource competition between modalities of input, central processing, and output"". human factors. 25 (2): 227–248. doi:10.1177/001872088302500209. issn 0018-7208. pmid 6862451. s2cid 1291342.wu, s. (2011). ""warranty claims analysis considering human factors"" (pdf). reliability engineering & system safety. 96: 131–138. doi:10.1016/j.ress.2010.07.010. wickens and hollands (2000). engineering psychology and human performance. discusses memory, attention, decision making, stress and human error, among other topics wilson & corlett, evaluation of human work a practical ergonomics methodology. warning: very technical and not a suitable \'intro\' to ergonomics zamprotta, luigi, la qualité comme philosophie de la production.interaction avec l\'ergonomie et perspectives futures, thèse de maîtrise ès sciences appliquées – informatique, institut d\'etudes supérieures l\'avenir, bruxelles, année universitaire 1992–93, tiu [1] press, independence, missouri (usa), 1994, isbn 0-89697-452-9peer-reviewed journals (numbers between brackets are the isi impact factor, followed by the date)  behavior & information technology (0.915, 2008) ergonomics (0.747, 2001–2003) ergonomics in design (-) applied ergonomics (1.713, 2015) human factors (1.37, 2015) international journal of industrial ergonomics (0.395, 2001–2003) human factors and ergonomics in manufacturing (0.311, 2001–2003) travail humain (0.260, 2001–2003) theoretical issues in ergonomics science (-) international journal of human factors and ergonomics (-) international journal of occupational safety and ergonomics (-)   == external links == directory of design support methods directory of design support methods engineering data compendium of human perception and performance index of non-government standards on human engineering... index of government standards on human engineering... niosh topic page on ergonomics and musculoskeletal disorders office ergonomics information from european agency for safety and health at work human factors standards & handbooks from the university of maryland department of mechanical engineering human factors and ergonomics resources human factors engineering collection, the university of alabama in huntsville archives and special collections')"
200,"Design Space Exploration (DSE) refers to systematic analysis and pruning of unwanted design points based on parameters of interest. While the term DSE can apply to any kind of system, we refer to electronic and embedded system design in this article.
Given the complex specification of electronic systems and the plethora of design choices ranging from the choice of components, number of components, operating modes of each of the components, connections between the components, choice of algorithm, etc.; design decisions need to be based on a systematic exploration process. However, the exploration process is complex because of a variety of ways in which the same functionality can be implemented. A tradeoff analysis between each of the implementation option based on a certain parameter of interest forms the basis of DSE. The parameter of interest could vary across systems, but the commonly used parameters are power, performance, and cost. Additional factors like size, shape, weight, etc. can be important for some handheld systems like cellphone and tablets. With growing usage of mobile devices, energy is also becoming a mainstream optimization parameter along with power and performance.
Owing to the complexity of the exploration process, researchers have proposed automated DSE where the exploration software is able to take decisions and comes up with the optimal solution. However, it is not possible to have an automated DSE for all kind of systems and hence there are semi-automated methods of DSE where the designer has to steer the tool after every iteration towards convergence. Since the exploration is a complex process which takes large computational time, researchers have developed exploration tools which can give an approximate analysis of the system behavior in a fraction of time compared to accurate analysis. Such tools are very important for quick comparison of design decisions and are becoming more important with increasing complexity of designs.
To simplify the complexity of DSE, researchers have been continuously striving to raise the abstractions of component and system definition to be able to cater to larger and complex systems. For example, instead of modeling a digital system at transistor or gate level, there have been attempts to use RTL or behavioral modeling. Further higher abstractions like SystemC or block diagram based modeling are also used depending on the system requirements. Modeling at higher abstractions allows fast exploration of various design choices for the lower level implementation.
The ability to operate on the space of design candidates makes DSE useful for many engineering tasks, such as rapid prototyping, optimization, and system integration.


== See also ==
Computer experiment
Design of experiment
MULTICUBE
Multifactor design of experiments software
Probabilistic design
Randomized block design


== References ==


== Further reading ==


== External links ==","pandas(index=200, _1=200, text='design space exploration (dse) refers to systematic analysis and pruning of unwanted design points based on parameters of interest. while the term dse can apply to any kind of system, we refer to electronic and embedded system design in this article. given the complex specification of electronic systems and the plethora of design choices ranging from the choice of components, number of components, operating modes of each of the components, connections between the components, choice of algorithm, etc.; design decisions need to be based on a systematic exploration process. however, the exploration process is complex because of a variety of ways in which the same functionality can be implemented. a tradeoff analysis between each of the implementation option based on a certain parameter of interest forms the basis of dse. the parameter of interest could vary across systems, but the commonly used parameters are power, performance, and cost. additional factors like size, shape, weight, etc. can be important for some handheld systems like cellphone and tablets. with growing usage of mobile devices, energy is also becoming a mainstream optimization parameter along with power and performance. owing to the complexity of the exploration process, researchers have proposed automated dse where the exploration software is able to take decisions and comes up with the optimal solution. however, it is not possible to have an automated dse for all kind of systems and hence there are semi-automated methods of dse where the designer has to steer the tool after every iteration towards convergence. since the exploration is a complex process which takes large computational time, researchers have developed exploration tools which can give an approximate analysis of the system behavior in a fraction of time compared to accurate analysis. such tools are very important for quick comparison of design decisions and are becoming more important with increasing complexity of designs. to simplify the complexity of dse, researchers have been continuously striving to raise the abstractions of component and system definition to be able to cater to larger and complex systems. for example, instead of modeling a digital system at transistor or gate level, there have been attempts to use rtl or behavioral modeling. further higher abstractions like systemc or block diagram based modeling are also used depending on the system requirements. modeling at higher abstractions allows fast exploration of various design choices for the lower level implementation. the ability to operate on the space of design candidates makes dse useful for many engineering tasks, such as rapid prototyping, optimization, and system integration.   == see also == computer experiment design of experiment multicube multifactor design of experiments software probabilistic design randomized block design   == references ==   == further reading ==   == external links ==')"
201,"Engineering Administration (EA) is a branch of engineering that is mainly concerned with the analysis and solution of operational and management problems using scientific and mathematical methods.
Engineering Administration is considered to be a subdiscipline of industrial engineering / systems engineering.  


== University programs ==


=== Undergraduate curriculum ===
In the United States the undergraduate degree earned is the Bachelor of Science (B.S.) in Engineering Administration. 


=== Postgraduate curriculum ===
The postgraduate degree earned is the Master of Science in Engineering Administration (MEA).


== Associations ==
INFORMS
Institute of Industrial Engineers


== See also ==
Industrial Engineering
Systems Engineering
Enterprise Engineering
Engineering Management
Business Engineering


== References ==


== External links ==
Bachelor of Engineering Administration
Master of Engineering Administration (MEA)","pandas(index=201, _1=201, text='engineering administration (ea) is a branch of engineering that is mainly concerned with the analysis and solution of operational and management problems using scientific and mathematical methods. engineering administration is considered to be a subdiscipline of industrial engineering / systems engineering.   == university programs == the postgraduate degree earned is the master of science in engineering administration (mea).   == associations == informs institute of industrial engineers   == see also == industrial engineering systems engineering enterprise engineering engineering management business engineering   == references ==   == external links == bachelor of engineering administration master of engineering administration (mea)')"
202,"Operations engineering is a branch of engineering that is mainly concerned with the analysis and optimization of operational  problems using scientific and mathematical methods. More frequently it has applications in the areas of Broadcasting/Industrial Engineering and also in the Creative and Technology Industries.
Operations engineering is considered to be a subdiscipline of Operations Research and Operations Management.


== Associations ==
INFORMS
industrial operation


== See also ==
Operations research
Systems engineering
Enterprise engineering
Engineering management
Business engineering","pandas(index=202, _1=202, text='operations engineering is a branch of engineering that is mainly concerned with the analysis and optimization of operational  problems using scientific and mathematical methods. more frequently it has applications in the areas of broadcasting/industrial engineering and also in the creative and technology industries. operations engineering is considered to be a subdiscipline of operations research and operations management.   == associations == informs industrial operation   == see also == operations research systems engineering enterprise engineering engineering management business engineering')"
203,"A demonstration plant is an industrial system used to validate an industrial process for commercialization. It is larger than a pilot plant, and is the final stage in research, development and demonstration of a new process. Demonstration plants are built in a range of sizes, and the term 'demonstration plant' can sometimes be used interchangeably with 'pilot plant.' However, demonstration plants are generally larger than pilot plants, and are often constructed following a successful trial in a pilot scale size. Demonstration plants are used to prove a process works at industrial scale, and is financially viable in its intended industry.


== Goals ==
The goals of a demonstration plant are generally as follows:

Prove a new technology using commercially available, pre-tested equipment.
Show a reasonable return on investment (ROI) for the capital that will be invested in a full-scale system, including the operational costs of running such a system.
In some cases, to start bringing product to market in significant enough amounts that production, distribution and target market viability can be established, including finalization of market testing.
Establish a viable product method that will endure the test of a true manufacturing operation.


== Design factors ==
Many of the same design techniques that are used for pilot plants are also used when developing demonstration plants. 3D modeling, chemical similitude studies, mass and energy balances, risk factors, computational fluid dynamics (CFD), and mathematical modeling are common techniques used to design demonstration modules before actual fabrication occurs.The emphasis in a demonstration plant is on using industrial equipment, rather than smaller-scale equipment, to prove process viability. A significant amount of product must be produced in equipment that will hold up over a long production lifetime and not be prohibitively expensive. A demonstration plant must show that enough end-product can be created to offset the costs of the commercial system over a period of time.


== See also ==
Pilot plant
Operations research
Chemical engineering
Process engineering


== References ==","pandas(index=203, _1=203, text=""a demonstration plant is an industrial system used to validate an industrial process for commercialization. it is larger than a pilot plant, and is the final stage in research, development and demonstration of a new process. demonstration plants are built in a range of sizes, and the term 'demonstration plant' can sometimes be used interchangeably with 'pilot plant.' however, demonstration plants are generally larger than pilot plants, and are often constructed following a successful trial in a pilot scale size. demonstration plants are used to prove a process works at industrial scale, and is financially viable in its intended industry.   == goals == the goals of a demonstration plant are generally as follows:  prove a new technology using commercially available, pre-tested equipment. show a reasonable return on investment (roi) for the capital that will be invested in a full-scale system, including the operational costs of running such a system. in some cases, to start bringing product to market in significant enough amounts that production, distribution and target market viability can be established, including finalization of market testing. establish a viable product method that will endure the test of a true manufacturing operation.   == design factors == many of the same design techniques that are used for pilot plants are also used when developing demonstration plants. 3d modeling, chemical similitude studies, mass and energy balances, risk factors, computational fluid dynamics (cfd), and mathematical modeling are common techniques used to design demonstration modules before actual fabrication occurs.the emphasis in a demonstration plant is on using industrial equipment, rather than smaller-scale equipment, to prove process viability. a significant amount of product must be produced in equipment that will hold up over a long production lifetime and not be prohibitively expensive. a demonstration plant must show that enough end-product can be created to offset the costs of the commercial system over a period of time.   == see also == pilot plant operations research chemical engineering process engineering   == references =="")"
204,"The Institute of Industrial and Systems Engineers (IISE), formerly the Institute of Industrial Engineers, is a professional society dedicated solely to the support of the industrial engineering profession and individuals involved with improving quality and productivity.
The institute was founded in 1948 as the American Institute of Industrial Engineers. In  1981, the name was changed to Institute of Industrial Engineers in order to reflect its international membership base. The name was changed again to the present Institute of Industrial and Systems Engineers in 2016 to reflect the changing scope of engineers working with large-scale, integrated systems.
Members include both college students and professionals. IISE holds annual regional and national conferences in the United States. IISE is headquartered in the United States in Peachtree Corners, Georgia, a suburb located northeast of Atlanta.:


== Publications ==
IISE publishes two magazines and five technical journals.
The Institute's flagship journal is IISE Transactions, which publishes papers that are grounded in science and mathematics, as well as motivated by engineering applications. The Engineering Economist is a quarterly refereed journal dealing with capital investments. IISE Transactions on Healthcare Systems Engineering focuses on research  in health systems. The Journal of Enterprise Transformation contains research related to enterprise transformation. IISE Transactions on Occupational Ergonomics and Human Factors is devoted to ergonomics and human factors research and techniques.
Industrial Management, the publication of the Institute's  Society for Engineering and Management System, is a quarterly magazine on engineering management topics, The award-winning member magazine is ISE (formerly Industrial Engineer) and is published monthly.


== References ==


== External links ==
Official website
IISE history
ISE (formerly Industrial Engineer) magazine


== See also ==","pandas(index=204, _1=204, text=""the institute of industrial and systems engineers (iise), formerly the institute of industrial engineers, is a professional society dedicated solely to the support of the industrial engineering profession and individuals involved with improving quality and productivity. the institute was founded in 1948 as the american institute of industrial engineers. in  1981, the name was changed to institute of industrial engineers in order to reflect its international membership base. the name was changed again to the present institute of industrial and systems engineers in 2016 to reflect the changing scope of engineers working with large-scale, integrated systems. members include both college students and professionals. iise holds annual regional and national conferences in the united states. iise is headquartered in the united states in peachtree corners, georgia, a suburb located northeast of atlanta.:   == publications == iise publishes two magazines and five technical journals. the institute's flagship journal is iise transactions, which publishes papers that are grounded in science and mathematics, as well as motivated by engineering applications. the engineering economist is a quarterly refereed journal dealing with capital investments. iise transactions on healthcare systems engineering focuses on research  in health systems. the journal of enterprise transformation contains research related to enterprise transformation. iise transactions on occupational ergonomics and human factors is devoted to ergonomics and human factors research and techniques. industrial management, the publication of the institute's  society for engineering and management system, is a quarterly magazine on engineering management topics, the award-winning member magazine is ise (formerly industrial engineer) and is published monthly.   == references ==   == external links == official website iise history ise (formerly industrial engineer) magazine   == see also =="")"
205,"A routing diagram or route diagram in the field of management engineering is a type of diagram, that shows a route through an accessible physical space. Routing diagrams are used in plant layout study, and manufacturing plant design.  


== Overview ==
A routing diagrams shows a route through a physical space. They are often considered a type of flow diagram, but they differ from flowcharts, that a routing is pictured in a physical layout. There is a similarity with design of Electrical equipment, where routing diagrams also in show ""the physical layout of the facility and equipment and how the circuit how the circuit to the various equipment is run."" A picture with a routing in geographical space is often called a route map. While the road map and transit map (such as the railway map, metro map, bus map, etc.) show all the roads or lines, the route map regularly shows one rad for a particular occasion. Likewise a ground plan or site map show all the space, buildings and/or rooms, the routing map shows one specific route on site.    
Routing diagrams are used in plant layout study. The routing diagram can consist of a floor plan with a trace attached, or a 3d cross section of a building with a trace. The routing diagram transforms into a flow diagram when the physical dimensions are taken out of the equation.


== References ==


== Further reading ==
Willard C. Brinton, ""Flow Charts,"" in Graphic presentation. New York city, Brinton associates, 1939; This is mainly about flow diagrams.
Charles Day, ""The Routing Diagram as a Basis for laying Out Industrial Plants."" Engineering Magazine, September, 1910. p. 809-821; Republished in: Industrial Plants, 1911. Chapter VII.
D. C. Eberhart. ""222.41 c. Diagrammatic routing,"" in: The Code of Federal Regulations of the United States of America; Title 14. U.S. Government Printing Office, 1964. p. 48
Karl G. Karsten, ""Route-Charts,"" in: Charts and graphs; an introduction to graphic methods in the control and analysis of statistics, 1923. p. 21-38


== External links ==
 Media related to Routing diagrams at Wikimedia Commons","pandas(index=205, _1=205, text='a routing diagram or route diagram in the field of management engineering is a type of diagram, that shows a route through an accessible physical space. routing diagrams are used in plant layout study, and manufacturing plant design.   == overview == a routing diagrams shows a route through a physical space. they are often considered a type of flow diagram, but they differ from flowcharts, that a routing is pictured in a physical layout. there is a similarity with design of electrical equipment, where routing diagrams also in show ""the physical layout of the facility and equipment and how the circuit how the circuit to the various equipment is run."" a picture with a routing in geographical space is often called a route map. while the road map and transit map (such as the railway map, metro map, bus map, etc.) show all the roads or lines, the route map regularly shows one rad for a particular occasion. likewise a ground plan or site map show all the space, buildings and/or rooms, the routing map shows one specific route on site. routing diagrams are used in plant layout study. the routing diagram can consist of a floor plan with a trace attached, or a 3d cross section of a building with a trace. the routing diagram transforms into a flow diagram when the physical dimensions are taken out of the equation.   == references ==   == further reading == willard c. brinton, ""flow charts,"" in graphic presentation. new york city, brinton associates, 1939; this is mainly about flow diagrams. charles day, ""the routing diagram as a basis for laying out industrial plants."" engineering magazine, september, 1910. p. 809-821; republished in: industrial plants, 1911. chapter vii. d. c. eberhart. ""222.41 c. diagrammatic routing,"" in: the code of federal regulations of the united states of america; title 14. u.s. government printing office, 1964. p. 48 karl g. karsten, ""route-charts,"" in: charts and graphs; an introduction to graphic methods in the control and analysis of statistics, 1923. p. 21-38   == external links == media related to routing diagrams at wikimedia commons')"
206,"In statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. It is related to the dose-response model in the medical literature.


== Description ==
Generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. One example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.Assume there exists a finite collection of multi-valued treatment status 
  
    
      
        T
        =
        {
        0
        ,
        1
        ,
        2
        ,
        …
        ,
        J
        ,
        }
      
    
    {\displaystyle T=\{0,1,2,\ldots ,J,\}}
   with J some fixed integer. As in the potential outcomes framework, denote  
  
    
      
        Y
        (
        j
        )
        ⊂
        R
      
    
    {\displaystyle Y(j)\subset R}
   the collection of potential outcomes under the treatment J, and 
  
    
      
        Y
        =
        
          
            ∑
            
              j
              =
              0
            
            
              J
            
          
          
            
              D
              
                j
              
            
            Y
            (
            j
            )
          
        
      
    
    {\displaystyle Y=\textstyle \sum _{j=0}^{J}\displaystyle D_{j}Y(j)}
   denotes the observed outcome and 
  
    
      
        
          D
          
            j
          
        
      
    
    {\displaystyle D_{j}}
   is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  A general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by Heckman and Vytlacil.Recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. In the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption.  Other recent work has focused more on the large sample properties of an estimator of the marginal mean treatment effect conditional on a treatment level in the context of a difference-in-differences model, and on the efficient estimation of multi-valued treatment effects in a semiparametric framework.


== References ==","pandas(index=206, _1=206, text='in statistics, in particular in the design of experiments, a multi-valued treatment is a treatment that can take on more than two values. it is related to the dose-response model in the medical literature.   == description == generally speaking, treatment levels may be finite or infinite as well as ordinal or cardinal, which leads to a large collection of possible treatment effects to be studied in applications. one example is the effect of different levels of program participation (e.g. full-time and part-time) in a job training program.assume there exists a finite collection of multi-valued treatment status    t =  is an indicator that equals 1 when the treatment equals j and 0 when it does not equal j, leading to a fundamental problem of causal inference.  a general framework that analyzes ordered choice models in terms of marginal treatment effects and average treatment effects has been extensively discussed by heckman and vytlacil.recent work in the econometrics and statistics literature has focused on estimation and inference for multivalued treatments and ignorability conditions for identifying the treatment effects. in the context of program evaluation, the propensity score has been generalized to allow for multi-valued treatments, while other work has also focused on the role of the conditional mean independence assumption.  other recent work has focused more on the large sample properties of an estimator of the marginal mean treatment effect conditional on a treatment level in the context of a difference-in-differences model, and on the efficient estimation of multi-valued treatment effects in a semiparametric framework.   == references ==')"
207,"Power plant engineering or power station engineering is a division of power engineering, and is defined as ""the engineering and technology required for the production of central station electric power."" The field is focused on the generation of power for industries and communities, not for household power production. The field is an interdisciplinary field, using the theoretical base of both mechanical and electrical engineering. The engineering aspect of power plant management has evolved with technology and has become progressively more complicated. The introduction of nuclear technology and the progression of other existing technologies have allowed power to be created in more ways and on a larger scale than was previously possible. The assignment of different types of engineers to the design, construction, and operation of a new power plant is dependent on the type of system being built such as whether it is a fossil fuel thermal power plant, nuclear power plant, hydroelectric plant, or solar plant.


== History ==

Power plant engineering got its start in the 1800s when small systems were used by individual factories to provide electrical power. Originally the only source of power came from DC, or direct current, systems. While this was suitable for business, electricity was not accessible for most of the public body. During these times, the coal powered steam engine was costly to run and there was no way for the power to be transmitted over distances. Hydroelectricity was one of the most utilized forms of power generation as water mills could be used to create power to transmit to small towns.It wasn't until the introduction of AC, or alternating current, power systems that allowed for the creation of power plants as we know them today. AC systems allowed power to be transmitted over larger distances than DC systems allowed and thus, large power stations were able to be created. One of the progenitors of long-distance power-transmission was the Lauffen to Frankfurt power plant which spanned 109 miles. The Lauffen-Frankfurt demonstrated how three-phase power could be effectively applied to transmit power over long distances. Three-phase power had been the progeny of years of research in power distribution and the Lauffen-Frankfurt was the first exhibition to show its real potential for future me="":0"" />
The engineering knowledge needed to perform these tasks enlists the help of several fields of engineering including mechanical, electrical, nuclear and civil engineers. When power plants were up and coming, engineering tasks needed to create these facilities mainly consisted of mechanical, civil, and electrical engineers. These disciplines allowed for the planning and construction of power plants. But when nuclear power plants were created it introduced nuclear engineers to perform the calculations necessary to maintain safety standards.


== Governing principles ==


=== First Law of Thermodynamics ===
In simple terms, the first law of thermodynamics states that energy cannot be created nor destroyed; however, power can be converted from one form of energy to another form of energy. This is especially important in power generation because power production in nearly all types of power plants relies upon the use of a generator.  Generators are used to convert mechanical energy into electrical energy; for example, wind turbines utilize a large blade connected to a shaft which turns the generator when rotated. The generator then creates electricity due to the interaction of a conductor within a magnetic field. In this case, the mechanical energy generated by the wind is converted, through the generator, into electric energy. Most power plants rely on these conversions to create usable electric power.


=== Second law of thermodynamics ===
The second law of thermodynamics conceptualizes that the entropy of a closed system can never decrease. As the law relates to power plants, it dictates that heat is to flow from a body at high temperature to a body at low temperature (the device in which electricity is being generated). This law is particularly pertinent to thermal power plants which derive their energy from the combustion of a fuel source.


== Types of power plants ==
All power plants are created with the same goal: to produce electric power as efficiently as possible. However, as technology has evolved, the sources of energy used in power plants has evolved as well. The introduction of more renewable/sustainable forms of energy has caused an increase in the improvement and creation of certain power plants.


=== Hydroelectric power plants ===

Hydroelectric power plants generate power using the force of water to turn generators. They can be categorized into three different types; impoundment, diversion and pumped storage. Impoundment and diversion hydroelectric power plants operate similarly in that each involves creating a barrier to keep water from flowing at an uncontrollable rate, and then controlling the flow rate of water to pass through turbines to create electricity at an ideal level. Mechanical engineers are in charge of calculating flow rates and other volumetric calculations necessary to turn the generators at the electrical engineers specifications. Pumped storage hydroelectric power plants operate in a similar manner but only function at peak hours of power demand. At calm hours the water is pumped uphill, then is released at peak hours to flow from a high to low elevation to turn turbines. The engineering knowledge required to assess the performance of pumped storage hydroelectric power plants is very similar to that of the impoundment and diversion power plants.


=== Thermal power plants ===

Thermal power plants are split into two different categories; those that create electricity by burning fuel and those that create electricity via prime mover. A common example of a thermal power plant that produces electricity by the consumption of fuel is the nuclear power plant. Nuclear power plants use a nuclear reactor's heat to turn water into steam. This steam is sent through a turbine which is connected to an electric generator to generate electricity. Nuclear power plants account for 20% of America's electricity generation. Another example of a fuel burning power plant is coal power plant. Coal power plants generate 50% of the United States' electricity supply. Coal power plants operate in a manner similar to nuclear power plants in that the heat from the burning coal powers a steam turbine and electric generator. There are several types of engineers that work in a Thermal Power Plant. Mechanical engineers maintain performance of the thermal power plants while keeping the plants in operation. Nuclear Engineer generally handle fuel efficiency and disposal of nuclear waste; however, in Nuclear Power Plants they work directly with nuclear equipment. Electrical Engineers deals with the power generating equipment as well as the calculations.


=== Solar power plants ===

Solar power plants derive their energy from sunlight, which is made accessible via photovoltaics (PV's). Photovoltaic panels, or solar panels, are constructed using photovoltaic cells which are made of silica materials that release electrons when they are warmed by the thermal energy of the sun. The new flow of electrons generates electricity within the cell.  While PV's are an efficient method of producing electricity, they do burn out after a decade and thus, must be replaced; however, their efficiency, cost of operation, and lack of noise/physical pollutants make them one of the cleanest and least expensive forms of energy. Solar power plants require the work of many facets of engineering; electrical engineers are especially crucial in constructing the solar panels and connecting them into a grid, computer engineers code the cells themselves so that electricity can be effectively and efficiently produced, and civil engineers play the very important role of identifying areas where solar plants are able to collect the most energy.


=== Wind power plants ===

Wind power plants, also known as wind turbines, derive their energy from the wind by connecting a generator to the fan blades and using the rotational motion caused by wind to power the generator. Then the generated power is fed back into the power grid. Wind power plants can be implemented on large, open expanses of land or on large bodies of water such as the oceans; they simply rely on being in areas that experience significant amounts of wind. Technically, wind turbines are a form of solar power in that they rely on pressure differentials caused by uneven heating of the earth's atmosphere. Wind turbines solicit the knowledge from mechanical, electrical, and civil engineers. Knowledge of fluid dynamics from the help of mechanical engineers is crucial in determining the viability of locations for wind turbines. Electrical engineers ensure that power generation and transmission is possible. Civil engineers are important in the construction and utilization of wind turbines.


== Education ==
Power plant engineering covers a broad spectrum of engineering disciplines. The field can solicit information from mechanical, electrical, nuclear, and civil engineers.


=== Mechanical ===
Mechanical engineers work to maintain and control machinery that used to power the plant. To work in this field, mechanical engineers require a bachelor's degree in Engineering and licenses passing both the Professional Engineering Exam (PE) and Fundamental Engineering Exam (FE). The mechanical engineers have additional roles that are needed to be considered based on their career. When working in thermal power plants, mechanical engineers make sure heavy machinery like boilers and turbines, are working in optimal condition and power is continually generated. Mechanical engineers also work with the operations of the plant. In nuclear and hydraulic power plants the engineers work to make sure that heavy machinery is maintained and preventive maintenance is performed.


=== Electrical ===
Electrical engineers work with electrical appliances while making sure electronic instruments and appliances are working in company and state level satisfaction. They require licenses passing both the Professional Engineering Exam (PE) and Fundamental Engineering Exam (FE).  It is also preferred that they have a bachelor's degree approved by the Accreditation Board of Engineering and Technology, Inc. (ABET) and field experience before getting an entry-level position.


=== Nuclear ===
Nuclear engineers develop and research methods, machinery and systems concerning radiation and energy in subatomic levels. They require on-site experience and a bachelor's degree in Engineering. These engineers work in Nuclear Power plants and require licenses for practice while working in the power plant. They require work experience, passing the Professional Engineering Exam(PE), Fundamental Engineering Exam (FE), and a degree from an Accreditation Board for Engineering and Technology, Inc (ABET) approved school.  Nuclear engineers work with the handling of nuclear material and operations of a nuclear power plant. These operations can range from handling of nuclear wastes, nuclear material experiments, and design of nuclear equipment.


=== Civil ===
Civil engineers focuses on the construction, expenses and building of the power plant. Civil Engineers require passing the Professional Engineering Exam (PE), Fundamental Engineering Exam (FE), and a degree from an Accreditation Board of Engineering and Technology, Inc. (ABET) approved school. They work with making sure the structure of the power plant, the location and the design and safety of the power plant.


=== Associations ===
While there are many disparities between the aforementioned engineering disciplines, they all cover material related to heat or electricity transmission. Obtaining a degree from an ABET accredited school in any one of these disciplines is essential to becoming a power plant engineer. There are also many associations which qualified engineers can join, including the American Society of Mechanical Engineers (ASME), the Institute of Electric and Electronic Engineers (IEEE), and the American Society of Power Engineers (ASOPE).


== Fields ==
Power plant operation and maintenance consists of optimizing the efficiency and power output of power plants and ensuring long term operation. These power plants are large scale, and used to supply power for communities and industry. Individual household electric power generators are not included.Power station design consists of the design of new power plant systems. There are many types of power plants, and each type requires specific expertise, as well as interdisciplinary teamwork, to build a modern system.


== See also ==
Power engineering
Mechanical engineering
Electrical engineering
Civil engineering
Photovoltaics
Thermal power station
Hydroelectricity
First law of thermodynamics
Second law of thermodynamics
Wind power


== References ==

Brighthub Engineering. Retrieved 2018-04-18.


== External links ==
American Society of Power Engineers
American Society of Mechanical Engineers
Institute of Electric and Electronics Engineers","pandas(index=207, _1=207, text='power plant engineering or power station engineering is a division of power engineering, and is defined as ""the engineering and technology required for the production of central station electric power."" the field is focused on the generation of power for industries and communities, not for household power production. the field is an interdisciplinary field, using the theoretical base of both mechanical and electrical engineering. the engineering aspect of power plant management has evolved with technology and has become progressively more complicated. the introduction of nuclear technology and the progression of other existing technologies have allowed power to be created in more ways and on a larger scale than was previously possible. the assignment of different types of engineers to the design, construction, and operation of a new power plant is dependent on the type of system being built such as whether it is a fossil fuel thermal power plant, nuclear power plant, hydroelectric plant, or solar plant.   == history ==  power plant engineering got its start in the 1800s when small systems were used by individual factories to provide electrical power. originally the only source of power came from dc, or direct current, systems. while this was suitable for business, electricity was not accessible for most of the public body. during these times, the coal powered steam engine was costly to run and there was no way for the power to be transmitted over distances. hydroelectricity was one of the most utilized forms of power generation as water mills could be used to create power to transmit to small towns.it wasn\'t until the introduction of ac, or alternating current, power systems that allowed for the creation of power plants as we know them today. ac systems allowed power to be transmitted over larger distances than dc systems allowed and thus, large power stations were able to be created. one of the progenitors of long-distance power-transmission was the lauffen to frankfurt power plant which spanned 109 miles. the lauffen-frankfurt demonstrated how three-phase power could be effectively applied to transmit power over long distances. three-phase power had been the progeny of years of research in power distribution and the lauffen-frankfurt was the first exhibition to show its real potential for future me="":0"" /> the engineering knowledge needed to perform these tasks enlists the help of several fields of engineering including mechanical, electrical, nuclear and civil engineers. when power plants were up and coming, engineering tasks needed to create these facilities mainly consisted of mechanical, civil, and electrical engineers. these disciplines allowed for the planning and construction of power plants. but when nuclear power plants were created it introduced nuclear engineers to perform the calculations necessary to maintain safety standards.   == governing principles == while there are many disparities between the aforementioned engineering disciplines, they all cover material related to heat or electricity transmission. obtaining a degree from an abet accredited school in any one of these disciplines is essential to becoming a power plant engineer. there are also many associations which qualified engineers can join, including the american society of mechanical engineers (asme), the institute of electric and electronic engineers (ieee), and the american society of power engineers (asope).   == fields == power plant operation and maintenance consists of optimizing the efficiency and power output of power plants and ensuring long term operation. these power plants are large scale, and used to supply power for communities and industry. individual household electric power generators are not included.power station design consists of the design of new power plant systems. there are many types of power plants, and each type requires specific expertise, as well as interdisciplinary teamwork, to build a modern system.   == see also == power engineering mechanical engineering electrical engineering civil engineering photovoltaics thermal power station hydroelectricity first law of thermodynamics second law of thermodynamics wind power   == references ==  brighthub engineering. retrieved 2018-04-18.   == external links == american society of power engineers american society of mechanical engineers institute of electric and electronics engineers')"
208,"Edward Cockey (1781–1860) was an industrial entrepreneur in Frome, Somerset, England, descended from a local family of metalworkers.


== Background ==
The early part of the nineteenth century was a hard time for Frome, industry declining over the years as its dependence on the wool trade fell.  In 1826 William Cobbett commented on what he found during one of his Rural Rides in his Political Register: 

These poor creatures at Frome have pawned all their things, or nearly all.  All their best clothes, their blankets and sheets, their looms; any little piece of furniture that they had…….all the tolerably good clothes their children had….though this is a sort of manufacture cannot come to a complete end; still it has received a blow from which it cannot possibly recover.
In this situation, any new employment prospect was welcomed. On 10 November 1831, Mr Penny, a bookseller, stationer and circulating library owner, lit his shop in 3 Bath Street with gas for the first time, gas supplied by the upcoming enterprise of a Cockey.


== Origins of the family ==
Lewis Cockey (1626-1711) was a brazier (a person who works in brass), bellfounder and clocksmith who worked in Warminster. His eldest son, William (1663–1748) became a clockmaker. By 1692 he moved to Wincanton, continuing making clocks (four of his lantern clocks are known today) and casting bells, as well repairing them. The youngest son, Edward (1669–1768) stayed with his father and became notable for his exceptionally complicated astronomical clocks, helped by local patronage, particularly for Lord Weymouth at Longleat.
The second son, Lewis Cockey Junior (1666–1703) was a pewterer and bellfounder who moved to work in Frome in 1682 and was buried in the local church there in 1703. Lewes lived at 45 Milk Street, known as ‘The Bell House’, probably using the space at the side, now its garage, for his bell casting. A foundry was soon established in the appropriately named Bell Lane, since demolished, a short distance from the Bell House.  His son, William continued the family tradition until his death in 1762. At least 23 towers in Somerset and over 40 in Wiltshire and Dorset have Cockey inscriptions on their bells. Their standard inscriptions were ‘William Cockey’ plus the year, and little else.  The fine ring at St John the Baptist, Frome contains such bells – the two trebles dated 1724, and the 6th dated 1746.  
However, one of the bells in the Chapel of St Lawrence, Warminster has a more unusual inscription amongst other bells in the church, including those cast either by his father or grandfather.
God made Cockey and Cockey made me
1743
After 1752 it seems the bells were cast elsewhere, placing the orders and collecting the accounts being undertaken in Frome.  Christopher Cockey (1748–1792) had 7 children, among them Edward.


== 19th century expansion ==
Edward Cockey established his own firm in 1816.  He is listed as a brazier and iron founder in the Market Place in 1812 through to 1820 and then a brazier, tin man and ironmonger working in Bath Street in 1821. The family diversified, Edward becoming a successful iron-founder, and began casting for the gas industry as well as building his own gas works at Welshmill, managed by his son, Henry.  Frome had gas street lighting as early as 1831. Over the years he held almost the whole of the trade of the West Country in the production of gas plant and carried out many contracts in Russia and elsewhere for gasholders and the like.in 1834 Edward and his son Henry were appointed as weights and measures inspectors for the Frome district of Somerset. The firm’s name has been seen on large cast iron weights. In 1844 Lawrence Hagley, a relative of Edward Cockey’s wife, was appointed the inspector in Frome. It was said that on Edward Cockey's daily inspection of the works if he met any item which displeased him, the person responsible, whether one of his sons, a foreman or workman, was liable to receive a heavy blow across the shoulders from a stick he always carried. Imperfect products were similarly battered.Edward was a stalwart of his community, which in part meant he took a lead in religious matters. His eldest son, also an Edward (1809–1880), had taken vows with the Anglican Church and became Warden of Wadham College, Oxford.  In his study there, the first meetings of opponents to the Anglo-Catholic Tractarian movement were held. In 1851 a new vicar arrived in Frome, Father Early Bennett.  He ""was one of the first Ritualists, and a most aggressive one; the church at that time, as with most others, was very Evangelical....Father Bennett made no bones about it, preached a violent Anglo-Catholic sermon,....proposed immediately to introduce vestments, incense, confession.....practically the whole congregation left, some, it is stated, walking out during the sermon. The Cockeys.....and other influential members immediately joined the then new Church of Holy Trinity and carried on the Evangelical tradition, becoming churchwardens there……""
By 1851 the company was employing 76 men and boys in the Palmer Street foundry, behind 10–15 Bath Street, as Edward Cockey & Sons. The company extended its work into iron foundry of all kinds: fences, gates, stairs, balustrades, boilers, valves, steam engines, roofs, gasometers.  The firm made mileposts for the Salisbury to Shaftesbury road. As a growing company it supported the Gas Institute's contribution to the Great Exhibition of 1851. The gas works at Welshmill were serviced after 1854 by a railway siding for coal delivery, installed on the North Somerset Railway. They took out patents for some of their products: a cheese press (1853) and fluid regulators for gas production (1857), which were still in use well into the 20th century.
After his death in 1860, two of his sons, Henry and Christopher Francis continued the management of the family business.  In 1861 it was exhibiting agricultural equipment at the Royal Agricultural Exhibition in Leeds.  In 1865, the firm was empowered to manufacture coke for sale. Gas pipe installation in Frome was not without its dangers. On the evening of 14 May 1871 a tremendous explosion took place next to the Ship at 6 Christchurch Street West: a 20 yard stretch of paving stones were torn up, a water closet exploded and two boys walking past were thrown into the air.  Others nearby were knocked to the ground, but no one was seriously injured. It seems a newly installed gas pipe had leaked into the town drains. In 1874 a newspaper report recorded:The construction of a large gasholder for the Portsea Gas Co.  The monster will be 162 feet in diameter & when fully extended, 54½ feet in height.  It will hold about 1,100,000 cubic feet of gas, or about 14 times the contents of the biggest gasholder of the Frome Gas Co.  The weight will be more than 300 tons & this great weight will float up & down in a tank of water on a bed of gas……..the rivets used in putting the parts together will exceed 14 tons in weight.  The whole weight of iron will be between 700 & 800 tons.
In 1883 it produced the ornate castings for the structure of the Victorian Gallery of Dorset County Museum. One unusual requirement in 1886 was to lay pipes in the town to supply a gas balloon for George Sanger’s circus.
In 1886 the family firm became a limited company, Edward Cockey & Sons Ltd., Henry remaining as managing director till his death in 1891, severing the last family connection. The original foundry, with its warehouse frontage of lifting crane and window-gates overlooking Palmer Street, had a space behind that was too small for their expanding business. In 1893 the work moved to Garston, east of the town centre, on to a new open site. The 1904 25"" OS maps shows an Iron Works immediately south of the east-west Frome-Radstock line (the North Somerset Railway), at the western end of the railway triangle, and immediately north of Garston Lane/Garston Street. A siding off the  line into the works was constructed.  The buildings were mainly concentrated towards the boundaries, leaving plenty of open space to pre-assemble gasometers, before dismantling them for delivery by rail.


== 20th century ==
From 1903, the 200 gas lamp standards erected by Cockey in Frome were converted to electricity, after a generating station was built and cables laid. They were then fitted with a replacement art nouveau leaf pattern lampholder.  Over 60 survive today, not all with the leaves, some with obtrusive modern lights, a number in poor condition of paintwork. Over 20 are listed Grade II by Historic England and attributed to Singer; a specimen entry illustrating the attribution is the one for the lamp post in front of No 9 Whittox Lane, near the centre of the town. Not all of the standards are Cockey originals. The town refers to them as 'Cockey Lamps' but records of Frome Urban District Council meetings suggest the art nouveau lamp heads were designed by Singer.By 1914 the limited company, with offices in London, were employing 250 people on the Garston site and elsewhere, working as gas engineers and contractors and making steel work. The foundry was used to pour metal into casts, making gas-holders, regulating valves and tar extractors, lamp standards and oil storage tanks.  In 1927 they constructed the first waterless gasholder for Ipswich Gas Works. The Frome Gas Company, founded by Edward Cockey, was taken over by that of the Bath Gas, Coke & Light Co in 1934, before nationalisation; over the years all signs of Frome's gas works were removed.
After WWI, describing themselves as 'Gas, Constructional and Chemical Engineers', gasholders and related engineering were installed at Newport, Dorchester, Widnes, Wells, Dursley, Glasgow, Margam, Bristol, and many other locations across the British Isles.  They specialised in their own patented wash scrubbers: for the recovery of ammonia, tar and carbonic acid, by-products of making gas from coke or coal.  Aside from the foundry there was a pattern shop for the full-size modelling of moulds or casts, a fettling shop for cleaning off the rough edges of castings and a steel plate works. During WWII, the Admiralty contracted the firm to fabricate components for Mulberry Harbours.One worker, John Stocker whose father operated the overhead crane in the foundry, said about the premises in 1946: 

""There were all these different machines, lathes, milling machines.   All the machines were very, very ancient and I think that was probably the reason why they went bust!""
The firm wound up voluntarily in April 1960 but its memory remains with bollards, gate-posts, drain covers and lamp standards, many displaying the name.Despite Edward having 16 children, there are no Cockeys left in Frome now.


== References ==


== External links ==
Frome Heritage Museum, Cockey exhibits
Frome Town Council, Cockey lamps
Photos of Cockey lamps","pandas(index=208, _1=208, text='edward cockey (1781–1860) was an industrial entrepreneur in frome, somerset, england, descended from a local family of metalworkers.   == background == the early part of the nineteenth century was a hard time for frome, industry declining over the years as its dependence on the wool trade fell.  in 1826 william cobbett commented on what he found during one of his rural rides in his political register:  these poor creatures at frome have pawned all their things, or nearly all.  all their best clothes, their blankets and sheets, their looms; any little piece of furniture that they had…….all the tolerably good clothes their children had….though this is a sort of manufacture cannot come to a complete end; still it has received a blow from which it cannot possibly recover. in this situation, any new employment prospect was welcomed. on 10 november 1831, mr penny, a bookseller, stationer and circulating library owner, lit his shop in 3 bath street with gas for the first time, gas supplied by the upcoming enterprise of a cockey.   == origins of the family == lewis cockey (1626-1711) was a brazier (a person who works in brass), bellfounder and clocksmith who worked in warminster. his eldest son, william (1663–1748) became a clockmaker. by 1692 he moved to wincanton, continuing making clocks (four of his lantern clocks are known today) and casting bells, as well repairing them. the youngest son, edward (1669–1768) stayed with his father and became notable for his exceptionally complicated astronomical clocks, helped by local patronage, particularly for lord weymouth at longleat. the second son, lewis cockey junior (1666–1703) was a pewterer and bellfounder who moved to work in frome in 1682 and was buried in the local church there in 1703. lewes lived at 45 milk street, known as ‘the bell house’, probably using the space at the side, now its garage, for his bell casting. a foundry was soon established in the appropriately named bell lane, since demolished, a short distance from the bell house.  his son, william continued the family tradition until his death in 1762. at least 23 towers in somerset and over 40 in wiltshire and dorset have cockey inscriptions on their bells. their standard inscriptions were ‘william cockey’ plus the year, and little else.  the fine ring at st john the baptist, frome contains such bells – the two trebles dated 1724, and the 6th dated 1746. however, one of the bells in the chapel of st lawrence, warminster has a more unusual inscription amongst other bells in the church, including those cast either by his father or grandfather. god made cockey and cockey made me 1743 after 1752 it seems the bells were cast elsewhere, placing the orders and collecting the accounts being undertaken in frome.  christopher cockey (1748–1792) had 7 children, among them edward.   == 19th century expansion == edward cockey established his own firm in 1816.  he is listed as a brazier and iron founder in the market place in 1812 through to 1820 and then a brazier, tin man and ironmonger working in bath street in 1821. the family diversified, edward becoming a successful iron-founder, and began casting for the gas industry as well as building his own gas works at welshmill, managed by his son, henry.  frome had gas street lighting as early as 1831. over the years he held almost the whole of the trade of the west country in the production of gas plant and carried out many contracts in russia and elsewhere for gasholders and the like.in 1834 edward and his son henry were appointed as weights and measures inspectors for the frome district of somerset. the firm’s name has been seen on large cast iron weights. in 1844 lawrence hagley, a relative of edward cockey’s wife, was appointed the inspector in frome. it was said that on edward cockey\'s daily inspection of the works if he met any item which displeased him, the person responsible, whether one of his sons, a foreman or workman, was liable to receive a heavy blow across the shoulders from a stick he always carried. imperfect products were similarly battered.edward was a stalwart of his community, which in part meant he took a lead in religious matters. his eldest son, also an edward (1809–1880), had taken vows with the anglican church and became warden of wadham college, oxford.  in his study there, the first meetings of opponents to the anglo-catholic tractarian movement were held. in 1851 a new vicar arrived in frome, father early bennett.  he ""was one of the first ritualists, and a most aggressive one; the church at that time, as with most others, was very evangelical....father bennett made no bones about it, preached a violent anglo-catholic sermon,....proposed immediately to introduce vestments, incense, confession.....practically the whole congregation left, some, it is stated, walking out during the sermon. the cockeys.....and other influential members immediately joined the then new church of holy trinity and carried on the evangelical tradition, becoming churchwardens there……"" by 1851 the company was employing 76 men and boys in the palmer street foundry, behind 10–15 bath street, as edward cockey & sons. the company extended its work into iron foundry of all kinds: fences, gates, stairs, balustrades, boilers, valves, steam engines, roofs, gasometers.  the firm made mileposts for the salisbury to shaftesbury road. as a growing company it supported the gas institute\'s contribution to the great exhibition of 1851. the gas works at welshmill were serviced after 1854 by a railway siding for coal delivery, installed on the north somerset railway. they took out patents for some of their products: a cheese press (1853) and fluid regulators for gas production (1857), which were still in use well into the 20th century. after his death in 1860, two of his sons, henry and christopher francis continued the management of the family business.  in 1861 it was exhibiting agricultural equipment at the royal agricultural exhibition in leeds.  in 1865, the firm was empowered to manufacture coke for sale. gas pipe installation in frome was not without its dangers. on the evening of 14 may 1871 a tremendous explosion took place next to the ship at 6 christchurch street west: a 20 yard stretch of paving stones were torn up, a water closet exploded and two boys walking past were thrown into the air.  others nearby were knocked to the ground, but no one was seriously injured. it seems a newly installed gas pipe had leaked into the town drains. in 1874 a newspaper report recorded:the construction of a large gasholder for the portsea gas co.  the monster will be 162 feet in diameter & when fully extended, 54½ feet in height.  it will hold about 1,100,000 cubic feet of gas, or about 14 times the contents of the biggest gasholder of the frome gas co.  the weight will be more than 300 tons & this great weight will float up & down in a tank of water on a bed of gas……..the rivets used in putting the parts together will exceed 14 tons in weight.  the whole weight of iron will be between 700 & 800 tons. in 1883 it produced the ornate castings for the structure of the victorian gallery of dorset county museum. one unusual requirement in 1886 was to lay pipes in the town to supply a gas balloon for george sanger’s circus. in 1886 the family firm became a limited company, edward cockey & sons ltd., henry remaining as managing director till his death in 1891, severing the last family connection. the original foundry, with its warehouse frontage of lifting crane and window-gates overlooking palmer street, had a space behind that was too small for their expanding business. in 1893 the work moved to garston, east of the town centre, on to a new open site. the 1904 25"" os maps shows an iron works immediately south of the east-west frome-radstock line (the north somerset railway), at the western end of the railway triangle, and immediately north of garston lane/garston street. a siding off the  line into the works was constructed.  the buildings were mainly concentrated towards the boundaries, leaving plenty of open space to pre-assemble gasometers, before dismantling them for delivery by rail.   == 20th century == from 1903, the 200 gas lamp standards erected by cockey in frome were converted to electricity, after a generating station was built and cables laid. they were then fitted with a replacement art nouveau leaf pattern lampholder.  over 60 survive today, not all with the leaves, some with obtrusive modern lights, a number in poor condition of paintwork. over 20 are listed grade ii by historic england and attributed to singer; a specimen entry illustrating the attribution is the one for the lamp post in front of no 9 whittox lane, near the centre of the town. not all of the standards are cockey originals. the town refers to them as \'cockey lamps\' but records of frome urban district council meetings suggest the art nouveau lamp heads were designed by singer.by 1914 the limited company, with offices in london, were employing 250 people on the garston site and elsewhere, working as gas engineers and contractors and making steel work. the foundry was used to pour metal into casts, making gas-holders, regulating valves and tar extractors, lamp standards and oil storage tanks.  in 1927 they constructed the first waterless gasholder for ipswich gas works. the frome gas company, founded by edward cockey, was taken over by that of the bath gas, coke & light co in 1934, before nationalisation; over the years all signs of frome\'s gas works were removed. after wwi, describing themselves as \'gas, constructional and chemical engineers\', gasholders and related engineering were installed at newport, dorchester, widnes, wells, dursley, glasgow, margam, bristol, and many other locations across the british isles.  they specialised in their own patented wash scrubbers: for the recovery of ammonia, tar and carbonic acid, by-products of making gas from coke or coal.  aside from the foundry there was a pattern shop for the full-size modelling of moulds or casts, a fettling shop for cleaning off the rough edges of castings and a steel plate works. during wwii, the admiralty contracted the firm to fabricate components for mulberry harbours.one worker, john stocker whose father operated the overhead crane in the foundry, said about the premises in 1946:  ""there were all these different machines, lathes, milling machines.   all the machines were very, very ancient and i think that was probably the reason why they went bust!"" the firm wound up voluntarily in april 1960 but its memory remains with bollards, gate-posts, drain covers and lamp standards, many displaying the name.despite edward having 16 children, there are no cockeys left in frome now.   == references ==   == external links == frome heritage museum, cockey exhibits frome town council, cockey lamps photos of cockey lamps')"
209,"The Gibbs Brothers Medal is awarded by the U.S. National Academy of Sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  It was established by a gift from William Francis Gibbs and Frederic Herbert Gibbs.


== Recipients ==
Jerome H. Milgram (2017)
For wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.Robert G. Keane, Jr. (2012)
For continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the U.S. Navy the most powerful in the world.Keith W. Tantlinger (2009)
For his visionary and innovative design of the cellular container ship and supporting systems that transformed the world's shipping fleet and facilitated the rapid expansion of global trade.Donald Liu (2006)
For first introducing finite element techniques into ship design and being the driving force behind the revolution in basing classification society rules on scientific principles.Alfred C. Malchiodi (2003)
For leading innovations in developing the naval architecture of submarines for the efficient utilization of advanced technology.Edward E. Horton (2001)
For visionary and innovative concept development and design of off-shore platforms, mooring systems, and related technology that have significantly influenced development of deep-water operations.Justin E. Kerwin (1999)
For his outstanding contributions in the field of naval architecture, including the development of computational methods used worldwide in propeller design.William B. Morgan (1997)
For his technical leadership in improving performance, quieting, and design of advanced marine propulsion systems, and development of large modern propulsion research and testing facilities.Owen H. Oakley (1995)
For his significant contributions to the field of naval architecture, especially in the design of naval ships, submarines, and advanced ship types and submersibles.Olin J. Stephens II (1993)
For his design of outstanding sailing vessels, including six defenders of the America's Cup and thousands of ocean-racing yachts, and for promoting the use of scientific knowledge and research in the field of naval architecture.Bruce G. Collipp (1991)
For his invention of the semisubmersible, offshore, floating drilling platform, and for his sustained pioneering leadership in devising innovative ocean-engineering technologies.Leslie A. Harlander (1988)
For his pioneering effort in the design of specialized vessels and cargo-handling equipment associated with * intermodal shipping by container systems.Matthew Galbraith Forrest (1979)John Charles Niedermair (1976)
For his outstanding contributions to the field of naval architecture and marine engineering.Phillip Eisenberg (1974)
For his work that is the basis of much of what is known about hydrofoils and how ships move smoothly.Henry A. Schade (1970)
For his outstanding contributions in the design, construction, and performance of ships.Alfred Adolf Heinrich Kiel (1967)
For his outstanding contributions in the field of naval architecture and marine engineering.Frederick Henry Todd (1965)
For his contributions to the theory of ship design through model experiments, and for his leadership in hydrodynamic research.


== See also ==
List of engineering awards
List of awards named after people


== References ==","pandas(index=209, _1=209, text='the gibbs brothers medal is awarded by the u.s. national academy of sciences for ""outstanding contributions in the field of naval architecture and marine engineering"".  it was established by a gift from william francis gibbs and frederic herbert gibbs.   == recipients == jerome h. milgram (2017) for wide-ranging original contributions to naval architecture in theoretical hydromechanics, education, yacht design, environmental protection, and the practical arts of ocean systems.robert g. keane, jr. (2012) for continued excellence as a naval architect over many years, exemplified by the outstanding naval warships that he had a major part in designing, helping to make the u.s. navy the most powerful in the world.keith w. tantlinger (2009) for his visionary and innovative design of the cellular container ship and supporting systems that transformed the world\'s shipping fleet and facilitated the rapid expansion of global trade.donald liu (2006) for first introducing finite element techniques into ship design and being the driving force behind the revolution in basing classification society rules on scientific principles.alfred c. malchiodi (2003) for leading innovations in developing the naval architecture of submarines for the efficient utilization of advanced technology.edward e. horton (2001) for visionary and innovative concept development and design of off-shore platforms, mooring systems, and related technology that have significantly influenced development of deep-water operations.justin e. kerwin (1999) for his outstanding contributions in the field of naval architecture, including the development of computational methods used worldwide in propeller design.william b. morgan (1997) for his technical leadership in improving performance, quieting, and design of advanced marine propulsion systems, and development of large modern propulsion research and testing facilities.owen h. oakley (1995) for his significant contributions to the field of naval architecture, especially in the design of naval ships, submarines, and advanced ship types and submersibles.olin j. stephens ii (1993) for his design of outstanding sailing vessels, including six defenders of the america\'s cup and thousands of ocean-racing yachts, and for promoting the use of scientific knowledge and research in the field of naval architecture.bruce g. collipp (1991) for his invention of the semisubmersible, offshore, floating drilling platform, and for his sustained pioneering leadership in devising innovative ocean-engineering technologies.leslie a. harlander (1988) for his pioneering effort in the design of specialized vessels and cargo-handling equipment associated with * intermodal shipping by container systems.matthew galbraith forrest (1979)john charles niedermair (1976) for his outstanding contributions to the field of naval architecture and marine engineering.phillip eisenberg (1974) for his work that is the basis of much of what is known about hydrofoils and how ships move smoothly.henry a. schade (1970) for his outstanding contributions in the design, construction, and performance of ships.alfred adolf heinrich kiel (1967) for his outstanding contributions in the field of naval architecture and marine engineering.frederick henry todd (1965) for his contributions to the theory of ship design through model experiments, and for his leadership in hydrodynamic research.   == see also == list of engineering awards list of awards named after people   == references ==')"
210,"Marine engineering includes the engineering of boats, ships, oil rigs and any other marine vessel or structure, as well as oceanographic engineering, oceanic engineering or ocean engineering. Specifically, marine engineering is the discipline of applying engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and on-board systems and oceanographic technology. It includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, such as surface ships and submarines.


== History ==
Archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. Modern marine engineering dates back to the beginning of the Industrial Revolution (early 1700s).
In 1712, Thomas Newcomen, a blacksmith, created a steam powered engine to pump water out of mines. In 1807, Robert Fulton successfully used a steam engine to propel a vessel through the water. Fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. The integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. Only twelve years after Fulton’s Clermont had her first voyage, the Savannah marked the first sea voyage from America to Europe. Around 50 years later the steam powered paddle wheels had a peak with the creation of the Great Eastern, which was as big as one of the cargo ships of today, 700 feet in length, weighing 22,000 tons. Paddle steamers would become the frontrunners of the steamship industry for the next thirty years till the next type of propulsion came around.In 1896, Henry L. Williams made the first oil platforms.


== Marine engineering specialties ==


=== Naval architect ===
Naval architects are concerned with the overall design of the ship and its propulsion through the water.


=== Mechanical engineering ===
Mechanical engineers design the main propulsion plant, the powering and mechanization aspects of the ship functions such as steering, anchoring, cargo handling, heating, ventilation, air conditioning interior and exterior communication, and other related requirements.  Electrical power generation and electrical power distribution systems are typically designed by their suppliers; only installation is the design responsibility of the marine engineer.


=== Oceanographic engineering ===
Oceanographic engineering is concerned with mechanical, electrical, and electronic, and computing technology deployed to support oceanography, and also falls under the umbrella of marine engineering, especially in Britain, where it is covered by the same professional organisation, the IMarEST.


=== Offshore engineering ===
Civil engineering for an offshore environment, the design and construction of fixed and floating marine structures, such as oil platforms and offshore wind farms is generally called offshore engineering.


== Challenges specific to marine engineering ==


=== Hydrodynamic loading ===
In the same way that civil engineers design to accommodate wind loads on building and bridges, maritime engineers design to accommodate a ship being flexed or a platform being struck by waves millions of times in its life.


=== Stability ===
A naval architect, like an airplane designer, is concerned with stability.  The naval architect's job is different, insofar as a ship operates in two fluids simultaneously: water and air. Engineers also face the challenge of balancing cargo as the mass of the ship increase and the center of gravity shifts higher as additional containers are stacked vertically. In addition, the weight of fuel presents a problem as the pitch of the ship cause the weight to shift with the liquid causing an imbalance. This offset is counteracted by water inside larger ballast tanks. Engineers are faced with the task of balancing and tracking the fuel and ballast water of a ship.


=== Corrosion ===
The chemical environment faced by ships and offshore structures is far harsher than nearly anywhere on land, save chemical plants.  Marine engineers are concerned with surface protection and preventing galvanic corrosion in every project.   Corrosion can be inhibited through cathodic protection by utilizing pieces of metal known as sacrificial anodes. A piece of metal such as zinc is used as the sacrificial anode as it becomes the anode in the chemical reaction. This causes the metal to corrode and not the ship’s hull. Another way to prevent corrosion is by sending a controlled amount of low DC current to the ship’s hull to prevent the process of electro-chemical corrosion. This changes the electrical charge of the ship’s hull to prevent electro-chemical corrosion.


=== Anti-fouling ===
Anti-fouling is the process of eliminating obstructive organisms from essential components of seawater systems. Marine organisms grow and attach to the surfaces of the outboard suction inlets used to obtain water for cooling systems. Electro-chlorination involves running high electrical current through sea water. The combination of current and sea water alters the chemical composition to create sodium hypochlorite to purge any bio-matter.  An electrolytic method of anti-fouling involves running electrical current through two anodes (Scardino, 2009). These anodes typically consist of copper and aluminum (or iron). The copper anode releases its ion into the water creating an environment that is too toxic for bio-matter. The second metal, aluminum, coats the inside of the pipes to help prevent corrosion. Other forms of marine growth such as mussels and algae may attach themselves to the bottom of a ship's hull. This causes the ship to have a less hydrodynamic shape since it would not be uniform and smooth around the hull. This creates the problem of less fuel efficiency as it slows down the vessel (IMO, 2018). This issue can be remedied by using special paint that prevent the growth of such organisms.


=== Pollution control ===


==== Sulfur emission ====
The burning of marine fuels has the potential to release harmful pollutants into the atmosphere. Ships burn marine diesel in addition to heavy fuel oil. Heavy fuel oil, being the heaviest of refined oils, releases sulfur dioxide when burned. Sulfur dioxide emissions have the potential to raise atmospheric and ocean acidity causing harm to marine life. However, heavy fuel oil may only be burned in international waters due to the pollution created. It is commercially advantageous due to the cost effectiveness compared to other marine fuels. It is prospected that heavy fuel oil will be phased out of commercial use by the year 2020 (Smith, 2018).


==== Oil and water discharge ====
Water, oil, and other substances collect at the bottom of the ship in what is known as the bilge. Bilge water is pumped overboard, but must pass a pollution threshold test of 15 ppm (parts per million) of oil to be discharged. Water is tested and either discharged if clean or recirculated to a holding tank to be separated before being tested again. The tank it is sent back to, the oily water separator, utilizes gravity to separate the fluids due to their viscosity. Ships over 400 gross tons  are required to carry the equipment to separate oil from bilge water. Further, as enforced by MARPOL, all ships over 400 gross tons and all oil tankers over 150 gross tons are require to log all oil transfer is an oil record book (EPA, 2011).


=== Cavitation ===
Cavitation is the process of forming an air bubble in a liquid due to the vaporization of that liquid cause by an area of low pressure. This area of low pressure lowers the boiling point of a liquid allowing it to vaporize into a gas. Cavitation can take place in pumps, which can cause damage to the impeller that moves the fluids through the system. Cavitation is also seen in propulsion. Low pressure pockets form on the surface of the propeller blades as its revolutions per minute increase (IIMS, 2015). Cavitation on the propeller causes a small but violent implosion which could warp the propeller blade. To remedy the issue, more blades allow the same amount of propulsion force but at a lower rate of revolutions. This is crucial for submarines as the propeller needs to keep the vessel relatively quiet to stay hidden. With more propeller blades, the vessel is able to achieve the same amount of propulsion force at lower shaft revolutions.


== Career ==
In 2012, the average annual earnings for marine engineers in the U.S. were $96,140 with average hourly earnings of $46.22.


== Industry growth ==
Marine engineering is predicted to grow approximately 12% from 2016 to 2026. Currently there are about 8,200 naval architects and marine engineers employed, however, this number is expected to increase to 9,200 by 2026 (BLS, 2017). This trend could be attributed to the demand in fossil fuels obtained through offshore drilling and mining. In addition, 90% of the world's trade is done overseas by close to 50,000 ships, all of which require engineers aboard and shoreside (ICS, 2017).


== Education ==

Maritime universities are dedicated to teaching and training students in maritime professions. Marine engineers generally have a bachelor's degree in marine engineering, marine engineering technology, or marine systems engineering. Practical training is valued by employers alongside the bachelor's degree.


=== Professional institutions ===
IMarEST
Society for Underwater Technology
IEEE Oceanic Engineering Society
Marine Engineering and Research Institute
Society of Naval Architects and Marine Engineers (SNAME) is a worldwide society that is focused on the advancement of the maritime industry. SNAME was founded in 1893.
American Society of Naval Engineers (ASNE)
Kunjali Marakkar School of Marine Engineering


== See also ==
Engine room
Engineering officer (ship) – Licensed mariner responsible for  propulsion plants and support systems
Marine architecture
Marine electronics
Naval architecture – Engineering discipline dealing with the design and construction of marine vessels
Oceanography – The study of the physical and biological aspects of the ocean


== References ==","pandas(index=210, _1=210, text=""marine engineering includes the engineering of boats, ships, oil rigs and any other marine vessel or structure, as well as oceanographic engineering, oceanic engineering or ocean engineering. specifically, marine engineering is the discipline of applying engineering sciences, including mechanical engineering, electrical engineering, electronic engineering, and computer science, to the development, design, operation and maintenance of watercraft propulsion and on-board systems and oceanographic technology. it includes but is not limited to power and propulsion plants, machinery, piping, automation and control systems for marine vehicles of any kind, such as surface ships and submarines.   == history == archimedes is traditionally regarded as the first marine engineer, having developed a number of marine engineering systems in antiquity. modern marine engineering dates back to the beginning of the industrial revolution (early 1700s). in 1712, thomas newcomen, a blacksmith, created a steam powered engine to pump water out of mines. in 1807, robert fulton successfully used a steam engine to propel a vessel through the water. fulton's ship used the engine to power a small wooden paddle wheel as its marine propulsion system. the integration of a steam engine into a watercraft to create a marine steam engine was the start of the marine engineering profession. only twelve years after fulton’s clermont had her first voyage, the savannah marked the first sea voyage from america to europe. around 50 years later the steam powered paddle wheels had a peak with the creation of the great eastern, which was as big as one of the cargo ships of today, 700 feet in length, weighing 22,000 tons. paddle steamers would become the frontrunners of the steamship industry for the next thirty years till the next type of propulsion came around.in 1896, henry l. williams made the first oil platforms.   == marine engineering specialties == imarest society for underwater technology ieee oceanic engineering society marine engineering and research institute society of naval architects and marine engineers (sname) is a worldwide society that is focused on the advancement of the maritime industry. sname was founded in 1893. american society of naval engineers (asne) kunjali marakkar school of marine engineering   == see also == engine room engineering officer (ship) – licensed mariner responsible for  propulsion plants and support systems marine architecture marine electronics naval architecture – engineering discipline dealing with the design and construction of marine vessels oceanography – the study of the physical and biological aspects of the ocean   == references =="")"
211,"The John Elder Professor of Naval Architecture and Ocean Engineering at Glasgow University, in Scotland, was founded in 1883 and endowed by Isabella Elder (1828-1905) in memory of her husband, John Elder, marine engineer and shipbuilder of Randolph, Elder & Co., (1824-1869).


== John Elder Professors of Naval Architecture and Ocean Engineering ==
Francis Elgar LLD (1883)
Philip Jenkins (1886)
Sir John Harvard Biles DSc LLD (1891)
Percy Archibald Hillhouse DSc (1921-1942)
Andrew McCance Robb DSc LLD (1944)
John Farquhar Christie Conn DSc (1957)
Douglas Faulkner BSc PhD RCNC FEng (1973)
Nigel D P Barltrop BSc CEng SICE MRINA


== See also ==
List of Professorships at the University of Glasgow


== References ==
Who, What and Where: The History and Constitution of the University of Glasgow compiled by Michael Moss, Moira Rankin and Lesley Richmond","pandas(index=211, _1=211, text='the john elder professor of naval architecture and ocean engineering at glasgow university, in scotland, was founded in 1883 and endowed by isabella elder (1828-1905) in memory of her husband, john elder, marine engineer and shipbuilder of randolph, elder & co., (1824-1869).   == john elder professors of naval architecture and ocean engineering == francis elgar lld (1883) philip jenkins (1886) sir john harvard biles dsc lld (1891) percy archibald hillhouse dsc (1921-1942) andrew mccance robb dsc lld (1944) john farquhar christie conn dsc (1957) douglas faulkner bsc phd rcnc feng (1973) nigel d p barltrop bsc ceng sice mrina   == see also == list of professorships at the university of glasgow   == references == who, what and where: the history and constitution of the university of glasgow compiled by michael moss, moira rankin and lesley richmond')"
212,"Marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. Heat exchangers can be used for a wide variety of uses. As the name implies, these can be used for heating as well as cooling. The two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  Maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.


== Types ==
Though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels.


=== Plate ===
Plate-type marine heat exchangers are designed with sets of multiple parallel plates that are compressed to form the main cooler unit. This type has a small footprint in comparison to other types of heat exchangers.  The plates are designed in such a way that when placed next to each other they create passageways to the fluid to flow between the plates. Gaskets are placed around the edge of each plate in order to prevent the mixing of the two fluids.  Due to the temperature and pressure constraints of the rubber used to make the gaskets plate type heat exchangers are used for low pressure, low temperature applications, under 290 psig (20 bar) and 300 degrees Fahrenheit (150 degrees Celsius).


=== Shell and tube ===
Shell and tube heat exchangers consist of a tube bundle which is placed inside the larger shell. Due to this design these exchanger require twice the footprint of the heat exchanger in order to perform maintenance.  Depending on the amount of cooling needed, shell and tube heat exchangers can be built in single or double pass configuration.  The number of pass refers to the number of times the fluid in the shell passes by the fluid in the tubes.  This is achieved by placing baffles in the shell that allow for the fluid to be directed.


== Uses ==
Heat exchangers on board vessels are used throughout many system.  Systems that use heat exchangers include lube oil, jacket water, steam systems and main seawater.  The systems are often interconnected by heat exchangers in order to remove heat generated from running equipment from the engine room.


=== Motor oil ===
Heat generated due to friction is carried away from the engine in the motor oil. The motor oil flows through a heat exchanger, where the heat is passed to a central engine room cooling loop, before the heat is rejected to the ocean.


=== Jacket water ===
Heat generated an engine's cylinders is transferred to a jacket water cooling system through the cylinder wall. In addition to cooling the cylinder walls, jacket water is often found as an insulator between the exhaust header and the engine room. Jacket water cooling systems can be cooled by a central cooling water loop or can be cooled directly by seawater.


=== Steam ===
Unlike most systems with heat exchangers, steam is used to heat other systems.  This is most common when a ship is left pierside for an extended period of time.  The steam system will be used to prevent condensation and rusting of vital engine room components. These heat exchangers are most often shell and tube heat exchangers due to the high temperature and pressures often utilized in steam systems.


=== Seawater ===
Seawater cooling is often the last stage of cooling on board a ship.  These coolers are oftentimes the largest on board a vessel in order to ensure maximum heat transfer to the seawater.  The seawater is then discharged overboard after passing through the coolers.


== Fouling ==
Maintenance of marine heat exchangers is important to ensure the small pathways in both types of coolers do not become fouled.  Depending on the system different types fouling may occur.  In oil based systems, an insufficient amount of cooling medium or inefficient flow of oil through the heater can cause the heater to become fouled.  Seawater coolers can often become fouled due to marine life present in the water or due to galvanic corrosion if the correct safety measures are not taken to prevent such occurrences.


== Maintenance ==
Regular maintenance of heat exchangers is important in order to maintain the heat exchanger's maximum efficiency.  Sacrificial anodes are necessary in cooling systems to prevent galvanic corrosion.  Anodes are often time made of Zinc and are replaced when they reach fifty percent wear.  Shell and tube heat exchangers require tubes to be plugged upon the detection of a leak.  This prevents the two liquids from mixing inside the heat exchangers.  In order to perform regular maintenance on a plate type heat exchanger, the plate stack is separated and the plates a cleaned to improve heat transfer.


== References ==","pandas(index=212, _1=212, text=""marine heat exchangers are no different than non-marine heat exchangers except for the simple fact that they are found aboard ships. heat exchangers can be used for a wide variety of uses. as the name implies, these can be used for heating as well as cooling. the two primary types of marine heat exchangers used aboard vessels in the maritime industry are plate, and shell and tube.  maintenance for heat exchangers prevents fouling and galvanic corrosion from dissimilar metals.   == types == though there are many more types of heat exchangers that are used shore side, plate and shell and tube heat exchangers are the most common type of heat exchangers found aboard ocean-going vessels. seawater cooling is often the last stage of cooling on board a ship.  these coolers are oftentimes the largest on board a vessel in order to ensure maximum heat transfer to the seawater.  the seawater is then discharged overboard after passing through the coolers.   == fouling == maintenance of marine heat exchangers is important to ensure the small pathways in both types of coolers do not become fouled.  depending on the system different types fouling may occur.  in oil based systems, an insufficient amount of cooling medium or inefficient flow of oil through the heater can cause the heater to become fouled.  seawater coolers can often become fouled due to marine life present in the water or due to galvanic corrosion if the correct safety measures are not taken to prevent such occurrences.   == maintenance == regular maintenance of heat exchangers is important in order to maintain the heat exchanger's maximum efficiency.  sacrificial anodes are necessary in cooling systems to prevent galvanic corrosion.  anodes are often time made of zinc and are replaced when they reach fifty percent wear.  shell and tube heat exchangers require tubes to be plugged upon the detection of a leak.  this prevents the two liquids from mixing inside the heat exchangers.  in order to perform regular maintenance on a plate type heat exchanger, the plate stack is separated and the plates a cleaned to improve heat transfer.   == references =="")"
213,"The tactical diameter of a ship is the distance:

travelled on sea surface
during a turning circle test
with maximum rudder angle
by the center of gravity of a ship
taken perpendicular to the initial track followed at approach speed
when the heading has changed by 180°The ratio of the tactical diameter divided by the ship's length between perpendiculars gives a dimensionless parameter which can be used to compare ships maneuverability.


== External links ==
http://ittc.sname.org/new%20recomendations/pdf%20Procedures%202008/7.5-04-02-01.pdf","pandas(index=213, _1=213, text=""the tactical diameter of a ship is the distance:  travelled on sea surface during a turning circle test with maximum rudder angle by the center of gravity of a ship taken perpendicular to the initial track followed at approach speed when the heading has changed by 180°the ratio of the tactical diameter divided by the ship's length between perpendiculars gives a dimensionless parameter which can be used to compare ships maneuverability.   == external links == http://ittc.sname.org/new%20recomendations/pdf%20procedures%202008/7.5-04-02-01.pdf"")"
214,"Lightening holes are holes in structural components of machines and buildings used by a variety of engineering disciplines to make structures lighter. The edges of the hole may be flanged to increase the rigidity and strength of the component. The holes can be circular, triangular, elliptical, or rectangular and should have rounded edges, but they should never have sharp corners, to avoid the risk of stress risers, and they must not be too close to the edge of a structural component.


== Usage ==


=== Aviation ===

Lightening holes are often used in the aviation industry. This allows the aircraft to be lightweight as possible, retaining the durability and airworthiness of the aircraft structure.


=== Maritime ===
Lightening holes have also been used in marine engineering to increase seaworthiness of the vessel.


=== Motorsports ===

Lightening holes became a prominent feature of motor racing in the 1920s and 1930s. Chassis members, suspension components, engine housings and even connecting rods were drilled with a range of holes, of sizes almost as large as the component.

""[The] wisdom of the day was to make everything along the lines of a brick shithouse [...] and then drill holes in the bits to lighten them.""
This drive towards lightening was based on a misunderstanding of the component's mechanical behaviour. The assumption for an H-beam was that all of the resistance to bending stresses was carried in the two top and bottom flanges of the girder, with the central web only carrying out a spacing function. The central web could thus be drilled indiscriminately, supposedly without weakening the overall girder. This was based on two fallacies: firstly that the only forces on the beam were simple bending forces in the plane of the web. In practice, a more complicated force, such as an unexpected torsional twisting from a sudden suspension bump overloaded the now-weakened central web and the lightened beam failed immediately. Secondly, the assumption that the ideal forces were separated into the top and bottom flanges was increasingly unrealistic with the development of stressed skin and monocoque designs, where loads were more evenly shared. In these designs there was no ""unloaded web"" that could be safely drilled.
In comparison to aircraft design, it was notable that motor racing lightening was carried out after the design of the components, usually without understanding the forces in the structure and without re-performing any stress calculations afterwards. Owing to the increasing number of dangerous failures, the practice of drilling lightening holes was forbidden by the race scrutineers. Lightening was still permitted, but only where it had been specified by the original designers, not by simply drilling holes afterwards.


=== Military ===
Lightening holes have been used in various military vehicles, aircraft, equipment and weaponry platforms. This allows equipment to be lighter in weight as well as increase the ruggedness and durability. They are usually made by drilling holes, pressed stamping or machining and can also save strategic materials and cost during wartime production.


=== Architecture ===

Lightening holes have been used on various architecture designs. During the 1980s and early 1990s, Lightening holes were fashionable and somewhat seen as futuristic and were used in the likes of industrial units, car showrooms, shopping precincts, sports centres etc. Parsons House in London is a notable building that uses lightening holes since its renovation in 1988. Ringwood Health & Leisure Centre in Hampshire is another notable example.


== See also ==
Honeycomb structure
Hollow structural section
Isogrid
Truss


== References ==


== External links ==
Tests Of Beams Having Webs With Large Circular Lightening Holes, by L. Ross Levin, National Advisory Committee for Aeronautics
The Strength And Stiffness Of Shear Webs With And Without Lightening Holes, by Paul Kuhn, National Advisory Committee for Aeronautics
The Strength And Stiffness Of Shear Webs With Round Lightening Holes Having 45° Flanges, by Paul Kuhn, National Advisory Committee for Aeronautics","pandas(index=214, _1=214, text='lightening holes are holes in structural components of machines and buildings used by a variety of engineering disciplines to make structures lighter. the edges of the hole may be flanged to increase the rigidity and strength of the component. the holes can be circular, triangular, elliptical, or rectangular and should have rounded edges, but they should never have sharp corners, to avoid the risk of stress risers, and they must not be too close to the edge of a structural component.   == usage == lightening holes have been used on various architecture designs. during the 1980s and early 1990s, lightening holes were fashionable and somewhat seen as futuristic and were used in the likes of industrial units, car showrooms, shopping precincts, sports centres etc. parsons house in london is a notable building that uses lightening holes since its renovation in 1988. ringwood health & leisure centre in hampshire is another notable example.   == see also == honeycomb structure hollow structural section isogrid truss   == references ==   == external links == tests of beams having webs with large circular lightening holes, by l. ross levin, national advisory committee for aeronautics the strength and stiffness of shear webs with and without lightening holes, by paul kuhn, national advisory committee for aeronautics the strength and stiffness of shear webs with round lightening holes having 45° flanges, by paul kuhn, national advisory committee for aeronautics')"
215,"The FORAN System is an integrated CAD/CAM/CAE system developed by SENER for the design and production of practically any naval ship and offshore unit. It is a multidisciplinary and integrated system that can be used in all the ship design and production phases and disciplines. The System collects all the information in a single database. FORAN is mainly focused on the design of: 

Merchants, Ro-Pax, Ro-Ro, bulk carriers, chemical tankers, container ships and cement and oil tankers.
Navy vessels (surface ships and submarines), in which the systems allows designers to carry out configuration control, analyze different design alternatives (prototypes), handle advanced hull forms and manage materials and special standards, as well as introducing customized criteria.
Specific vessels, tugs and workboats, hotel vessels, fishing vessels, fish transport vessels, oceanographic vessels, etc.
For use in the offshore industry such as floating platforms (both anchored and fixed), staff transportation services, anchor vessels and vessels for applications such as supply, rescue, firefighting or anti-pollution.The latest version of the integrated CAD/CAM/CAE system is FORAN V80; however, FORAN V70 is still used widely.


== FORAN V70 ==
Common tools: this version supports Unicode characters; this functionality enables entering text and generating information in languages using non Latin characters such as Chinese, Russian or Korean. FORAN dialogues and menu names can also be translated. Moreover, this update includes FVIEWER, a virtual reality module that replace the former VISUAL3D module. This application takes advantage of state-of-the-art graphic cards capabilities and allows the management of huge amount of information data. 
Drafting: This update includes also a new 2D environment, based in the QCAD application and compatible with AutoCAD, developed to be used in the modules for the norms and structure standards definition (FNORM), and in the General Arrangement module (FGA) and for the definition of electrical and P&I diagrams. It also includes developments for the interim products drawings, symbolic drawings and the 3D model views drawings.
Project: One of the most relevant developments of the new version is the General Arrangement module (FGA) for spaces and general ship arrangement definition, both in 2D or 3D environments, with all data stored in the FORAN database. The application allows generating the general arrangement drawing in an efficient way.  On the other hand, the module for the probabilistic damage stability calculations (FSUBD) offers now the possibility to consider intermediate stages of flooding, according to SOLAS standard. The automatic assignment of spaces to subzones has also been improved. 
Hull structure: A FNORM module for the definition of standards of structure is provided with a user interface, including multi dock windows and snap points; furthermore, it allows adding geometrical restrictions and includes the possibility of layer management. The increase of the lengths of the identifications and descriptions of blocks, materials and geometrical norms, as well as the hierarchical structure for the definition of the standards and geometrical norms are other capabilities of this module. Moreover, following features must be highlighted:  hull structure modeling: an algorithm to represent corrugated parts more accurately, commands for checking the edge preparation of plates and profiles, options for the definition of face bars and an algorithm to represent more accurately curved shell and deck plates. Regarding profiles and plates nesting, the NEST module allows the nesting of identical parts assigned to different interim products and keeps information to recognize each individual part. 
Outfitting: FORAN V70 includes piping design tools for pipeline routing. Some important characteristics featured in the new update regard the polygonal lines, which are no more needed, the pipelines that are now routed dynamically displaying the pipeline as a solid model with significant snap points of the model. The version incorporates design functionalities adapted to the production circumstances in each shipyard, such as a command for smart splitting of pipe segments based on the standard pipe length defined in the components library, or checking utilities to control the spool fabrication restrictions before generating drawings and greater flexibility for the creation of sets of piping elements.
Electrical design: The electrical design module allows now to generate cable conduits for special non-standard cross-section cableways and to define conduits with cables inside cable trays, considering them in the cross-section filling calculations. In the cable routing the definition of cable splitting has also been improved and the management of cables partially routed has a better functionality in the connection between cables and terminal blocks.
Product lifecycle management (PLM):  FORAN V70 allows the integration with different PLM Systems, thanks to a neutral solution built with standards based on CORBA and web services.


== FORAN FVIEWER VR ==
The use of a Virtual Reality (VR) environment offers significant advantages in shipbuilding. The most important advantage is the possibility to review the model and to find out errors at early design stages, with an important cost reduction. VR allows an intuitive and quick evaluation of the model, queries, measurement of distances, ergonomic studies, collision detection, design changes evaluation, simulation of mounting, dismantling and operation tasks, etc. The viewer, called FORAN FVIEWER VR, which is part of the FORAN System, includes stereoscopic capability as feature, which means that it allows the 3D navigation around the model of a ship, being possible to use it with tracking devices. The module can be used during a 3D navigation with a great user-model interaction. The solution can be used in any kind of VR room as well as in portable solutions, workstations, etc.


== Head Mounted Display (HMD) ==
The development of applications in a Virtual Reality environment within the shipbuilding industry is booming nowadays. After the development of the second generation of the viewer for interactive navigation through the ship 3D model -FORAN FVIEWER VR- SENER and Ingevideo have developed a Head Mounted Display (HMD).A Virtual Reality HMD is a device that lets the user to view and interact with 3D simulation environments, in this case applied to 3D models generated in FORAN. The possibility of working in virtual reality environments provides benefits and cost reductions, since it allows virtual screening in an interactive and intuitive way.


== FORAN system references ==
The FORAN system is used in more than 150 ship design offices and shipyards in 30 countries. Some of its most significant references are:

Strategic projection ship for Navantia.
Cruise salvage vessel for the China Ship Development and Design Center (CSDDC).
CVF, aircraft carrier for the Royal Navy (client: BAE Systems Babcock Marine).
“Bahía Uno”, oil tanker for Astilleros de Murueta.
“Ruiloba”, container ship for 1,350 TEU for the client Hijos de J. Barreras Shipyard.
Frigate F-310 built at the Navantia shipyards for the Norwegian Royal Navy.
Semi-submersible platform GM 400 for Global Maritime.


== See also ==
Comparison of computer-aided design editors
List of 3D computer graphics software
List of 3D rendering software
List of 3D modeling software


== References ==","pandas(index=215, _1=215, text='the foran system is an integrated cad/cam/cae system developed by sener for the design and production of practically any naval ship and offshore unit. it is a multidisciplinary and integrated system that can be used in all the ship design and production phases and disciplines. the system collects all the information in a single database. foran is mainly focused on the design of:  merchants, ro-pax, ro-ro, bulk carriers, chemical tankers, container ships and cement and oil tankers. navy vessels (surface ships and submarines), in which the systems allows designers to carry out configuration control, analyze different design alternatives (prototypes), handle advanced hull forms and manage materials and special standards, as well as introducing customized criteria. specific vessels, tugs and workboats, hotel vessels, fishing vessels, fish transport vessels, oceanographic vessels, etc. for use in the offshore industry such as floating platforms (both anchored and fixed), staff transportation services, anchor vessels and vessels for applications such as supply, rescue, firefighting or anti-pollution.the latest version of the integrated cad/cam/cae system is foran v80; however, foran v70 is still used widely.   == foran v70 == common tools: this version supports unicode characters; this functionality enables entering text and generating information in languages using non latin characters such as chinese, russian or korean. foran dialogues and menu names can also be translated. moreover, this update includes fviewer, a virtual reality module that replace the former visual3d module. this application takes advantage of state-of-the-art graphic cards capabilities and allows the management of huge amount of information data. drafting: this update includes also a new 2d environment, based in the qcad application and compatible with autocad, developed to be used in the modules for the norms and structure standards definition (fnorm), and in the general arrangement module (fga) and for the definition of electrical and p&i diagrams. it also includes developments for the interim products drawings, symbolic drawings and the 3d model views drawings. project: one of the most relevant developments of the new version is the general arrangement module (fga) for spaces and general ship arrangement definition, both in 2d or 3d environments, with all data stored in the foran database. the application allows generating the general arrangement drawing in an efficient way.  on the other hand, the module for the probabilistic damage stability calculations (fsubd) offers now the possibility to consider intermediate stages of flooding, according to solas standard. the automatic assignment of spaces to subzones has also been improved. hull structure: a fnorm module for the definition of standards of structure is provided with a user interface, including multi dock windows and snap points; furthermore, it allows adding geometrical restrictions and includes the possibility of layer management. the increase of the lengths of the identifications and descriptions of blocks, materials and geometrical norms, as well as the hierarchical structure for the definition of the standards and geometrical norms are other capabilities of this module. moreover, following features must be highlighted:  hull structure modeling: an algorithm to represent corrugated parts more accurately, commands for checking the edge preparation of plates and profiles, options for the definition of face bars and an algorithm to represent more accurately curved shell and deck plates. regarding profiles and plates nesting, the nest module allows the nesting of identical parts assigned to different interim products and keeps information to recognize each individual part. outfitting: foran v70 includes piping design tools for pipeline routing. some important characteristics featured in the new update regard the polygonal lines, which are no more needed, the pipelines that are now routed dynamically displaying the pipeline as a solid model with significant snap points of the model. the version incorporates design functionalities adapted to the production circumstances in each shipyard, such as a command for smart splitting of pipe segments based on the standard pipe length defined in the components library, or checking utilities to control the spool fabrication restrictions before generating drawings and greater flexibility for the creation of sets of piping elements. electrical design: the electrical design module allows now to generate cable conduits for special non-standard cross-section cableways and to define conduits with cables inside cable trays, considering them in the cross-section filling calculations. in the cable routing the definition of cable splitting has also been improved and the management of cables partially routed has a better functionality in the connection between cables and terminal blocks. product lifecycle management (plm):  foran v70 allows the integration with different plm systems, thanks to a neutral solution built with standards based on corba and web services.   == foran fviewer vr == the use of a virtual reality (vr) environment offers significant advantages in shipbuilding. the most important advantage is the possibility to review the model and to find out errors at early design stages, with an important cost reduction. vr allows an intuitive and quick evaluation of the model, queries, measurement of distances, ergonomic studies, collision detection, design changes evaluation, simulation of mounting, dismantling and operation tasks, etc. the viewer, called foran fviewer vr, which is part of the foran system, includes stereoscopic capability as feature, which means that it allows the 3d navigation around the model of a ship, being possible to use it with tracking devices. the module can be used during a 3d navigation with a great user-model interaction. the solution can be used in any kind of vr room as well as in portable solutions, workstations, etc.   == head mounted display (hmd) == the development of applications in a virtual reality environment within the shipbuilding industry is booming nowadays. after the development of the second generation of the viewer for interactive navigation through the ship 3d model -foran fviewer vr- sener and ingevideo have developed a head mounted display (hmd).a virtual reality hmd is a device that lets the user to view and interact with 3d simulation environments, in this case applied to 3d models generated in foran. the possibility of working in virtual reality environments provides benefits and cost reductions, since it allows virtual screening in an interactive and intuitive way.   == foran system references == the foran system is used in more than 150 ship design offices and shipyards in 30 countries. some of its most significant references are:  strategic projection ship for navantia. cruise salvage vessel for the china ship development and design center (csddc). cvf, aircraft carrier for the royal navy (client: bae systems babcock marine). “bahía uno”, oil tanker for astilleros de murueta. “ruiloba”, container ship for 1,350 teu for the client hijos de j. barreras shipyard. frigate f-310 built at the navantia shipyards for the norwegian royal navy. semi-submersible platform gm 400 for global maritime.   == see also == comparison of computer-aided design editors list of 3d computer graphics software list of 3d rendering software list of 3d modeling software   == references ==')"
216,"Parbuckle salvage, or parbuckling, is the righting of a sunken vessel using rotational leverage.  A common operation with smaller watercraft, parbuckling is also employed to right large vessels. In 1943, the USS Oklahoma was rotated nearly 180 degrees to upright after being sunk in the attack on Pearl Harbor, and the Italian cruise ship Costa Concordia was successfully parbuckled off the west coast of Italy in September 2013, the largest salvage operation of that kind to date.


== Mechanical advantage and difficulties ==

While the mechanical advantage used by a laborer to parbuckle a cask up an incline is 2:1, parbuckling salvage is not so limited.  Each of the 21 winches used to roll the Oklahoma used cables that passed through two 17-part tackle assemblies (17:1 advantage).  Eight 28-inch (710 mm) diameter sheaves, eight 24-inch (610 mm) diameter sheaves, and one 20-inch (510 mm) diameter sheave comprised just half the mechanical effort.A major concern during salvage is preventing rotational torque from becoming a transverse force moving the ship sideways.  USS Utah, lost like the Oklahoma in the Pearl Harbor attack, was meant to be recovered by a similar rotation after the Oklahoma.  As the Utah was rotated, however, its hull did not catch on the harbor bottom, and the vessel slid towards Ford Island.  The Utah recovery effort was abandoned.


== Righting of Oklahoma ==
Oklahoma weighed about 35,000 short tons (32,000 metric tons). Twenty-one electric winches were installed on Ford Island, anchored in concrete foundations. They operated in unison. Each winch pulled about 20 short tons (18 metric tons) by a wire operated through a block system which gave an advantage of seventeen, for a total pull of 21×20×17, or 7,140 short tons (6,480 metric tons). In order to increase the leverage, the wire passed over a wooden strut arrangement (a bent) which stood on the bottom of the ship about 40 feet (12 meters) high. Oil had been removed from the ship through the bottom. The ship was lightened by air inside the hull. There was a large amount of weight in the ship which may have been removed prior to righting, but not all could be accessed. About one-third of the ammunition was taken off together with some of the machinery. The blades of the two propellers were also taken off, but more to avoid damage to them than to reduce weight. Tests were made to check whether restraining forces should be used to prevent sliding toward Ford Island. It was indicated that the soil under the aft part of the ship prevented sliding, whereas the bow section rested in soupy mud which permitted it. To prevent sliding about 2200 tons of coral soil were deposited near the bow section. During righting, excess soil under the starboard side was washed away by high-pressure jets operated by divers. The ship rolled as it should have and was right-side up by 16 June 1943, the work having started 8 March 1943. The mean draft of the ship after righting was c. 50 feet (15 meters).


== Righting of Costa Concordia ==

Following its capsizing and sinking in January 2012, the hull of Costa Concordia lay starboard side to the seaward face of a small outcropping very near the mouth of the harbor of Giglio, Italy, resting precariously on the incline to deeper water.  To right the vessel, four key pieces of apparatus were required:

a ""holdback"" system of chains attached to the island on one end and the hull on the other to ensure Costa Concordia rolled in place;
a man-made ledge inserted into the island face to provide a landing surface for the vessel;
a series of sponsons attached to the hull's port side so as, when flooded, to increase the torque on the hull and to unburden the strand jacks;
an arrangement of cables rising from the edge of the ledge over the sponsons on the port side of the hull.Tensioning the cables started the roll of the ship.  At about the halfway-to-vertical position the sponsons were filled with seawater, and Costa Concordia completed its roll to upright upon the ledge.  The hull was rotated 65 degrees to become vertical.Parbuckling was accomplished in three phases:

Freeing the hull
Phase of rotation using cables
Rotation by ballasting with sponsonsAt the completion of parbuckling, Costa Concordia rested on the ledge at a depth of 30 meters (98 feet).


=== Holdback system ===
The holdback system consisted of 56 chains in total, of which 22 chains were attached to the port side to go under the hull to the island. Each chain was 58 meters (190 ft) long and weighed about 26 metric tons (29 short tons). Each link weighed 205 kilograms (452 pounds).


=== Ledge ===
The ledge was part steel and part grout.  There were six steel platforms. The three larger platforms measured 35 by 40 meters (115 by 131 feet) each; the three smaller platforms measured 15 by 5 meters (49 by 16 feet) each. The 6 platforms were supported by 21 pillars of 1.6 meters (5.2 feet) diameter each and plunged for an average of 9 meters (30 feet) in the granite sea face of Giglio.  The grout filled the space between the land side of the platforms and the sea bed. It totaled 1,180 individual bags with a volume of over 12,000 cubic meters (16,000 cubic yards) and over 16,000 metric tons (18,000 short tons) in weight. The grout bags contained an ""ecofriendly cement,"" and were built with eyelets to aid post-recovery cleanup.


=== Sponsons ===
Eleven steel sponsons were installed on the port side of the hull: two long horizontal sponsons; two long vertical sponsons and seven short vertical sponsons. 

Each long horizontal sponson
measured 33 by 11.5 by 10.5 meters (108 by 38 by 34 ft),
weighed about 540 metric tons (600 short tons),
provided 3,600 cubic meters (4,700 cubic yards) of buoyancy.
Each long vertical sponson
measured 33 by 11.5 by 10.5 meters (108 by 38 by 34 ft),
weighed of about 523 metric tons (577 short tons),
provided about 3,600 cubic meters (4,700 cubic yards) of buoyancy.
Each short vertical sponson
measured 21.8 by 11.5 by 10.5 meters (72 by 38 by 34 ft),
weighed about 400 metric tons (440 short tons),
provided about 2,400 cubic meters (3,100 cubic yards) of buoyancy.Two steel ""blister"" tanks were connected together at the hull's bow. They measured 23 meters (75 feet) in length, 20 meters (66 feet) in height each, and had a total breadth of about 36 meters (118 feet). The whole blister structure (the two blister tanks, the tubular frame and the three anchor pipes) weighed about 1,700 metric tons (1,900 short tons). They provided a net buoyancy of 4,500 metric tons (5,000 short tons) to the bow section.


=== Cables ===
The cable system provided a force of about 23,800 metric tons (26,200 short tons) to start the Costa Concordia's rotation.


=== Phase 1 – freeing the hull ===
The hull of Costa Concordia rested on two spurs of rock, and was severely deformed from the weight of the ship pressing down on the spurs. This phase began when the strand jacks exerted force and the ship started to return to an upright position. This was ""without doubt one of the most delicate phases of the entire recovery plan.""


=== Phase 2 – rotation using cables ===
This phase began when the hull lifted from the seabed. Rotation continued by tensioning the cables operated by the strand jacks, and continued until the sponson water intakes reached sea level.


=== Phase 3 – rotation by ballasting with sponsons ===
The hull continued to rotate, pulled down by the weight of seawater added to the sponsons. The strand jacks and cables went slack. Redundant systems were designed as a guard against failure.  For example, two seawater inlet valves were provided to each sponson.


== List of parbuckle-salvaged vessels ==

USS Oklahoma
MV Rocknes
RPS Rajah Soliman
USS Oglala
RMS Empress of Canada
MS Herald of Free Enterprise
USS Reuben James
MV Janra
MV Repubblica di Genova
MSC Napoli's separated stern section
Barge Larvik Rock 
Fishing trawler Nieuwpoort 28
Fishing vessel Sandy Point
MS Costa Concordia
Jackup work barge Sep Orion


== References ==


== External links ==
FFPV Rocknes salvage
Pearl Harbor Raid, 7 December 1941 Salvage of USS Oklahoma, 1942–44
Salvage of the battleship USS Oklahoma following the attack on Pearl Harbor 1942–46
The Parbuckling Project: Concordia wreck removal project informative website
March 6, 1987 (the salvage of the Herald of Free Enterprise)
German website with a time-lapse video of the parbuckling of the Costa Concordia.","pandas(index=216, _1=216, text='parbuckle salvage, or parbuckling, is the righting of a sunken vessel using rotational leverage.  a common operation with smaller watercraft, parbuckling is also employed to right large vessels. in 1943, the uss oklahoma was rotated nearly 180 degrees to upright after being sunk in the attack on pearl harbor, and the italian cruise ship costa concordia was successfully parbuckled off the west coast of italy in september 2013, the largest salvage operation of that kind to date.   == mechanical advantage and difficulties ==  while the mechanical advantage used by a laborer to parbuckle a cask up an incline is 2:1, parbuckling salvage is not so limited.  each of the 21 winches used to roll the oklahoma used cables that passed through two 17-part tackle assemblies (17:1 advantage).  eight 28-inch (710 mm) diameter sheaves, eight 24-inch (610 mm) diameter sheaves, and one 20-inch (510 mm) diameter sheave comprised just half the mechanical effort.a major concern during salvage is preventing rotational torque from becoming a transverse force moving the ship sideways.  uss utah, lost like the oklahoma in the pearl harbor attack, was meant to be recovered by a similar rotation after the oklahoma.  as the utah was rotated, however, its hull did not catch on the harbor bottom, and the vessel slid towards ford island.  the utah recovery effort was abandoned.   == righting of oklahoma == oklahoma weighed about 35,000 short tons (32,000 metric tons). twenty-one electric winches were installed on ford island, anchored in concrete foundations. they operated in unison. each winch pulled about 20 short tons (18 metric tons) by a wire operated through a block system which gave an advantage of seventeen, for a total pull of 21×20×17, or 7,140 short tons (6,480 metric tons). in order to increase the leverage, the wire passed over a wooden strut arrangement (a bent) which stood on the bottom of the ship about 40 feet (12 meters) high. oil had been removed from the ship through the bottom. the ship was lightened by air inside the hull. there was a large amount of weight in the ship which may have been removed prior to righting, but not all could be accessed. about one-third of the ammunition was taken off together with some of the machinery. the blades of the two propellers were also taken off, but more to avoid damage to them than to reduce weight. tests were made to check whether restraining forces should be used to prevent sliding toward ford island. it was indicated that the soil under the aft part of the ship prevented sliding, whereas the bow section rested in soupy mud which permitted it. to prevent sliding about 2200 tons of coral soil were deposited near the bow section. during righting, excess soil under the starboard side was washed away by high-pressure jets operated by divers. the ship rolled as it should have and was right-side up by 16 june 1943, the work having started 8 march 1943. the mean draft of the ship after righting was c. 50 feet (15 meters).   == righting of costa concordia ==  following its capsizing and sinking in january 2012, the hull of costa concordia lay starboard side to the seaward face of a small outcropping very near the mouth of the harbor of giglio, italy, resting precariously on the incline to deeper water.  to right the vessel, four key pieces of apparatus were required:  a ""holdback"" system of chains attached to the island on one end and the hull on the other to ensure costa concordia rolled in place; a man-made ledge inserted into the island face to provide a landing surface for the vessel; a series of sponsons attached to the hull\'s port side so as, when flooded, to increase the torque on the hull and to unburden the strand jacks; an arrangement of cables rising from the edge of the ledge over the sponsons on the port side of the hull.tensioning the cables started the roll of the ship.  at about the halfway-to-vertical position the sponsons were filled with seawater, and costa concordia completed its roll to upright upon the ledge.  the hull was rotated 65 degrees to become vertical.parbuckling was accomplished in three phases:  freeing the hull phase of rotation using cables rotation by ballasting with sponsonsat the completion of parbuckling, costa concordia rested on the ledge at a depth of 30 meters (98 feet). the hull continued to rotate, pulled down by the weight of seawater added to the sponsons. the strand jacks and cables went slack. redundant systems were designed as a guard against failure.  for example, two seawater inlet valves were provided to each sponson.   == list of parbuckle-salvaged vessels ==  uss oklahoma mv rocknes rps rajah soliman uss oglala rms empress of canada ms herald of free enterprise uss reuben james mv janra mv repubblica di genova msc napoli\'s separated stern section barge larvik rock fishing trawler nieuwpoort 28 fishing vessel sandy point ms costa concordia jackup work barge sep orion   == references ==   == external links == ffpv rocknes salvage pearl harbor raid, 7 december 1941 salvage of uss oklahoma, 1942–44 salvage of the battleship uss oklahoma following the attack on pearl harbor 1942–46 the parbuckling project: concordia wreck removal project informative website march 6, 1987 (the salvage of the herald of free enterprise) german website with a time-lapse video of the parbuckling of the costa concordia.')"
217,"A Butterworth cover (also Butterworth hatch and Butterworth plate) is a hatch on the deck of a cargo vessel that is used to seal a small opening that admits to the space below.In oil tankers and other marine vessels used for transporting fluid products, there are small service openings though the deck into each tank to allow access for miscellaneous reasons such as sampling, inspection, gauging and cleaning. When the service opening is not in use, it is sealed by a removable cover plate, commonly referred to as a Butterworth cover. Butterworth hatches are not the main access hatches, but are the servicing hatches, and are generally closed with a metal cover plate with a gasket that is fastened to the deck by a number of bolts which stick up from the deck. Holes on the edges of the plate fit over these bolts and the cover is fastened down with nuts or dogs.
Vapor accidents can occur when pressure builds up in the tank below and the Butterworth cover is carelessly removed. Butterworth hatches have been responsible for a number of marine losses. In the United States, regulations prescribe requirements for Butterworth hatches and covers.


== References ==","pandas(index=217, _1=217, text='a butterworth cover (also butterworth hatch and butterworth plate) is a hatch on the deck of a cargo vessel that is used to seal a small opening that admits to the space below.in oil tankers and other marine vessels used for transporting fluid products, there are small service openings though the deck into each tank to allow access for miscellaneous reasons such as sampling, inspection, gauging and cleaning. when the service opening is not in use, it is sealed by a removable cover plate, commonly referred to as a butterworth cover. butterworth hatches are not the main access hatches, but are the servicing hatches, and are generally closed with a metal cover plate with a gasket that is fastened to the deck by a number of bolts which stick up from the deck. holes on the edges of the plate fit over these bolts and the cover is fastened down with nuts or dogs. vapor accidents can occur when pressure builds up in the tank below and the butterworth cover is carelessly removed. butterworth hatches have been responsible for a number of marine losses. in the united states, regulations prescribe requirements for butterworth hatches and covers.   == references ==')"
218,"The National Science Foundation's (NSF) Ocean Observatories Initiative (OOI) Regional Scale Nodes (RSN) component is an electro-optically cabled underwater observatory that directly connects to the global Internet.  It is the largest cable-linked seabed observatory in the world, and also the first of its kind in the United States.
Located on the southern part of the Juan de Fuca plate, off the coast of Washington and Oregon, it is the first ocean observatory to span a tectonic plate.
RSN utilizes several high-power, high-bandwidth sub-sea terminals called primary nodes which are linked together by fiber-optic cable and provide support to oceanographic sensors at key locations.
Upon completion of the network in 2014, RSN will cover a distance of over 900 kilometers at depths of up to 3000 meters.  Implementation of the OOI Regional Scale Nodes is led by the University of Washington's (UW) School of Oceanography, the UW Applied Physics Laboratory, and L-3 MariPro.
Live RSN data from >100 seafloor and water column instruments will be made available live on the Internet.  This will allow both scientists and the general public to study long-term changes in ocean systems over the next 25 years.
Construction of RSN will be completed in 2014.  Efforts are substantially aided by the crews of ROPOS (Remotely Operated Platform for Observation Sciences.  The 83-day VISIONS ’14 expedition aboard the 274-foot global-class R/V Thomas G. Thompson is responsible for the observatory's final implementation.


== Overview ==
The Regional Scale Nodes (RSN) is a component of the National Science Foundation's (NSF's) Ocean Observatories Initiative (OOI). The NSF's OOI is managed and coordinated by the OOI Project Office at the Consortium for Ocean Leadership (COL) in Washington, D.C.  The UW, located in Seattle, Washington, is the RSN Implementing Organization for the COL.
The vision behind RSN is to launch a new era of scientific discovery and understanding of the oceans.
The RSN consists of two infrastructures: primary and secondary.  The primary infrastructure network, which was designed, qualified, manufactured, and installed in 2012 by L-3 Maripro, consists of a shore facility located in Pacific City, Oregon; two fiber-optic cable lines covering a distance of 800 kilometers, and seven primary science nodes.
The RSN system delivers 200 kilowatts of power and 240Gbit/s of TCP/IP Internet data communications to the seven primary science nodes.  RSN is designed to last for 25 years and is capable of significant expansion to serve future science needs.


== History ==
Prior to the emergence of underwater cabled observatories, oceanographers and other researchers studying the global ocean tended to rely on the use of research vessels and manned submersibles in order to collect data.  This was followed by a shift toward Remote Operated Vehicles (ROV's) and space-based research satellites.  The limitation to these methods was that they were either not cost-effective, or data could only be collected for short durations.  While the importance of expedition-based exploration was recognized, a solution was needed.
In 1987, the concept of utilizing high-power, high-bandwidth underwater cabled observatories emerged as a long-term, cost-effective solution for conducting real-time monitoring of ocean systems.
In the early 1990s, the United States and Canada formed an agreement to develop a plate-scale submarine electro-optically cabled ocean observatory in the northeast Pacific Ocean.  This region is home to the smallest of Earth's tectonic plates – the Juan de Fuca plate.  The small size and close coastal proximity of the Juan de Fuca plate presents a unique opportunity to observe the dynamic systems in submarine volcano regions.
The partnership between the U.S. and Canada developed into a plan to build a Canadian cabled array that would cover the upper 1/3 of the Juan de Fuca plate, and a U.S. system spanning the lower 2/3 of the plate (cite).  Together, this plate-scale observatory would be called NEPTUNE (Northeast Pacific Time Series Underwater Networked Experiments) and would provide continuous observations for 25 years.
By the mid-2000s, NEPTUNE Canada had received full funding and their cabled array was completed and online by 2009.  It was brought under the umbrella network of Ocean Networks Canada (ONC).  Meanwhile, NEPTUNE U.S. was renamed to Regional Scale Nodes and became a component of the OOI.  It is slated for completion in 2014.  Both NEPTUNE Canada and RSN will be integrated through the ONC's digital infrastructure and the OOI Cyberinfrastructure providing real-time access to anyone connected to the Internet.


== Scientific Motivation ==
The scientific goals of RSN are significant.  A vast array of natural phenomena that occur throughout the world's oceans and seafloor are found in the Northeast Pacific Ocean. As a whole, the mission of RSN is to provide a human telepresence in the ocean that will serve researchers, students, educators, policymakers, and the public.  Scientists will be able to conduct local investigations of such global processes as major ocean currents, active earthquake zones, creation of new seafloor, and rich environments of marine plants and animals.

RSN is also designed to help anticipate both short and long-term ocean-generated threats and opportunities.  Notably, RSN will be able to monitor the tectonic activity along the plate boundary.  There is hope that seismic sensors could be installed at key areas along the spreading center which would serve as an early warning system for earthquakes and tsunamis.
The existence of a long-term cabled observatory will allow for long-term measurements of biological communities.  In particular, the Juan de Fuca plate's divergent plate boundary has resulted in the existence of seafloor hydrothermal vents ecosystems, and other similar groups.  These deep sea communities, thriving in extremely harsh environments, pose a number of unsolved scientific questions which RSN will be capable of investigating.


== Infrastructure ==

Primary Infrastructure
The primary infrastructure of RSN consists of seven primary nodes which were installed in 2012 by L-3 Maripro.  They are terminal points which help distribute power and bandwidth to the networks of deployed sensors.
Approximately 900 kilometers of cable (referred to as backbone cable) have been used to connect the primary nodes together.  These cables make landfall at the shore station in Pacific, City, Oregon.
In 2005, over 175 scientists across the United States responded to a Request for Assistance from the National Science Foundation to develop a cabled observatory on the Juan de Fuca Plate.  Nodes are located at pre-selected experimental sites throughout the Juan de Fuca plate.  Axial Seamount, Hydrate Ridge on the Cascadia Margin and shallow water sites west of Newport, Oregon (the Endurance Array) all have primary nodes installed.  The primary nodes are all located in environmentally benign areas.
Nodes also convert the 10kVdc voltage levels from the backbone cable to 375Vdc which is then directed to the secondary infrastructure. The 375V switching systems and Node telemetry systems were designed and manufactured by Texcel Technology Plc based in England. The software to manage the ports and telemetry protection systems was also supplied by Texcel as an element manager sitting under a Network Management System (NMS).
The primary nodes have a number of extra ports which offer the potential for large-scale future expansion (>100 kilometers).
Secondary Infrastructure
The converted 375Vdc voltage from the primary nodes is then directed at low-and medium-power nodes and junction boxes.  The nodes and junction boxes (similar to power strips) offer direct power and communications to the instruments at the experimental sites.  In concert, these parts make up the RSN secondary infrastructure. 
Extension cables are used to link the primary nodes to the secondary infrastructure, providing power and communications.
Equipment is linked using wet-mate connectors.  Different types of cable were installed depending on load requirements.  Bandwidth from these cables ranges from 10 Gbit/s to 1 Gbit/s.
During the VISIONS ’13 expedition to continue construction of RSN, over 22,000 meters of extension cables were installed on the ocean floor.  The cables all successfully went online.
Upon completion in 2014, over 100 cabled seafloor and water column instruments will be operational.  These instruments will allow monitoring of biological, chemical, geological, and geophysical processes in the ocean.  The secondary infrastructure will also include six mooring systems for water-column profilers.  
Cables are frequently deployed all across the world in ocean basins and margins.  They have considerably long lifetimes.  The backbone cable was installed in the summer of 2011.  The commercial cable-laying ship, TE SubCom Dependable, carried out this phase of the project.
Special environmental requirements were also taken into account.  Certain cables are substantially well-armored, especially those deployed in volcanic areas, such as Axial Seamount.


== Instruments ==

In order to fully understand complex ocean systems, a wide variety of sensor arrays, capable of surviving for long periods of time in harsh conditions, are necessary.  A suite of sensors (over 100) were selected and strategically placed throughout RSN.  They are located at Axial Seamount, Hydrate Ridge, and also on the water-column moorings.
Instruments connected to the RSN include:  

Conductivity Temperature Depth (located on profilers)
Dissolved Oxygen,
3-D Single Point Current Meter
Temperature
Fluorometers
CDOM,
Chlorophyll-a,
Optical BackscatterThe instruments are the final spot of each regional network branch.


=== Cyberinfrastructure ===

The Regional Scale Nodes is connected into the OOI Cyberinfrastructure.
The Cyberinfrastructure component of the OOI links marine infrastructure to scientists and users.  The OOI Cyberinfrastructure manages and integrates data from all the different OOI sensors.  It will provide a common operating infrastructure, the Integrated Observatory Network (ION), connecting and coordinating the operations of the marine components (global, regional, and coastal scale arrays). It will also provide resource management, observatory mission command and control, product production, data management and distribution (including strong data provenance), and centrally available collaboration tools.
The Integrated Observatory Network (ION) connects and coordinates the operations of the OOI marine components with the scientific and educational pursuits of oceanographic research communities.  The cyberinfrastructure is being designed and constructed by the University of California, San Diego.


== Status ==
Construction of RSN is ongoing.  As of September 19, 2014, the primary infrastructure and most of the secondary infrastructure was successfully in place, and OOI RSN and UW APL crews were working to complete the vertical moorings for the shallow profiler.


== Outreach ==
The University of Washington has welcomed student participation in the implementation of RSN.  As of 2014, there have been eight expeditions in which students have had the opportunity to work aboard the R/V Thomas G. Thompson and witness the construction of the cabled observatory.  During these cruises, students develop projects utilizing the array of technology and scientific equipment on board.
Students who participate in these expeditions go on to share their experiences with others.
In 2014, over 30 graduate and undergraduate students worked alongside the researchers, engineers, educators, and crew during the 83-day VISIONS ’14 expedition.


== References ==

Carr, Geoffrey (15 November 2007). ""Visiting Neptune's kingdom"". The Economist. Retrieved 17 September 2014.
Delaney, John; Alan Chave (January 2000). ""NEPTUNE: A Fiber-Optic 'Telescope' to Inner Space"". Oceanus. Retrieved 17 September 2014.
Thomson, Ashley (7 March 2014). ""Canadian Scientific Submersible Facility and University of Washington sign 2014 agreement for installation of US Regional Scale Nodes cabled observatory"". Canadian Scientific Submersible Facility. Retrieved 18 September 2014.
Delaney, John; Deborah Kelley; S. Kim Juniper (20 March 2014). ""Establishing a new era of submarine volcanic observatories:Cabling Axial Seamount and the Endeavour Segment of the Juan de Fuca Ridge"". Marine Geology. 352: 426–450. Bibcode:2014MGeol.352..426K. doi:10.1016/j.margeo.2014.03.010.
Capotosto, David (16 June 2014). ""DeepWater Buoyancy Chosen for Ocean Observatories Initiative"" (PDF). DeepWater Buoyancy. Retrieved 18 September 2014.
Hickey, Hannah (18 September 2013). ""Cables, instruments installed in the deep sea off Pacific Northwest coast"". UW Today. Retrieved 18 September 2014.
Soper, Taylor (18 September 2013). ""Underwater lab: UW researchers install 14 miles of cables, cameras, sensors on ocean floor"". GeekWire. Retrieved 18 September 2014.
Hickey, Hannah (1 July 2013). ""Work this summer extends reach of cabled deep-ocean observatory"". UW Daily. Retrieved 18 September 2014.
Ahearn, Ashley (17 April 2013). ""Getting Ready For World's Largest Underwater Observatory"". KUOW.org. Retrieved 18 September 2014.
Theodoric, Meyer (7 September 2012). ""UW leads ambitious effort to monitor the ocean"". The Seattle Times. Retrieved 18 September 2014.
Brown, Molly (31 July 2012). ""UW team deploying power and Internet to the sea floor"". GeekWire. Retrieved 18 September 2014.
Yardley, William (4 September 2007). ""'Bringing the Ocean to the World,' in High-Def"". The New York Times. Retrieved 18 September 2014.
Witze, Alexandra (25 September 2013). ""Marine science: Oceanography's billion-dollar baby"". Nature. Retrieved 18 September 2014.
""Zenoss Selected for Cutting-Edge 25-Year Research Project in the Pacific Ocean"". MarketWired. 6 April 2011. Retrieved 18 September 2014.
Barone, Jennifer (25 June 2012). ""Once We Wire It Up, What Will the Ocean Tell Us?"". Discover Magazine. Retrieved 18 September 2014.
Boa, Susan (17 May 2007). ""Ocean observing contracts awarded to UC San Diego and University of Washington"". Joint Oceanographic Institutions. Retrieved 18 September 2014.
Boa, Susan (17 May 2007). ""Ocean observing contracts awarded to UC San Diego and University of Washington"". Joint Oceanographic Institutions. Retrieved 18 September 2014.
Truitt, Jack (30 April 2013). ""UW helps lead the way for the next generation of oceanography"". University of Washington - The Daily. Retrieved 18 September 2014.
""Oceans lined with research cable"". BBC News. 2 October 2007. Retrieved 18 September 2014.
""A Sea Change for U.S. Oceanography"". Middle East Technical University Institute of Marine Sciences. 3 October 2013. Retrieved 18 September 2014.
Hoffmann, Linn J.; Eike Breitbarth; Phillip W. Boyd; Keith A. Hunter (6 December 2012). ""Influence of ocean warming and acidification on trace metal biogeochemistry Seamount and the Endeavour Segment of the Juan de Fuca Ridge"". Marine Ecology Progress Series. 470: 191–205. doi:10.3354/meps10082.
Sabine, Christopher; Feely, Richard A.; Gruber, Nicolas; Key, Robert M.; Lee, Kitack; Bullister, John L.; Wanninkhof, Rik; Wong, C. S.; Wallace, Douglas W. R.; Tilbrook, Bronte; Millero, Frank J.; Peng, Tsung-Hung; Kozyr, Alexander; Ono, Tsueno; Rios, Aida F. (2004). ""The Oceanic Sink for Anthropogenic CO2"". Science. 305 (5682): 367–71. Bibcode:2004Sci...305..367S. doi:10.1126/science.1097403. hdl:10261/52596. PMID 15256665.


== External links ==
VISIONS '14: Completing Construction of the Regional Scale Nodes
UW Applied Physics Laboratory RSN Page
Ocean Observatories Initiative Home Page
Oregon Station University - OOI Endurance Array
University of Washington - OOI Regional Component 
University of California, San Diego - OOI CyberInfrastructure 
Woods Hole Oceanographic Institution - OOI Coastal Global Component 
University of Washington School of Oceanography 
University of Washington Applied Physics Laboratory 
L-3 MariPro Home Page
Consortium for Ocean Leadership Home Page
R/V Thomas G. Thompson Home Page
CSSF-ROPOS Home Page
Texcel Technology Plc SubSea Page","pandas(index=218, _1=218, text='the national science foundation\'s (nsf) ocean observatories initiative (ooi) regional scale nodes (rsn) component is an electro-optically cabled underwater observatory that directly connects to the global internet.  it is the largest cable-linked seabed observatory in the world, and also the first of its kind in the united states. located on the southern part of the juan de fuca plate, off the coast of washington and oregon, it is the first ocean observatory to span a tectonic plate. rsn utilizes several high-power, high-bandwidth sub-sea terminals called primary nodes which are linked together by fiber-optic cable and provide support to oceanographic sensors at key locations. upon completion of the network in 2014, rsn will cover a distance of over 900 kilometers at depths of up to 3000 meters.  implementation of the ooi regional scale nodes is led by the university of washington\'s (uw) school of oceanography, the uw applied physics laboratory, and l-3 maripro. live rsn data from >100 seafloor and water column instruments will be made available live on the internet.  this will allow both scientists and the general public to study long-term changes in ocean systems over the next 25 years. construction of rsn will be completed in 2014.  efforts are substantially aided by the crews of ropos (remotely operated platform for observation sciences.  the 83-day visions ’14 expedition aboard the 274-foot global-class r/v thomas g. thompson is responsible for the observatory\'s final implementation.   == overview == the regional scale nodes (rsn) is a component of the national science foundation\'s (nsf\'s) ocean observatories initiative (ooi). the nsf\'s ooi is managed and coordinated by the ooi project office at the consortium for ocean leadership (col) in washington, d.c.  the uw, located in seattle, washington, is the rsn implementing organization for the col. the vision behind rsn is to launch a new era of scientific discovery and understanding of the oceans. the rsn consists of two infrastructures: primary and secondary.  the primary infrastructure network, which was designed, qualified, manufactured, and installed in 2012 by l-3 maripro, consists of a shore facility located in pacific city, oregon; two fiber-optic cable lines covering a distance of 800 kilometers, and seven primary science nodes. the rsn system delivers 200 kilowatts of power and 240gbit/s of tcp/ip internet data communications to the seven primary science nodes.  rsn is designed to last for 25 years and is capable of significant expansion to serve future science needs.   == history == prior to the emergence of underwater cabled observatories, oceanographers and other researchers studying the global ocean tended to rely on the use of research vessels and manned submersibles in order to collect data.  this was followed by a shift toward remote operated vehicles (rov\'s) and space-based research satellites.  the limitation to these methods was that they were either not cost-effective, or data could only be collected for short durations.  while the importance of expedition-based exploration was recognized, a solution was needed. in 1987, the concept of utilizing high-power, high-bandwidth underwater cabled observatories emerged as a long-term, cost-effective solution for conducting real-time monitoring of ocean systems. in the early 1990s, the united states and canada formed an agreement to develop a plate-scale submarine electro-optically cabled ocean observatory in the northeast pacific ocean.  this region is home to the smallest of earth\'s tectonic plates – the juan de fuca plate.  the small size and close coastal proximity of the juan de fuca plate presents a unique opportunity to observe the dynamic systems in submarine volcano regions. the partnership between the u.s. and canada developed into a plan to build a canadian cabled array that would cover the upper 1/3 of the juan de fuca plate, and a u.s. system spanning the lower 2/3 of the plate (cite).  together, this plate-scale observatory would be called neptune (northeast pacific time series underwater networked experiments) and would provide continuous observations for 25 years. by the mid-2000s, neptune canada had received full funding and their cabled array was completed and online by 2009.  it was brought under the umbrella network of ocean networks canada (onc).  meanwhile, neptune u.s. was renamed to regional scale nodes and became a component of the ooi.  it is slated for completion in 2014.  both neptune canada and rsn will be integrated through the onc\'s digital infrastructure and the ooi cyberinfrastructure providing real-time access to anyone connected to the internet.   == scientific motivation == the scientific goals of rsn are significant.  a vast array of natural phenomena that occur throughout the world\'s oceans and seafloor are found in the northeast pacific ocean. as a whole, the mission of rsn is to provide a human telepresence in the ocean that will serve researchers, students, educators, policymakers, and the public.  scientists will be able to conduct local investigations of such global processes as major ocean currents, active earthquake zones, creation of new seafloor, and rich environments of marine plants and animals.  rsn is also designed to help anticipate both short and long-term ocean-generated threats and opportunities.  notably, rsn will be able to monitor the tectonic activity along the plate boundary.  there is hope that seismic sensors could be installed at key areas along the spreading center which would serve as an early warning system for earthquakes and tsunamis. the existence of a long-term cabled observatory will allow for long-term measurements of biological communities.  in particular, the juan de fuca plate\'s divergent plate boundary has resulted in the existence of seafloor hydrothermal vents ecosystems, and other similar groups.  these deep sea communities, thriving in extremely harsh environments, pose a number of unsolved scientific questions which rsn will be capable of investigating.   == infrastructure ==  primary infrastructure the primary infrastructure of rsn consists of seven primary nodes which were installed in 2012 by l-3 maripro.  they are terminal points which help distribute power and bandwidth to the networks of deployed sensors. approximately 900 kilometers of cable (referred to as backbone cable) have been used to connect the primary nodes together.  these cables make landfall at the shore station in pacific, city, oregon. in 2005, over 175 scientists across the united states responded to a request for assistance from the national science foundation to develop a cabled observatory on the juan de fuca plate.  nodes are located at pre-selected experimental sites throughout the juan de fuca plate.  axial seamount, hydrate ridge on the cascadia margin and shallow water sites west of newport, oregon (the endurance array) all have primary nodes installed.  the primary nodes are all located in environmentally benign areas. nodes also convert the 10kvdc voltage levels from the backbone cable to 375vdc which is then directed to the secondary infrastructure. the 375v switching systems and node telemetry systems were designed and manufactured by texcel technology plc based in england. the software to manage the ports and telemetry protection systems was also supplied by texcel as an element manager sitting under a network management system (nms). the primary nodes have a number of extra ports which offer the potential for large-scale future expansion (>100 kilometers). secondary infrastructure the converted 375vdc voltage from the primary nodes is then directed at low-and medium-power nodes and junction boxes.  the nodes and junction boxes (similar to power strips) offer direct power and communications to the instruments at the experimental sites.  in concert, these parts make up the rsn secondary infrastructure. extension cables are used to link the primary nodes to the secondary infrastructure, providing power and communications. equipment is linked using wet-mate connectors.  different types of cable were installed depending on load requirements.  bandwidth from these cables ranges from 10 gbit/s to 1 gbit/s. during the visions ’13 expedition to continue construction of rsn, over 22,000 meters of extension cables were installed on the ocean floor.  the cables all successfully went online. upon completion in 2014, over 100 cabled seafloor and water column instruments will be operational.  these instruments will allow monitoring of biological, chemical, geological, and geophysical processes in the ocean.  the secondary infrastructure will also include six mooring systems for water-column profilers. cables are frequently deployed all across the world in ocean basins and margins.  they have considerably long lifetimes.  the backbone cable was installed in the summer of 2011.  the commercial cable-laying ship, te subcom dependable, carried out this phase of the project. special environmental requirements were also taken into account.  certain cables are substantially well-armored, especially those deployed in volcanic areas, such as axial seamount.   == instruments ==  in order to fully understand complex ocean systems, a wide variety of sensor arrays, capable of surviving for long periods of time in harsh conditions, are necessary.  a suite of sensors (over 100) were selected and strategically placed throughout rsn.  they are located at axial seamount, hydrate ridge, and also on the water-column moorings. instruments connected to the rsn include:  conductivity temperature depth (located on profilers) dissolved oxygen, 3-d single point current meter temperature fluorometers cdom, chlorophyll-a, optical backscatterthe instruments are the final spot of each regional network branch. the regional scale nodes is connected into the ooi cyberinfrastructure. the cyberinfrastructure component of the ooi links marine infrastructure to scientists and users.  the ooi cyberinfrastructure manages and integrates data from all the different ooi sensors.  it will provide a common operating infrastructure, the integrated observatory network (ion), connecting and coordinating the operations of the marine components (global, regional, and coastal scale arrays). it will also provide resource management, observatory mission command and control, product production, data management and distribution (including strong data provenance), and centrally available collaboration tools. the integrated observatory network (ion) connects and coordinates the operations of the ooi marine components with the scientific and educational pursuits of oceanographic research communities.  the cyberinfrastructure is being designed and constructed by the university of california, san diego.   == status == construction of rsn is ongoing.  as of september 19, 2014, the primary infrastructure and most of the secondary infrastructure was successfully in place, and ooi rsn and uw apl crews were working to complete the vertical moorings for the shallow profiler.   == outreach == the university of washington has welcomed student participation in the implementation of rsn.  as of 2014, there have been eight expeditions in which students have had the opportunity to work aboard the r/v thomas g. thompson and witness the construction of the cabled observatory.  during these cruises, students develop projects utilizing the array of technology and scientific equipment on board. students who participate in these expeditions go on to share their experiences with others. in 2014, over 30 graduate and undergraduate students worked alongside the researchers, engineers, educators, and crew during the 83-day visions ’14 expedition.   == references ==  carr, geoffrey (15 november 2007). ""visiting neptune\'s kingdom"". the economist. retrieved 17 september 2014. delaney, john; alan chave (january 2000). ""neptune: a fiber-optic \'telescope\' to inner space"". oceanus. retrieved 17 september 2014. thomson, ashley (7 march 2014). ""canadian scientific submersible facility and university of washington sign 2014 agreement for installation of us regional scale nodes cabled observatory"". canadian scientific submersible facility. retrieved 18 september 2014. delaney, john; deborah kelley; s. kim juniper (20 march 2014). ""establishing a new era of submarine volcanic observatories:cabling axial seamount and the endeavour segment of the juan de fuca ridge"". marine geology. 352: 426–450. bibcode:2014mgeol.352..426k. doi:10.1016/j.margeo.2014.03.010. capotosto, david (16 june 2014). ""deepwater buoyancy chosen for ocean observatories initiative"" (pdf). deepwater buoyancy. retrieved 18 september 2014. hickey, hannah (18 september 2013). ""cables, instruments installed in the deep sea off pacific northwest coast"". uw today. retrieved 18 september 2014. soper, taylor (18 september 2013). ""underwater lab: uw researchers install 14 miles of cables, cameras, sensors on ocean floor"". geekwire. retrieved 18 september 2014. hickey, hannah (1 july 2013). ""work this summer extends reach of cabled deep-ocean observatory"". uw daily. retrieved 18 september 2014. ahearn, ashley (17 april 2013). ""getting ready for world\'s largest underwater observatory"". kuow.org. retrieved 18 september 2014. theodoric, meyer (7 september 2012). ""uw leads ambitious effort to monitor the ocean"". the seattle times. retrieved 18 september 2014. brown, molly (31 july 2012). ""uw team deploying power and internet to the sea floor"". geekwire. retrieved 18 september 2014. yardley, william (4 september 2007). ""\'bringing the ocean to the world,\' in high-def"". the new york times. retrieved 18 september 2014. witze, alexandra (25 september 2013). ""marine science: oceanography\'s billion-dollar baby"". nature. retrieved 18 september 2014. ""zenoss selected for cutting-edge 25-year research project in the pacific ocean"". marketwired. 6 april 2011. retrieved 18 september 2014. barone, jennifer (25 june 2012). ""once we wire it up, what will the ocean tell us?"". discover magazine. retrieved 18 september 2014. boa, susan (17 may 2007). ""ocean observing contracts awarded to uc san diego and university of washington"". joint oceanographic institutions. retrieved 18 september 2014. boa, susan (17 may 2007). ""ocean observing contracts awarded to uc san diego and university of washington"". joint oceanographic institutions. retrieved 18 september 2014. truitt, jack (30 april 2013). ""uw helps lead the way for the next generation of oceanography"". university of washington - the daily. retrieved 18 september 2014. ""oceans lined with research cable"". bbc news. 2 october 2007. retrieved 18 september 2014. ""a sea change for u.s. oceanography"". middle east technical university institute of marine sciences. 3 october 2013. retrieved 18 september 2014. hoffmann, linn j.; eike breitbarth; phillip w. boyd; keith a. hunter (6 december 2012). ""influence of ocean warming and acidification on trace metal biogeochemistry seamount and the endeavour segment of the juan de fuca ridge"". marine ecology progress series. 470: 191–205. doi:10.3354/meps10082. sabine, christopher; feely, richard a.; gruber, nicolas; key, robert m.; lee, kitack; bullister, john l.; wanninkhof, rik; wong, c. s.; wallace, douglas w. r.; tilbrook, bronte; millero, frank j.; peng, tsung-hung; kozyr, alexander; ono, tsueno; rios, aida f. (2004). ""the oceanic sink for anthropogenic co2"". science. 305 (5682): 367–71. bibcode:2004sci...305..367s. doi:10.1126/science.1097403. hdl:10261/52596. pmid 15256665.   == external links == visions \'14: completing construction of the regional scale nodes uw applied physics laboratory rsn page ocean observatories initiative home page oregon station university - ooi endurance array university of washington - ooi regional component university of california, san diego - ooi cyberinfrastructure woods hole oceanographic institution - ooi coastal global component university of washington school of oceanography university of washington applied physics laboratory l-3 maripro home page consortium for ocean leadership home page r/v thomas g. thompson home page cssf-ropos home page texcel technology plc subsea page')"
219,"A Marine pump is a pump which is used on board a vessel (ship) or an offshore platform.


== Upper category ==
It is a kind of general equipment, usually driven by an electrical motor, refer to pump category.
It is widely used as a machine in marine industry, refer to marine industry category.


== General ==
A pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action. A marine pump is an important auxiliary equipment in marine industry and ship building industry. It is widely used in all kinds of marine vessels, such as barges, tug boats, containers, carriers, ships, vessels, fixed offshore structure, drilling jack-up rigs and so on.
These marine pumps can be serviced for cooling, circulating, ballast, general service(G/S), fire-fighting, boiler feed water, condensate water, fresh(drinking) water, sanitary water, bilge & sludge, F.O. transfer, L.O. transfer, F.O. and F.S. cargo pumping, cargo stripping, hydrophone tank unit, sewage treatment unit, oil water separator, incinerator, fresh water generator, and so on.


== Type ==
Marine pump is named by its usage or application, it covers a lot of types of pumps, such as:
The marine centrifugal pump is used to transport fluids by the conversion of rotational kinetic energy to the hydrodynamic energy of the fluid flow. The rotational energy typically comes from an engine or electric motor. The fluid enters the pump impeller along or near to the rotating axis and is accelerated by the impeller, flowing radially outward into a diffuser or volute chamber (casing), from where it exits.
The gear pump is a marine gear pump that uses the meshing of gears to pump fluid by displacement. It is one of the most common types of pumps for hydraulic fluid power applications. Gear pumps are also widely used in chemical installations to pump high viscosity fluids. There are two main variations; external gear pumps which use two external spur gears, and internal gear pumps which use an external and an internal spur gears (internal spur gear teeth face inwards, see below). Gear pumps are positive displacement (or fixed displacement), meaning they pump a constant amount of fluid for each revolution. Some gear pumps are designed to function as either a motor or a pump.
The screw pump is a positive-displacement (PD) pump that uses one or several screws to move fluids or solids along the screw(s) axis. In its simplest form (the Archimedes' screw pump), a single screw rotates in a cylindrical cavity, thereby moving the material along the screw's spindle.


== See also ==
Handy billy


== References ==","pandas(index=219, _1=219, text=""a marine pump is a pump which is used on board a vessel (ship) or an offshore platform.   == upper category == it is a kind of general equipment, usually driven by an electrical motor, refer to pump category. it is widely used as a machine in marine industry, refer to marine industry category.   == general == a pump is a device that moves fluids (liquids or gases), or sometimes slurries, by mechanical action. a marine pump is an important auxiliary equipment in marine industry and ship building industry. it is widely used in all kinds of marine vessels, such as barges, tug boats, containers, carriers, ships, vessels, fixed offshore structure, drilling jack-up rigs and so on. these marine pumps can be serviced for cooling, circulating, ballast, general service(g/s), fire-fighting, boiler feed water, condensate water, fresh(drinking) water, sanitary water, bilge & sludge, f.o. transfer, l.o. transfer, f.o. and f.s. cargo pumping, cargo stripping, hydrophone tank unit, sewage treatment unit, oil water separator, incinerator, fresh water generator, and so on.   == type == marine pump is named by its usage or application, it covers a lot of types of pumps, such as: the marine centrifugal pump is used to transport fluids by the conversion of rotational kinetic energy to the hydrodynamic energy of the fluid flow. the rotational energy typically comes from an engine or electric motor. the fluid enters the pump impeller along or near to the rotating axis and is accelerated by the impeller, flowing radially outward into a diffuser or volute chamber (casing), from where it exits. the gear pump is a marine gear pump that uses the meshing of gears to pump fluid by displacement. it is one of the most common types of pumps for hydraulic fluid power applications. gear pumps are also widely used in chemical installations to pump high viscosity fluids. there are two main variations; external gear pumps which use two external spur gears, and internal gear pumps which use an external and an internal spur gears (internal spur gear teeth face inwards, see below). gear pumps are positive displacement (or fixed displacement), meaning they pump a constant amount of fluid for each revolution. some gear pumps are designed to function as either a motor or a pump. the screw pump is a positive-displacement (pd) pump that uses one or several screws to move fluids or solids along the screw(s) axis. in its simplest form (the archimedes' screw pump), a single screw rotates in a cylindrical cavity, thereby moving the material along the screw's spindle.   == see also == handy billy   == references =="")"
220,"Airbag launching refers to a method to launch vessels with marine air bags. It is a methodology for ship launching utilizing air bags.The Xiao Qinghe shipyard launched a tank barge with marine airbags on January 20, 1981 and it is known as the first use of marine airbags.
This kind of ship launching method has the advantages of requiring less permanent infrastructure, risk, and cost. The airbags provide support to the hull of the ship, air bags rolling motion take a vessel launch into water, thus it is arguably safer than other options like sideways launching.
Unlike most other launching methods that are fixed infrastructures, airbag launching has relatively less limitations and can be used in versatile ways. It overcomes the disadvantage of fixed track launching in which capacity of shipbuilding and ship repairing is limited by the fixed infrastructures especially in small and medium-sized shipyards.


== Airbags ==
Ship launching airbags are specialized air bags that are used for launching marine vessels. These air bags are made of synthetic-tire-cord reinforcement layers and rubber layers, and are also known as marine airbags. They were invented in 1980. The first known use of marine airbags occurred on January 20, 1981 with the launch of a tank barge from the Xiao Qinghe shipyard. From then on, more and more shipyards, especially in China and Southeast Asia, began to use air bags to launch small and medium-sized vessels.
In recent years, higher strength materials have been used in air bag production, allowing them to have much more bearing capacity. Hence, they have begun to be used at the launchings of larger vessels. In October 2011, the successful launch of one vessel with a deadweight tonnage (DWT) of 75000 tonnes set a world record for ship launches utilizing air bags. The following year, on June 6, 2012, the ship ""He Ming"" (IMO number 9657105), with a DWT of 73541 tonnes, total length of 224.8m, breadth of 34m, and depth of 18.5m, also launched successfully using air bags.


=== Air bag structure ===

Marine air bags consist of three parts:

Air bag body: the main, cylindrical part of the air bag after being fully inflated with compressed air,
Air bag heads: conical parts connecting the body and the mouths of the air bag, and
Air bag mouths: either metal valves mounted on both ends of the air bag for inflating with air or a valve on one end and a metal lug on the other end used to haul the air bag.


=== Air bag size and model ===


==== Size ====
The diameter of an air bag varies; sizes include 0.8 m, 1.0 m, 1.2 m, 1.5 m, 1.8 m, etc.
The length of an air bag is specified by the customer when it is manufactured.


==== Model ====
Air bags are commonly divided into three to six tire-cord reinforcement layers. There can be more layers, though there are normally less than ten.


==== Air bag bearing capacity ====
The maximum load carrying capacity of an air bag, which is the maximum load under which it will suffer no permanent deformation or damage, can be found as follows:

Let:

  
    
      
        D
      
    
    {\displaystyle D}
   be the original diameter of the air bag

  
    
      
        H
      
    
    {\displaystyle H}
   be the height of the compressed air bag

  
    
      
        
          W
          
            1
          
        
      
    
    {\displaystyle W_{1}}
   be the width of the air bag contacting the ship bottom

  
    
      
        
          P
          
            1
          
        
      
    
    {\displaystyle P_{1}}
   be the internal pressure of the air bag

  
    
      
        L
      
    
    {\displaystyle L}
   be the effective length of the air bag
Given these variables,

  
    
      
        
          bearing capacity
        
        =
        
          P
          
            1
          
        
        ×
        
          W
          
            1
          
        
        ×
        L
      
    
    {\displaystyle {\text{bearing capacity}}=P_{1}\times W_{1}\times L}
  


==== Air bag materials ====
Ship launching air bags are constructed of synthetic-tire-cord layers; inner and outer rubber layers are sometimes added. All materials used are vulcanized.


=== Air bag test ===
Air tightness test: Without carrying any load, fill the air bag till the internal pressure of the air bag reaches rated working pressure. After 1 hour. The pressure loss should be less than 5 % of initial pressure.
Bursting test： Fill the air bag with water until the air bag bursts. The water pressure at the time of bursting shall be no less than three times of rated working pressure.


== The arrangements of air bags ==


=== Launching type ===
Based on the ship shapes, the ship may be launched, using air bags, by either end launching type  or side launching type


=== End launching type ===
There are three ways to arrange air bags when using the end launching type. They are (1) linear arrangement (see Figure A.1), (2) staggered arrangement (see Figure A.2), and (3) two-lines arrangement (see Figure A.3). As for which arrangement to use, it will depend on the ship's width and the length of the air bags.
When the ship's width is not greater than the effective length of the air bags, the linear arrangement shall be selected

When the ship's width is greater than the effective length of an air bag and less than the effective length of two air bags, the staggered arrangement can be selected.

When the ship's width is greater than the combined effective length of two air bags, or for special ship such as catamaran HSC or split hopper barge, the two lines arrangement shall be selected. The distance between the near ends of two air bags is greater than 0,2 m.


=== Side launching type ===
For small flat-bottom ships, the side launching method may be utilized (see Figure A.4)


== Air bags quantity ==
Air bags shall meet the requirements of ISO 14409According to the weight of the ship being launched, the quantity of the air bags needed for this operation must be calculated in accordance with Formula (1):

where
N is the quantity of air bags used for ship launching;
K1 is a coefficient, in general, K1 ≥ 1,2;
Q is the weight of the ship (ton);
g is acceleration of gravity (m/s2), g = 9,8;
Cb is the block coefficient of the ship being launched;
R is the allowable unit bearing capacity of the air bags (kN/m);
Ld is the contact length between the bottom of the ship and the body of the air bag at the midship section (m).
For ship shifting, 2 to 4 additional air bags shall be made ready and available.
The centre to centre distance between two neighbouring air bags should be less than or equal to that found in Formula (2) and equal to or be greater than that found in Formula (3).

where
L is the actual length of the ship bottom that can make contact with the air bags (m);
N is the quantity of air bags used for ship launching;
k is a coefficient, k = 1 for steel ships, k = 0,8 for wooden, aluminium and glass-fibre-reinforced ships;
D is the nominal diameter of air bags (m).


== Slipway ==
The gradient and the length of the slipway shall be determined according to the size of the ship and the hydrological condition of the area water.
The bearing capacity of the slipway shall be at least twice as strong as the working pressure of air bags.
For ships of more than 3 000 tons of length, more than 120m,the slipway shall be constructed with reinforced concrete and the height difference between the right and left sides shall be less than 20 mm. For ships of more than 1 000 tons but less than or equal to 3 000 tons in weight, or more than 90 m but less than or equal to 120 m in length, the slipway shall be constructed with cement concrete and the height difference between the right and left sides shall be less than 50 mm. For ships of not more than 1 000 tons in weight or not more than 90 m in length, the slipway may be an earthen slope and shall be compacted even by rollers. The height difference between the right and left sides shall be less than 80 mm.
The main slipway shall enable the ship to glide automatically when the ship is off the tow. The auxiliary slipway shall be determined according to the ship type, the water level at time of launching, the diameter of the air bags, and the safety requirements.


== Towing arrangement ==
A windlass shall be used to control the movement of the ship. Tow system that comprises windlass, steel wire rope and pulley set shall be securely fastened to the ground anchor in front of the berth.
In general, a slow windlass shall be selected for ship launching. The veering speed of the windlass shall be 9 m/min to 13 m/min.
The forces of the windlass and the steel wire rope should be calculated carefully by technicians of ship yards or air bags company.


== References ==","pandas(index=220, _1=220, text='airbag launching refers to a method to launch vessels with marine air bags. it is a methodology for ship launching utilizing air bags.the xiao qinghe shipyard launched a tank barge with marine airbags on january 20, 1981 and it is known as the first use of marine airbags. this kind of ship launching method has the advantages of requiring less permanent infrastructure, risk, and cost. the airbags provide support to the hull of the ship, air bags rolling motion take a vessel launch into water, thus it is arguably safer than other options like sideways launching. unlike most other launching methods that are fixed infrastructures, airbag launching has relatively less limitations and can be used in versatile ways. it overcomes the disadvantage of fixed track launching in which capacity of shipbuilding and ship repairing is limited by the fixed infrastructures especially in small and medium-sized shipyards.   == airbags == ship launching airbags are specialized air bags that are used for launching marine vessels. these air bags are made of synthetic-tire-cord reinforcement layers and rubber layers, and are also known as marine airbags. they were invented in 1980. the first known use of marine airbags occurred on january 20, 1981 with the launch of a tank barge from the xiao qinghe shipyard. from then on, more and more shipyards, especially in china and southeast asia, began to use air bags to launch small and medium-sized vessels. in recent years, higher strength materials have been used in air bag production, allowing them to have much more bearing capacity. hence, they have begun to be used at the launchings of larger vessels. in october 2011, the successful launch of one vessel with a deadweight tonnage (dwt) of 75000 tonnes set a world record for ship launches utilizing air bags. the following year, on june 6, 2012, the ship ""he ming"" (imo number 9657105), with a dwt of 73541 tonnes, total length of 224.8m, breadth of 34m, and depth of 18.5m, also launched successfully using air bags. for small flat-bottom ships, the side launching method may be utilized (see figure a.4)   == air bags quantity == air bags shall meet the requirements of iso 14409according to the weight of the ship being launched, the quantity of the air bags needed for this operation must be calculated in accordance with formula (1):  where n is the quantity of air bags used for ship launching; k1 is a coefficient, in general, k1 ≥ 1,2; q is the weight of the ship (ton); g is acceleration of gravity (m/s2), g = 9,8; cb is the block coefficient of the ship being launched; r is the allowable unit bearing capacity of the air bags (kn/m); ld is the contact length between the bottom of the ship and the body of the air bag at the midship section (m). for ship shifting, 2 to 4 additional air bags shall be made ready and available. the centre to centre distance between two neighbouring air bags should be less than or equal to that found in formula (2) and equal to or be greater than that found in formula (3).  where l is the actual length of the ship bottom that can make contact with the air bags (m); n is the quantity of air bags used for ship launching; k is a coefficient, k = 1 for steel ships, k = 0,8 for wooden, aluminium and glass-fibre-reinforced ships; d is the nominal diameter of air bags (m).   == slipway == the gradient and the length of the slipway shall be determined according to the size of the ship and the hydrological condition of the area water. the bearing capacity of the slipway shall be at least twice as strong as the working pressure of air bags. for ships of more than 3 000 tons of length, more than 120m,the slipway shall be constructed with reinforced concrete and the height difference between the right and left sides shall be less than 20 mm. for ships of more than 1 000 tons but less than or equal to 3 000 tons in weight, or more than 90 m but less than or equal to 120 m in length, the slipway shall be constructed with cement concrete and the height difference between the right and left sides shall be less than 50 mm. for ships of not more than 1 000 tons in weight or not more than 90 m in length, the slipway may be an earthen slope and shall be compacted even by rollers. the height difference between the right and left sides shall be less than 80 mm. the main slipway shall enable the ship to glide automatically when the ship is off the tow. the auxiliary slipway shall be determined according to the ship type, the water level at time of launching, the diameter of the air bags, and the safety requirements.   == towing arrangement == a windlass shall be used to control the movement of the ship. tow system that comprises windlass, steel wire rope and pulley set shall be securely fastened to the ground anchor in front of the berth. in general, a slow windlass shall be selected for ship launching. the veering speed of the windlass shall be 9 m/min to 13 m/min. the forces of the windlass and the steel wire rope should be calculated carefully by technicians of ship yards or air bags company.   == references ==')"
221,"With the introduction of the marine propeller back in the early 19th century, cavitation during operation has always been a limiting factor on efficiency of ships. Cavitation in marine propellers develops when the propeller operates at a high speed and reduces efficiency of the propeller. Ever since the introduction of the propeller, solutions for cavitation have been developed and tested. 


== Nozzle System ==
As the name suggests, this system uses a set of nozzles to help reduce and prevent the likelihood of cavitation in propellers. This system was developed by Samsung Shipping which is based in South Korea. In order to reduce the possibility of cavitation happening in marine propellers, a set of nozzles are placed on the hull of the ship directly in front of the propeller. These nozzles spray out compressed air over the propeller that creates “a macro bubble”. This bubble completely encompasses the propeller that is in operation. With the differing characteristics of the seawater outside of the bubble and the air inside, a zone develops that has the ability to reduce the “resonance frequency”. Due to this reduction, cavitation is less likely to occur during operation of a marine propeller.To determine how effective this nozzle system could be, multiple tests were carried out with the nozzles and without them. In these tests, it was discovered that the resonance frequencies and the likelihood of cavitation could be reduced by up to 75%. Those who conducted these tests also tried two different arrangements of the nozzles to find out which one was more effective. The first arrangement used only one nozzle, which though it used considerably less power than the other option, it was not nearly as successful. The multi-nozzle system, on the other hand, gave much better results but required more power to operate.While this nozzle system has major drawbacks particularly in its power requirements, the possibility of cavitation in the operation of marine propellers is reduced considerably. Thus, to some ship owners and operators, the cost of installing these nozzles and operating them is outweighed by the benefits of increased efficiency in their propellers.


== Air-Filled Rubber Membrane ==
The Air-Filled Rubber Membrane uses the same principles as the Nozzle System to reduce cavitation in marine propellers. Since the Nozzle System requires a large source of energy to operate, the creators sought to create a system that has the same results but is cheaper to operate. This membrane builds on the lessons learned in designing the Nozzle System and uses a pocket of air to prevent cavitation but does not require nozzles or compressors. While at the same time limiting the cost of operation, this membrane provides just as much protection against cavitation as the nozzles do.The Air-Filled Rubber Membrane is placed directly behind an operating marine propeller in the hull. As described before, the differing characteristics of the air in the membrane and the seawater around it reduce the resonance frequency, which in turn increases the point at which cavitation is encountered. The membrane is specially designed which, along with the use of rubber, furthers the effect of reducing the frequency. This membrane is cheaper to operate than the Propeller Control System+ and the Nozzle System but is not as effective as the PCS+ in reducing cavitation.


== Different Materials for Propellers ==
This solution focuses on the materials that marine propellers are created from which is a direct factor in cavitation. While redesigning propellers would only garner an extra one or two percent efficiency in operation, changing the materials a propeller is made from has greater effects. The most common blend that marine propellers are created from is the nickel aluminum bronze blend. While this blend can resist erosion which is why it is so common, it cannot properly handle cavitation.However, this is beginning to change. The Royal Netherlands Navy for one is starting to experiment with composite materials like resins or carbon fiber. These materials, when formed into a propeller, are flexible enough under pressure to “deflect,” which can reduce cavitation. Other  options are made from carbon fiber, epoxy resin, or even glass, and are able to produce “a hydro elastic effect”. Since these new propellers can flex and are not nearly as rigid under pressure, the risk of cavitation is reduced.While replacing propellers would be the most efficient on ships that are currently under construction, the benefits from newer propeller materials could outweigh costs of replacing current marine propellers. Despite the initial cost of the propellers, this solution costs nothing to operate making it more feasible to shipping around the globe.


== References ==","pandas(index=221, _1=221, text='with the introduction of the marine propeller back in the early 19th century, cavitation during operation has always been a limiting factor on efficiency of ships. cavitation in marine propellers develops when the propeller operates at a high speed and reduces efficiency of the propeller. ever since the introduction of the propeller, solutions for cavitation have been developed and tested.   == nozzle system == as the name suggests, this system uses a set of nozzles to help reduce and prevent the likelihood of cavitation in propellers. this system was developed by samsung shipping which is based in south korea. in order to reduce the possibility of cavitation happening in marine propellers, a set of nozzles are placed on the hull of the ship directly in front of the propeller. these nozzles spray out compressed air over the propeller that creates “a macro bubble”. this bubble completely encompasses the propeller that is in operation. with the differing characteristics of the seawater outside of the bubble and the air inside, a zone develops that has the ability to reduce the “resonance frequency”. due to this reduction, cavitation is less likely to occur during operation of a marine propeller.to determine how effective this nozzle system could be, multiple tests were carried out with the nozzles and without them. in these tests, it was discovered that the resonance frequencies and the likelihood of cavitation could be reduced by up to 75%. those who conducted these tests also tried two different arrangements of the nozzles to find out which one was more effective. the first arrangement used only one nozzle, which though it used considerably less power than the other option, it was not nearly as successful. the multi-nozzle system, on the other hand, gave much better results but required more power to operate.while this nozzle system has major drawbacks particularly in its power requirements, the possibility of cavitation in the operation of marine propellers is reduced considerably. thus, to some ship owners and operators, the cost of installing these nozzles and operating them is outweighed by the benefits of increased efficiency in their propellers.   == air-filled rubber membrane == the air-filled rubber membrane uses the same principles as the nozzle system to reduce cavitation in marine propellers. since the nozzle system requires a large source of energy to operate, the creators sought to create a system that has the same results but is cheaper to operate. this membrane builds on the lessons learned in designing the nozzle system and uses a pocket of air to prevent cavitation but does not require nozzles or compressors. while at the same time limiting the cost of operation, this membrane provides just as much protection against cavitation as the nozzles do.the air-filled rubber membrane is placed directly behind an operating marine propeller in the hull. as described before, the differing characteristics of the air in the membrane and the seawater around it reduce the resonance frequency, which in turn increases the point at which cavitation is encountered. the membrane is specially designed which, along with the use of rubber, furthers the effect of reducing the frequency. this membrane is cheaper to operate than the propeller control systemand the nozzle system but is not as effective as the pcsin reducing cavitation.   == different materials for propellers == this solution focuses on the materials that marine propellers are created from which is a direct factor in cavitation. while redesigning propellers would only garner an extra one or two percent efficiency in operation, changing the materials a propeller is made from has greater effects. the most common blend that marine propellers are created from is the nickel aluminum bronze blend. while this blend can resist erosion which is why it is so common, it cannot properly handle cavitation.however, this is beginning to change. the royal netherlands navy for one is starting to experiment with composite materials like resins or carbon fiber. these materials, when formed into a propeller, are flexible enough under pressure to “deflect,” which can reduce cavitation. other  options are made from carbon fiber, epoxy resin, or even glass, and are able to produce “a hydro elastic effect”. since these new propellers can flex and are not nearly as rigid under pressure, the risk of cavitation is reduced.while replacing propellers would be the most efficient on ships that are currently under construction, the benefits from newer propeller materials could outweigh costs of replacing current marine propellers. despite the initial cost of the propellers, this solution costs nothing to operate making it more feasible to shipping around the globe.   == references ==')"
222,"A marine thruster is a device for producing directed hydrodynamic thrust mounted on a marine vehicle, primarily for maneuvering or propulsion. There are a variety of different types of marine thrusters and each of them plays a role in the maritime industry. Marine thrusters come in many different shapes and sizes, for example screw propellers, Voith-Schneider propellers, waterjets, ducted propellers, tunnel bow thrusters, and stern thrusters, azimuth thrusters, rim-driven thrusters, ROV and submersible drive units. A marine thruster consists of a propeller or impeller which may be encased in some kind of tunnel or ducting that directs the flow of water to produce a resultant force intended to obtain movement in the desired direction or resist forces which would cause unwanted movement. The two subcategories of marine thrusters are for propulsion and maneuvering, the maneuvering thruster typically in the form of bow or stern thrusters and propulsion thrusters ranging from Azimuth thrusters to Rim Drive thrusters.


== Positioning Thrusters ==

Positioning thrusters come in applications, Bow thrusters at the forward end of the vessel, and stern thrusters mounted aft on the boat. Their purpose is to maneuver or position the boat to a greater precision than the propulsion device can accomplish. Their positioning along the length of the vessel allows for directed leteral thrust ahead and astern of the centre of lateral resistance so that the vessel may be maneuvered away from obstructions in its path, or towards a desired position, especially when coming to or away from a dock. These positioning thrusters are usually significantly smaller than the main propulsion thrusters because they only have to do small adjustments rather than moving the whole vessel at speed. Both bow and stern thrusters may be housed in through-hull tunnels. Depending on the size of the motors driving these propellers, they could draw an insignificant amount of power or a large amount of power that requires much caution to operate. Another smaller subset of positioning thrusters is those used for maneuvering unmanned aquatic vehicles like Guanay II AUV tested by scientists from Spain (Masmitja, 2018).


== Propulsion Thrusters ==

Propulsion thrusters are those thrusters which provide longitudinal motion for vessels as an alternative to traditional propellers. There are a variety of types of propulsion thrusters but the most common form is the azimuth thruster, that can rotate 360 degrees on a vertical axis to optionally produce thrust for maneuvering. (Lindborg, 1997). The amount of thrust produced is controllable. There are variants of azimuth thrusters such as CRP thrusters which have two contra-rotating Azimuth thrusters or Swing-Up Azimuth thrusters that can be retracted when not in use to reduce drag on the vessel (Wartsila Encyclopedia). Other propulsion thrusters like outboard thrusters which can be easily put in and out of service, rim drive thrusters that are driven via the external ring with the blades mounted on the inner face of the ring with their tips towards the center, or tilted thrusters pointed away from the hull to minimize interaction with the ship and increase thruster efficiency. The choice between using thrusters or traditional propellers to propel marine vessels is a compromise between versatility and efficiency. Propellers are designed to work in-line with a propulsion plant and produce one-directional thrust while thrusters are more customizable and have a more versatile application. They have this versatility at the cost of complexity and lower efficiency – they are not as robust as propellers and typically have applications on smaller vessels that don’t require as much power.


== Reference List ==","pandas(index=222, _1=222, text='a marine thruster is a device for producing directed hydrodynamic thrust mounted on a marine vehicle, primarily for maneuvering or propulsion. there are a variety of different types of marine thrusters and each of them plays a role in the maritime industry. marine thrusters come in many different shapes and sizes, for example screw propellers, voith-schneider propellers, waterjets, ducted propellers, tunnel bow thrusters, and stern thrusters, azimuth thrusters, rim-driven thrusters, rov and submersible drive units. a marine thruster consists of a propeller or impeller which may be encased in some kind of tunnel or ducting that directs the flow of water to produce a resultant force intended to obtain movement in the desired direction or resist forces which would cause unwanted movement. the two subcategories of marine thrusters are for propulsion and maneuvering, the maneuvering thruster typically in the form of bow or stern thrusters and propulsion thrusters ranging from azimuth thrusters to rim drive thrusters.   == positioning thrusters ==  positioning thrusters come in applications, bow thrusters at the forward end of the vessel, and stern thrusters mounted aft on the boat. their purpose is to maneuver or position the boat to a greater precision than the propulsion device can accomplish. their positioning along the length of the vessel allows for directed leteral thrust ahead and astern of the centre of lateral resistance so that the vessel may be maneuvered away from obstructions in its path, or towards a desired position, especially when coming to or away from a dock. these positioning thrusters are usually significantly smaller than the main propulsion thrusters because they only have to do small adjustments rather than moving the whole vessel at speed. both bow and stern thrusters may be housed in through-hull tunnels. depending on the size of the motors driving these propellers, they could draw an insignificant amount of power or a large amount of power that requires much caution to operate. another smaller subset of positioning thrusters is those used for maneuvering unmanned aquatic vehicles like guanay ii auv tested by scientists from spain (masmitja, 2018).   == propulsion thrusters ==  propulsion thrusters are those thrusters which provide longitudinal motion for vessels as an alternative to traditional propellers. there are a variety of types of propulsion thrusters but the most common form is the azimuth thruster, that can rotate 360 degrees on a vertical axis to optionally produce thrust for maneuvering. (lindborg, 1997). the amount of thrust produced is controllable. there are variants of azimuth thrusters such as crp thrusters which have two contra-rotating azimuth thrusters or swing-up azimuth thrusters that can be retracted when not in use to reduce drag on the vessel (wartsila encyclopedia). other propulsion thrusters like outboard thrusters which can be easily put in and out of service, rim drive thrusters that are driven via the external ring with the blades mounted on the inner face of the ring with their tips towards the center, or tilted thrusters pointed away from the hull to minimize interaction with the ship and increase thruster efficiency. the choice between using thrusters or traditional propellers to propel marine vessels is a compromise between versatility and efficiency. propellers are designed to work in-line with a propulsion plant and produce one-directional thrust while thrusters are more customizable and have a more versatile application. they have this versatility at the cost of complexity and lower efficiency – they are not as robust as propellers and typically have applications on smaller vessels that don’t require as much power.   == reference list ==')"
223,"The IEEE Journal of Oceanic Engineering is a journal published by the Institute of Electrical and Electronics Engineers.  The journal's editor in chief is Professor Ross Chapman, of the University of Victoria.
It is included in the Science Citation Index Expanded; its impact factor is 2.90.


== References ==","pandas(index=223, _1=223, text=""the ieee journal of oceanic engineering is a journal published by the institute of electrical and electronics engineers.  the journal's editor in chief is professor ross chapman, of the university of victoria. it is included in the science citation index expanded; its impact factor is 2.90.   == references =="")"
224,"Bridge Operations (or Operational) Quality Assurance (BOQA - pronounced BOUQUA or / bəʊ'kwɒ /) is a methodology utilised in shipping and which originates from the similar FOQA/FDM (Flight Operations Quality Assurance/Flight Data Monitoring) concept in aviation. BOQA is a methodology with which ship owners/operators, ship Captains, and other associated shipping stakeholders can automatically and systematically monitor, track, trend and analyse operational quality of (seagoing) vessels. The main target with BOQA is to provide a non-punitive platform for proactive analysis of vessel data to enable the enhancement of maritime safety . The BOQA methodology can be used in both conventional manned ships and in autonomous or unmanned vessels providing that adequate data sources are available.


== History ==
The original template for BOQA was laid out when Royal Caribbean approached Aerobytes Limited (a market leader in FOQA) to collaborate to provide a similar product for the maritime industry. https://www.aerobytes.co.uk/boqa/.  Discussions were held as to what breaches of performance should be detected and two recorders were installed on RCCL vessels and discussions were also held with Carnival group to develop the BOQA concept. Aerobytes decided to focus on it's core business of aviation and RCCL and Carnival went on to develop their own systems along with a few other companies who saw the potential. 
At present BOQA is not mandated and therefore there are no strict rules as to what an effective BOQA system should contain, but once the enormous potential is realised it is entirely possible that might change. 


== Description ==
BOQA is best developed as a non-punitive company-internal methodology or process, which has the overall target of assisting the ship Captain and the ship operator to maintain a high level of safety and operational quality.
BOQA has been described as:
A system which delivers 24/7 electronic monitoring and electronic alarms when set operational parameters are deviated from
During normal operations, BOQA will collect and analyse digital operational data direct from ships operation equipment
BOQA data is unique because it can provide objective information that is not available through other methods
A BOQA program can identify operational situations in which there is increased risk, allowing operators to take early corrective action before that risk results in an incident or accident
The BOQA program is a tool in the operators overall operational risk assessment and prevention program
BOQA, being proactive in identifying and addressing risk will enhance safety
The Oil Companies International Marine Forum (OCIMF) published in June 2013 the ""Recommendations on the Proactive Use of Voyage Data Recorder Information"" and went forth with submitting these recommendations as an info paper to the IMO (NAV 59/INF.9) 12/07/2013 with the title ""The proactive use of Voyage Data Recorder (VDR) information"". Main aspect with this paper was to use the ship's VDR as a data source for proactive safety monitoring. This paper called for:

Routine transmission of VDR data ashore
Auto-analysis of data
Alerts on non-conformance...and as such the paper described in essence a BOQA solution.


== Methodology ==
BOQA is used to objectively monitor the human operational performance and external factors, such as weather and other traffic around the vessel, by utilising various types of sensor and external data and comparing this data with defined best-practices and Standard operating procedure (SOPs). BOQA is usually not used to monitor the internal technical performance of the vessel machinery and equipment-base, which in most cases already has a wide range of proprietary monitoring and alarm functions.
BOQA is often a software system consisting of shipboard real-time data collection and sensing, automated data transmission between the vessel and shore and a shore-based system which receives, combines, analyses, alerts and stores the data according to defined rules and logic.
BOQA usually includes three ""time-domains"", ie:

a historical database of data and events and which is used for trend- and pattern analysis
a real-time reactive stream analytics component which can react to events or deviations in real- or near-real-time (NRT)
a predictive and pro-active component, which assists the operator in taking actions before an event occurs


=== Event types ===
BOQA is a continuously developing methodology, much in the same way as FOQA. At present time BOQA solutions are known to be able monitor some of the following various event types some of which might include:

Cross-track error or deviation from a defined navigational route (the Costa Concordia disaster is known to be caused by such deviation)
Safety corridor, ie is the ship sailing within its defined safe waters
Collision risk based on Proximity (ship too close to own ship), CPA (Closest point of approach which measures the time and distance to a potential collision) and BCR (Bow Cross Range)
Entry into a restricted or protected area
Severe list, heel or roll
Excessive accelerations
Excessive turns (high ROT)
Crash stop
Heavy weather at present position and along a planned route (this is a predictive component)
Sudden change in atmospheric pressure (which is an indicator of oncoming change in weather)
Speed monitoring, which can be used for example in Charterparty Compliance monitoring
Unscheduled stops along the route, which may lead to schedule-deviations and unwanted changes in arrival time
Proximity to a shore line, ice edge or iceberg
Rescue boat launch
AIS (Automatic identification system) status change
Black-out
Port inactivity
Excessive use of engine controls
Under keel clearance
Excessive rudder angles
Traffic Separation Scheme violations


=== Inputs ===
BOQA relies on data from various sources. Some data sources could be but are not restricted to:

Ship's AIS transponders (Automatic identification system), which give location, speed and course of the vessel itself and the surrounding traffic
Ship's GPS (Global Positioning System), which also gives location, speed and course
Ship's VDR (Voyage Data Recorder)
Motion sensors
Barometric pressure sensors
Real-time weather data from sensors
Weather forecast data
Ship's navigational ECDIS (Electronic Chart Display and Information System) route
External geographic data, such as restricted areas, PSSAs (Marine protected area) or TSS (Traffic Separation Scheme)
Ship's electronic logbook


== Applications ==
BOQA is not yet known to be officially mandated or regulated by any official maritime bodies, such as International Maritime Organization, Classification societies or Flag state administrations.
Royal Caribbean Cruise line stated in their 2012 Stewardship report that they were evaluating a BOQA system. 
Carnival Corporation & plc is known to have a large scale in-house developed BOQA solution, which consists of a data-system (Neptune) and a 24/7 manned Fleet Operations Centre in three locations around the world.AS Tallink Group is known to be in the process of testing a BOQA solution.


== Literature ==
NTSB Forum 25.-26.3.2914: Cruise Ships: Examining Safety, Operations and Oversight Forum, Capt. David Christie – Carnival Corporation Innovative Techniques to Enhance Safety
OCIMF (Oil Companies International Marine Forum). Recommendations on the Proactive Use of Voyage Data Recorder Information (Ver 2).pdf
AWS Public sector blog ""Maritime Operations – Automating Operational Quality Assurance with AWS and Open Data""


== References ==","pandas(index=224, _1=224, text='bridge operations (or operational) quality assurance (boqa - pronounced bouqua or / bəʊ\'kwɒ /) is a methodology utilised in shipping and which originates from the similar foqa/fdm (flight operations quality assurance/flight data monitoring) concept in aviation. boqa is a methodology with which ship owners/operators, ship captains, and other associated shipping stakeholders can automatically and systematically monitor, track, trend and analyse operational quality of (seagoing) vessels. the main target with boqa is to provide a non-punitive platform for proactive analysis of vessel data to enable the enhancement of maritime safety . the boqa methodology can be used in both conventional manned ships and in autonomous or unmanned vessels providing that adequate data sources are available.   == history == the original template for boqa was laid out when royal caribbean approached aerobytes limited (a market leader in foqa) to collaborate to provide a similar product for the maritime industry. https://www.aerobytes.co.uk/boqa/.  discussions were held as to what breaches of performance should be detected and two recorders were installed on rccl vessels and discussions were also held with carnival group to develop the boqa concept. aerobytes decided to focus on it\'s core business of aviation and rccl and carnival went on to develop their own systems along with a few other companies who saw the potential. at present boqa is not mandated and therefore there are no strict rules as to what an effective boqa system should contain, but once the enormous potential is realised it is entirely possible that might change.   == description == boqa is best developed as a non-punitive company-internal methodology or process, which has the overall target of assisting the ship captain and the ship operator to maintain a high level of safety and operational quality. boqa has been described as: a system which delivers 24/7 electronic monitoring and electronic alarms when set operational parameters are deviated from during normal operations, boqa will collect and analyse digital operational data direct from ships operation equipment boqa data is unique because it can provide objective information that is not available through other methods a boqa program can identify operational situations in which there is increased risk, allowing operators to take early corrective action before that risk results in an incident or accident the boqa program is a tool in the operators overall operational risk assessment and prevention program boqa, being proactive in identifying and addressing risk will enhance safety the oil companies international marine forum (ocimf) published in june 2013 the ""recommendations on the proactive use of voyage data recorder information"" and went forth with submitting these recommendations as an info paper to the imo (nav 59/inf.9) 12/07/2013 with the title ""the proactive use of voyage data recorder (vdr) information"". main aspect with this paper was to use the ship\'s vdr as a data source for proactive safety monitoring. this paper called for:  routine transmission of vdr data ashore auto-analysis of data alerts on non-conformance...and as such the paper described in essence a boqa solution.   == methodology == boqa is used to objectively monitor the human operational performance and external factors, such as weather and other traffic around the vessel, by utilising various types of sensor and external data and comparing this data with defined best-practices and standard operating procedure (sops). boqa is usually not used to monitor the internal technical performance of the vessel machinery and equipment-base, which in most cases already has a wide range of proprietary monitoring and alarm functions. boqa is often a software system consisting of shipboard real-time data collection and sensing, automated data transmission between the vessel and shore and a shore-based system which receives, combines, analyses, alerts and stores the data according to defined rules and logic. boqa usually includes three ""time-domains"", ie:  a historical database of data and events and which is used for trend- and pattern analysis a real-time reactive stream analytics component which can react to events or deviations in real- or near-real-time (nrt) a predictive and pro-active component, which assists the operator in taking actions before an event occurs boqa relies on data from various sources. some data sources could be but are not restricted to:  ship\'s ais transponders (automatic identification system), which give location, speed and course of the vessel itself and the surrounding traffic ship\'s gps (global positioning system), which also gives location, speed and course ship\'s vdr (voyage data recorder) motion sensors barometric pressure sensors real-time weather data from sensors weather forecast data ship\'s navigational ecdis (electronic chart display and information system) route external geographic data, such as restricted areas, pssas (marine protected area) or tss (traffic separation scheme) ship\'s electronic logbook   == applications == boqa is not yet known to be officially mandated or regulated by any official maritime bodies, such as international maritime organization, classification societies or flag state administrations. royal caribbean cruise line stated in their 2012 stewardship report that they were evaluating a boqa system. carnival corporation & plc is known to have a large scale in-house developed boqa solution, which consists of a data-system (neptune) and a 24/7 manned fleet operations centre in three locations around the world.as tallink group is known to be in the process of testing a boqa solution.   == literature == ntsb forum 25.-26.3.2914: cruise ships: examining safety, operations and oversight forum, capt. david christie – carnival corporation innovative techniques to enhance safety ocimf (oil companies international marine forum). recommendations on the proactive use of voyage data recorder information (ver 2).pdf aws public sector blog ""maritime operations – automating operational quality assurance with aws and open data""   == references ==')"
225,"Ángel Amadeo Labruna, (28 September 1918 – 19 September 1983), was an Argentine football player and coach, who played as a forward. With 295 goals scored in official matches, Labruna is the 2nd all-time top scorer of Primera División after Paraguayan Arsenio Erico. Labruna was also part of the celebrated River Plate offense, nicknamed La Máquina (The Machine), and he was considered one of the best South-American footballers of his generation.


== Biography ==
Labruna was born in Buenos Aires, Argentina.


== Career ==


=== Playing career ===

Labruna's debut in the Primera División was in replacement of José Manuel Moreno who had been suspended by the club, wearing the number 10 shirt. The match disputed on 18 June 1939 in La Plata against Estudiantes, which defeated River by 1–0.
His goals scored and outstanding performances caused that Moreno had to play on the right side of the field when he was allowed to play again. Labruna played in River for 20 years, winning 9 domestic championships with the team (1941, 1942, 1945, 1947, 1952, 1953, 1955, 1956, 1957) and being the top scorer twice (1943 with 23 goals and 1945 with 25).He was part of his club's legendary team along with Juan Carlos Muñoz, José Manuel Moreno, Adolfo Pedernera, and Félix Loustau, where he played as an inside-left forward. Although this attacking line only disputed 18 games with those players, they were regarded as one of the best forward line in the history of Argentine football. They were nicknamed La Máquina (The Machine) due to their skills with the ball and synchronized play. Coach and former player Carlos Peucelle said that his team was formed by ""A goalkeeper and 10 forwards"", using an imaginary ""1–10"".
Labruna holds a number of records for River Plate, including his record of 16 goals in the superclásico derby with fierce rival Boca Juniors.
In 1959, Labruna left River Plate having defended club's colors in 515 matches and scoring 317 goals, 293 in goals what made him the all-time highest goalscorer in the Argentine first division along with Arsenio Erico, a record that remains nowadays. He later played two seasons in the Chilean C.S.D. Rangers, and Uruguayan team Rampla Juniors of Montevideo, before returning to Argentina to finish his career at Platense, when he was 43 years old.
Labruna played 37 matches for the Argentina national team, scoring 17 goals. He also won two South American Championships (1946 and 1955) and as a nearly 40-year-old he played in the final phase of 1958 FIFA World Cup held in Sweden.As other great players of his generation, Labruna could not participate in other World Cups due to the event's suspension during World War II and later for the decision taken by the Argentine Football Association, which did not compete in the World Cups of Brazil and Switzerland.


=== Coaching career ===

After ending his career as a player he became Assistant Coach and Coach in River Plate, Defensores de Belgrano, Platense, Rosario Central (where he won his first Nacional championship, in 1971), Talleres de Córdoba, Racing Club, Lanús, Chacarita and Argentinos Juniors.
In 1975 River called Labruna to offer him work as coach. Labruna won two championships that same year, breaking a ""curse"" of 18 years without titles. Labruna's period in charge of River Plate brought the club much domestic success, a side endowed with players such as Daniel Passarella, Norberto Alonso and Leopoldo Luque.


== Personal life ==
Labruna had two sons, Daniel (dead in 1969) and Omar, who worked with Ramón Díaz in River Plate and then coached Olimpo de Bahía Blanca and other teams.
Labruna died on September 19, 1983 from a heart attack, at 64 years old. He is buried at La Chacarita Cemetery in Buenos Aires. Every September 28, River Plate's fans celebrate the ""International River Plate Fan's Day"" as a tribute to one of the club's greatest idols.


== Honours ==


=== Player ===


==== Club ====
River PlatePrimera División: 1941, 1942, 1945, 1947, 1952, 1953, 1955, 1956, 1957
Copa Ibarguren: 1937, 1941, 1942
Copa Adrián C. Escobar: 1941
Copa Aldao: 1941, 1945, 1947


==== International ====
ArgentinaCampeonato Sudamericano: 1946, 1955


=== Manager ===
Rosario CentralPrimera División: Nacional 1971River PlatePrimera División: Metropolitano 1975, Nacional 1975, Metropolitano 1977, Nacional 1979, Metropolitano 1979, Metropolitano 1980


== References ==


== External links ==
 Media related to Angel Labruna at Wikimedia Commons

El Feo Labruna – Tribute webpage (in Spanish)","pandas(index=225, _1=225, text='ángel amadeo labruna, (28 september 1918 – 19 september 1983), was an argentine football player and coach, who played as a forward. with 295 goals scored in official matches, labruna is the 2nd all-time top scorer of primera división after paraguayan arsenio erico. labruna was also part of the celebrated river plate offense, nicknamed la máquina (the machine), and he was considered one of the best south-american footballers of his generation.   == biography == labruna was born in buenos aires, argentina.   == career == rosario centralprimera división: nacional 1971river plateprimera división: metropolitano 1975, nacional 1975, metropolitano 1977, nacional 1979, metropolitano 1979, metropolitano 1980   == references ==   == external links == media related to angel labruna at wikimedia commons  el feo labruna – tribute webpage (in spanish)')"
226,"The CMD – Costruzioni Motori Diesel S.p.A. (en. Construction Diesel Engines) or more simply CMD is an Italian company that designs, develops, builds and markets marine engines, with its brand FNM Marine.


== History ==
The Company was founded in 1971, under the name Fratelli Negri Macchine Diesel Sud (FNM), on the initiative of the Negri brothers. Initially, the activity focused on the revision of earthmoving machines, then expanding in the mid-70s, in the installation of diesel engines on used cars. Furthermore, towards the end of the decade, collaboration with FIAT began, which still represents an important slice of the company's business. In 2013, indeed, the company signed an agreement to produce engine heads for Maserati and Jeep.In 1980, the GD 178 AT 1.3 supercharged diesel engine, entirely designed and built by FNM, was presented at the Turin International Motor Show.
In 1991, CMD Costruzioni Motori Diesel was set up, which would acquire the entire FNM business branch and related know-how. In the 1990s, CMD inaugurated the Atella 1 plant, expanding its business producing and selling marine diesel engines. In the 2000s the company opened two new production plants, Atella 2 in 2004 and Morra De Sanctis in 2005. In 2017 67% of the share capital was held by the Chinese Loncin Holdings group, while the remaining 33% was in Italian hands. The official dealer is AS Labruna which is based in Monopoli, Apulia.


== Engines ==
The company builds various types of marine inboard motor including hybrids.


== Awards ==
Premio ADI (Association for Industrial Design) 2019, at the 59° Salone Nautico di Genova thanks to the engine Blue Hybrid System.


== See also ==
AS Labruna
Inboard motor
Fiat Chrysler Automobiles


== References ==","pandas(index=226, _1=226, text=""the cmd – costruzioni motori diesel s.p.a. (en. construction diesel engines) or more simply cmd is an italian company that designs, develops, builds and markets marine engines, with its brand fnm marine.   == history == the company was founded in 1971, under the name fratelli negri macchine diesel sud (fnm), on the initiative of the negri brothers. initially, the activity focused on the revision of earthmoving machines, then expanding in the mid-70s, in the installation of diesel engines on used cars. furthermore, towards the end of the decade, collaboration with fiat began, which still represents an important slice of the company's business. in 2013, indeed, the company signed an agreement to produce engine heads for maserati and jeep.in 1980, the gd 178 at 1.3 supercharged diesel engine, entirely designed and built by fnm, was presented at the turin international motor show. in 1991, cmd costruzioni motori diesel was set up, which would acquire the entire fnm business branch and related know-how. in the 1990s, cmd inaugurated the atella 1 plant, expanding its business producing and selling marine diesel engines. in the 2000s the company opened two new production plants, atella 2 in 2004 and morra de sanctis in 2005. in 2017 67% of the share capital was held by the chinese loncin holdings group, while the remaining 33% was in italian hands. the official dealer is as labruna which is based in monopoli, apulia.   == engines == the company builds various types of marine inboard motor including hybrids.   == awards == premio adi (association for industrial design) 2019, at the 59° salone nautico di genova thanks to the engine blue hybrid system.   == see also == as labruna inboard motor fiat chrysler automobiles   == references =="")"
227,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.Commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. Incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. For example, asking whether a kilogram is larger than an hour is meaningless.
Any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
The concept of physical dimension, and of dimensional analysis, was introduced by Joseph Fourier in 1822.


== Concrete numbers and base units ==
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof.
A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units.
Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). The newton is defined as 1 N = 1 kg⋅m⋅s−2.


=== Percentages and derivatives ===
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as ""hundredths"", since 1% = 1/100.
Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:

position (x) has the dimension L (length);
derivative of position with respect to time (dx/dt, velocity) has dimension LT−1—length from position, time due to the gradient;
the second derivative (d2x/dt2 = d(dx/dt) / dt, acceleration) has dimension LT−2.In economics, one distinguishes between stocks and flows: a stock has units of ""units"" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of ""units/time"" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.


== Conversion factor ==

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and 100 kPa = 1 bar. The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to 100 kPa / 1 bar = 1.  Since any quantity can be multiplied by 1 without changing it, the expression ""100 kPa / 1 bar"" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, 5 bar × 100 kPa / 1 bar = 500 kPa because 5 × 100 / 1 = 500, and bar/bar cancels out, so 5 bar = 500 kPa.


== Dimensional homogeneity ==

The most basic rule of dimensional analysis is that of dimensional homogeneity.
Only commensurable quantities (physical quantities having the same dimension) may be compared, equated, added, or subtracted.However, the dimensions form an abelian group under multiplication, so:

One may take ratios of incommensurable quantities (quantities with different dimensions), and multiply or divide them.For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometre being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful expression only quantities of the same dimension can be added, subtracted, or compared. For example, if mman, mrat and Lman denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression mman + mrat is meaningful, but the heterogeneous expression mman + Lman is meaningless. However, mman/L2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
This has the implication that most mathematical functions, particularly the transcendental functions, must have a dimensionless quantity, a pure number, as the argument and must return a dimensionless number as a result. This is clear because many transcendental functions can be expressed as an infinite power series with dimensionless coefficients.

  
    
      
        f
        (
        x
        )
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        =
        
          a
          
            0
          
        
        +
        
          a
          
            1
          
        
        x
        +
        
          a
          
            2
          
        
        
          x
          
            2
          
        
        +
        
          a
          
            3
          
        
        
          x
          
            3
          
        
        +
        ⋯
      
    
    {\displaystyle f(x)=\sum _{n=0}^{\infty }a_{n}x^{n}=a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}+\cdots }
  All powers of x must have the same dimension for the terms to be commensurable. But if x is not dimensionless, then the different powers of x will have different, incommensurable dimensions. However, power functions including root functions may have a dimensional argument and will return a result having dimension that is the same power applied to the argument dimension. This is because power functions and root functions are, loosely, just an expression of multiplication of quantities.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension L2MT−2, they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.


== The factor-label method for converting units ==
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:

  
    
      
        
          
            
              10
               
              
                
                  mile
                
              
            
            
              1
               
              
                
                  hour
                
              
            
          
        
        ×
        
          
            
              1609.344
              
                 meter
              
            
            
              1
               
              
                
                  mile
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  hour
                
              
            
            
              3600
              
                 second
              
            
          
        
        =
        4.4704
         
        
          
            meter
            second
          
        
        .
      
    
    {\displaystyle {\frac {10\ {\cancel {\text{mile}}}}{1\ {\cancel {\text{hour}}}}}\times {\frac {1609.344{\text{ meter}}}{1\ {\cancel {\text{mile}}}}}\times {\frac {1\ {\cancel {\text{hour}}}}{3600{\text{ second}}}}=4.4704\ {\frac {\text{meter}}{\text{second}}}.}
  Each conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as ""mile"" is the numerator in the original fraction and 
  
    
      
        1
         
        
          mile
        
        =
        1609.344
         
        
          meter
        
      
    
    {\displaystyle 1\ {\text{mile}}=1609.344\ {\text{meter}}}
  , ""mile"" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields 
  
    
      
        
          
            
              1
               
              
                mile
              
            
            
              1
               
              
                mile
              
            
          
        
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle {\frac {1\ {\text{mile}}}{1\ {\text{mile}}}}={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  , which when simplified results in the dimensionless 
  
    
      
        1
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle 1={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  . Multiplying any quantity (physical quantity or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units mile and hour, 10 miles per hour converts to 4.4704 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., 
  
    
      
        
          
            
              NO
            
            
              x
            
          
        
      
    
    {\displaystyle \color {Blue}{\ce {NO}}_{x}}
  ) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of 
  
    
      
        
          
            NO
          
          
            x
          
        
      
    
    {\displaystyle {\ce {NO}}_{x}}
   by using the following information as shown below:

NOx concentration
= 10 parts per million by volume = 10 ppmv = 10 volumes/106 volumes
NOx molar mass
= 46 kg/kmol = 46 g/mol
Flow rate of flue gas
= 20 cubic meters per minute = 20 m3/min
The flue gas exits the furnace at 0 °C temperature and 101.325 kPa absolute pressure.
The molar volume of a gas at 0 °C temperature and 101.325 kPa is 22.414 m3/kmol.
  
    
      
        
          
            
              1000
               
              
                
                  g
                   
                  NO
                
                
                  x
                
              
            
            
              1
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              46
               
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              22.414
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              10
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              
                10
                
                  6
                
              
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
          
        
        ×
        
          
            
              20
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
            
              1
               
              
                
                  minute
                
              
            
          
        
        ×
        
          
            
              60
               
              
                
                  minute
                
              
            
            
              1
               
              
                hour
              
            
          
        
        =
        24.63
         
        
          
            
              
                g
                 
                NO
              
              
                x
              
            
            hour
          
        
      
    
    {\displaystyle {\frac {1000\ {\ce {g\ NO}}_{x}}{1{\cancel {{\ce {kg\ NO}}_{x}}}}}\times {\frac {46\ {\cancel {{\ce {kg\ NO}}_{x}}}}{1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}}\times {\frac {1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}{22.414\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}}\times {\frac {10\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}{10^{6}\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}}\times {\frac {20\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}{1\ {\cancel {\ce {minute}}}}}\times {\frac {60\ {\cancel {\ce {minute}}}}{1\ {\ce {hour}}}}=24.63\ {\frac {{\ce {g\ NO}}_{x}}{\ce {hour}}}}
  After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.


=== Checking equations that involve dimensions ===
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.
For example, check the Universal Gas Law equation of PV = nRT, when:

the pressure P is in pascals (Pa)
the volume V is in cubic meters (m3)
the amount of substance n is in moles (mol)
the universal gas law constant R is 8.3145 Pa⋅m3/(mol⋅K)
the temperature T is in kelvins (K)
  
    
      
        
          Pa
          ⋅
          
            m
            
              3
            
          
        
        =
        
          
            
              
                mol
              
            
            1
          
        
        ×
        
          
            
              Pa
              ⋅
              
                m
                
                  3
                
              
            
            
              
                
                  
                    mol
                  
                
              
               
              
                
                  
                    K
                  
                
              
            
          
        
        ×
        
          
            
              
                K
              
            
            1
          
        
      
    
    {\displaystyle {\ce {Pa.m^3}}={\frac {\cancel {{\ce {mol}}}}{1}}\times {\frac {{\ce {Pa.m^3}}}{{\cancel {{\ce {mol}}}}\ {\cancel {{\ce {K}}}}}}\times {\frac {\cancel {{\ce {K}}}}{1}}}
  As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions — dimensional  adjusters — that can then be assigned physical significance. It is important to point out that such ‘mathematical manipulation’ is   neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck's constant, a fundamental constant of the universe, was ‘discovered’ as a purely mathematical abstraction or representation that built on the Rayleigh-Jeans Equation for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment – not earlier.


=== Limitations ===
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (
  
    
      
        x
        ↦
        a
        x
        +
        b
      
    
    {\displaystyle x\mapsto ax+b}
  , rather than a linear transform 
  
    
      
        x
        ↦
        a
        x
      
    
    {\displaystyle x\mapsto ax}
  ) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature T[F] in degrees Fahrenheit to a numerical quantity value T[C] in degrees Celsius, this formula may be used:

T[C] = (T[F] − 32) × 5/9.To convert T[C] in degrees Celsius to T[F] in degrees Fahrenheit, this formula may be used:

T[F] = (T[C] × 9/5) + 32.


== Applications ==
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.


=== Mathematics ===
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an n-ball (the solid ball in n dimensions), or the area of its surface, the n-sphere: being an n-dimensional figure, the volume scales as 
  
    
      
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle x^{n},}
   while the surface area, being 
  
    
      
        (
        n
        −
        1
        )
      
    
    {\displaystyle (n-1)}
  -dimensional, scales as 
  
    
      
        
          x
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle x^{n-1}.}
   Thus the volume of the n-ball in terms of the radius is 
  
    
      
        
          C
          
            n
          
        
        
          r
          
            n
          
        
        ,
      
    
    {\displaystyle C_{n}r^{n},}
   for some constant 
  
    
      
        
          C
          
            n
          
        
        .
      
    
    {\displaystyle C_{n}.}
   Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.


=== Finance, economics, and accounting ===
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

For example, the P/E ratio has dimensions of time (units of years), and can be interpreted as ""years of earnings to earn the price paid"".
In economics, debt-to-GDP ratio also has units of years (debt has units of currency, GDP has units of currency/year).
In financial analysis, some bond duration types also have dimension of time (unit of years) and can be interpreted as ”years to balance point between interest payments and nominal repayment”.
Velocity of money has units of 1/years (GDP/money supply has units of currency/year over currency): how often a unit of currency circulates per year.
Interest rates are often expressed as a percentage, but more properly percent per annum, which has dimensions of 1/years.


=== Fluid mechanics ===
In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:

Reynolds number (Re), generally important in all types of fluid problems:

  
    
      
        
          R
          e
        
        =
        
          
            
              ρ
              
              u
              d
            
            μ
          
        
      
    
    {\displaystyle \mathrm {Re} ={\frac {\rho \,ud}{\mu }}}
  .
Froude number (Fr), modeling flow with a free surface:

  
    
      
        
          F
          r
        
        =
        
          
            u
            
              g
              
              L
            
          
        
        .
      
    
    {\displaystyle \mathrm {Fr} ={\frac {u}{\sqrt {g\,L}}}.}
  
Euler number (Eu), used in problems in which pressure is of interest:

  
    
      
        
          E
          u
        
        =
        
          
            
              Δ
              p
            
            
              ρ
              
                u
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle \mathrm {Eu} ={\frac {\Delta p}{\rho u^{2}}}.}
  
Mach number (Ma), important in high speed flows where the velocity approaches or exceeds the local speed of sound:

  
    
      
        
          M
          a
        
        =
        
          
            u
            c
          
        
        ,
      
    
    {\displaystyle \mathrm {Ma} ={\frac {u}{c}},}
   where: c is the local speed of sound.


== History ==
The origins of dimensional analysis have been disputed by historians.The first written application of dimensional analysis has been credited to an article of François Daviet at the Turin Academy of Science. Daviet had the master Lagrange as teacher. 
His fundamental works are contained in acta of the Academy dated 1799.This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p.39). In the second edition of 1833, Poisson explicitly introduces the term dimension instead of the Daviet homogeneity.
In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like F = ma should be independent of the units employed to measure the physical variables.
Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be ""the three fundamental units"", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant G is taken as unity, thereby defining M = L3T−2. By assuming a form of Coulomb's law in which Coulomb's constant ke is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were Q = L3/2M1/2T−1, which, after substituting his M = L3T−2 equation for mass, results in charge having the same dimensions as mass, viz. Q = L3T−2.
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize.  It was used for the first time (Pesic 2005) in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue.  Rayleigh first published the technique in his 1877 book The Theory of Sound.The original meaning of the word dimension, in Fourier's Theorie de la Chaleur, was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT−2, instead of just the exponents.


== Mathematical formulation ==
The Buckingham π theorem describes how every physically meaningful equation involving n variables can be equivalently rewritten as an equation of n − m dimensionless parameters, where m is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.


=== Definition ===
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The dimension of a physical quantity is more fundamental than some scale unit used to express the amount of that physical quantity.  For example, mass is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.
There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity Q is given by 

  
    
      
        
          dim
        
         
        
          Q
        
        =
        
          
            
              L
            
          
          
            a
          
        
        
          
            
              M
            
          
          
            b
          
        
        
          
            
              T
            
          
          
            c
          
        
        
          
            
              I
            
          
          
            d
          
        
        
          
            
              Θ
            
          
          
            e
          
        
        
          
            
              N
            
          
          
            f
          
        
        
          
            
              J
            
          
          
            g
          
        
      
    
    {\displaystyle {\text{dim}}~{Q}={\mathsf {L}}^{a}{\mathsf {M}}^{b}{\mathsf {T}}^{c}{\mathsf {I}}^{d}{\mathsf {\Theta }}^{e}{\mathsf {N}}^{f}{\mathsf {J}}^{g}}
  where a, b, c, d, e, f, g are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electric current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.
As examples, the dimension of the physical quantity speed v is

  
    
      
        
          dim
        
         
        v
        =
        
          
            length
            time
          
        
        =
        
          
            
              L
            
            
              T
            
          
        
        =
        
          
            
              L
              T
            
          
          
            −
            1
          
        
      
    
    {\displaystyle {\text{dim}}~v={\frac {\text{length}}{\text{time}}}={\frac {\mathsf {L}}{\mathsf {T}}}={\mathsf {LT}}^{-1}}
  and the dimension of the physical quantity force F is

  
    
      
        
          dim
        
         
        F
        =
        
          mass
        
        ×
        
          acceleration
        
        =
        
          mass
        
        ×
        
          
            length
            
              
                time
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
            
            
              
                
                  T
                
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
              T
            
          
          
            −
            2
          
        
      
    
    {\displaystyle {\text{dim}}~F={\text{mass}}\times {\text{acceleration}}={\text{mass}}\times {\frac {\text{length}}{{\text{time}}^{2}}}={\frac {\mathsf {ML}}{{\mathsf {T}}^{2}}}={\mathsf {MLT}}^{-2}}
  The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.
There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.


=== Mathematical properties ===

The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; L0 = 1, and the inverse to L is 1/L or L−1. L raised to any rational power p is a member of the group, having an inverse of L−p or 1/Lp.  The operation of the group is multiplication, having the usual rules for handling exponents (Ln × Lm = Ln+m).
This group can be described as a vector space over the rational numbers, with for example dimensional symbol MiLjTk corresponding to the vector (i, j, k). When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., m) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., πm}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity X can be expressed in the general form

  
    
      
        X
        =
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        (
        
          π
          
            i
          
        
        
          )
          
            
              k
              
                i
              
            
          
        
        
        .
      
    
    {\displaystyle X=\prod _{i=1}^{m}(\pi _{i})^{k_{i}}\,.}
  Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

  
    
      
        f
        (
        
          π
          
            1
          
        
        ,
        
          π
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          π
          
            m
          
        
        )
        =
        0
        
        .
      
    
    {\displaystyle f(\pi _{1},\pi _{2},...,\pi _{m})=0\,.}
  Knowing this restriction can be a powerful tool for obtaining new insight into the system.


=== Mechanics ===
The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T2], L, M, while the latter can be expressed as M, L, [T = (ML/F)1/2].
On the other hand, length, velocity and time (L, V, T) do not form a set of base dimensions for mechanics, for two reasons:

There is no way to obtain mass – or anything derived from it, such as force – without introducing another base dimension (thus, they do not span the space).
Velocity, being expressible in terms of length and time (V = L/T), is redundant (the set is not linearly independent).


=== Other fields of physics and chemistry ===
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge.  In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ.  In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ 6.02×1023 mol−1) is defined as a base dimension, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.


=== Polynomials and transcendental functions ===
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities.  (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does not hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials (xn) of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for x2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for x2 + x, the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless.  For example,

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          (
          
            −
            9.8
             
            
              
                meter
                
                  
                    second
                  
                  
                    2
                  
                
              
            
          
          )
        
        ⋅
        
          t
          
            2
          
        
        +
        
          (
          
            500
             
            
              
                meter
                second
              
            
          
          )
        
        ⋅
        t
        .
      
    
    {\displaystyle {\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot t^{2}+\left(500\ {\frac {\text{meter}}{\text{second}}}\right)\cdot t.}
  This is the height to which an object rises in time t if the acceleration of gravity is 9.8 meter per second per second and the initial upward speed is 500 meter per second.  It is not necessary for t to be in seconds.  For example, suppose t = 0.01 minutes.  Then the first term would be

  
    
      
        
          
            
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                
                  (
                  
                    −
                    9.8
                     
                    
                      
                        meter
                        
                          
                            second
                          
                          
                            2
                          
                        
                      
                    
                  
                  )
                
                ⋅
                (
                0.01
                
                   minute
                
                
                  )
                  
                    2
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                
                  
                    (
                    
                      
                        minute
                        second
                      
                    
                    )
                  
                  
                    2
                  
                
                ⋅
                
                  meter
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                ⋅
                
                  60
                  
                    2
                  
                
                ⋅
                
                  meter
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot (0.01{\text{ minute}})^{2}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\left({\frac {\text{minute}}{\text{second}}}\right)^{2}\cdot {\text{meter}}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\cdot 60^{2}\cdot {\text{meter}}.\end{aligned}}}
  


=== Incorporating units ===
The value of a dimensional physical quantity Z is written as the product of a unit [Z] within the dimension and a dimensionless numerical factor, n.

  
    
      
        Z
        =
        n
        ×
        [
        Z
        ]
        =
        n
        [
        Z
        ]
      
    
    {\displaystyle Z=n\times [Z]=n[Z]}
  When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

  
    
      
        1
         
        
          
            ft
          
        
        =
        0.3048
         
        
          
            m
          
        
         
      
    
    {\displaystyle 1\ {\mbox{ft}}=0.3048\ {\mbox{m}}\ }
    is identical to 
  
    
      
        1
        =
        
          
            
              0.3048
               
              
                
                  m
                
              
            
            
              1
               
              
                
                  ft
                
              
            
          
        
        .
         
      
    
    {\displaystyle 1={\frac {0.3048\ {\mbox{m}}}{1\ {\mbox{ft}}}}.\ }
  The factor 
  
    
      
        0.3048
         
        
          
            
              m
            
            
              ft
            
          
        
      
    
    {\displaystyle 0.3048\ {\frac {\mbox{m}}{\mbox{ft}}}}
   is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.


=== Position vs displacement ===

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:

adding two displacements should yield a new displacement (walking ten paces then twenty paces gets you thirty paces forward),
adding a displacement to a position should yield a new position (walking one block down the street from an intersection gets you to the next intersection),
subtracting two positions should yield a displacement,
but one may not add two positions.This illustrates the subtle distinction between affine quantities (ones modeled by an affine space, such as position) and vector quantities (ones modeled by a vector space, such as displacement).

Vector quantities may be added to each other, yielding a new vector quantity, and a vector quantity may be added to a suitable affine quantity (a vector space acts on an affine space), yielding a new affine quantity.
Affine quantities cannot be added, but may be subtracted, yielding relative quantities which are vectors, and these relative differences may then be added to each other or to an affine quantity.Properly then, positions have dimension of affine length, while displacements have dimension of vector length. To assign a number to an affine unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a vector unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,

−273.15 °C ≘ 0 K = 0 °R ≘ −459.67 °F,where the symbol ≘ means corresponds to, since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.
For temperature differences,

1 K = 1 °C ≠ 1 °F = 1 °R.(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.


=== Orientation and frame of reference ===
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a direction. (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.


== Examples ==


=== A simple example: period of a harmonic oscillator ===
What is the period of oscillation T of a mass m attached to an ideal linear spring with spring constant k suspended in gravity of strength g? That period is the solution for T of some dimensionless equation in the variables T, m, k, and g.
The four quantities have the following dimensions:  T  [T];  m  [M]; k [M/T2]; and  g [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, 
  
    
      
        
          G
          
            1
          
        
      
    
    {\displaystyle G_{1}}
   = 
  
    
      
        
          T
          
            2
          
        
        k
        
          /
        
        m
      
    
    {\displaystyle T^{2}k/m}
   [T2 · M/T2 / M = 1], and putting 
  
    
      
        
          G
          
            1
          
        
        =
        C
      
    
    {\displaystyle G_{1}=C}
   for some dimensionless constant C gives the dimensionless equation sought.  The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term ""group"" means ""collection"" rather than mathematical group.  They are often called dimensionless numbers as well.
Note that the variable g does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines g with k, m, and T, because g is the only quantity that involves the dimension L. This implies that in this problem the g is irrelevant. Dimensional analysis can sometimes yield strong statements about the irrelevance of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of g: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way:  
  
    
      
        T
        =
        κ
        
          
            
              
                m
                k
              
            
          
        
      
    
    {\displaystyle T=\kappa {\sqrt {\tfrac {m}{k}}}}
  , for some dimensionless constant κ (equal to 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\sqrt {C}}}
   from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (g, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be ""complete"" – although it still may involve unknown dimensionless constants, such as κ.


=== A more complex example: energy of a vibrating wire ===
Consider the case of a vibrating wire of length ℓ (L) vibrating with an amplitude A (L).  The wire has a linear density ρ (M/L) and is under tension s (ML/T2), and we want to know the energy E (ML2/T2) in the wire.  Let π1 and π2 be two dimensionless products of powers of the variables chosen, given by

  
    
      
        
          
            
              
                
                  π
                  
                    1
                  
                
              
              
                
                =
                
                  
                    E
                    
                      A
                      s
                    
                  
                
              
            
            
              
                
                  π
                  
                    2
                  
                
              
              
                
                =
                
                  
                    ℓ
                    A
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\pi _{1}&={\frac {E}{As}}\\\pi _{2}&={\frac {\ell }{A}}.\end{aligned}}}
  The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation

  
    
      
        F
        
          (
          
            
              
                E
                
                  A
                  s
                
              
            
            ,
            
              
                ℓ
                A
              
            
          
          )
        
        =
        0
        ,
      
    
    {\displaystyle F\left({\frac {E}{As}},{\frac {\ell }{A}}\right)=0,}
  where F is some unknown function, or, equivalently as

  
    
      
        E
        =
        A
        s
        f
        
          (
          
            
              ℓ
              A
            
          
          )
        
        ,
      
    
    {\displaystyle E=Asf\left({\frac {\ell }{A}}\right),}
  where f is some other unknown function.  Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension.  Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function f.  But our experiments are simpler than in the absence of dimensional analysis.  We'd perform none to verify that the energy is proportional to the tension.  Or perhaps we might guess that the energy is proportional to ℓ, and so infer that E = ℓs.  The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex.  Consider, for example, a small pebble sitting on the bed of a river.  If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water.  At what critical velocity will this occur?  Sorting out the guessed variables is not so easy as before.  But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.


=== A third example: demand versus capacity for a rotating disc ===

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness t (L) and radius R (L).  The disc has a density ρ (M/L3), rotates at an angular velocity ω (T−1) and this leads to a stress S (ML−1T−2) in the material.  There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid.  As the disc becomes thicker relative to the radius then the plane stress solution breaks down.  If the disc is restrained axially on its free faces then a state of plane strain will occur.  However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case.  An engineer might, therefore, be interested in establishing a relationship between the five variables.  Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:

demand/capacity = ρR2ω2/S
thickness/radius or aspect ratio = t/RThrough the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure.  As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs


== Extensions ==


=== Huntley's extension: directed dimensions and quantity of matter ===
Huntley (Huntley 1967) has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank 
  
    
      
        m
      
    
    {\displaystyle m}
   of the dimensional matrix. He introduced two approaches to doing so:

The magnitudes of the components of a vector are to be considered dimensionally independent. For example, rather than an undifferentiated length dimension L, we may have Lx represent dimension in the x-direction, and so forth. This requirement stems ultimately from the requirement that each component of a physically meaningful equation (scalar, vector, or tensor) must be dimensionally consistent.
Mass as a measure of the quantity of matter is to be considered dimensionally independent from mass as a measure of inertia.As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   and a horizontal velocity component 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
  , both dimensioned as LT−1, R, the distance travelled, having dimension L, and g the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range R may be written:

  
    
      
        R
        ∝
        
          V
          
            x
          
          
            a
          
        
        
        
          V
          
            y
          
          
            b
          
        
        
        
          g
          
            c
          
        
        .
        
      
    
    {\displaystyle R\propto V_{\text{x}}^{a}\,V_{\text{y}}^{b}\,g^{c}.\,}
  Or dimensionally

  
    
      
        
          
            L
          
        
        =
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            a
            +
            b
          
        
        
          
            (
            
              
                
                  L
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
        
      
    
    {\displaystyle {\mathsf {L}}=\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{a+b}\left({\frac {\mathsf {L}}{{\mathsf {T}}^{2}}}\right)^{c}\,}
  from which we may deduce that 
  
    
      
        a
        +
        b
        +
        c
        =
        1
      
    
    {\displaystyle a+b+c=1}
   and 
  
    
      
        a
        +
        b
        +
        2
        c
        =
        0
      
    
    {\displaystyle a+b+2c=0}
  , which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
   will be dimensioned as LxT−1, 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   as LyT−1, R as Lx and g as LyT−2. The dimensional equation becomes:

  
    
      
        
          
            
              L
            
          
          
            
              x
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      x
                    
                  
                
                
                  T
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
      
    
    {\displaystyle {\mathsf {L}}_{\mathrm {x} }=\left({\frac {{\mathsf {L}}_{\mathrm {x} }}{\mathsf {T}}}\right)^{a}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{\mathsf {T}}}\right)^{b}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{{\mathsf {T}}^{2}}}\right)^{c}}
  and we may solve completely as 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
  , 
  
    
      
        b
        =
        1
      
    
    {\displaystyle b=1}
   and 
  
    
      
        c
        =
        −
        1
      
    
    {\displaystyle c=-1}
  . The increase in deductive power gained by the use of directed length dimensions is apparent.
In his second approach, Huntley holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity (a) proportional to inertial mass, but (b) not implicating inertial properties. No further restrictions are added to its definition.
For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables

  
    
      
        
          
            
              m
              ˙
            
          
        
      
    
    {\displaystyle {\dot {m}}}
   the mass flow rate with dimension MT−1

  
    
      
        
          p
          
            x
          
        
      
    
    {\displaystyle p_{\text{x}}}
   the pressure gradient along the pipe with dimension ML−2T−2
ρ the density with dimension ML−3
η the dynamic fluid viscosity with dimension ML−1T−1
r the radius of the pipe with dimension LThere are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be 
  
    
      
        
          π
          
            1
          
        
        =
        
          
            
              m
              ˙
            
          
        
        
          /
        
        η
        r
      
    
    {\displaystyle \pi _{1}={\dot {m}}/\eta r}
   and 
  
    
      
        
          π
          
            2
          
        
        =
        
          p
          
            
              x
            
          
        
        ρ
        
          r
          
            5
          
        
        
          /
        
        
          
            
              
                m
                ˙
              
            
          
          
            2
          
        
      
    
    {\displaystyle \pi _{2}=p_{\mathrm {x} }\rho r^{5}/{\dot {m}}^{2}}
   and we may express the dimensional equation as

  
    
      
        C
        =
        
          π
          
            1
          
        
        
          π
          
            2
          
          
            a
          
        
        =
        
          (
          
            
              
                
                  m
                  ˙
                
              
              
                η
                r
              
            
          
          )
        
        
          
            (
            
              
                
                  
                    p
                    
                      
                        x
                      
                    
                  
                  ρ
                  
                    r
                    
                      5
                    
                  
                
                
                  
                    
                      
                        m
                        ˙
                      
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
      
    
    {\displaystyle C=\pi _{1}\pi _{2}^{a}=\left({\frac {\dot {m}}{\eta r}}\right)\left({\frac {p_{\mathrm {x} }\rho r^{5}}{{\dot {m}}^{2}}}\right)^{a}}
  where C and a are undetermined constants. If we draw a distinction between inertial mass with dimension 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{\text{i}}}
   and quantity of matter with dimension 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{\text{m}}}
  , then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

  
    
      
        C
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              ρ
              
                r
                
                  4
                
              
            
            
              η
              
                
                  
                    m
                    ˙
                  
                
              
            
          
        
      
    
    {\displaystyle C={\frac {p_{\mathrm {x} }\rho r^{4}}{\eta {\dot {m}}}}}
  where now only C is an undetermined constant (found to be equal to 
  
    
      
        π
        
          /
        
        8
      
    
    {\displaystyle \pi /8}
   by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements (a) and (b) he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.
Huntley's concept of directed length dimensions however has some serious limitations:

It does not deal well with vector equations involving the cross product,
nor does it handle well the use of angles as physical variables.It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the ""symmetry"" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of ""symmetry"" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?
Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.


=== Siano's extension: orientational analysis ===
Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin (x, y) = (0, 0) at a speed v and angle θ above the x-axis, with the force of gravity directed along the negative y-axis. It is desired to find the range R, at which point the mass returns to the x-axis. Conventional analysis will yield the dimensionless variable π = R g/v2, but offers no insight into the relationship between R and θ.
Siano (1985-I, 1985-II) has suggested that the directed dimensions of Huntley be replaced by using orientational symbols 1x 1y 1z to denote vector directions, and an orientationless symbol 10. Thus, Huntley's Lx becomes L 1x with L specifying the dimension of length, and 1x specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own.  Along with the requirement that 1i−1 = 1i, the following multiplication table for the orientation symbols results:

  
    
      
        
          
            
              
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  
                    1
                    
                      z
                    
                  
                
              
            
            
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
            
            
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
            
            
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
            
            
              
                
                  
                    1
                    
                      z
                    
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{c|cccc}&\mathbf {1_{0}} &\mathbf {1_{\text{x}}} &\mathbf {1_{\text{y}}} &\mathbf {1_{\text{z}}} \\\hline \mathbf {1_{0}} &1_{0}&1_{\text{x}}&1_{\text{y}}&1_{\text{z}}\\\mathbf {1_{\text{x}}} &1_{\text{x}}&1_{0}&1_{\text{z}}&1_{\text{y}}\\\mathbf {1_{\text{y}}} &1_{\text{y}}&1_{\text{z}}&1_{0}&1_{\text{x}}\\\mathbf {1_{\text{z}}} &1_{\text{z}}&1_{\text{y}}&1_{\text{x}}&1_{0}\end{array}}}
  Note that the orientational symbols form a group (the Klein four-group or ""Viergruppe""). In this system, scalars always have the same orientation as the identity element, independent of the ""symmetry of the problem"".  Physical quantities that are vectors have the orientation expected:  a force or a velocity in the z-direction has the orientation of 1z.  For angles, consider an angle θ that lies in the z-plane.  Form a right triangle in the z-plane with θ being one of the acute angles.  The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y.  Since (using ~ to indicate orientational equivalence) tan(θ) = θ + ... ~ 1y/1x we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable.  Analogous reasoning forces the conclusion that sin(θ) has orientation 1z while cos(θ) has orientation 10.  These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form a cos(θ) + b sin(θ), where a and b are real scalars. Note that an expression such as 
  
    
      
        sin
        ⁡
        (
        θ
        +
        π
        
          /
        
        2
        )
        =
        cos
        ⁡
        (
        θ
        )
      
    
    {\displaystyle \sin(\theta +\pi /2)=\cos(\theta )}
   is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:

  
    
      
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            +
            b
            
            
              1
              
                z
              
            
          
          )
        
        =
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            b
            
            
              1
              
                z
              
            
          
          )
        
        +
        sin
        ⁡
        
          (
          
            b
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            a
            
            
              1
              
                z
              
            
          
          )
        
        ,
      
    
    {\displaystyle \sin \left(a\,1_{\text{z}}+b\,1_{\text{z}}\right)=\sin \left(a\,1_{\text{z}})\cos(b\,1_{\text{z}}\right)+\sin \left(b\,1_{\text{z}})\cos(a\,1_{\text{z}}\right),}
  which for 
  
    
      
        a
        =
        θ
      
    
    {\displaystyle a=\theta }
   and 
  
    
      
        b
        =
        π
        
          /
        
        2
      
    
    {\displaystyle b=\pi /2}
   yields 
  
    
      
        sin
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        +
        [
        π
        
          /
        
        2
        ]
        
        
          1
          
            z
          
        
        )
        =
        
          1
          
            z
          
        
        cos
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        )
      
    
    {\displaystyle \sin(\theta \,1_{\text{z}}+[\pi /2]\,1_{\text{z}})=1_{\text{z}}\cos(\theta \,1_{\text{z}})}
  . Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is 
  
    
      
        
          1
          
            0
          
        
      
    
    {\displaystyle 1_{0}}
  .
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems.  In this approach one sets up the dimensional equation and solves it as far as one can.  If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral.  This puts it into ""normal form"".  The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension 1z and the range of the projectile R will be of the form:

  
    
      
        R
        =
        
          g
          
            a
          
        
        
        
          v
          
            b
          
        
        
        
          θ
          
            c
          
        
        
           which means 
        
        
          
            L
          
        
        
        
          1
          
            
              x
            
          
        
        ∼
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                  
                    1
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
        
          1
          
            
              z
            
          
          
            c
          
        
        .
        
      
    
    {\displaystyle R=g^{a}\,v^{b}\,\theta ^{c}{\text{ which means }}{\mathsf {L}}\,1_{\mathrm {x} }\sim \left({\frac {{\mathsf {L}}\,1_{\text{y}}}{{\mathsf {T}}^{2}}}\right)^{a}\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{b}\,1_{\mathsf {z}}^{c}.\,}
  Dimensional homogeneity will now correctly yield a = −1 and b = 2, and orientational homogeneity requires that 
  
    
      
        
          1
          
            x
          
        
        
          /
        
        (
        
          1
          
            y
          
          
            a
          
        
        
          1
          
            z
          
          
            c
          
        
        )
        =
        
          1
          
            z
          
          
            c
            +
            1
          
        
        =
        1
      
    
    {\displaystyle 1_{x}/(1_{y}^{a}1_{z}^{c})=1_{z}^{c+1}=1}
  . In other words, that c must be an odd integer. In fact the required function of theta will be sin(θ)cos(θ) which is a series consisting of odd powers of θ.
It is seen that the Taylor series of sin(θ) and cos(θ) are orientationally homogeneous using the above multiplication table, while expressions like cos(θ) + sin(θ) and exp(θ) are not, and are (correctly) deemed unphysical.
Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.


== Dimensionless concepts ==


=== Constants ===

The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
   in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation.  Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity.  This observation can allow one to sometimes make ""back of the envelope"" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.


=== Formalisms ===
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
   ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on ""dimensional grounds"" that the non-analytical part of the free energy per lattice site should be 
  
    
      
        ∼
        1
        
          /
        
        
          ξ
          
            d
          
        
      
    
    {\displaystyle \sim 1/\xi ^{d}}
   where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: c, ħ, and G, in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants ħ, c, and G (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit 
  
    
      
        c
        →
        ∞
      
    
    {\displaystyle c\rightarrow \infty }
  ,  
  
    
      
        ℏ
        →
        0
      
    
    {\displaystyle \hbar \rightarrow 0}
   and 
  
    
      
        G
        →
        0
      
    
    {\displaystyle G\rightarrow 0}
  . In problems involving a gravitational field the latter limit should be taken such that the field stays finite.


== Dimensional equivalences ==
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.


=== SI units ===


=== Natural units ===

If c = ħ = 1, where c is the speed of light and ħ is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length L, mass M and time T can be expressed (dimensionally) as a power of energy E, because length, mass and time can be expressed using speed v, action S, and energy E:

  
    
      
        M
        =
        
          
            E
            
              v
              
                2
              
            
          
        
        ,
        
        L
        =
        
          
            
              S
              v
            
            E
          
        
        ,
        
        t
        =
        
          
            S
            E
          
        
      
    
    {\displaystyle M={\frac {E}{v^{2}}},\quad L={\frac {Sv}{E}},\quad t={\frac {S}{E}}}
  though speed and action are dimensionless (v = c = 1 and S = ħ = 1) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:

  
    
      
        
          
            
              E
            
          
          
            n
          
        
        =
        
          
            
              M
            
          
          
            p
          
        
        
          
            
              L
            
          
          
            q
          
        
        
          
            
              T
            
          
          
            r
          
        
        =
        
          
            
              E
            
          
          
            p
            −
            q
            −
            r
          
        
      
    
    {\displaystyle {\mathsf {E}}^{n}={\mathsf {M}}^{p}{\mathsf {L}}^{q}{\mathsf {T}}^{r}={\mathsf {E}}^{p-q-r}}
  This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge e though other choices are possible.


== See also ==
Buckingham π theorem
Dimensionless numbers in fluid mechanics
Fermi estimate — used to teach dimensional analysis
Rayleigh's method of dimensional analysis
Similitude (model) — an application of dimensional analysis
System of measurement


=== Related areas of math ===
Covariance and contravariance of vectors
Exterior algebra
Geometric algebra
Quantity calculus


=== Programming languages ===
Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML,  and later in F#. There are implementations for Haskell, OCaml, and Rust, Python, and a code checker for Fortran.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.


== Notes ==


== References ==
Barenblatt, G. I. (1996), Scaling, Self-Similarity, and Intermediate Asymptotics, Cambridge, UK: Cambridge University Press, ISBN 978-0-521-43522-2
Bhaskar, R.; Nigam, Anil (1990), ""Qualitative Physics Using Dimensional Analysis"", Artificial Intelligence, 45 (1–2): 73–111, doi:10.1016/0004-3702(90)90038-2
Bhaskar, R.; Nigam, Anil (1991), ""Qualitative Explanations of Red Giant Formation"", The Astrophysical Journal, 372: 592–6, Bibcode:1991ApJ...372..592B, doi:10.1086/170003
Boucher; Alves (1960), ""Dimensionless Numbers"", Chemical Engineering Progress, 55: 55–64
Bridgman, P. W. (1922), Dimensional Analysis, Yale University Press, ISBN 978-0-548-91029-0
Buckingham, Edgar (1914), ""On Physically Similar Systems: Illustrations of the Use of Dimensional Analysis"", Physical Review, 4 (4): 345–376, Bibcode:1914PhRv....4..345B, doi:10.1103/PhysRev.4.345, hdl:10338.dmlcz/101743
Drobot, S. (1953–1954), ""On the foundations of dimensional analysis"" (PDF), Studia Mathematica, 14: 84–99, doi:10.4064/sm-14-1-84-99
Gibbings, J.C. (2011), Dimensional Analysis, Springer, ISBN 978-1-84996-316-9
Hart, George W. (1994), ""The theory of dimensioned matrices"",  in Lewis, John G. (ed.), Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, SIAM, pp. 186–190, ISBN 978-0-89871-336-7 As postscript
Hart, George W. (1995), Multidimensional Analysis: Algebras and Systems for Science and Engineering, Springer-Verlag, ISBN 978-0-387-94417-3
Huntley, H. E. (1967), Dimensional Analysis, Dover, LOC 67-17978
Klinkenberg, A. (1955), ""Dimensional systems and systems of units in physics with special reference to chemical engineering: Part I. The principles according to which dimensional systems and systems of units are constructed"", Chemical Engineering Science, 4 (3): 130–140, 167–177, doi:10.1016/0009-2509(55)80004-8
Langhaar, Henry L. (1951), Dimensional Analysis and Theory of Models, Wiley, ISBN 978-0-88275-682-0
Mendez, P.F.; Ordóñez, F. (September 2005), ""Scaling Laws From Statistical Data and Dimensional Analysis"", Journal of Applied Mechanics, 72 (5): 648–657, Bibcode:2005JAM....72..648M, CiteSeerX 10.1.1.422.610, doi:10.1115/1.1943434
Moody, L. F. (1944), ""Friction Factors for Pipe Flow"", Transactions of the American Society of Mechanical Engineers, 66 (671)
Murphy, N. F. (1949), ""Dimensional Analysis"", Bulletin of the Virginia Polytechnic Institute, 42 (6)
Perry, J. H.;  et al. (1944), ""Standard System of Nomenclature for Chemical Engineering Unit Operations"", Transactions of the American Institute of Chemical Engineers, 40 (251)
Pesic, Peter (2005), Sky in a Bottle, MIT Press, pp. 227–8, ISBN 978-0-262-16234-0
Petty, G. W. (2001), ""Automated computation and consistency checking of physical dimensions and units in scientific programs"", Software – Practice and Experience, 31 (11): 1067–76, doi:10.1002/spe.401, S2CID 206506776
Porter, Alfred W. (1933), The Method of Dimensions (3rd ed.), Methuen
J. W. Strutt (3rd Baron Rayleigh) (1915), ""The Principle of Similitude"", Nature, 95 (2368): 66–8, Bibcode:1915Natur..95...66R, doi:10.1038/095066c0
Siano, Donald (1985), ""Orientational Analysis – A Supplement to Dimensional Analysis – I"", Journal of the Franklin Institute, 320 (6): 267–283, doi:10.1016/0016-0032(85)90031-6
Siano, Donald (1985), ""Orientational Analysis, Tensor Analysis and The Group Properties of the SI Supplementary Units – II"", Journal of the Franklin Institute, 320 (6): 285–302, doi:10.1016/0016-0032(85)90032-8
Silberberg, I. H.; McKetta, J. J. Jr. (1953), ""Learning How to Use Dimensional Analysis"", Petroleum Refiner, 32 (4): 5, (5): 147, (6): 101, (7): 129
Van Driest, E. R. (March 1946), ""On Dimensional Analysis and the Presentation of Data in Fluid Flow Problems"", Journal of Applied Mechanics, 68 (A–34)
Whitney, H. (1968), ""The Mathematics of Physical Quantities, Parts I and II"", American Mathematical Monthly, 75 (2): 115–138, 227–256, doi:10.2307/2315883, JSTOR 2315883
Vignaux, GA (1992),  Erickson, Gary J.; Neudorfer, Paul O. (eds.), Dimensional Analysis in Data Modelling, Kluwer Academic, ISBN 978-0-7923-2031-9
Kasprzak, Wacław; Lysik, Bertold; Rybaczuk, Marek (1990), Dimensional Analysis in the Identification of Mathematical Models, World Scientific, ISBN 978-981-02-0304-7


== External links ==
List of dimensions for variety of physical quantities
Unicalc Live web calculator doing units conversion by dimensional analysis
A C++ implementation of compile-time dimensional analysis in the Boost open-source libraries
Buckingham’s pi-theorem
Quantity System calculator for units conversion based on dimensional approach
Units, quantities, and fundamental constants project dimensional analysis maps
Bowley, Roger (2009). ""[ ] Dimensional Analysis"". Sixty Symbols. Brady Haran for the University of Nottingham.
Dureisseix, David (2019). An introduction to dimensional analysis (lecture). INSA Lyon.


=== Converting units ===
Unicalc Live web calculator doing units conversion by dimensional analysis
Math Skills Review
U.S. EPA tutorial
A Discussion of Units
Short Guide to Unit Conversions
Canceling Units Lesson
Chapter 11: Behavior of Gases Chemistry: Concepts and Applications, Denton Independent School District
Air Dispersion Modeling Conversions and Formulas
www.gnu.org/software/units  free program, very practical","pandas(index=227, _1=227, text='in engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. the conversion of units from one dimensional unit to another is often easier within the metric or si system than in others, due to the regular 10-base in all units. dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. for example, asking whether a kilogram is larger than an hour is meaningless. any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. it also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation. the concept of physical dimension, and of dimensional analysis, was introduced by joseph fourier in 1822.   == concrete numbers and base units == many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof. a set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. for example, units for length and time are normally chosen as base units. units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units. sometimes the names of units obscure the fact that they are derived units. for example, a newton (n) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). the newton is defined as 1 n = 1 kg⋅m⋅s−2. unicalc live web calculator doing units conversion by dimensional analysis math skills review u.s. epa tutorial a discussion of units short guide to unit conversions canceling units lesson chapter 11: behavior of gases chemistry: concepts and applications, denton independent school district air dispersion modeling conversions and formulas www.gnu.org/software/units  free program, very practical')"
228,"In physics and thermodynamics, an equation of state is a thermodynamic equation relating state variables which describe the state of matter under a given set of physical conditions, such as pressure, volume, temperature (PVT), or internal energy. Equations of state are useful in describing the properties of fluids, mixtures of fluids, solids, and the interior of stars.


== Overview ==
At present, there is no single equation of state that accurately predicts the properties of all substances under all conditions. An example of an equation of state correlates densities of gases and liquids to temperatures and pressures, known as the ideal gas law, which is roughly accurate for weakly polar gases at low pressures and moderate temperatures. This equation becomes increasingly inaccurate at higher pressures and lower temperatures, and fails to predict condensation from a gas to a liquid.
Another common use is in modeling the interior of stars, including neutron stars, dense matter (quark–gluon plasmas) and radiation fields. A related concept is the perfect fluid equation of state used in cosmology.
Equations of state can also describe solids, including the transition of solids from one crystalline state to another.
In a practical context, equations of state are instrumental for PVT calculations in process engineering problems, such as petroleum gas/liquid equilibrium calculations. A successful PVT model based on a fitted equation of state can be helpful to determine the state of the flow regime, the parameters for handling the reservoir fluids, and pipe sizing.
Measurements of equation-of-state parameters, especially at high pressures, can be made using lasers.


== Historical ==


=== Boyle's law (1662) ===
Boyle's Law was perhaps the first expression of an equation of state. In 1662, the Irish physicist and chemist Robert Boyle performed a series of experiments employing a J-shaped glass tube, which was sealed on one end. Mercury was added to the tube, trapping a fixed quantity of air in the short, sealed end of the tube. Then the volume of gas was measured as additional mercury was added to the tube. The pressure of the gas could be determined by the difference between the mercury level in the short end of the tube and that in the long, open end. Through these experiments, Boyle noted that the gas volume varied inversely with the pressure. In mathematical form, this can be stated as:

  
    
      
        p
        V
        =
        
          c
          o
          n
          s
          t
          a
          n
          t
        
        .
        
        
      
    
    {\displaystyle pV=\mathrm {constant} .\,\!}
  The above relationship has also been attributed to Edme Mariotte and is sometimes referred to as Mariotte's law. However, Mariotte's work was not published until 1676.


=== Charles's law or Law of Charles and Gay-Lussac (1787) ===
In 1787 the French physicist Jacques Charles found that oxygen, nitrogen, hydrogen, carbon dioxide, and air expand to roughly the same extent over the same 80-kelvin interval. Later, in 1802, Joseph Louis Gay-Lussac published results of similar experiments, indicating a linear relationship between volume and temperature (Charles's Law):

  
    
      
        
          
            
              V
              
                1
              
            
            
              T
              
                1
              
            
          
        
        =
        
          
            
              V
              
                2
              
            
            
              T
              
                2
              
            
          
        
        .
      
    
    {\displaystyle {\frac {V_{1}}{T_{1}}}={\frac {V_{2}}{T_{2}}}.}
  


=== Dalton's law of partial pressures (1801) ===
Dalton's Law of partial pressure states that the pressure of a mixture of gases is equal to the sum of the pressures of all of the constituent gases alone.
Mathematically, this can be represented for n species as:

  
    
      
        
          p
          
            total
          
        
        =
        
          p
          
            1
          
        
        +
        
          p
          
            2
          
        
        +
        ⋯
        +
        
          p
          
            n
          
        
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          p
          
            i
          
        
        .
      
    
    {\displaystyle p_{\text{total}}=p_{1}+p_{2}+\cdots +p_{n}=\sum _{i=1}^{n}p_{i}.}
  


=== The ideal gas law (1834) ===
In 1834, Émile Clapeyron combined Boyle's Law and Charles' law into the first statement of the ideal gas law. Initially, the law was formulated as pVm = R(TC + 267) (with temperature expressed in degrees Celsius), where R is the gas constant. However, later work revealed that the number should actually be closer to 273.2, and then the Celsius scale was defined with 0 °C = 273.15 K, giving:

  
    
      
        p
        
          V
          
            m
          
        
        =
        R
        
          (
          
            
              T
              
                C
              
            
            +
            273.15
             
            
              

              
              
                ∘
              
            
            
              C
            
          
          )
        
        .
      
    
    {\displaystyle pV_{m}=R\left(T_{C}+273.15\ {}^{\circ }{\text{C}}\right).}
  


=== Van der Waals equation of state (1873) ===
In 1873, J. D. van der Waals introduced the first equation of state derived by the assumption of a finite volume occupied by the constituent molecules.  His new formula revolutionized the study of equations of state, and was most famously continued via the Redlich–Kwong equation of state and the Soave modification of Redlich-Kwong.


== General form of an equation of state ==
For a given amount of substance contained in a system, the temperature, volume, and pressure are not independent quantities; they are connected by a relationship of the general form

  
    
      
        f
        (
        p
        ,
        V
        ,
        T
        )
        =
        0
      
    
    {\displaystyle f(p,V,T)=0}
  An equation used to model this relationship is called an equation of state. In the following sections major equations of state are described, and the variables used here are defined as follows. Any consistent set of units may be used, although SI units are preferred. Absolute temperature refers to use of the Kelvin (K) or Rankine (°R) temperature scales, with zero being absolute zero.

  
    
      
         
        p
      
    
    {\displaystyle \ p}
  , pressure (absolute)

  
    
      
         
        V
      
    
    {\displaystyle \ V}
  , volume

  
    
      
         
        n
      
    
    {\displaystyle \ n}
  , number of moles of a substance

  
    
      
         
        
          V
          
            m
          
        
      
    
    {\displaystyle \ V_{m}}
  , 
  
    
      
        
          
            V
            n
          
        
      
    
    {\displaystyle {\frac {V}{n}}}
  , molar volume, the volume of 1 mole of gas or liquid

  
    
      
         
        T
      
    
    {\displaystyle \ T}
  , absolute temperature

  
    
      
         
        R
      
    
    {\displaystyle \ R}
  , ideal gas constant ≈ 8.3144621 J/mol·K

  
    
      
         
        
          p
          
            c
          
        
      
    
    {\displaystyle \ p_{c}}
  , pressure at the critical point

  
    
      
         
        
          V
          
            c
          
        
      
    
    {\displaystyle \ V_{c}}
  , molar volume at the critical point

  
    
      
         
        
          T
          
            c
          
        
      
    
    {\displaystyle \ T_{c}}
  , absolute temperature at the critical point


== Classical ideal gas law ==
The classical ideal gas law may be written

  
    
      
        p
        V
        =
        n
        R
        T
        .
      
    
    {\displaystyle pV=nRT.}
  In the form shown above, the equation of state is thus

  
    
      
        f
        (
        p
        ,
        V
        ,
        T
        )
        =
        p
        V
        −
        n
        R
        T
        =
        0
      
    
    {\displaystyle f(p,V,T)=pV-nRT=0}
  .If the calorically perfect gas approximation is used, then the ideal gas law may also be expressed as follows

  
    
      
        p
        =
        ρ
        (
        γ
        −
        1
        )
        e
      
    
    {\displaystyle p=\rho (\gamma -1)e}
  where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
    is the density, 
  
    
      
        γ
        =
        
          C
          
            p
          
        
        
          /
        
        
          C
          
            v
          
        
      
    
    {\displaystyle \gamma =C_{p}/C_{v}}
   is the adiabatic index (ratio of specific heats), 
  
    
      
        e
        =
        
          C
          
            v
          
        
        T
      
    
    {\displaystyle e=C_{v}T}
   is the internal energy per unit mass (the ""specific internal energy""), 
  
    
      
        
          C
          
            v
          
        
      
    
    {\displaystyle C_{v}}
   is the specific heat at constant volume, and 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   is the specific heat at constant pressure.


== Quantum ideal gas law ==
Since for atomic and molecular gases, the classical ideal gas law is well suited in most cases, let us describe the equation of state for elementary particles with mass 
  
    
      
        m
      
    
    {\displaystyle m}
   and spin 
  
    
      
        s
      
    
    {\displaystyle s}
   that takes into account of quantum effects. In the following, the upper sign will always correspond to Fermi-Dirac statistics and the lower sign to Bose–Einstein statistics. The equation of state of such gases with 
  
    
      
        N
      
    
    {\displaystyle N}
   particles occupying a volume 
  
    
      
        V
      
    
    {\displaystyle V}
   with temperature 
  
    
      
        T
      
    
    {\displaystyle T}
   and pressure 
  
    
      
        p
      
    
    {\displaystyle p}
   is given by

  
    
      
        p
        =
        
          
            
              (
              2
              s
              +
              1
              )
              
                
                  2
                  
                    m
                    
                      3
                    
                  
                  
                    k
                    
                      B
                    
                    
                      5
                    
                  
                  
                    T
                    
                      5
                    
                  
                
              
            
            
              3
              
                π
                
                  2
                
              
              
                ℏ
                
                  3
                
              
            
          
        
        
          ∫
          
            0
          
          
            ∞
          
        
        
          
            
              
                z
                
                  3
                  
                    /
                  
                  2
                
              
              
              
                d
              
              z
            
            
              
                e
                
                  z
                  −
                  μ
                  
                    /
                  
                  (
                  
                    k
                    
                      B
                    
                  
                  T
                  )
                
              
              ±
              1
            
          
        
      
    
    {\displaystyle p={\frac {(2s+1){\sqrt {2m^{3}k_{B}^{5}T^{5}}}}{3\pi ^{2}\hbar ^{3}}}\int _{0}^{\infty }{\frac {z^{3/2}\,\mathrm {d} z}{e^{z-\mu /(k_{B}T)}\pm 1}}}
  where 
  
    
      
        
          k
          
            B
          
        
      
    
    {\displaystyle k_{B}}
   is the Boltzmann constant and 
  
    
      
        μ
        (
        T
        ,
        N
        
          /
        
        V
        )
      
    
    {\displaystyle \mu (T,N/V)}
   the chemical potential is given by the following implicit function

  
    
      
        
          
            N
            V
          
        
        =
        
          
            
              (
              2
              s
              +
              1
              )
              (
              m
              
                k
                
                  B
                
              
              T
              
                )
                
                  3
                  
                    /
                  
                  2
                
              
            
            
              
                
                  2
                
              
              
                π
                
                  2
                
              
              
                ℏ
                
                  3
                
              
            
          
        
        
          ∫
          
            0
          
          
            ∞
          
        
        
          
            
              
                z
                
                  1
                  
                    /
                  
                  2
                
              
              
              
                d
              
              z
            
            
              
                e
                
                  z
                  −
                  μ
                  
                    /
                  
                  (
                  
                    k
                    
                      B
                    
                  
                  T
                  )
                
              
              ±
              1
            
          
        
        .
      
    
    {\displaystyle {\frac {N}{V}}={\frac {(2s+1)(mk_{B}T)^{3/2}}{{\sqrt {2}}\pi ^{2}\hbar ^{3}}}\int _{0}^{\infty }{\frac {z^{1/2}\,\mathrm {d} z}{e^{z-\mu /(k_{B}T)}\pm 1}}.}
  In the limiting case where 
  
    
      
        
          e
          
            μ
            
              /
            
            (
            
              k
              
                B
              
            
            T
            )
          
        
        ≪
        1
      
    
    {\displaystyle e^{\mu /(k_{B}T)}\ll 1}
  , this equation of state will reduce to that of the classical ideal gas. It can be shown that the above equation of state in the limit 
  
    
      
        
          e
          
            μ
            
              /
            
            (
            
              k
              
                B
              
            
            T
            )
          
        
        ≪
        1
      
    
    {\displaystyle e^{\mu /(k_{B}T)}\ll 1}
   reduces to

  
    
      
        p
        V
        =
        N
        
          k
          
            B
          
        
        T
        
          [
          
            1
            ±
            
              
                
                  π
                  
                    3
                    
                      /
                    
                    2
                  
                
                
                  2
                  (
                  2
                  s
                  +
                  1
                  )
                
              
            
            
              
                
                  N
                  
                    ℏ
                    
                      3
                    
                  
                
                
                  V
                  (
                  m
                  
                    k
                    
                      B
                    
                  
                  T
                  
                    )
                    
                      3
                      
                        /
                      
                      2
                    
                  
                
              
            
            +
            ⋯
          
          ]
        
      
    
    {\displaystyle pV=Nk_{B}T\left[1\pm {\frac {\pi ^{3/2}}{2(2s+1)}}{\frac {N\hbar ^{3}}{V(mk_{B}T)^{3/2}}}+\cdots \right]}
  With a fixed number density 
  
    
      
        N
        
          /
        
        V
      
    
    {\displaystyle N/V}
  , decreasing the temperature causes in Fermi gas, a increase in  the value for pressure from its classical value implying an effective repulsion between particles (this is an apparent repulsion due to quantum exchange effects not because of actual interactions between particles since in ideal gas, interactional forces are neglected) and in Bose gas, a decrease in pressure from its classical value implying an effective attraction.


== Cubic equations of state ==
Cubic equations of state are called such because they can be rewritten as a cubic function of 
  
    
      
        
          V
          
            m
          
        
      
    
    {\displaystyle V_{m}}
  .


=== Van der Waals equation of state ===
The Van der Waals equation of state may be written:

  
    
      
        
          (
          
            p
            +
            
              
                a
                
                  V
                  
                    m
                  
                  
                    2
                  
                
              
            
          
          )
        
        
          (
          
            
              V
              
                m
              
            
            −
            b
          
          )
        
        =
        R
        T
      
    
    {\displaystyle \left(p+{\frac {a}{V_{m}^{2}}}\right)\left(V_{m}-b\right)=RT}
  where 
  
    
      
        
          V
          
            m
          
        
      
    
    {\displaystyle V_{m}}
   is molar volume. The substance-specific constants 
  
    
      
        a
      
    
    {\displaystyle a}
   and 
  
    
      
        b
      
    
    {\displaystyle b}
   can be calculated from the critical properties 
  
    
      
        
          p
          
            c
          
        
      
    
    {\displaystyle p_{c}}
  , 
  
    
      
        
          T
          
            c
          
        
      
    
    {\displaystyle T_{c}}
  , and 
  
    
      
        
          V
          
            c
          
        
      
    
    {\displaystyle V_{c}}
   (noting that 
  
    
      
        
          V
          
            c
          
        
      
    
    {\displaystyle V_{c}}
   is the molar volume at the critical point) as:

  
    
      
        a
        =
        3
        
          p
          
            c
          
        
        
        
          V
          
            c
          
          
            2
          
        
      
    
    {\displaystyle a=3p_{c}\,V_{c}^{2}}
  

  
    
      
        b
        =
        
          
            
              V
              
                c
              
            
            3
          
        
        .
      
    
    {\displaystyle b={\frac {V_{c}}{3}}.}
  Also written as

  
    
      
        a
        =
        
          
            
              27
              (
              R
              
              
                T
                
                  c
                
              
              
                )
                
                  2
                
              
            
            
              64
              
                p
                
                  c
                
              
            
          
        
      
    
    {\displaystyle a={\frac {27(R\,T_{c})^{2}}{64p_{c}}}}
  

  
    
      
        b
        =
        
          
            
              R
              
              
                T
                
                  c
                
              
            
            
              8
              
                p
                
                  c
                
              
            
          
        
        .
      
    
    {\displaystyle b={\frac {R\,T_{c}}{8p_{c}}}.}
  Proposed in 1873, the van der Waals equation of state was one of the first to perform markedly better than the ideal gas law. In this landmark equation 
  
    
      
        a
      
    
    {\displaystyle a}
   is called the attraction parameter and 
  
    
      
        b
      
    
    {\displaystyle b}
   the repulsion parameter or the effective molecular volume. While the equation is definitely superior to the ideal gas law and does predict the formation of a liquid phase, the agreement with experimental data is limited for conditions where the liquid forms. While the van der Waals equation is commonly referenced in text-books and papers for historical reasons, it is now obsolete. Other modern equations of only slightly greater complexity are much more accurate.
The van der Waals equation may be considered as the ideal gas law, ""improved"" due to two independent reasons:

Molecules are thought as particles with volume, not material points. Thus 
  
    
      
        
          V
          
            m
          
        
      
    
    {\displaystyle V_{m}}
   cannot be too little, less than some constant. So we get (
  
    
      
        
          V
          
            m
          
        
        −
        b
      
    
    {\displaystyle V_{m}-b}
  ) instead of 
  
    
      
        
          V
          
            m
          
        
      
    
    {\displaystyle V_{m}}
  .
While ideal gas molecules do not interact, we consider molecules attracting others within a distance of several molecules' radii. It makes no effect inside the material, but surface molecules are attracted into the material from the surface. We see this as diminishing of pressure on the outer shell (which is used in the ideal gas law), so we write (
  
    
      
        p
        +
      
    
    {\displaystyle p+}
   something) instead of 
  
    
      
        p
      
    
    {\displaystyle p}
  . To evaluate this ‘something’, let's examine an additional force acting on an element of gas surface. While the force acting on each surface molecule is ~
  
    
      
        ρ
      
    
    {\displaystyle \rho }
  , the force acting on the whole element is ~
  
    
      
        
          ρ
          
            2
          
        
      
    
    {\displaystyle \rho ^{2}}
  ~
  
    
      
        
          
            1
            
              V
              
                m
              
              
                2
              
            
          
        
      
    
    {\displaystyle {\frac {1}{V_{m}^{2}}}}
  .With the reduced state variables, i.e. 
  
    
      
        
          V
          
            r
          
        
        =
        
          V
          
            m
          
        
        
          /
        
        
          V
          
            c
          
        
      
    
    {\displaystyle V_{r}=V_{m}/V_{c}}
  , 
  
    
      
        
          P
          
            r
          
        
        =
        P
        
          /
        
        
          P
          
            c
          
        
      
    
    {\displaystyle P_{r}=P/P_{c}}
   and 
  
    
      
        
          T
          
            r
          
        
        =
        T
        
          /
        
        
          T
          
            c
          
        
      
    
    {\displaystyle T_{r}=T/T_{c}}
  , the reduced form of the Van der Waals equation can be formulated:

  
    
      
        
          (
          
            
              P
              
                r
              
            
            +
            
              
                3
                
                  V
                  
                    r
                  
                  
                    2
                  
                
              
            
          
          )
        
        
          (
          
            3
            
              V
              
                r
              
            
            −
            1
          
          )
        
        =
        8
        
          T
          
            r
          
        
      
    
    {\displaystyle \left(P_{r}+{\frac {3}{V_{r}^{2}}}\right)\left(3V_{r}-1\right)=8T_{r}}
  The benefit of this form is that for given 
  
    
      
        
          T
          
            r
          
        
      
    
    {\displaystyle T_{r}}
   and 
  
    
      
        
          P
          
            r
          
        
      
    
    {\displaystyle P_{r}}
  , the reduced volume of the liquid and gas can be calculated directly using Cardano's method for the reduced cubic form:

  
    
      
        
          V
          
            r
          
          
            3
          
        
        −
        
          (
          
            
              
                1
                3
              
            
            +
            
              
                
                  8
                  
                    T
                    
                      r
                    
                  
                
                
                  3
                  
                    P
                    
                      r
                    
                  
                
              
            
          
          )
        
        
          V
          
            r
          
          
            2
          
        
        +
        
          
            
              3
              
                V
                
                  r
                
              
            
            
              P
              
                r
              
            
          
        
        −
        
          
            1
            
              P
              
                r
              
            
          
        
        =
        0
      
    
    {\displaystyle V_{r}^{3}-\left({\frac {1}{3}}+{\frac {8T_{r}}{3P_{r}}}\right)V_{r}^{2}+{\frac {3V_{r}}{P_{r}}}-{\frac {1}{P_{r}}}=0}
  For 
  
    
      
        
          P
          
            r
          
        
        <
        1
      
    
    {\displaystyle P_{r}<1}
   and 
  
    
      
        
          T
          
            r
          
        
        <
        1
      
    
    {\displaystyle T_{r}<1}
  , the system is in a state of vapor–liquid equilibrium. The reduced cubic equation of state yields in that case 3 solutions. The largest and the lowest solution are the gas and liquid reduced volume.


=== Redlich-Kwong equation of state ===

  
    
      
        
          
            
              
                p
              
              
                
                =
                
                  
                    
                      R
                      
                      T
                    
                    
                      
                        V
                        
                          m
                        
                      
                      −
                      b
                    
                  
                
                −
                
                  
                    a
                    
                      
                        
                          T
                        
                      
                      
                      
                        V
                        
                          m
                        
                      
                      
                        (
                        
                          
                            V
                            
                              m
                            
                          
                          +
                          b
                        
                        )
                      
                    
                  
                
              
            
            
              
                a
              
              
                
                ≈
                0.42748
                
                  
                    
                      
                        R
                        
                          2
                        
                      
                      
                      
                        T
                        
                          c
                        
                        
                          
                            5
                            2
                          
                        
                      
                    
                    
                      p
                      
                        c
                      
                    
                  
                
              
            
            
              
                b
              
              
                
                ≈
                0.08664
                
                  
                    
                      R
                      
                      
                        T
                        
                          c
                        
                      
                    
                    
                      p
                      
                        c
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}p&={\frac {R\,T}{V_{m}-b}}-{\frac {a}{{\sqrt {T}}\,V_{m}\left(V_{m}+b\right)}}\\[3pt]a&\approx 0.42748{\frac {R^{2}\,T_{c}^{\frac {5}{2}}}{p_{c}}}\\[3pt]b&\approx 0.08664{\frac {R\,T_{c}}{p_{c}}}\end{aligned}}}
  Introduced in 1949, the Redlich-Kwong equation of state was a considerable improvement over other equations of the time. It is still of interest primarily due to its relatively simple form. While superior to the van der Waals equation of state, it performs poorly with respect to the liquid phase and thus cannot be used for accurately calculating vapor–liquid equilibria. However, it can be used in conjunction with separate liquid-phase correlations for this purpose.
The Redlich-Kwong equation is adequate for calculation of gas phase properties when the ratio of the pressure to the critical pressure (reduced pressure) is less than about one-half of the ratio of the temperature to the critical temperature (reduced temperature):

  
    
      
        
          
            p
            
              p
              
                c
              
            
          
        
        <
        
          
            T
            
              2
              
                T
                
                  c
                
              
            
          
        
        .
      
    
    {\displaystyle {\frac {p}{p_{c}}}<{\frac {T}{2T_{c}}}.}
  


=== Soave modification of Redlich-Kwong ===

  
    
      
        p
        =
        
          
            
              R
              
              T
            
            
              
                V
                
                  m
                
              
              −
              b
            
          
        
        −
        
          
            
              a
              
              α
            
            
              
                V
                
                  m
                
              
              
                (
                
                  
                    V
                    
                      m
                    
                  
                  +
                  b
                
                )
              
            
          
        
      
    
    {\displaystyle p={\frac {R\,T}{V_{m}-b}}-{\frac {a\,\alpha }{V_{m}\left(V_{m}+b\right)}}}
  
  
    
      
        a
        =
        
          
            
              0.42747
              
              
                R
                
                  2
                
              
              
              
                T
                
                  c
                
                
                  2
                
              
            
            
              P
              
                c
              
            
          
        
      
    
    {\displaystyle a={\frac {0.42747\,R^{2}\,T_{c}^{2}}{P_{c}}}}
  

  
    
      
        b
        =
        
          
            
              0.08664
              
              R
              
              
                T
                
                  c
                
              
            
            
              P
              
                c
              
            
          
        
      
    
    {\displaystyle b={\frac {0.08664\,R\,T_{c}}{P_{c}}}}
  

  
    
      
        α
        =
        
          
            (
            
              1
              +
              
                (
                
                  0.48508
                  +
                  1.55171
                  
                  ω
                  −
                  0.15613
                  
                  
                    ω
                    
                      2
                    
                  
                
                )
              
              
                (
                
                  1
                  −
                  
                    T
                    
                      r
                    
                    
                      
                      0.5
                    
                  
                
                )
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle \alpha =\left(1+\left(0.48508+1.55171\,\omega -0.15613\,\omega ^{2}\right)\left(1-T_{r}^{\,0.5}\right)\right)^{2}}
  
  
    
      
        
          T
          
            r
          
        
        =
        
          
            T
            
              T
              
                c
              
            
          
        
      
    
    {\displaystyle T_{r}={\frac {T}{T_{c}}}}
  Where ω is the acentric factor for the species.
This formulation for 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is due to Graboski and Daubert.  The original formulation from Soave is:

  
    
      
        α
        =
        
          
            (
            
              1
              +
              
                (
                
                  0.480
                  +
                  1.574
                  
                  ω
                  −
                  0.176
                  
                  
                    ω
                    
                      2
                    
                  
                
                )
              
              
                (
                
                  1
                  −
                  
                    T
                    
                      r
                    
                    
                      
                      0.5
                    
                  
                
                )
              
            
            )
          
          
            2
          
        
      
    
    {\displaystyle \alpha =\left(1+\left(0.480+1.574\,\omega -0.176\,\omega ^{2}\right)\left(1-T_{r}^{\,0.5}\right)\right)^{2}}
  for hydrogen:

  
    
      
        α
        =
        1.202
        exp
        ⁡
        
          (
          
            −
            0.30288
            
            
              T
              
                r
              
            
          
          )
        
        .
      
    
    {\displaystyle \alpha =1.202\exp \left(-0.30288\,T_{r}\right).}
  We can also write it in the polynomial form, with:

  
    
      
        A
        =
        
          
            
              a
              
              α
              
              P
            
            
              
                R
                
                  2
                
              
              
              
                T
                
                  2
                
              
            
          
        
      
    
    {\displaystyle A={\frac {a\,\alpha \,P}{R^{2}\,T^{2}}}}
  

  
    
      
        B
        =
        
          
            
              b
              
              P
            
            
              R
              
              T
            
          
        
      
    
    {\displaystyle B={\frac {b\,P}{R\,T}}}
  then we have:

  
    
      
        0
        =
        
          Z
          
            3
          
        
        −
        
          Z
          
            2
          
        
        +
        Z
        
          (
          
            A
            −
            B
            −
            
              B
              
                2
              
            
          
          )
        
        −
        A
        B
        
      
    
    {\displaystyle 0=Z^{3}-Z^{2}+Z\left(A-B-B^{2}\right)-AB\;}
  where 
  
    
      
        R
      
    
    {\displaystyle R}
   is the universal gas constant and Z=PV/(RT) is the compressibility factor.
In 1972 G. Soave replaced the 1/√T term of the Redlich-Kwong equation with a function α(T,ω) involving the temperature and the acentric factor (the resulting equation is also known as the Soave-Redlich-Kwong equation of state; SRK EOS). The α function was devised to fit the vapor pressure data of hydrocarbons and the equation does fairly well for these materials.
Note especially that this replacement changes the definition of a slightly, as the 
  
    
      
        
          T
          
            c
          
        
      
    
    {\displaystyle T_{c}}
   is now to the second power.


=== Volume translation of Peneloux et al. (1982) ===
The SRK EOS may be written as

  
    
      
        p
        =
        
          
            
              R
              
              T
            
            
              
                V
                
                  m
                  ,
                  
                    SRK
                  
                
              
              −
              b
            
          
        
        −
        
          
            a
            
              
                V
                
                  m
                  ,
                  
                    SRK
                  
                
              
              
                (
                
                  
                    V
                    
                      m
                      ,
                      
                        SRK
                      
                    
                  
                  +
                  b
                
                )
              
            
          
        
      
    
    {\displaystyle p={\frac {R\,T}{V_{m,{\text{SRK}}}-b}}-{\frac {a}{V_{m,{\text{SRK}}}\left(V_{m,{\text{SRK}}}+b\right)}}}
  where

  
    
      
        
          
            
              
                a
              
              
                
                =
                
                  a
                  
                    c
                  
                
                
                α
              
            
            
              
                
                  a
                  
                    c
                  
                
              
              
                
                ≈
                0.42747
                
                  
                    
                      
                        R
                        
                          2
                        
                      
                      
                      
                        T
                        
                          c
                        
                        
                          2
                        
                      
                    
                    
                      P
                      
                        c
                      
                    
                  
                
              
            
            
              
                b
              
              
                
                ≈
                0.08664
                
                  
                    
                      R
                      
                      
                        T
                        
                          c
                        
                      
                    
                    
                      P
                      
                        c
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}a&=a_{c}\,\alpha \\a_{c}&\approx 0.42747{\frac {R^{2}\,T_{c}^{2}}{P_{c}}}\\b&\approx 0.08664{\frac {R\,T_{c}}{P_{c}}}\end{aligned}}}
  where 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   and other parts of the SRK EOS is defined in the SRK EOS section.
A downside of the SRK EOS, and other cubic EOS, is that the liquid molar volume is significantly less accurate than the gas molar volume. Peneloux et alios (1982) proposed a simple correction for this by introducing a volume translation

  
    
      
        
          V
          
            m
            ,
            
              SRK
            
          
        
        =
        
          V
          
            m
          
        
        +
        c
      
    
    {\displaystyle V_{m,{\text{SRK}}}=V_{m}+c}
  where 
  
    
      
        c
      
    
    {\displaystyle c}
   is an additional fluid component parameter that translates the molar volume slightly. On the liquid branch of the EOS, a small change in molar volume corresponds to a large change in pressure. On the gas branch of the EOS, a small change in molar volume corresponds to a much smaller change in pressure than for the liquid branch. Thus, the perturbation of the molar gas volume is small. Unfortunately, there are two versions that occur in science and industry.
In the first version only 
  
    
      
        
          V
          
            m
            ,
            
              SRK
            
          
        
      
    
    {\displaystyle V_{m,{\text{SRK}}}}
   is translated, and the EOS becomes

  
    
      
        p
        =
        
          
            
              R
              
              T
            
            
              
                V
                
                  m
                
              
              +
              c
              −
              b
            
          
        
        −
        
          
            a
            
              
                (
                
                  
                    V
                    
                      m
                    
                  
                  +
                  c
                
                )
              
              
                (
                
                  
                    V
                    
                      m
                    
                  
                  +
                  c
                  +
                  b
                
                )
              
            
          
        
      
    
    {\displaystyle p={\frac {R\,T}{V_{m}+c-b}}-{\frac {a}{\left(V_{m}+c\right)\left(V_{m}+c+b\right)}}}
  In the second version both 
  
    
      
        
          V
          
            m
            ,
            
              SRK
            
          
        
      
    
    {\displaystyle V_{m,{\text{SRK}}}}
   and 
  
    
      
        
          b
          
            SRK
          
        
      
    
    {\displaystyle b_{\text{SRK}}}
   are translated, or the translation of 
  
    
      
        
          V
          
            m
            ,
            
              SRK
            
          
        
      
    
    {\displaystyle V_{m,{\text{SRK}}}}
   is followed by a renaming of the composite parameter b − c. This gives

  
    
      
        
          
            
              
                
                  b
                  
                    SRK
                  
                
              
              
                
                =
                b
                +
                c
                
                
                  or
                
                
                b
                −
                c
                ↷
                b
              
            
            
              
                p
              
              
                
                =
                
                  
                    
                      R
                      
                      T
                    
                    
                      
                        V
                        
                          m
                        
                      
                      −
                      b
                    
                  
                
                −
                
                  
                    a
                    
                      
                        (
                        
                          
                            V
                            
                              m
                            
                          
                          +
                          c
                        
                        )
                      
                      
                        (
                        
                          
                            V
                            
                              m
                            
                          
                          +
                          2
                          c
                          +
                          b
                        
                        )
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}b_{\text{SRK}}&=b+c\quad {\text{or}}\quad b-c\curvearrowright b\\p&={\frac {R\,T}{V_{m}-b}}-{\frac {a}{\left(V_{m}+c\right)\left(V_{m}+2c+b\right)}}\end{aligned}}}
  The c-parameter of a fluid mixture is calculated by

  
    
      
        c
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          z
          
            i
          
        
        
          c
          
            i
          
        
      
    
    {\displaystyle c=\sum _{i=1}^{n}z_{i}c_{i}}
  The c-parameter of the individual fluid components in a petroleum gas and oil can be estimated by the correlation

  
    
      
        
          c
          
            i
          
        
        ≈
        0.40768
         
        
          
            
              R
              
                T
                
                  c
                  i
                
              
            
            
              P
              
                c
                i
              
            
          
        
        
          (
          
            0.29441
            −
            
              Z
              
                
                  RA
                
                ,
                i
              
            
          
          )
        
      
    
    {\displaystyle c_{i}\approx 0.40768\ {\frac {RT_{ci}}{P_{ci}}}\left(0.29441-Z_{{\text{RA}},i}\right)}
  where the Rackett compressibility factor 
  
    
      
        
          Z
          
            
              RA
            
            ,
            i
          
        
      
    
    {\displaystyle Z_{{\text{RA}},i}}
   can be estimated by

  
    
      
        
          Z
          
            
              RA
            
            ,
            i
          
        
        ≈
        0.29056
        −
        0.08775
         
        
          ω
          
            i
          
        
      
    
    {\displaystyle Z_{{\text{RA}},i}\approx 0.29056-0.08775\ \omega _{i}}
  A nice feature with the volume translation method of Peneloux et al. (1982) is that it does not affect the vapor-liquid equilibrium calculations. This method of volume translation can also be applied to other cubic EOSs if the c-parameter correlation is adjusted to match the selected EOS.


=== Peng–Robinson equation of state ===

  
    
      
        
          
            
              
                p
              
              
                
                =
                
                  
                    
                      R
                      
                      T
                    
                    
                      
                        V
                        
                          m
                        
                      
                      −
                      b
                    
                  
                
                −
                
                  
                    
                      a
                      
                      α
                    
                    
                      
                        V
                        
                          m
                        
                        
                          2
                        
                      
                      +
                      2
                      b
                      
                        V
                        
                          m
                        
                      
                      −
                      
                        b
                        
                          2
                        
                      
                    
                  
                
              
            
            
              
                a
              
              
                
                ≈
                0.45724
                
                  
                    
                      
                        R
                        
                          2
                        
                      
                      
                      
                        T
                        
                          c
                        
                        
                          2
                        
                      
                    
                    
                      p
                      
                        c
                      
                    
                  
                
              
            
            
              
                b
              
              
                
                ≈
                0.07780
                
                  
                    
                      R
                      
                      
                        T
                        
                          c
                        
                      
                    
                    
                      p
                      
                        c
                      
                    
                  
                
              
            
            
              
                α
              
              
                
                =
                
                  
                    (
                    
                      1
                      +
                      κ
                      
                        (
                        
                          1
                          −
                          
                            T
                            
                              r
                            
                            
                              
                                1
                                2
                              
                            
                          
                        
                        )
                      
                    
                    )
                  
                  
                    2
                  
                
              
            
            
              
                κ
              
              
                
                ≈
                0.37464
                +
                1.54226
                
                ω
                −
                0.26992
                
                
                  ω
                  
                    2
                  
                
              
            
            
              
                
                  T
                  
                    r
                  
                
              
              
                
                =
                
                  
                    T
                    
                      T
                      
                        c
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}p&={\frac {R\,T}{V_{m}-b}}-{\frac {a\,\alpha }{V_{m}^{2}+2bV_{m}-b^{2}}}\\[3pt]a&\approx 0.45724{\frac {R^{2}\,T_{c}^{2}}{p_{c}}}\\[3pt]b&\approx 0.07780{\frac {R\,T_{c}}{p_{c}}}\\[3pt]\alpha &=\left(1+\kappa \left(1-T_{r}^{\frac {1}{2}}\right)\right)^{2}\\[3pt]\kappa &\approx 0.37464+1.54226\,\omega -0.26992\,\omega ^{2}\\[3pt]T_{r}&={\frac {T}{T_{c}}}\end{aligned}}}
  In polynomial form:

  
    
      
        A
        =
        
          
            
              α
              a
              p
            
            
              
                R
                
                  2
                
              
              
              
                T
                
                  2
                
              
            
          
        
      
    
    {\displaystyle A={\frac {\alpha ap}{R^{2}\,T^{2}}}}
  

  
    
      
        B
        =
        
          
            
              b
              p
            
            
              R
              T
            
          
        
      
    
    {\displaystyle B={\frac {bp}{RT}}}
  

  
    
      
        
          Z
          
            3
          
        
        −
        (
        1
        −
        B
        )
        
          Z
          
            2
          
        
        +
        
          (
          
            A
            −
            2
            B
            −
            3
            
              B
              
                2
              
            
          
          )
        
        Z
        −
        
          (
          
            A
            B
            −
            
              B
              
                2
              
            
            −
            
              B
              
                3
              
            
          
          )
        
        =
        0
      
    
    {\displaystyle Z^{3}-(1-B)Z^{2}+\left(A-2B-3B^{2}\right)Z-\left(AB-B^{2}-B^{3}\right)=0}
  where 
  
    
      
        ω
      
    
    {\displaystyle \omega }
   is the acentric factor of the species, 
  
    
      
        R
      
    
    {\displaystyle R}
   is the universal gas constant and 
  
    
      
        Z
        =
        P
        V
        
          /
        
        n
        R
        T
      
    
    {\displaystyle Z=PV/nRT}
   is compressibility factor.
The Peng–Robinson equation of state (PR EOS) was developed in 1976 at The University of Alberta by Ding-Yu Peng and Donald Robinson in order to satisfy the following goals:
The parameters should be expressible in terms of the critical properties and the acentric factor.
The model should provide reasonable accuracy near the critical point, particularly for calculations of the compressibility factor and liquid density.
The mixing rules should not employ more than a single binary interaction parameter, which should be independent of temperature, pressure, and composition.
The equation should be applicable to all calculations of all fluid properties in natural gas processes.For the most part the Peng–Robinson equation exhibits performance similar to the Soave equation, although it is generally superior in predicting the liquid densities of many materials, especially nonpolar ones. The departure functions of the Peng–Robinson equation are given on a separate article.
The analytic values of its characteristic constants are:

  
    
      
        
          Z
          
            c
          
        
        =
        
          
            1
            32
          
        
        
          (
          
            11
            −
            2
            
              
                7
              
            
            sinh
            ⁡
            
              (
              
                
                  
                    1
                    3
                  
                
                arsinh
                ⁡
                
                  (
                  
                    
                      13
                      
                        7
                        
                          
                            7
                          
                        
                      
                    
                  
                  )
                
              
              )
            
          
          )
        
        ≈
        0.307401
      
    
    {\displaystyle Z_{c}={\frac {1}{32}}\left(11-2{\sqrt {7}}\sinh \left({\frac {1}{3}}\operatorname {arsinh} \left({\frac {13}{7{\sqrt {7}}}}\right)\right)\right)\approx 0.307401}
  

  
    
      
        
          b
          ′
        
        =
        
          
            b
            
              V
              
                m
                ,
                c
              
            
          
        
        =
        
          
            1
            3
          
        
        
          (
          
            
              
                8
              
            
            sinh
            ⁡
            
              (
              
                
                  
                    1
                    3
                  
                
                arsinh
                ⁡
                
                  (
                  
                    
                      8
                    
                  
                  )
                
              
              )
            
            −
            1
          
          )
        
        ≈
        0.253077
        ≈
        
          
            0.07780
            
              Z
              
                c
              
            
          
        
      
    
    {\displaystyle b'={\frac {b}{V_{m,c}}}={\frac {1}{3}}\left({\sqrt {8}}\sinh \left({\frac {1}{3}}\operatorname {arsinh} \left({\sqrt {8}}\right)\right)-1\right)\approx 0.253077\approx {\frac {0.07780}{Z_{c}}}}
  

  
    
      
        
          
            
              
                P
                
                  c
                
              
              
                V
                
                  m
                  ,
                  c
                
                
                  2
                
              
            
            
              a
              
              
                b
                ′
              
            
          
        
        =
        
          
            3
            8
          
        
        
          (
          
            1
            +
            cosh
            ⁡
            
              (
              
                
                  
                    1
                    3
                  
                
                arcosh
                ⁡
                (
                3
                )
              
              )
            
          
          )
        
        ≈
        0.816619
        ≈
        
          
            
              Z
              
                c
              
              
                2
              
            
            
              0.45724
              
              
                b
                ′
              
            
          
        
      
    
    {\displaystyle {\frac {P_{c}V_{m,c}^{2}}{a\,b'}}={\frac {3}{8}}\left(1+\cosh \left({\frac {1}{3}}\operatorname {arcosh} (3)\right)\right)\approx 0.816619\approx {\frac {Z_{c}^{2}}{0.45724\,b'}}}
  


=== Peng–Robinson-Stryjek-Vera equations of state ===


==== PRSV1 ====
A modification to the attraction term in the Peng–Robinson equation of state published by Stryjek and Vera in 1986 (PRSV) significantly improved the model's accuracy by introducing an adjustable pure component parameter and by modifying the polynomial fit of the acentric factor.The modification is:

  
    
      
        
          
            
              
                κ
              
              
                
                =
                
                  κ
                  
                    0
                  
                
                +
                
                  κ
                  
                    1
                  
                
                
                  (
                  
                    1
                    +
                    
                      T
                      
                        r
                      
                      
                        
                          1
                          2
                        
                      
                    
                  
                  )
                
                
                  (
                  
                    0.7
                    −
                    
                      T
                      
                        r
                      
                    
                  
                  )
                
              
            
            
              
                
                  κ
                  
                    0
                  
                
              
              
                
                =
                0.378893
                +
                1.4897153
                
                ω
                −
                0.17131848
                
                
                  ω
                  
                    2
                  
                
                +
                0.0196554
                
                
                  ω
                  
                    3
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\kappa &=\kappa _{0}+\kappa _{1}\left(1+T_{r}^{\frac {1}{2}}\right)\left(0.7-T_{r}\right)\\\kappa _{0}&=0.378893+1.4897153\,\omega -0.17131848\,\omega ^{2}+0.0196554\,\omega ^{3}\end{aligned}}}
  where 
  
    
      
        
        
          κ
          
            1
          
        
      
    
    {\displaystyle \,\kappa _{1}}
   is an adjustable pure component parameter.  Stryjek and Vera published pure component parameters for many compounds of industrial interest in their original journal article. At reduced temperatures above 0.7, they recommend to set 
  
    
      
        
        
          κ
          
            1
          
        
        =
        0
      
    
    {\displaystyle \,\kappa _{1}=0}
   and simply use 
  
    
      
        κ
        =
        
          κ
          
            0
          
        
      
    
    {\displaystyle \kappa =\kappa _{0}}
  . For alcohols and water the value of 
  
    
      
        
        
          κ
          
            1
          
        
      
    
    {\displaystyle \,\kappa _{1}}
   may be used up to the critical temperature and set to zero at higher temperatures.


==== PRSV2 ====
A subsequent modification published in 1986 (PRSV2) further improved the model's accuracy by introducing two additional pure component parameters to the previous attraction term modification.The modification is:

  
    
      
        
          
            
              
                κ
              
              
                
                =
                
                  κ
                  
                    0
                  
                
                +
                
                  [
                  
                    
                      κ
                      
                        1
                      
                    
                    +
                    
                      κ
                      
                        2
                      
                    
                    
                      (
                      
                        
                          κ
                          
                            3
                          
                        
                        −
                        
                          T
                          
                            r
                          
                        
                      
                      )
                    
                    
                      (
                      
                        1
                        −
                        
                          T
                          
                            r
                          
                          
                            
                              1
                              2
                            
                          
                        
                      
                      )
                    
                  
                  ]
                
                
                  (
                  
                    1
                    +
                    
                      T
                      
                        r
                      
                      
                        
                          1
                          2
                        
                      
                    
                  
                  )
                
                
                  (
                  
                    0.7
                    −
                    
                      T
                      
                        r
                      
                    
                  
                  )
                
              
            
            
              
                
                  κ
                  
                    0
                  
                
              
              
                
                =
                0.378893
                +
                1.4897153
                
                ω
                −
                0.17131848
                
                
                  ω
                  
                    2
                  
                
                +
                0.0196554
                
                
                  ω
                  
                    3
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\kappa &=\kappa _{0}+\left[\kappa _{1}+\kappa _{2}\left(\kappa _{3}-T_{r}\right)\left(1-T_{r}^{\frac {1}{2}}\right)\right]\left(1+T_{r}^{\frac {1}{2}}\right)\left(0.7-T_{r}\right)\\\kappa _{0}&=0.378893+1.4897153\,\omega -0.17131848\,\omega ^{2}+0.0196554\,\omega ^{3}\end{aligned}}}
  where 
  
    
      
        
        
          κ
          
            1
          
        
      
    
    {\displaystyle \,\kappa _{1}}
  , 
  
    
      
        
        
          κ
          
            2
          
        
      
    
    {\displaystyle \,\kappa _{2}}
  , and 
  
    
      
        
        
          κ
          
            3
          
        
      
    
    {\displaystyle \,\kappa _{3}}
   are adjustable pure component parameters.
PRSV2 is particularly advantageous for VLE calculations.  While PRSV1 does offer an advantage over the Peng–Robinson model for describing thermodynamic behavior, it is still not accurate enough, in general, for phase equilibrium calculations.  The highly non-linear behavior of phase-equilibrium calculation methods tends to amplify what would otherwise be acceptably small errors.  It is therefore recommended that PRSV2 be used for equilibrium calculations when applying these models to a design.  However, once the equilibrium state has been determined, the phase specific thermodynamic values at equilibrium may be determined by one of several simpler models with a reasonable degree of accuracy.One thing to note is that in the PRSV equation, the parameter fit is done in a particular temperature range which is usually below the critical temperature. Above the critical temperature, the PRSV alpha function tends to diverge and become arbitrarily large instead of tending towards 0. Because of this, alternate equations for alpha should be employed above the critical point. This is especially important for systems containing hydrogen which is often found at temperatures far above its critical point. Several alternate formulations have been proposed. Some well known ones are by Twu et al or by Mathias and Copeman.


=== Peng-Robinson-Babalola equation of state (PRB) ===
Babalola  modified the Peng–Robinson Equation of state as:

  
    
      
        P
        =
        
          (
          
            
              
                R
                T
              
              
                v
                −
                b
              
            
          
          )
        
        −
        
          [
          
            
              
                (
                
                  a
                  
                    1
                  
                
                P
                +
                
                  a
                  
                    2
                  
                
                )
                α
              
              
                v
                (
                v
                +
                b
                )
                +
                b
                (
                v
                −
                b
                )
              
            
          
          ]
        
      
    
    {\displaystyle P=\left({\frac {RT}{v-b}}\right)-\left[{\frac {(a_{1}P+a_{2})\alpha }{v(v+b)+b(v-b)}}\right]}
  
The attractive force parameter ‘a’, which was considered to be a constant with respect to pressure in Peng–Robinson EOS. The modification, in which parameter ‘a’ was treated as a variable with respect to pressure for multicomponent multi-phase high density reservoir systems was to improve accuracy in the prediction of properties of complex reservoir fluids for PVT modeling. The variation was represented with a linear equation where a1 and a2 represent the slope and the intercept respectively of the straight line obtained when values of parameter ‘a’ are plotted against pressure.
This modification increases the accuracy of Peng–Robinson equation of state for heavier fluids particularly at pressure ranges (>30MPa) and eliminates the need for tuning the original Peng-Robinson equation of state. Values for a


=== Elliott, Suresh, Donohue equation of state ===
The Elliott, Suresh, and Donohue (ESD) equation of state was proposed in 1990. The equation seeks to correct a shortcoming in the Peng–Robinson EOS in that there was an inaccuracy in the van der Waals repulsive term. The EOS accounts for the effect of the shape of a non-polar molecule and can be extended to polymers with the addition of an extra term (not shown). The EOS itself was developed through modeling computer simulations and should capture the essential physics of the size, shape, and hydrogen bonding.

  
    
      
        
          
            
              p
              
                V
                
                  m
                
              
            
            
              R
              T
            
          
        
        =
        Z
        =
        1
        +
        
          Z
          
            
              r
              e
              p
            
          
        
        +
        
          Z
          
            
              a
              t
              t
            
          
        
      
    
    {\displaystyle {\frac {pV_{m}}{RT}}=Z=1+Z^{\rm {rep}}+Z^{\rm {att}}}
  where:

  
    
      
        
          Z
          
            
              r
              e
              p
            
          
        
        =
        
          
            
              4
              c
              η
            
            
              1
              −
              1.9
              η
            
          
        
      
    
    {\displaystyle Z^{\rm {rep}}={\frac {4c\eta }{1-1.9\eta }}}
  
  
    
      
        
          Z
          
            
              a
              t
              t
            
          
        
        =
        −
        
          
            
              
                z
                
                  m
                
              
              q
              η
              Y
            
            
              1
              +
              
                k
                
                  1
                
              
              η
              Y
            
          
        
      
    
    {\displaystyle Z^{\rm {att}}=-{\frac {z_{m}q\eta Y}{1+k_{1}\eta Y}}}
  and

  
    
      
        c
      
    
    {\displaystyle c}
   is a ""shape factor"", with 
  
    
      
        c
        =
        1
      
    
    {\displaystyle c=1}
   for spherical molecules
For non-spherical molecules, the following relation is suggested
  
    
      
        c
        =
        1
        +
        3.535
        ω
        +
        0.533
        
          ω
          
            2
          
        
      
    
    {\displaystyle c=1+3.535\omega +0.533\omega ^{2}}
   where 
  
    
      
        ω
      
    
    {\displaystyle \omega }
   is the acentric factor.The reduced number density 
  
    
      
        η
      
    
    {\displaystyle \eta }
   is defined as 
  
    
      
        η
        =
        
          
            
              
                v
                
                  ∗
                
              
              n
            
            V
          
        
      
    
    {\displaystyle \eta ={\frac {v^{*}n}{V}}}
  where

  
    
      
        
          v
          
            ∗
          
        
      
    
    {\displaystyle v^{*}}
   is the characteristic size parameter

  
    
      
        n
      
    
    {\displaystyle n}
   is the number of molecules

  
    
      
        V
      
    
    {\displaystyle V}
   is the volume of the containerThe characteristic size parameter is related to the shape parameter 
  
    
      
        c
      
    
    {\displaystyle c}
   through

  
    
      
        
          v
          
            ∗
          
        
        =
        
          
            
              k
              
                T
                
                  c
                
              
            
            
              P
              
                c
              
            
          
        
        Φ
      
    
    {\displaystyle v^{*}={\frac {kT_{c}}{P_{c}}}\Phi }
  where

  
    
      
        Φ
        =
        
          
            
              0.0312
              +
              0.087
              (
              c
              −
              1
              )
              +
              0.008
              (
              c
              −
              1
              
                )
                
                  2
                
              
            
            
              1.000
              +
              2.455
              (
              c
              −
              1
              )
              +
              0.732
              (
              c
              −
              1
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle \Phi ={\frac {0.0312+0.087(c-1)+0.008(c-1)^{2}}{1.000+2.455(c-1)+0.732(c-1)^{2}}}}
   and 
  
    
      
        k
      
    
    {\displaystyle k}
   is Boltzmann's constant.Noting the relationships between Boltzmann's constant and the Universal gas constant, and observing that the number of molecules can be expressed in terms of Avogadro's number and the molar mass, the reduced number density 
  
    
      
        η
      
    
    {\displaystyle \eta }
   can be expressed in terms of the molar volume as

  
    
      
        η
        =
        
          
            
              R
              
                T
                
                  c
                
              
            
            
              P
              
                c
              
            
          
        
        Φ
        
          
            1
            
              V
              
                m
              
            
          
        
        .
      
    
    {\displaystyle \eta ={\frac {RT_{c}}{P_{c}}}\Phi {\frac {1}{V_{m}}}.}
  The shape parameter 
  
    
      
        q
      
    
    {\displaystyle q}
   appearing in the Attraction term and the term 
  
    
      
        Y
      
    
    {\displaystyle Y}
   are given by

  
    
      
        q
        =
        1
        +
        
          k
          
            3
          
        
        (
        c
        −
        1
        )
      
    
    {\displaystyle q=1+k_{3}(c-1)}
   (and is hence also equal to 1 for spherical molecules).
  
    
      
        Y
        =
        exp
        ⁡
        
          (
          
            
              ϵ
              
                k
                T
              
            
          
          )
        
        −
        
          k
          
            2
          
        
      
    
    {\displaystyle Y=\exp \left({\frac {\epsilon }{kT}}\right)-k_{2}}
  where 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   is the depth of the square-well potential and is given by

  
    
      
        
          
            ϵ
            k
          
        
        =
        
          
            
              1.000
              +
              0.945
              (
              c
              −
              1
              )
              +
              0.134
              (
              c
              −
              1
              
                )
                
                  2
                
              
            
            
              1.023
              +
              2.225
              (
              c
              −
              1
              )
              +
              0.478
              (
              c
              −
              1
              
                )
                
                  2
                
              
            
          
        
      
    
    {\displaystyle {\frac {\epsilon }{k}}={\frac {1.000+0.945(c-1)+0.134(c-1)^{2}}{1.023+2.225(c-1)+0.478(c-1)^{2}}}}
  
  
    
      
        
          z
          
            m
          
        
      
    
    {\displaystyle z_{m}}
  , 
  
    
      
        
          k
          
            1
          
        
      
    
    {\displaystyle k_{1}}
  , 
  
    
      
        
          k
          
            2
          
        
      
    
    {\displaystyle k_{2}}
   and 
  
    
      
        
          k
          
            3
          
        
      
    
    {\displaystyle k_{3}}
   are constants in the equation of state:

  
    
      
        
          z
          
            m
          
        
        =
        9.49
      
    
    {\displaystyle z_{m}=9.49}
   for spherical molecules (c=1)

  
    
      
        
          k
          
            1
          
        
        =
        1.7745
      
    
    {\displaystyle k_{1}=1.7745}
   for spherical molecules (c=1)

  
    
      
        
          k
          
            2
          
        
        =
        1.0617
      
    
    {\displaystyle k_{2}=1.0617}
   for spherical molecules (c=1)

  
    
      
        
          k
          
            3
          
        
        =
        1.90476.
      
    
    {\displaystyle k_{3}=1.90476.}
  The model can be extended to associating components and mixtures of nonassociating components. Details are in the paper by J.R. Elliott, Jr. et al. (1990).


=== Cubic-Plus-Association ===
The Cubic-Plus-Association (CPA) equation of state combines the Soave-Redlich-Kwong equation with an association term from Wertheim theory. The development of the equation began in 1995 as a research project that was funded by Shell, and in 1996 an article was published which presented the CPA equation of state.

  
    
      
        P
        =
        
          
            
              R
              T
            
            
              (
              V
              −
              b
              )
            
          
        
        −
        
          
            a
            
              V
              (
              V
              +
              b
              )
            
          
        
        +
        
          
            
              R
              T
            
            V
          
        
        ρ
        
          ∑
          
            A
          
        
        
          [
          
            
              
                1
                
                  X
                  
                    A
                  
                
              
            
            −
            
              
                1
                2
              
            
          
          ]
        
        
          
            
              ∂
              
                X
                
                  A
                
              
            
            
              ∂
              ρ
            
          
        
      
    
    {\displaystyle P={\frac {RT}{(V-b)}}-{\frac {a}{V(V+b)}}+{\frac {RT}{V}}\rho \sum _{A}\left[{\frac {1}{X^{A}}}-{\frac {1}{2}}\right]{\frac {\partial X^{A}}{\partial \rho }}}
  In the association term 
  
    
      
        
          X
          
            A
          
        
      
    
    {\displaystyle X^{A}}
   is the mole fraction of molecules not bonded at site A.


== Non-cubic equations of state ==


=== Dieterici equation of state ===

  
    
      
        p
        (
        V
        −
        b
        )
        =
        R
        T
        
          e
          
            −
            
              
                a
                
                  R
                  T
                  V
                
              
            
          
        
      
    
    {\displaystyle p(V-b)=RTe^{-{\frac {a}{RTV}}}}
  where a is associated with the interaction between molecules and b takes into account the finite size of the molecules, similar to the Van der Waals equation.
The reduced coordinates are:

  
    
      
        
          T
          
            c
          
        
        =
        
          
            a
            
              4
              R
              b
            
          
        
        ,
         
        
          p
          
            c
          
        
        =
        
          
            a
            
              4
              
                b
                
                  2
                
              
              
                e
                
                  2
                
              
            
          
        
        ,
         
        
          V
          
            c
          
        
        =
        2
        b
        .
      
    
    {\displaystyle T_{c}={\frac {a}{4Rb}},\ p_{c}={\frac {a}{4b^{2}e^{2}}},\ V_{c}=2b.}
  


== Virial equations of state ==


=== Virial equation of state ===

  
    
      
        
          
            
              p
              
                V
                
                  m
                
              
            
            
              R
              T
            
          
        
        =
        A
        +
        
          
            B
            
              V
              
                m
              
            
          
        
        +
        
          
            C
            
              V
              
                m
              
              
                2
              
            
          
        
        +
        
          
            D
            
              V
              
                m
              
              
                3
              
            
          
        
        +
        ⋯
      
    
    {\displaystyle {\frac {pV_{m}}{RT}}=A+{\frac {B}{V_{m}}}+{\frac {C}{V_{m}^{2}}}+{\frac {D}{V_{m}^{3}}}+\cdots }
  Although usually not the most convenient equation of state, the virial equation is important because it can be derived directly from statistical mechanics. This equation is also called the Kamerlingh Onnes equation. If appropriate assumptions are made about the mathematical form of intermolecular forces, theoretical expressions can be developed for each of the coefficients. A is the first virial coefficient, which has a constant value of 1 and makes the statement that when volume is large, all fluids behave like ideal gases. The second virial coefficient B corresponds to interactions between pairs of molecules, C to triplets, and so on. Accuracy can be increased indefinitely by considering higher order terms.  The coefficients B, C, D, etc. are functions of temperature only.
One of the most accurate equations of state is that from Benedict-Webb-Rubin-Starling shown next. It was very close to a virial equation of state. If the exponential term in it is expanded to two Taylor terms, a virial equation can be derived:

  
    
      
        p
        =
        ρ
        R
        T
        +
        
          (
          
            
              B
              
                0
              
            
            R
            T
            −
            
              A
              
                0
              
            
            −
            
              
                
                  C
                  
                    0
                  
                
                
                  T
                  
                    2
                  
                
              
            
            +
            
              
                
                  D
                  
                    0
                  
                
                
                  T
                  
                    3
                  
                
              
            
            −
            
              
                
                  E
                  
                    0
                  
                
                
                  T
                  
                    4
                  
                
              
            
          
          )
        
        
          ρ
          
            2
          
        
        +
        
          (
          
            b
            R
            T
            −
            a
            −
            
              
                d
                T
              
            
            +
            
              
                c
                
                  T
                  
                    2
                  
                
              
            
          
          )
        
        
          ρ
          
            3
          
        
        +
        α
        
          (
          
            a
            +
            
              
                d
                T
              
            
          
          )
        
        
          ρ
          
            6
          
        
      
    
    {\displaystyle p=\rho RT+\left(B_{0}RT-A_{0}-{\frac {C_{0}}{T^{2}}}+{\frac {D_{0}}{T^{3}}}-{\frac {E_{0}}{T^{4}}}\right)\rho ^{2}+\left(bRT-a-{\frac {d}{T}}+{\frac {c}{T^{2}}}\right)\rho ^{3}+\alpha \left(a+{\frac {d}{T}}\right)\rho ^{6}}
  Note that in this virial equation, the fourth and fifth virial terms are zero. The second virial coefficient is monotonically decreasing as temperature is lowered. The third virial coefficient is monotonically increasing as temperature is lowered.


=== The BWR equation of state ===

  
    
      
        p
        =
        ρ
        R
        T
        +
        
          (
          
            
              B
              
                0
              
            
            R
            T
            −
            
              A
              
                0
              
            
            −
            
              
                
                  C
                  
                    0
                  
                
                
                  T
                  
                    2
                  
                
              
            
            +
            
              
                
                  D
                  
                    0
                  
                
                
                  T
                  
                    3
                  
                
              
            
            −
            
              
                
                  E
                  
                    0
                  
                
                
                  T
                  
                    4
                  
                
              
            
          
          )
        
        
          ρ
          
            2
          
        
        +
        
          (
          
            b
            R
            T
            −
            a
            −
            
              
                d
                T
              
            
          
          )
        
        
          ρ
          
            3
          
        
        +
        α
        
          (
          
            a
            +
            
              
                d
                T
              
            
          
          )
        
        
          ρ
          
            6
          
        
        +
        
          
            
              c
              
                ρ
                
                  3
                
              
            
            
              T
              
                2
              
            
          
        
        
          (
          
            1
            +
            γ
            
              ρ
              
                2
              
            
          
          )
        
        exp
        ⁡
        
          (
          
            −
            γ
            
              ρ
              
                2
              
            
          
          )
        
      
    
    {\displaystyle p=\rho RT+\left(B_{0}RT-A_{0}-{\frac {C_{0}}{T^{2}}}+{\frac {D_{0}}{T^{3}}}-{\frac {E_{0}}{T^{4}}}\right)\rho ^{2}+\left(bRT-a-{\frac {d}{T}}\right)\rho ^{3}+\alpha \left(a+{\frac {d}{T}}\right)\rho ^{6}+{\frac {c\rho ^{3}}{T^{2}}}\left(1+\gamma \rho ^{2}\right)\exp \left(-\gamma \rho ^{2}\right)}
  where

p is pressure
ρ is molar densityValues of the various parameters for 15 substances can be found in K.E. Starling (1973). Fluid Properties for Light Petroleum Systems. Gulf Publishing Company.


=== Lee-Kesler equation of state ===
The Lee-Kesler equation of state is based on the corresponding states principle, and is a modification of the BWR equation of state.


== SAFT equations of state ==
Statistical associating fluid theory (SAFT) equations of state predict the effect of molecular size and shape and hydrogen bonding on fluid properties and phase behavior.  The SAFT equation of state was developed using statistical mechanical methods (in particular perturbation theory) to describe the interactions between molecules in a system. The idea of a SAFT equation of state was first proposed by Chapman et al. in 1988 and 1989.  Many different versions of the SAFT equation of state have been proposed, but all use the same chain and association terms derived by Chapman. SAFT equations of state represent molecules as chains of typically spherical particles that interact with one another through short range repulsion, long range attraction, and hydrogen bonding between specific sites. One popular version of the SAFT equation of state includes the effect of chain length on the shielding of the dispersion interactions between molecules (PC-SAFT). In general, SAFT equations give more accurate results than traditional cubic equations of state, especially for systems containing liquids or solids.


== Multiparameter equations of state ==


=== Helmholtz Function form ===
Multiparameter equations of state (MEOS) can be used to represent pure fluids with high accuracy, in both the liquid and gaseous states. MEOS's represent the Helmholtz function of the fluid as the sum of ideal gas and residual terms. Both terms are explicit in reduced temperature and reduced density - thus:

  
    
      
        
          
            
              a
              (
              T
              ,
              ρ
              )
            
            
              R
              T
            
          
        
        =
        
          
            
              
                a
                
                  o
                
              
              (
              T
              ,
              ρ
              )
              +
              
                a
                
                  r
                
              
              (
              T
              ,
              ρ
              )
            
            
              R
              T
            
          
        
        =
        
          α
          
            o
          
        
        (
        τ
        ,
        δ
        )
        +
        
          α
          
            r
          
        
        (
        τ
        ,
        δ
        )
      
    
    {\displaystyle {\frac {a(T,\rho )}{RT}}={\frac {a^{o}(T,\rho )+a^{r}(T,\rho )}{RT}}=\alpha ^{o}(\tau ,\delta )+\alpha ^{r}(\tau ,\delta )}
  where:

  
    
      
        τ
        =
        
          
            
              T
              
                r
              
            
            T
          
        
        ,
        δ
        =
        
          
            ρ
            
              ρ
              
                r
              
            
          
        
      
    
    {\displaystyle \tau ={\frac {T_{r}}{T}},\delta ={\frac {\rho }{\rho _{r}}}}
  The reduced density and temperature are typically, though not always, the critical values for the pure fluid.
Other thermodynamic functions can be derived from the MEOS by using appropriate derivatives of the Helmholtz function; hence, because integration of the MEOS is not required, there are few restrictions as to the functional form of the ideal or residual terms. Typical MEOS use upwards of 50 fluid specific parameters, but are able to represent the fluid's properties with high accuracy. MEOS are available currently for about 50 of the most common industrial fluids including refrigerants. Mixture models also exist.


== Other equations of state of interest ==


=== Stiffened equation of state ===
When considering water under very high pressures, in situations such as underwater nuclear explosions, sonic shock lithotripsy, and sonoluminescence, the stiffened equation of state is often used:

  
    
      
        p
        =
        ρ
        (
        γ
        −
        1
        )
        e
        −
        γ
        
          p
          
            0
          
        
        
      
    
    {\displaystyle p=\rho (\gamma -1)e-\gamma p^{0}\,}
  where 
  
    
      
        e
      
    
    {\displaystyle e}
   is the internal energy per unit mass, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   is an empirically determined constant typically taken to be about 6.1, and 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p^{0}}
   is another constant, representing the molecular attraction between water molecules. The magnitude of the correction is about 2 gigapascals (20,000 atmospheres).
The equation is stated in this form because the speed of sound in water is given by 
  
    
      
        
          c
          
            2
          
        
        =
        γ
        
          (
          
            p
            +
            
              p
              
                0
              
            
          
          )
        
        
          /
        
        ρ
      
    
    {\displaystyle c^{2}=\gamma \left(p+p^{0}\right)/\rho }
  .
Thus water behaves as though it is an ideal gas that is already under about 20,000 atmospheres (2 GPa) pressure, and explains why water is commonly assumed to be incompressible: when the external pressure changes from 1 atmosphere to 2 atmospheres (100 kPa to 200 kPa), the water behaves as an ideal gas would when changing from 20,001 to 20,002 atmospheres (2000.1 MPa to 2000.2 MPa).
This equation mispredicts the specific heat capacity of water but few simple alternatives are available for severely nonisentropic processes such as strong shocks.


=== Ultrarelativistic equation of state ===
An ultrarelativistic fluid has equation of state

  
    
      
        p
        =
        
          ρ
          
            m
          
        
        
          c
          
            s
          
          
            2
          
        
      
    
    {\displaystyle p=\rho _{m}c_{s}^{2}}
  where 
  
    
      
        p
      
    
    {\displaystyle p}
   is the pressure, 
  
    
      
        
          ρ
          
            m
          
        
      
    
    {\displaystyle \rho _{m}}
   is the mass density, and 
  
    
      
        
          c
          
            s
          
        
      
    
    {\displaystyle c_{s}}
   is the speed of sound.


=== Ideal Bose equation of state ===
The equation of state for an ideal Bose gas is

  
    
      
        p
        
          V
          
            m
          
        
        =
        R
        T
         
        
          
            
              
                
                  Li
                
                
                  α
                  +
                  1
                
              
              (
              z
              )
            
            
              ζ
              (
              α
              )
            
          
        
        
          
            (
            
              
                T
                
                  T
                  
                    c
                  
                
              
            
            )
          
          
            α
          
        
      
    
    {\displaystyle pV_{m}=RT~{\frac {{\text{Li}}_{\alpha +1}(z)}{\zeta (\alpha )}}\left({\frac {T}{T_{c}}}\right)^{\alpha }}
  where α is an exponent specific to the system (e.g. in the absence of a potential field, α = 3/2), z is exp(μ/kT) where μ is the chemical potential, Li is the polylogarithm, ζ is the Riemann zeta function, and Tc is the critical temperature at which a Bose–Einstein condensate begins to form.


=== Jones–Wilkins–Lee equation of state for explosives (JWL equation) ===
The equation of state from Jones–Wilkins–Lee is used to describe the detonation products of explosives.

  
    
      
        p
        =
        A
        
          (
          
            1
            −
            
              
                ω
                
                  
                    R
                    
                      1
                    
                  
                  V
                
              
            
          
          )
        
        exp
        ⁡
        (
        −
        
          R
          
            1
          
        
        V
        )
        +
        B
        
          (
          
            1
            −
            
              
                ω
                
                  
                    R
                    
                      2
                    
                  
                  V
                
              
            
          
          )
        
        exp
        ⁡
        
          (
          
            −
            
              R
              
                2
              
            
            V
          
          )
        
        +
        
          
            
              ω
              
                e
                
                  0
                
              
            
            V
          
        
      
    
    {\displaystyle p=A\left(1-{\frac {\omega }{R_{1}V}}\right)\exp(-R_{1}V)+B\left(1-{\frac {\omega }{R_{2}V}}\right)\exp \left(-R_{2}V\right)+{\frac {\omega e_{0}}{V}}}
  The ratio 
  
    
      
        V
        =
        
          ρ
          
            e
          
        
        
          /
        
        ρ
      
    
    {\displaystyle V=\rho _{e}/\rho }
   is defined by using 
  
    
      
        
          ρ
          
            e
          
        
      
    
    {\displaystyle \rho _{e}}
   = density of the explosive (solid part) and 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   = density of the detonation products. The parameters 
  
    
      
        A
      
    
    {\displaystyle A}
  , 
  
    
      
        B
      
    
    {\displaystyle B}
  , 
  
    
      
        
          R
          
            1
          
        
      
    
    {\displaystyle R_{1}}
  , 
  
    
      
        
          R
          
            2
          
        
      
    
    {\displaystyle R_{2}}
   and 
  
    
      
        ω
      
    
    {\displaystyle \omega }
   are given by several references. In addition, the initial density (solid part) 
  
    
      
        
          ρ
          
            0
          
        
      
    
    {\displaystyle \rho _{0}}
  , speed of detonation 
  
    
      
        
          V
          
            D
          
        
      
    
    {\displaystyle V_{D}}
  , Chapman–Jouguet pressure 
  
    
      
        
          P
          
            C
            J
          
        
      
    
    {\displaystyle P_{CJ}}
   and the chemical energy of the explosive 
  
    
      
        
          e
          
            0
          
        
      
    
    {\displaystyle e_{0}}
   are given in such references. These parameters are obtained by fitting the JWL-EOS to experimental results.  Typical parameters for some explosives are listed in the table below.


== Equations of state for solids and liquids ==
Common abbreviations: 
  
    
      
        η
        =
        
          
            (
            
              
                V
                
                  V
                  
                    0
                  
                
              
            
            )
          
          
            
              1
              3
            
          
        
         
        ,
         
         
        
          K
          
            0
          
          
            ′
          
        
        =
        
          
            
              d
              
                K
                
                  0
                
              
            
            
              d
              p
            
          
        
      
    
    {\displaystyle \eta =\left({\frac {V}{V_{0}}}\right)^{\frac {1}{3}}~,~~K_{0}^{\prime }={\frac {dK_{0}}{dp}}}
  

Tait equation for water and other liquids.  Several equations are referred to as the Tait equation.
Murnaghan equation of state
  
    
      
        p
        (
        V
        )
        =
        
          
            
              K
              
                0
              
            
            
              K
              
                0
              
              ′
            
          
        
        
          [
          
            
              η
              
                −
                3
                
                  K
                  
                    0
                  
                  ′
                
              
            
            −
            1
          
          ]
        
      
    
    {\displaystyle p(V)={\frac {K_{0}}{K_{0}'}}\left[\eta ^{-3K_{0}'}-1\right]}
  Birch–Murnaghan equation of state
  
    
      
        p
        (
        V
        )
        =
        
          
            
              3
              
                K
                
                  0
                
              
            
            2
          
        
        
          (
          
            
              
                1
                −
                
                  η
                  
                    2
                  
                
              
              
                η
                
                  7
                
              
            
          
          )
        
        
          {
          
            1
            +
            
              
                3
                4
              
            
            
              (
              
                
                  K
                  
                    0
                  
                  ′
                
                −
                4
              
              )
            
            
              (
              
                
                  
                    1
                    −
                    
                      η
                      
                        2
                      
                    
                  
                  
                    η
                    
                      2
                    
                  
                
              
              )
            
          
          }
        
      
    
    {\displaystyle p(V)={\frac {3K_{0}}{2}}\left({\frac {1-\eta ^{2}}{\eta ^{7}}}\right)\left\{1+{\frac {3}{4}}\left(K_{0}'-4\right)\left({\frac {1-\eta ^{2}}{\eta ^{2}}}\right)\right\}}
  Stacey-Brennan-Irvine equation of state (falsely often refer to Rose-Vinet equation of state)
  
    
      
        p
        (
        V
        )
        =
        3
        
          K
          
            0
          
        
        
          (
          
            
              
                1
                −
                η
              
              
                η
                
                  2
                
              
            
          
          )
        
        exp
        ⁡
        
          [
          
            
              
                3
                2
              
            
            
              (
              
                
                  K
                  
                    0
                  
                  ′
                
                −
                1
              
              )
            
            (
            1
            −
            η
            )
          
          ]
        
      
    
    {\displaystyle p(V)=3K_{0}\left({\frac {1-\eta }{\eta ^{2}}}\right)\exp \left[{\frac {3}{2}}\left(K_{0}'-1\right)(1-\eta )\right]}
  Modified Rydberg equation of state (more reasonable form for strong compression)
  
    
      
        p
        (
        V
        )
        =
        3
        
          K
          
            0
          
        
        
          (
          
            
              
                1
                −
                η
              
              
                η
                
                  5
                
              
            
          
          )
        
        exp
        ⁡
        
          [
          
            
              
                3
                2
              
            
            
              (
              
                
                  K
                  
                    0
                  
                  ′
                
                −
                3
              
              )
            
            (
            1
            −
            η
            )
          
          ]
        
      
    
    {\displaystyle p(V)=3K_{0}\left({\frac {1-\eta }{\eta ^{5}}}\right)\exp \left[{\frac {3}{2}}\left(K_{0}'-3\right)(1-\eta )\right]}
  Adapted Polynomial equation of state  (second order form = AP2, adapted for extreme compression)
  
    
      
        p
        (
        V
        )
        =
        3
        
          K
          
            0
          
        
        
          (
          
            
              
                1
                −
                η
              
              
                η
                
                  5
                
              
            
          
          )
        
        exp
        ⁡
        
          [
          
            
              c
              
                0
              
            
            (
            1
            −
            η
            )
          
          ]
        
        
          {
          
            1
            +
            
              c
              
                2
              
            
            η
            (
            1
            −
            η
            )
          
          }
        
      
    
    {\displaystyle p(V)=3K_{0}\left({\frac {1-\eta }{\eta ^{5}}}\right)\exp \left[c_{0}(1-\eta )\right]\left\{1+c_{2}\eta (1-\eta )\right\}}
  
with

  
    
      
        
          c
          
            0
          
        
        =
        −
        ln
        ⁡
        
          (
          
            
              
                3
                
                  K
                  
                    0
                  
                
              
              
                p
                
                  FG0
                
              
            
          
          )
        
         
        ,
         
         
        
          p
          
            FG0
          
        
        =
        
          a
          
            0
          
        
        
          
            (
            
              
                Z
                
                  V
                  
                    0
                  
                
              
            
            )
          
          
            
              5
              3
            
          
        
         
        ,
         
         
        
          c
          
            2
          
        
        =
        
          
            3
            2
          
        
        
          (
          
            
              K
              
                0
              
              ′
            
            −
            3
          
          )
        
        −
        
          c
          
            0
          
        
      
    
    {\displaystyle c_{0}=-\ln \left({\frac {3K_{0}}{p_{\text{FG0}}}}\right)~,~~p_{\text{FG0}}=a_{0}\left({\frac {Z}{V_{0}}}\right)^{\frac {5}{3}}~,~~c_{2}={\frac {3}{2}}\left(K_{0}'-3\right)-c_{0}}
  
where 
  
    
      
        
          a
          
            0
          
        
      
    
    {\displaystyle a_{0}}
   = 0.02337 GPa.nm5. The total number of electrons 
  
    
      
        Z
      
    
    {\displaystyle Z}
   in the initial volume 
  
    
      
        
          V
          
            0
          
        
      
    
    {\displaystyle V_{0}}
   determines the Fermi gas pressure 
  
    
      
        
          p
          
            FG0
          
        
      
    
    {\displaystyle p_{\text{FG0}}}
  , which provides for the correct behavior at extreme compression. So far there are no known ""simple"" solids that require higher order terms.Adapted polynomial equation of state   (third order form = AP3)
  
    
      
        p
        (
        V
        )
        =
        3
        
          K
          
            0
          
        
        
          (
          
            
              
                1
                −
                η
              
              
                η
                
                  5
                
              
            
          
          )
        
        exp
        ⁡
        
          [
          
            
              c
              
                0
              
            
            (
            1
            −
            η
            )
          
          ]
        
        
          {
          
            1
            +
            
              c
              
                2
              
            
            η
            (
            1
            −
            η
            )
            +
            
              c
              
                3
              
            
            η
            (
            1
            −
            η
            
              )
              
                2
              
            
          
          }
        
      
    
    {\displaystyle p(V)=3K_{0}\left({\frac {1-\eta }{\eta ^{5}}}\right)\exp \left[c_{0}(1-\eta )\right]\left\{1+c_{2}\eta (1-\eta )+c_{3}\eta (1-\eta )^{2}\right\}}
  Johnson–Holmquist equation of state
  
    
      
        p
        (
        V
        )
        =
        
          
            {
            
              
                
                  
                    k
                    
                      1
                    
                  
                   
                  ξ
                  +
                  
                    k
                    
                      2
                    
                  
                   
                  
                    ξ
                    
                      2
                    
                  
                  +
                  
                    k
                    
                      3
                    
                  
                   
                  
                    ξ
                    
                      3
                    
                  
                  +
                  Δ
                  p
                
                
                  
                  
                    Compression
                  
                
              
              
                
                  
                    k
                    
                      1
                    
                  
                   
                  ξ
                
                
                  
                  
                    Tension
                  
                
              
            
            
          
        
         
        ;
         
         
        ξ
        :=
        
          
            
              
                
              
              
                
                  
                    V
                    
                      0
                    
                  
                
              
            
            
              
                
              
              
                
                  V
                
              
            
          
        
        −
        1
      
    
    {\displaystyle p(V)={\begin{cases}k_{1}~\xi +k_{2}~\xi ^{2}+k_{3}~\xi ^{3}+\Delta p&\qquad {\text{Compression}}\\k_{1}~\xi &\qquad {\text{Tension}}\end{cases}}~;~~\xi :={\cfrac {V_{0}}{V}}-1}
  Mie–Grüneisen equation of state (for a more detailed discussion see ref.)
  
    
      
        p
        (
        V
        )
        −
        
          p
          
            0
          
        
        =
        
          
            Γ
            V
          
        
        
          (
          
            e
            −
            
              e
              
                0
              
            
          
          )
        
      
    
    {\displaystyle p(V)-p_{0}={\frac {\Gamma }{V}}\left(e-e_{0}\right)}
  Anton-Schmidt equation of state
  
    
      
        p
        (
        V
        )
        =
        −
        β
        
          
            (
            
              
                V
                
                  V
                  
                    0
                  
                
              
            
            )
          
          
            n
          
        
        ln
        ⁡
        
          (
          
            
              V
              
                V
                
                  0
                
              
            
          
          )
        
      
    
    {\displaystyle p(V)=-\beta \left({\frac {V}{V_{0}}}\right)^{n}\ln \left({\frac {V}{V_{0}}}\right)}
  
where 
  
    
      
        β
        =
        
          K
          
            0
          
        
      
    
    {\displaystyle \beta =K_{0}}
   is the bulk modulus at equilibrium volume 
  
    
      
        
          V
          
            0
          
        
      
    
    {\displaystyle V_{0}}
   and 
  
    
      
        n
        =
        −
        
          
            
              K
              
                0
              
              ′
            
            2
          
        
      
    
    {\displaystyle n=-{\frac {K'_{0}}{2}}}
   typically about −2 is often related to the Grüneisen parameter by 
  
    
      
        n
        =
        −
        
          
            1
            6
          
        
        −
        
          γ
          
            G
          
        
      
    
    {\displaystyle n=-{\frac {1}{6}}-\gamma _{G}}
  


== See also ==
Gas laws
Departure function
Table of thermodynamic equations
Real gas
Cluster Expansion


== References ==


== External links ==
Elliott & Lira, (1999). Introductory Chemical Engineering Thermodynamics, Prentice Hall.","pandas(index=228, _1=228, text='in physics and thermodynamics, an equation of state is a thermodynamic equation relating state variables which describe the state of matter under a given set of physical conditions, such as pressure, volume, temperature (pvt), or internal energy. equations of state are useful in describing the properties of fluids, mixtures of fluids, solids, and the interior of stars.   == overview == at present, there is no single equation of state that accurately predicts the properties of all substances under all conditions. an example of an equation of state correlates densities of gases and liquids to temperatures and pressures, known as the ideal gas law, which is roughly accurate for weakly polar gases at low pressures and moderate temperatures. this equation becomes increasingly inaccurate at higher pressures and lower temperatures, and fails to predict condensation from a gas to a liquid. another common use is in modeling the interior of stars, including neutron stars, dense matter (quark–gluon plasmas) and radiation fields. a related concept is the perfect fluid equation of state used in cosmology. equations of state can also describe solids, including the transition of solids from one crystalline state to another. in a practical context, equations of state are instrumental for pvt calculations in process engineering problems, such as petroleum gas/liquid equilibrium calculations. a successful pvt model based on a fitted equation of state can be helpful to determine the state of the flow regime, the parameters for handling the reservoir fluids, and pipe sizing. measurements of equation-of-state parameters, especially at high pressures, can be made using lasers.   == historical == boyle\'s law was perhaps the first expression of an equation of state. in 1662, the irish physicist and chemist robert boyle performed a series of experiments employing a j-shaped glass tube, which was sealed on one end. mercury was added to the tube, trapping a fixed quantity of air in the short, sealed end of the tube. then the volume of gas was measured as additional mercury was added to the tube. the pressure of the gas could be determined by the difference between the mercury level in the short end of the tube and that in the long, open end. through these experiments, boyle noted that the gas volume varied inversely with the pressure. in mathematical form, this can be stated as:     p v =  c o n s t a n t  .         == see also == gas laws departure function table of thermodynamic equations real gas cluster expansion   == references ==   == external links == elliott & lira, (1999). introductory chemical engineering thermodynamics, prentice hall.')"
229,"A kludge or kluge () is a workaround or quick-and-dirty solution that is clumsy, inelegant, inefficient, difficult to extend and hard to maintain. This term is used in diverse fields such as computer science, aerospace engineering, Internet slang, evolutionary neuroscience, and government. It is similar in meaning to the naval term jury rig.


== Etymology ==
The word has alternate spellings (kludge and kluge), pronunciations ( and , rhyming with judge and stooge respectively, and several proposed etymologies.


=== Jackson W. Granholm ===
The Oxford English Dictionary (2nd ed., 1989), cites Jackson W. Granholm's 1962 ""How to Design a Kludge"" article in the American computer magazine Datamation.
kludge /kluːdʒ/ Also kluge. [J. W. Granholm's jocular invention: see first quot.; cf. also bodge v., fudge v.]'An ill-assorted collection of poorly-matching parts, forming a distressing whole' (Granholm); esp. in Computing, a machine, system, or program that has been improvised or 'bodged' together; a hastily improvised and poorly thought-out solution to a fault or 'bug'. ...
The OED entry also includes the verb kludge (""to improvise with a kludge or kludges"") and kludgemanship (""skill in designing or applying kludges"").

Granholm humorously imagined a fictitious source for the term:Phineas Burling is the Chief calligrapher with the Fink and Wiggles Publishing Company, Inc. . . . According to Burling, the word ""kludge"" first appeared in the English language in the early fifteen-hundreds. . . .
The word ""kludge"" is, according to Burling, derived from the same root as the German ""klug"" (Dutch kloog, Swedish Klag, Danish Klog, Gothic Klaugen, Lettish Kladnis and Sanskrit Veklaunn), originally meaning ""smart"" or ""witty"". In the typical machinations of language in evolutionary growth, the word ""Kludge"" eventually came to mean ""not so smart"" or ""pretty ridiculous"". . . . Today ""kludge"" forms one of the most beloved words in design terminology, and it stands ready for handy application to the work of anyone who gins up 110-volt circuitry to plug into the 220 VAC source. The building of a Kludge, however, is not work for amateurs.Although OED accepts Granholm's coinage of the term, there are examples of its use before the 1960s.


=== Germanic sources ===
American Yiddish speakers use klug (קלוג) to mean ""too smart by half"", the reflected meaning of German klug (""clever"").  This may explain the idea of clever but clumsy and temporary, as well as the pronunciation variation from German.Cf. German Kloß (""clod"", diminutive Klößchen), Low Saxon klut, klute, Dutch kluit, perhaps related to Low German diminutive klütje (""dumpling, clod""), Danish Jutland dialect klyt (""piece of bad workmanship, kludge""), and Standard Danish kludder (""mess, disorder"").
Arguments against the derivation from German klug:

There is no equivalent usage in German
Both pronunciations contain the soft ""g"" (dʒ) not present in German
The word emerges in English only in the 20th century
The alleged Swedish translation, klag, is incorrect and would properly be spelled klok.


=== Paper feeder ===
Another hypothesis dates to 1907, ""when John Brandtjen convinced two young machinists from Oslo, Norway named Abel and Eneval Kluge to service and install presses for his fledgling printing equipment firm"". In 1919, the brothers invented an automatic feeder for printing presses which was a success, though ""temperamental, subject to frequent breakdowns, and devilishly difficult to repair — but oh, so clever!"" The Kluge brothers continued to innovate, and the company remained active as of 2020. Given that the feeder bore the Kluge name, it seems reasonable that it became a byword for over-complex mechanical contraptions. 


=== Acronym ===
Other suggested folk etymologies or backronyms for kludge or kluge are: klumsy, lame, ugly, dumb, but good enough; or klutzy lashup, under-going engineering.


== Kludge vs kluge ==
The Jargon File (a.k.a. The New Hacker's Dictionary), a glossary of computer programmer slang maintained by Eric S. Raymond, differentiates kludge from kluge and cites usage examples predating 1962. Kluge seems to have the sense of overcomplicated, while kludge has only the sense of poorly done. 

kludge /kluhj/
n. Incorrect (though regrettably common) spelling of kluge (US). These two words have been confused in American usage since the early 1960s, and widely confounded in Great Britain since the end of World War II.
[TMRC] A crock that works. (A long-ago Datamation article by Jackson Granholme  [sic] similarly said: ""An ill-assorted collection of poorly matching parts, forming a distressing whole."")
v. To use a kludge to get around a problem. ""I've kludged around it for now, but I'll fix it up properly later."" 
This Jargon File entry notes kludge apparently derives via British military slang from Scots cludge or cludgie meaning ""a common toilet"", and became confused with U.S. kluge during or after World War II.

kluge: /klooj/ [from the German 'klug', clever; poss. related to Polish & Russian 'klucz' (a key, a hint, a main point)]
n. A Rube Goldberg (or Heath Robinson) device, whether in hardware or software.
n. A clever programming trick intended to solve a particular nasty case in an expedient, if not clear, manner. Often used to repair bugs. Often involves ad-hockery and verges on being a crock.
n. Something that works for the wrong reason.
vt. To insert a kluge into a program. ""I've kluged this routine to get around that weird bug, but there's probably a better way.""
[WPI] n. A feature that is implemented in a rude manner. 
This entry notes kluge, which is now often spelled kludge, ""was the original spelling, reported around computers as far back as the mid-1950s and, at that time, used exclusively of hardware kluges"". 
Kluge ""was common Navy slang in the World War II era for any piece of electronics that worked well on shore but consistently failed at sea"". A summary of a 1947 article in the New York Folklore Quarterly states: 

On being drafted into the navy, Murgatroyd gave his profession as ""kluge maker"". . . . Whenever Murgatroyd was asked what he was doing, he said he was making a kluge, and actually he was one of the world's best kluge makers. Not wanting to seem ignorant, his superiors kept giving him commendations and promotions. . . . One day . . .  the admiral asked him what a kluge was – the first person ever to do so. Murgatroyd said it was hard to explain, but he would make one so the admiral could see what it was. After a couple of days, he returned with a complex object.
""Interesting,"" said the admiral, ""but what does it do?"" In reply, Murgatroyd dropped it over the side of the ship. As the thing sank, it went ""kluge"".
The Jargon File further includes kluge around ""to avoid a bug or difficult condition by inserting a kluge"", kluge up ""to lash together a quick hack to perform a task"". 
After Granholm's 1962 ""How to Design a Kludge"" article popularized the kluge variant kludge, both were interchangeably used and confused. The Jargon File concludes:

The result of this history is a tangle. Many younger U.S. hackers pronounce the word as /klooj/ but spell it, incorrectly for its meaning and pronunciation, as 'kludge'. ... British hackers mostly learned /kluhj/ orally, use it in a restricted negative sense and are at least consistent. European hackers have mostly learned the word from written American sources and tend to pronounce it /kluhj/ but use the wider American meaning! Some observers consider this mess appropriate in view of the word's meaning. 


== Industries ==


=== Aerospace engineering ===
In aerospace, a kludge was a temporary design using separate commonly available components that were not flightworthy in order to proof the design and enable concurrent software development while the integrated components were developed and manufactured. The term was in common enough use to appear in a fictional movie about the US space program.Perhaps the ultimate kludge was the first US space station, Skylab. Its two major components, the Saturn Workshop and the Apollo Telescope Mount, began development as separate projects (the SWS was kludged from the S-IVB stage of the Saturn 1B and Saturn V launch vehicles, the ATM was kludged from an early design for the descent stage of the Apollo Lunar Module). Later the SWS and ATM were folded into the Apollo Applications Program, but the components were to have been launched separately, then docked in orbit. In the final design, the SWS and ATM were launched together, but for the single-launch concept to work, the ATM had to pivot 90 degrees on a truss structure from its launch position to its on-orbit orientation, clearing the way for the crew to dock its Apollo Command/Service Module at the axial docking port of the Multiple Docking Adapter.
The Airlock Module's manufacturer, McDonnell Douglas, even recycled the hatch design from its Gemini spacecraft and kludged what was originally designed for the conical Gemini Command Module onto the cylindrical Skylab Airlock Module. The Skylab project, managed by the National Aeronautics and Space Administration's Marshall Space Flight Center, was seen by the Manned Spacecraft Center (later Johnson Space Center) as an invasion of its historical role as the NASA center for manned spaceflight.  Thus, MSC personnel missed no opportunity to disparage the Skylab project, calling it ""the kludge"".


=== Computer science ===
In modern computing terminology, a ""kludge"" (or often a ""hack"") is a solution to a problem, the performance of a task, or a system fix which is inefficient, inelegant (""hacky""), or even incomprehensible, but which somehow works. It is similar to a workaround, but quick and ugly. To ""kludge around something"" is to avoid a bug or difficulty by building a kludge, perhaps exploiting properties of the bug itself. A kludge is often used to modify a working system while avoiding fundamental changes, or to ensure backwards compatibility. Hack can also be used with a positive connotation, for a quick solution to a frustrating problem.A kludge is often used to fix an unanticipated problem in an earlier kludge; this is essentially a kind of cruft.
A solution might be a kludge if it fails in corner cases. An intimate knowledge of the problem domain and execution environment is typically required to build a corner-case kludge. More commonly, a kludge is a heuristic which was expected to work almost always, but ends up failing often. 
A 1960s Soviet anecdote tells of a computer part which needed a slightly delayed signal to work. Rather than setting up a timing system, the kludge was to connect long coils of internal wires to slow the electrical signal.
Another type of kludge is the evasion of an unknown problem or bug in a computer program. Rather than continue to struggle to diagnose and fix the bug, the programmer may write additional code to compensate. For example, if a variable keeps ending up doubled, a kludge may be to add later code that divides by two rather than to search for the original incorrect computation.
In computer networking, use of NAT (Network Address Translation) (RFC 1918) or PAT (Port Address Translation) to cope with the shortage of IPv4 addresses is an example of a kludge.
In FidoNet terminology, kludge refers to a piece of control data embedded inside a message.


=== Evolutionary neuroscience ===

The kludge or kluge metaphor has been adapted in fields such as evolutionary neuroscience, particularly in reference to the human brain.
The neuroscientist David Linden discusses how intelligent design proponents have misconstrued brain anatomy.

The transcendent aspects of our human experience, the things that touch our emotional and cognitive core, were not given to us by a Great Engineer. These are not the latest design features of an impeccably crafted brain. Rather, at every turn, brain design has been a kludge, a workaround, a jumble, a pastiche. The things we hold highest in our human experience (love, memory, dreams, and a predisposition for religious thought) result from a particular agglomeration of ad hoc solutions that have been piled on through millions of years of evolution history. It's not that we have fundamentally human thoughts and feelings despite the kludgy design of the brain as molded by the twists and turns of evolutionary history. Rather, we have them precisely because of that history. 
The research psychologist Gary Marcus's book Kluge: The Haphazard Construction of the Human Mind compares evolutionary kluges with engineering ones like manifold vacuum-powered windshield wipers – when you accelerated or drove uphill, ""Your wipers slowed to a crawl, or even stopped working altogether.""

For instance, the vertebrate eye's retina that is installed backward, facing the back of the head rather than the front. As a result, all kinds of stuff gets in its way, including a bunch of wiring that passes through the eye and leaves us with a pair of blind spots, one in each eye.


== Other uses ==
In John Varley's 1985 short story ""Press Enter_"", the antagonist, a reclusive hacker, adopts the identity Charles Kluge.
In the science fiction television series Andromeda, genetically engineered human beings called Nietzscheans use the term disparagingly to refer to genetically unmodified humans.
In a 2012 article, political scientist Steven Teles used the term ""kludgeocracy"" to criticize the complexity of social welfare policy in the United States. Teles argues that institutional and political obstacles to passing legislation often drive policy makers to accept expedient fixes rather than carefully thought out reforms.


== See also ==
Bricolage
Jugaad, an Indian version of ""kludge""
Bodging and Jury rigging, two English terms of similar meaning.
MacGyver
Unintended consequence
Drop-in replacement


== References ==


== External links ==
First Usage of ""Kludge"" on UseNET (26 May 1981)
First Usage of ""Kluge"" on UseNET (14 December 1981)
The Jargon File: Kludge
World Wide Words: Kludge
Work-arounds, Make-work, and Kludges, Philip Koopman and Robert R. Hoffman","pandas(index=229, _1=229, text='a kludge or kluge () is a workaround or quick-and-dirty solution that is clumsy, inelegant, inefficient, difficult to extend and hard to maintain. this term is used in diverse fields such as computer science, aerospace engineering, internet slang, evolutionary neuroscience, and government. it is similar in meaning to the naval term jury rig.   == etymology == the word has alternate spellings (kludge and kluge), pronunciations ( and , rhyming with judge and stooge respectively, and several proposed etymologies. the kludge or kluge metaphor has been adapted in fields such as evolutionary neuroscience, particularly in reference to the human brain. the neuroscientist david linden discusses how intelligent design proponents have misconstrued brain anatomy.  the transcendent aspects of our human experience, the things that touch our emotional and cognitive core, were not given to us by a great engineer. these are not the latest design features of an impeccably crafted brain. rather, at every turn, brain design has been a kludge, a workaround, a jumble, a pastiche. the things we hold highest in our human experience (love, memory, dreams, and a predisposition for religious thought) result from a particular agglomeration of ad hoc solutions that have been piled on through millions of years of evolution history. it\'s not that we have fundamentally human thoughts and feelings despite the kludgy design of the brain as molded by the twists and turns of evolutionary history. rather, we have them precisely because of that history. the research psychologist gary marcus\'s book kluge: the haphazard construction of the human mind compares evolutionary kluges with engineering ones like manifold vacuum-powered windshield wipers – when you accelerated or drove uphill, ""your wipers slowed to a crawl, or even stopped working altogether.""  for instance, the vertebrate eye\'s retina that is installed backward, facing the back of the head rather than the front. as a result, all kinds of stuff gets in its way, including a bunch of wiring that passes through the eye and leaves us with a pair of blind spots, one in each eye.   == other uses == in john varley\'s 1985 short story ""press enter_"", the antagonist, a reclusive hacker, adopts the identity charles kluge. in the science fiction television series andromeda, genetically engineered human beings called nietzscheans use the term disparagingly to refer to genetically unmodified humans. in a 2012 article, political scientist steven teles used the term ""kludgeocracy"" to criticize the complexity of social welfare policy in the united states. teles argues that institutional and political obstacles to passing legislation often drive policy makers to accept expedient fixes rather than carefully thought out reforms.   == see also == bricolage jugaad, an indian version of ""kludge"" bodging and jury rigging, two english terms of similar meaning. macgyver unintended consequence drop-in replacement   == references ==   == external links == first usage of ""kludge"" on usenet (26 may 1981) first usage of ""kluge"" on usenet (14 december 1981) the jargon file: kludge world wide words: kludge work-arounds, make-work, and kludges, philip koopman and robert r. hoffman')"
230,"Mechanical engineering is an engineering branch that combines engineering physics and mathematics principles with materials science to design, analyze, manufacture, and maintain mechanical systems.  It is one of the oldest and broadest of the engineering branches.
The mechanical engineering field requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, structural analysis, and electricity.  In addition to these core principles, mechanical engineers use tools such as computer-aided design (CAD), computer-aided manufacturing (CAM), and product lifecycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, aircraft, watercraft, robotics, medical devices, weapons, and others.  It is the branch of engineering that involves the design, production, and operation of machinery.Mechanical engineering emerged as a field during the Industrial Revolution in Europe in the 18th century; however, its development can be traced back several thousand years around the world.  In the 19th century, developments in physics led to the development of mechanical engineering science. The field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology.  It also overlaps with aerospace engineering, metallurgical engineering, civil engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts.  Mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modelling of biological systems.


== History ==

The application of mechanical engineering can be seen in the archives of various ancient and medieval societies. The six classic simple machines were known in the ancient Near East. The wedge and the inclined plane (ramp) were known since prehistoric times. The wheel, along with the wheel and axle mechanism, was invented in Mesopotamia (modern Iraq) during the 5th millennium BC. The lever mechanism first appeared around 5,000 years ago in the Near East, where it was used in a simple balance scale, and to move large objects in ancient Egyptian technology. The lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in Mesopotamia circa 3000 BC. The earliest evidence of pulleys date back to Mesopotamia in the early 2nd millennium BC.The Sakia was developed in the Kingdom of Kush during the 4th century BC. It relied on animal power reducing the tow on the requirement of human energy. Reservoirs in the form of Hafirs were developed in Kush to store water and boost irrigation. Bloomeries and blast furnaces were developed during the seventh century BC in Meroe. Kushite sundials applied mathematics in the form of advanced trigonometry.The earliest practical water-powered machines, the water wheel and watermill, first appeared in the Persian Empire, in what are now Iraq and Iran, by the early 4th century BC. In ancient Greece, the works of Archimedes (287–212 BC) influenced mechanics in the Western tradition. In Roman Egypt, Heron of Alexandria (c. 10–70 AD) created the first steam-powered device (Aeolipile). In China, Zhang Heng (78–139 AD) improved a water clock and invented a seismometer, and Ma Jun (200–265 AD) invented a chariot with differential gears. The medieval Chinese horologist and engineer Su Song (1020–1101 AD) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval European clocks. He also invented the world's first known endless power-transmitting chain drive.During the Islamic Golden Age (7th to 15th century), Muslim inventors made remarkable contributions in the field of mechanical technology. Al-Jazari, who was one of them, wrote his famous Book of Ingenious Devices in 1206 and presented many mechanical designs. Al-Jazari is also the first known person to create devices such as the crankshaft and camshaft, which now form the basics of many mechanisms.During the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in England. Sir Isaac Newton formulated Newton's Laws of Motion and developed Calculus, the mathematical basis of physics. Newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as Sir Edmond Halley, much to the benefit of all mankind. Gottfried Wilhelm Leibniz is also credited with creating Calculus during this time period.During the early 19th century industrial revolution, machine tools were developed in England, Germany, and Scotland. This allowed mechanical engineering to develop as a separate field within engineering. They brought with them manufacturing machines and the engines to power them. The first British professional society of mechanical engineers was formed in 1847 Institution of Mechanical Engineers, thirty years after the civil engineers formed the first such professional society Institution of Civil Engineers. On the European continent, Johann von Zimmermann (1820–1901) founded the first factory for grinding machines in Chemnitz, Germany in 1848.
In the United States, the American Society of Mechanical Engineers (ASME) was formed in 1880, becoming the third such professional engineering society, after the American Society of Civil Engineers (1852) and the American Institute of Mining Engineers (1871). The first schools in the United States to offer an engineering education were the United States Military Academy in 1817, an institution now known as Norwich University in 1819, and Rensselaer Polytechnic Institute in 1825. Education in mechanical engineering has historically been based on a strong foundation in mathematics and science.


== Education ==

Degrees in mechanical engineering are offered at various universities worldwide. Mechanical engineering programs typically take four to five years of study depending on the place and university and result in a Bachelor of Engineering (B.Eng. or B.E.), Bachelor of Science (B.Sc. or B.S.), Bachelor of Science Engineering (B.Sc.Eng.), Bachelor of Technology (B.Tech.), Bachelor of Mechanical Engineering (B.M.E.), or Bachelor of Applied Science (B.A.Sc.) degree, in or with emphasis in mechanical engineering. In Spain, Portugal and most of South America, where neither B.S. nor B.Tech. programs have been adopted, the formal name for the degree is ""Mechanical Engineer"", and the course work is based on five or six years of training. In Italy the course work is based on five years of education, and training, but in order to qualify as an Engineer one has to pass a state exam at the end of the course. In Greece, the coursework is based on a five-year curriculum and the requirement of a 'Diploma' Thesis, which upon completion a 'Diploma' is awarded rather than a B.Sc.In the United States, most undergraduate mechanical engineering programs are accredited by the Accreditation Board for Engineering and Technology (ABET) to ensure similar course requirements and standards among universities. The ABET web site lists 302 accredited mechanical engineering programs as of 11 March 2014. Mechanical engineering programs in Canada are accredited by the Canadian Engineering Accreditation Board (CEAB), and most other countries offering engineering degrees have similar accreditation societies.
In Australia, mechanical engineering degrees are awarded as Bachelor of Engineering (Mechanical) or similar nomenclature, although there are an increasing number of specialisations. The degree takes four years of full-time study to achieve. To ensure quality in engineering degrees, Engineers Australia accredits engineering degrees awarded by Australian universities in accordance with the global Washington Accord. Before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm.  Similar systems are also present in South Africa and are overseen by the Engineering Council of South Africa (ECSA).
In India, to become an engineer, one needs to have an engineering degree like a B.Tech or B.E, have a diploma in engineering, or by completing a course in an engineering trade like fitter from the Industrial Training Institute (ITIs) to receive a ""ITI Trade Certificate"" and also pass the All India Trade Test (AITT) with an engineering trade conducted by the National Council of Vocational Training (NCVT) by which one is awarded a ""National Trade Certificate"".  A similar system is used in Nepal.Some mechanical engineers go on to pursue a postgraduate degree such as a Master of Engineering, Master of Technology, Master of Science, Master of Engineering Management (M.Eng.Mgt. or M.E.M.), a Doctor of Philosophy in engineering (Eng.D. or Ph.D.) or an engineer's degree. The master's and engineer's degrees may or may not include research. The Doctor of Philosophy includes a significant research component and is often viewed as the entry point to academia.  The Engineer's degree exists at a few institutions at an intermediate level between the master's degree and the doctorate.


=== Coursework ===
Standards set by each country's accreditation society are intended to provide uniformity in fundamental subject material, promote competence among graduating engineers, and to maintain confidence in the engineering profession as a whole.  Engineering programs in the U.S., for example, are required by ABET to show that their students can ""work professionally in both thermal and mechanical systems areas."" The specific courses required to graduate, however, may differ from program to program. Universities and Institutes of technology will often combine multiple subjects into a single class or split a subject into multiple classes, depending on the faculty available and the university's major area(s) of research.
The fundamental subjects of mechanical engineering usually include:

Mathematics (in particular, calculus, differential equations, and linear algebra)
Basic physical sciences (including physics and chemistry)
Statics and dynamics
Strength of materials and solid mechanics
Materials engineering, Composites
Thermodynamics, heat transfer, energy conversion, and HVAC
Fuels, combustion, Internal combustion engine
Fluid mechanics (including fluid statics and fluid dynamics)
Mechanism and Machine design (including kinematics and dynamics)
Instrumentation and measurement
Manufacturing engineering, technology, or processes
Vibration, control theory and control engineering
Hydraulics and Pneumatics
Mechatronics and robotics
Engineering design and product design
Drafting, computer-aided design (CAD) and computer-aided manufacturing (CAM)Mechanical engineers are also expected to understand and be able to apply basic concepts from chemistry, physics, Tribology, chemical engineering, civil engineering, and electrical engineering. All mechanical engineering programs include multiple semesters of mathematical classes including calculus, and advanced mathematical concepts including differential equations, partial differential equations, linear algebra, abstract algebra, and differential geometry, among others.
In addition to the core mechanical engineering curriculum, many mechanical engineering programs offer more specialized programs and classes, such as control systems, robotics, transport and logistics, cryogenics, fuel technology, automotive engineering, biomechanics, vibration, optics and others, if a separate department does not exist for these subjects.Most mechanical engineering programs also require varying amounts of research or community projects to gain practical problem-solving experience. In the United States it is common for mechanical engineering students to complete one or more internships while studying, though this is not typically mandated by the university. Cooperative education is another option. Future work skills research puts demand on study components that feed student's creativity and innovation.


== Job duties ==
Mechanical engineers research, design, develop, build, and test mechanical and thermal devices, including tools, engines, and machines.
Mechanical engineers typically do the following:

Analyze problems to see how mechanical and thermal devices might help solve the problem.
Design or redesign mechanical and thermal devices using analysis and computer-aided design.
Develop and test prototypes of devices they design.
Analyze the test results and change the design as needed.
Oversee the manufacturing process for the device.Mechanical engineers design and oversee the manufacturing of many products ranging from medical devices to new batteries.  They also design power-producing machines such as electric generators, internal combustion engines, and steam and gas turbines as well as power-using machines, such as refrigeration and air-conditioning systems.
Like other engineers, mechanical engineers use computers to help create and analyze designs, run simulations and test how a machine is likely to work.


=== License and regulation ===
Engineers may seek license by a state, provincial, or national government. The purpose of this process is to ensure that engineers possess the necessary technical knowledge, real-world experience, and knowledge of the local legal system to practice engineering at a professional level. Once certified, the engineer is given the title of Professional Engineer (United States, Canada, Japan, South Korea, Bangladesh and South Africa), Chartered Engineer (in the United Kingdom, Ireland, India and Zimbabwe), Chartered Professional Engineer (in Australia and New Zealand) or European Engineer (much of the European Union).
In the U.S., to become a licensed Professional Engineer (PE), an engineer must pass the comprehensive FE (Fundamentals of Engineering) exam, work a minimum of 4 years as an Engineering Intern (EI) or Engineer-in-Training (EIT), and pass the ""Principles and Practice"" or PE (Practicing Engineer or Professional Engineer) exams.  The requirements and steps of this process are set forth by the National Council of Examiners for Engineering and Surveying (NCEES), a composed of engineering and land surveying licensing boards representing all U.S. states and territories.
In the UK, current graduates require a BEng plus an appropriate master's degree or an integrated MEng degree, a minimum of 4 years post graduate on the job competency development and a peer reviewed project report to become a Chartered Mechanical Engineer (CEng, MIMechE) through the Institution of Mechanical Engineers. CEng MIMechE can also be obtained via an examination route administered by the City and Guilds of London Institute.In most developed countries, certain engineering tasks, such as the design of bridges, electric power plants, and chemical plants, must be approved by a professional engineer or a chartered engineer. ""Only a licensed engineer, for instance, may prepare, sign, seal and submit engineering plans and drawings to a public authority for approval, or to seal engineering work for public and private clients."" This requirement can be written into state and provincial legislation, such as in the Canadian provinces, for example the Ontario or Quebec's Engineer Act.In other countries, such as Australia, and the UK, no such legislation exists; however, practically all certifying bodies maintain a code of ethics independent of legislation, that they expect all members to abide by or risk expulsion.


=== Salaries and workforce statistics ===
The total number of engineers employed in the U.S. in 2015 was roughly 1.6 million. Of these, 278,340 were mechanical engineers (17.28%), the largest discipline by size.  In 2012, the median annual income of mechanical engineers in the U.S. workforce was $80,580. The median income was highest when working for the government ($92,030), and lowest in education ($57,090).  In 2014, the total number of mechanical engineering jobs was projected to grow 5% over the next decade. As of 2009, the average starting salary was $58,800 with a bachelor's degree.


== Subdisciplines ==
The field of mechanical engineering can be thought of as a collection of many mechanical engineering science disciplines. Several of these subdisciplines which are typically taught at the undergraduate level are listed below, with a brief explanation and the most common application of each. Some of these subdisciplines are unique to mechanical engineering, while others are a combination of mechanical engineering and one or more other disciplines. Most work that a mechanical engineer does uses skills and techniques from several of these subdisciplines, as well as specialized subdisciplines. Specialized subdisciplines, as used in this article, are more likely to be the subject of graduate studies or on-the-job training than undergraduate research. Several specialized subdisciplines are discussed in this section.


=== Mechanics ===

Mechanics is, in the most general sense, the study of forces and their effect upon matter. Typically, engineering mechanics is used to analyze and predict the acceleration and deformation (both elastic and plastic) of objects under known forces (also called loads) or stresses. Subdisciplines of mechanics include

Statics, the study of non-moving bodies under known loads, how forces affect static bodies
Dynamics the study of how forces affect moving bodies. Dynamics includes kinematics (about movement, velocity, and acceleration) and kinetics (about forces and resulting accelerations).
Mechanics of materials, the study of how different materials deform under various types of stress
Fluid mechanics, the study of how fluids react to forces
Kinematics, the study of the motion of bodies (objects) and systems (groups of objects), while ignoring the forces that cause the motion.  Kinematics is often used in the design and analysis of mechanisms.
Continuum mechanics, a method of applying mechanics that assumes that objects are continuous (rather than discrete)Mechanical engineers typically use mechanics in the design or analysis phases of engineering. If the engineering project were the design of a vehicle, statics might be employed to design the frame of the vehicle, in order to evaluate where the stresses will be most intense. Dynamics might be used when designing the car's engine, to evaluate the forces in the pistons and cams as the engine cycles. Mechanics of materials might be used to choose appropriate materials for the frame and engine. Fluid mechanics might be used to design a ventilation system for the vehicle (see HVAC), or to design the intake system for the engine.


=== Mechatronics and robotics ===

Mechatronics is a combination of mechanics and electronics.  It is an interdisciplinary branch of mechanical engineering, electrical engineering and software engineering that is concerned with integrating electrical and mechanical engineering to create hybrid systems.  In this way, machines can be automated through the use of electric motors, servo-mechanisms, and other electrical systems in conjunction with special software. A common example of a mechatronics system is a CD-ROM drive. Mechanical systems open and close the drive, spin the CD and move the laser, while an optical system reads the data on the CD and converts it to bits. Integrated software controls the process and communicates the contents of the CD to the computer.
Robotics is the application of mechatronics to create robots, which are often used in industry to perform tasks that are dangerous, unpleasant, or repetitive. These robots may be of any shape and size, but all are preprogrammed and interact physically with the world. To create a robot, an engineer typically employs kinematics (to determine the robot's range of motion) and mechanics (to determine the stresses within the robot).
Robots are used extensively in industrial engineering. They allow businesses to save money on labor, perform tasks that are either too dangerous or too precise for humans to perform them economically, and to ensure better quality. Many companies employ assembly lines of robots, especially in Automotive Industries and some factories are so robotized that they can run by themselves. Outside the factory, robots have been employed in bomb disposal, space exploration, and many other fields. Robots are also sold for various residential applications, from recreation to domestic applications.


=== Structural analysis ===

Structural analysis is the branch of mechanical engineering (and also civil engineering) devoted to examining why and how objects fail and to fix the objects and their performance. Structural failures occur in two general modes: static failure, and fatigue failure. Static structural failure occurs when, upon being loaded (having a force applied) the object being analyzed either breaks or is deformed plastically, depending on the criterion for failure. Fatigue failure occurs when an object fails after a number of repeated loading and unloading cycles. Fatigue failure occurs because of imperfections in the object: a microscopic crack on the surface of the object, for instance, will grow slightly with each cycle (propagation) until the crack is large enough to cause ultimate failure.Failure is not simply defined as when a part breaks, however; it is defined as when a part does not operate as intended. Some systems, such as the perforated top sections of some plastic bags, are designed to break. If these systems do not break, failure analysis might be employed to determine the cause.
Structural analysis is often used by mechanical engineers after a failure has occurred, or when designing to prevent failure. Engineers often use online documents and books such as those published by ASM to aid them in determining the type of failure and possible causes.
Once theory is applied to a mechanical design, physical testing is often performed to verify calculated results. Structural analysis may be used in an office when designing parts, in the field to analyze failed parts, or in laboratories where parts might undergo controlled failure tests.


=== Thermodynamics and thermo-science ===

Thermodynamics is an applied science used in several branches of engineering, including mechanical and chemical engineering. At its simplest, thermodynamics is the study of energy, its use and transformation through a system. Typically, engineering thermodynamics is concerned with changing energy from one form to another. As an example, automotive engines convert chemical energy (enthalpy) from the fuel into heat, and then into mechanical work that eventually turns the wheels.
Thermodynamics principles are used by mechanical engineers in the fields of heat transfer, thermofluids, and energy conversion. Mechanical engineers use thermo-science to design engines and power plants, heating, ventilation, and air-conditioning (HVAC) systems, heat exchangers, heat sinks, radiators, refrigeration, insulation, and others.


=== Design and drafting ===

Drafting or technical drawing is the means by which mechanical engineers design products and create instructions for manufacturing parts. A technical drawing can be a computer model or hand-drawn schematic showing all the dimensions necessary to manufacture a part, as well as assembly notes, a list of required materials, and other pertinent information. A U.S. mechanical engineer or skilled worker who creates technical drawings may be referred to as a drafter or draftsman. Drafting has historically been a two-dimensional process, but computer-aided design (CAD) programs now allow the designer to create in three dimensions.
Instructions for manufacturing a part must be fed to the necessary machinery, either manually, through programmed instructions, or through the use of a computer-aided manufacturing (CAM) or combined CAD/CAM program. Optionally, an engineer may also manually manufacture a part using the technical drawings. However, with the advent of computer numerically controlled (CNC) manufacturing, parts can now be fabricated without the need for constant technician input. Manually manufactured parts generally consist of spray coatings, surface finishes, and other processes that cannot economically or practically be done by a machine.
Drafting is used in nearly every subdiscipline of mechanical engineering, and by many other branches of engineering and architecture. Three-dimensional models created using CAD software are also commonly used in finite element analysis (FEA) and computational fluid dynamics (CFD).


== Modern tools ==

Many mechanical engineering companies, especially those in industrialized nations, have begun to incorporate computer-aided engineering (CAE) programs into their existing design and analysis processes, including 2D and 3D solid modeling computer-aided design (CAD). This method has many benefits, including easier and more exhaustive visualization of products, the ability to create virtual assemblies of parts, and the ease of use in designing mating interfaces and tolerances.
Other CAE programs commonly used by mechanical engineers include product lifecycle management (PLM) tools and analysis tools used to perform complex simulations. Analysis tools may be used to predict product response to expected loads, including fatigue life and manufacturability. These tools include finite element analysis (FEA), computational fluid dynamics (CFD), and computer-aided manufacturing (CAM).
Using CAE programs, a mechanical design team can quickly and cheaply iterate the design process to develop a product that better meets cost, performance, and other constraints. No physical prototype need be created until the design nears completion, allowing hundreds or thousands of designs to be evaluated, instead of a relative few. In addition, CAE analysis programs can model complicated physical phenomena which cannot be solved by hand, such as viscoelasticity, complex contact between mating parts, or non-Newtonian flows.
As mechanical engineering begins to merge with other disciplines, as seen in mechatronics, multidisciplinary design optimization (MDO) is being used with other CAE programs to automate and improve the iterative design process.  MDO tools wrap around existing CAE processes, allowing product evaluation to continue even after the analyst goes home for the day. They also utilize sophisticated optimization algorithms to more intelligently explore possible designs, often finding better, innovative solutions to difficult multidisciplinary design problems.


== Areas of research ==
Mechanical engineers are constantly pushing the boundaries of what is physically possible in order to produce safer, cheaper, and more efficient machines and mechanical systems. Some technologies at the cutting edge of mechanical engineering are listed below (see also exploratory engineering).


=== Micro electro-mechanical systems (MEMS) ===
Micron-scale mechanical components such as springs, gears, fluidic and heat transfer devices are fabricated from a variety of substrate materials such as silicon, glass and polymers like SU8. Examples of MEMS components are the accelerometers that are used as car airbag sensors, modern cell phones, gyroscopes for precise positioning and microfluidic devices used in biomedical applications.


=== Friction stir welding (FSW) ===

Friction stir welding, a new type of welding, was discovered in 1991 by The Welding Institute (TWI).  The innovative steady state (non-fusion) welding technique joins materials previously un-weldable, including several aluminum alloys.  It plays an important role in the future construction of airplanes, potentially replacing rivets.  Current uses of this technology to date include welding the seams of the aluminum main Space Shuttle external tank, Orion Crew Vehicle, Boeing Delta II and Delta IV Expendable Launch Vehicles and the SpaceX Falcon 1 rocket, armor plating for amphibious assault ships, and welding the wings and fuselage panels of the new Eclipse 500 aircraft from Eclipse Aviation among an increasingly growing pool of uses.


=== Composites ===

Composites or composite materials are a combination of materials which provide different physical characteristics than either material separately. Composite material research within mechanical engineering typically focuses on designing (and, subsequently, finding applications for) stronger or more rigid materials while attempting to reduce weight, susceptibility to corrosion, and other undesirable factors. Carbon fiber reinforced composites, for instance, have been used in such diverse applications as spacecraft and fishing rods.


=== Mechatronics ===
Mechatronics is the synergistic combination of mechanical engineering, electronic engineering, and software engineering. The discipline of mechatronics began as a way to combine mechanical principles with electrical engineering. Mechatronic concepts are used in the majority of electro-mechanical systems. Typical electro-mechanical sensors used in mechatronics are strain gauges, thermocouples, and pressure transducers.


=== Nanotechnology ===

At the smallest scales, mechanical engineering becomes nanotechnology—one speculative goal of which is to create a molecular assembler to build molecules and materials via mechanosynthesis. For now that goal remains within exploratory engineering.  Areas of current mechanical engineering research in nanotechnology include nanofilters, nanofilms, and nanostructures, among others.


=== Finite element analysis ===

Finite Element Analysis is a computational tool used to estimate stress, strain, and deflection of solid bodies. It uses a mesh setup with user-defined sizes to measure physical quantities at a node. The more nodes there are, the higher the precision. This field is not new, as the basis of Finite Element Analysis (FEA) or Finite Element Method (FEM) dates back to 1941.  But the evolution of computers has made FEA/FEM a viable option for analysis of structural problems. Many commercial codes such as NASTRAN, ANSYS, and ABAQUS are widely used in industry for research and the design of components. Some 3D modeling and CAD software packages have added FEA modules.  In the recent times, cloud simulation platforms like SimScale are becoming more common.
Other techniques such as finite difference method (FDM) and finite-volume method (FVM) are employed to solve problems relating heat and mass transfer, fluid flows, fluid surface interaction, etc.


=== Biomechanics ===

Biomechanics is the application of mechanical principles to biological systems, such as humans, animals, plants, organs, and cells.  Biomechanics also aids in creating prosthetic limbs and artificial organs for humans. Biomechanics is closely related to engineering, because it often uses traditional engineering sciences to analyze biological systems. Some simple applications of Newtonian mechanics and/or materials sciences can supply correct approximations to the mechanics of many biological systems.
In the past decade, reverse engineering of materials found in nature such as bone matter has gained funding in academia. The structure of bone matter is optimized for its purpose of bearing a large amount of compressive stress per unit weight. The goal is to replace crude steel with bio-material for structural design.
Over the past decade the Finite element method (FEM) has also entered the Biomedical sector highlighting further engineering aspects of Biomechanics. FEM has since then established itself as an alternative to in vivo surgical assessment and gained the wide acceptance of academia. The main advantage of Computational Biomechanics lies in its ability to determine the endo-anatomical response of an anatomy, without being subject to ethical restrictions. This has led FE modelling to the point of becoming ubiquitous in several fields of Biomechanics while several projects have even adopted an open source philosophy (e.g. BioSpine).


=== Computational fluid dynamics ===

Computational fluid dynamics, usually abbreviated as CFD, is a branch of fluid mechanics that uses numerical methods and algorithms to solve and analyze problems that involve fluid flows. Computers are used to perform the calculations required to simulate the interaction of liquids and gases with surfaces defined by boundary conditions. With high-speed supercomputers, better solutions can be achieved. Ongoing research yields software that improves the accuracy and speed of complex simulation scenarios such as turbulent flows. Initial validation of such software is performed using a wind tunnel with the final validation coming in full-scale testing, e.g. flight tests.


=== Acoustical engineering ===

Acoustical engineering is one of many other sub-disciplines of mechanical engineering and is the application of acoustics. Acoustical engineering is the study of Sound and Vibration. These engineers work effectively to reduce noise pollution in mechanical devices and in buildings by soundproofing or removing sources of unwanted noise. The study of acoustics can range from designing a more efficient hearing aid, microphone, headphone, or recording studio to enhancing the sound quality of an orchestra hall. Acoustical engineering also deals with the vibration of different mechanical systems.


== Related fields ==
Manufacturing engineering, aerospace engineering and automotive engineering are grouped with mechanical engineering at times. A bachelor's degree in these areas will typically have a difference of a few specialized classes.


== See also ==

Lists
Associations
Wikibooks


== References ==


== Further reading ==
Burstall, Aubrey F. (1965). A History of Mechanical Engineering. The MIT Press. ISBN 978-0-262-52001-0.
Marks' Standard Handbook for Mechanical Engineers (11 ed.). McGraw-Hill. 2007. ISBN 978-0-07-142867-5.
Oberg, Erik; Franklin D. Jones; Holbrook L. Horton; Henry H. Ryffel; Christopher McCauley (2016). Machinery's Handbook (30th ed.). New York: Industrial Press Inc. ISBN 978-0-8311-3091-6.


== External links ==
 Quotations related to Mechanical engineering at Wikiquote","pandas(index=230, _1=230, text='mechanical engineering is an engineering branch that combines engineering physics and mathematics principles with materials science to design, analyze, manufacture, and maintain mechanical systems.  it is one of the oldest and broadest of the engineering branches. the mechanical engineering field requires an understanding of core areas including mechanics, dynamics, thermodynamics, materials science, structural analysis, and electricity.  in addition to these core principles, mechanical engineers use tools such as computer-aided design (cad), computer-aided manufacturing (cam), and product lifecycle management to design and analyze manufacturing plants, industrial equipment and machinery, heating and cooling systems, transport systems, aircraft, watercraft, robotics, medical devices, weapons, and others.  it is the branch of engineering that involves the design, production, and operation of machinery.mechanical engineering emerged as a field during the industrial revolution in europe in the 18th century; however, its development can be traced back several thousand years around the world.  in the 19th century, developments in physics led to the development of mechanical engineering science. the field has continually evolved to incorporate advancements; today mechanical engineers are pursuing developments in such areas as composites, mechatronics, and nanotechnology.  it also overlaps with aerospace engineering, metallurgical engineering, civil engineering, electrical engineering, manufacturing engineering, chemical engineering, industrial engineering, and other engineering disciplines to varying amounts.  mechanical engineers may also work in the field of biomedical engineering, specifically with biomechanics, transport phenomena, biomechatronics, bionanotechnology, and modelling of biological systems.   == history ==  the application of mechanical engineering can be seen in the archives of various ancient and medieval societies. the six classic simple machines were known in the ancient near east. the wedge and the inclined plane (ramp) were known since prehistoric times. the wheel, along with the wheel and axle mechanism, was invented in mesopotamia (modern iraq) during the 5th millennium bc. the lever mechanism first appeared around 5,000 years ago in the near east, where it was used in a simple balance scale, and to move large objects in ancient egyptian technology. the lever was also used in the shadoof water-lifting device, the first crane machine, which appeared in mesopotamia circa 3000 bc. the earliest evidence of pulleys date back to mesopotamia in the early 2nd millennium bc.the sakia was developed in the kingdom of kush during the 4th century bc. it relied on animal power reducing the tow on the requirement of human energy. reservoirs in the form of hafirs were developed in kush to store water and boost irrigation. bloomeries and blast furnaces were developed during the seventh century bc in meroe. kushite sundials applied mathematics in the form of advanced trigonometry.the earliest practical water-powered machines, the water wheel and watermill, first appeared in the persian empire, in what are now iraq and iran, by the early 4th century bc. in ancient greece, the works of archimedes (287–212 bc) influenced mechanics in the western tradition. in roman egypt, heron of alexandria (c. 10–70 ad) created the first steam-powered device (aeolipile). in china, zhang heng (78–139 ad) improved a water clock and invented a seismometer, and ma jun (200–265 ad) invented a chariot with differential gears. the medieval chinese horologist and engineer su song (1020–1101 ad) incorporated an escapement mechanism into his astronomical clock tower two centuries before escapement devices were found in medieval european clocks. he also invented the world\'s first known endless power-transmitting chain drive.during the islamic golden age (7th to 15th century), muslim inventors made remarkable contributions in the field of mechanical technology. al-jazari, who was one of them, wrote his famous book of ingenious devices in 1206 and presented many mechanical designs. al-jazari is also the first known person to create devices such as the crankshaft and camshaft, which now form the basics of many mechanisms.during the 17th century, important breakthroughs in the foundations of mechanical engineering occurred in england. sir isaac newton formulated newton\'s laws of motion and developed calculus, the mathematical basis of physics. newton was reluctant to publish his works for years, but he was finally persuaded to do so by his colleagues, such as sir edmond halley, much to the benefit of all mankind. gottfried wilhelm leibniz is also credited with creating calculus during this time period.during the early 19th century industrial revolution, machine tools were developed in england, germany, and scotland. this allowed mechanical engineering to develop as a separate field within engineering. they brought with them manufacturing machines and the engines to power them. the first british professional society of mechanical engineers was formed in 1847 institution of mechanical engineers, thirty years after the civil engineers formed the first such professional society institution of civil engineers. on the european continent, johann von zimmermann (1820–1901) founded the first factory for grinding machines in chemnitz, germany in 1848. in the united states, the american society of mechanical engineers (asme) was formed in 1880, becoming the third such professional engineering society, after the american society of civil engineers (1852) and the american institute of mining engineers (1871). the first schools in the united states to offer an engineering education were the united states military academy in 1817, an institution now known as norwich university in 1819, and rensselaer polytechnic institute in 1825. education in mechanical engineering has historically been based on a strong foundation in mathematics and science.   == education ==  degrees in mechanical engineering are offered at various universities worldwide. mechanical engineering programs typically take four to five years of study depending on the place and university and result in a bachelor of engineering (b.eng. or b.e.), bachelor of science (b.sc. or b.s.), bachelor of science engineering (b.sc.eng.), bachelor of technology (b.tech.), bachelor of mechanical engineering (b.m.e.), or bachelor of applied science (b.a.sc.) degree, in or with emphasis in mechanical engineering. in spain, portugal and most of south america, where neither b.s. nor b.tech. programs have been adopted, the formal name for the degree is ""mechanical engineer"", and the course work is based on five or six years of training. in italy the course work is based on five years of education, and training, but in order to qualify as an engineer one has to pass a state exam at the end of the course. in greece, the coursework is based on a five-year curriculum and the requirement of a \'diploma\' thesis, which upon completion a \'diploma\' is awarded rather than a b.sc.in the united states, most undergraduate mechanical engineering programs are accredited by the accreditation board for engineering and technology (abet) to ensure similar course requirements and standards among universities. the abet web site lists 302 accredited mechanical engineering programs as of 11 march 2014. mechanical engineering programs in canada are accredited by the canadian engineering accreditation board (ceab), and most other countries offering engineering degrees have similar accreditation societies. in australia, mechanical engineering degrees are awarded as bachelor of engineering (mechanical) or similar nomenclature, although there are an increasing number of specialisations. the degree takes four years of full-time study to achieve. to ensure quality in engineering degrees, engineers australia accredits engineering degrees awarded by australian universities in accordance with the global washington accord. before the degree can be awarded, the student must complete at least 3 months of on the job work experience in an engineering firm.  similar systems are also present in south africa and are overseen by the engineering council of south africa (ecsa). in india, to become an engineer, one needs to have an engineering degree like a b.tech or b.e, have a diploma in engineering, or by completing a course in an engineering trade like fitter from the industrial training institute (itis) to receive a ""iti trade certificate"" and also pass the all india trade test (aitt) with an engineering trade conducted by the national council of vocational training (ncvt) by which one is awarded a ""national trade certificate"".  a similar system is used in nepal.some mechanical engineers go on to pursue a postgraduate degree such as a master of engineering, master of technology, master of science, master of engineering management (m.eng.mgt. or m.e.m.), a doctor of philosophy in engineering (eng.d. or ph.d.) or an engineer\'s degree. the master\'s and engineer\'s degrees may or may not include research. the doctor of philosophy includes a significant research component and is often viewed as the entry point to academia.  the engineer\'s degree exists at a few institutions at an intermediate level between the master\'s degree and the doctorate. acoustical engineering is one of many other sub-disciplines of mechanical engineering and is the application of acoustics. acoustical engineering is the study of sound and vibration. these engineers work effectively to reduce noise pollution in mechanical devices and in buildings by soundproofing or removing sources of unwanted noise. the study of acoustics can range from designing a more efficient hearing aid, microphone, headphone, or recording studio to enhancing the sound quality of an orchestra hall. acoustical engineering also deals with the vibration of different mechanical systems.   == related fields == manufacturing engineering, aerospace engineering and automotive engineering are grouped with mechanical engineering at times. a bachelor\'s degree in these areas will typically have a difference of a few specialized classes.   == see also ==  lists associations wikibooks   == references ==   == further reading == burstall, aubrey f. (1965). a history of mechanical engineering. the mit press. isbn 978-0-262-52001-0. marks\' standard handbook for mechanical engineers (11 ed.). mcgraw-hill. 2007. isbn 978-0-07-142867-5. oberg, erik; franklin d. jones; holbrook l. horton; henry h. ryffel; christopher mccauley (2016). machinery\'s handbook (30th ed.). new york: industrial press inc. isbn 978-0-8311-3091-6.   == external links == quotations related to mechanical engineering at wikiquote')"
231,"Microelectromechanical systems (MEMS), also written as micro-electro-mechanical systems (or microelectronic and microelectromechanical systems) and the related micromechatronics and microsystems constitute the technology of microscopic devices, particularly those with moving parts. They merge at the nanoscale into nanoelectromechanical systems (NEMS) and nanotechnology. MEMS are also referred to as micromachines in Japan and microsystem technology (MST) in Europe.
MEMS are made up of components between 1 and 100 micrometers in size (i.e., 0.001 to 0.1 mm), and MEMS devices generally range in size from 20 micrometres to a millimetre (i.e., 0.02 to 1.0 mm), although components arranged in arrays (e.g., digital micromirror devices) can be more than 1000 mm2. 
They usually consist of a central unit that processes data (an integrated circuit chip such as microprocessor) and several components that interact with the surroundings (such as microsensors). Because of the large surface area to volume ratio of MEMS, forces produced by ambient electromagnetism (e.g., electrostatic charges and magnetic moments), and fluid dynamics (e.g., surface tension and viscosity) are more important design considerations than with larger scale mechanical devices. MEMS technology is distinguished from molecular nanotechnology or molecular electronics in that the latter must also consider surface chemistry.
The potential of very small machines was appreciated before the technology existed that could make them (see, for example, Richard Feynman's famous 1959 lecture There's Plenty of Room at the Bottom). MEMS became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. These include molding and plating, wet etching (KOH, TMAH) and dry etching (RIE and DRIE), electrical discharge machining (EDM), and other technologies capable of manufacturing small devices.


== History ==
MEMS technology has roots in the silicon revolution, which can be traced back to two important silicon semiconductor inventions from 1959: the monolithic integrated circuit (IC) chip by Robert Noyce at Fairchild Semiconductor, and the MOSFET (metal-oxide-semiconductor field-effect transistor, or MOS transistor) by Mohamed M. Atalla and Dawon Kahng at Bell Labs. MOSFET scaling, the miniaturisation of MOSFETs on IC chips, led to the miniaturisation of electronics (as predicted by Moore's law and Dennard scaling). This laid the foundations for the miniaturisation of mechanical systems, with the development of micromachining technology based on silicon semiconductor technology, as engineers began realizing that silicon chips and MOSFETs could interact and communicate with the surroundings and process things such as chemicals, motions and light. One of the first silicon pressure sensors was isotropically micromachined by Honeywell in 1962.An early example of a MEMS device is the resonant-gate transistor, an adaptation of the MOSFET, developed by Harvey C. Nathanson in 1965. Another early example is the resonistor, an electromechanical monolithic resonator patented by Raymond J. Wilfinger between 1966 and 1971. During the 1970s to early 1980s, a number of MOSFET microsensors were developed for measuring physical, chemical, biological and environmental parameters.


== Types ==
There are two basic types of MEMS switch technology: capacitive and ohmic.  A capacitive MEMS switch is developed using a moving plate or sensing element, which changes the capacitance. Ohmic switches are controlled by electrostatically controlled cantilevers.  Ohmic MEMS switches can fail from metal fatigue of the MEMS actuator (cantilever) and contact wear, since cantilevers can deform over time.


== Materials for MEMS manufacturing ==
The fabrication of MEMS evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes.


=== Silicon ===
Silicon is the material used to create most integrated circuits used in consumer electronics in the modern industry. The economies of scale, ready availability of inexpensive high-quality materials, and ability to incorporate electronic functionality make silicon attractive for a wide variety of MEMS applications. Silicon also has significant advantages engendered through its material properties. In single crystal form, silicon is an almost perfect Hookean material, meaning that when it is flexed there is virtually no hysteresis and hence almost no energy dissipation. As well as making for highly repeatable motion, this also makes silicon very reliable as it suffers very little fatigue and can have service lifetimes in the range of billions to trillions of cycles without breaking. Semiconductor nanostructures based on silicon are gaining increasing importance in the field of microelectronics and MEMS in particular. Silicon nanowires, fabricated through the thermal oxidation of silicon, are of further interest in electrochemical conversion and storage, including nanowire batteries and photovoltaic systems.


=== Polymers ===
Even though the electronics industry provides an economy of scale for the silicon industry, crystalline silicon is still a complex and relatively expensive material to produce. Polymers on the other hand can be produced in huge volumes, with a great variety of material characteristics. MEMS devices can be made from polymers by processes such as injection molding, embossing or stereolithography and are especially well suited to microfluidic applications such as disposable blood testing cartridges.


=== Metals ===
Metals can also be used to create MEMS elements. While metals do not have some of the advantages displayed by silicon in terms of mechanical properties, when used within their limitations, metals can exhibit very high degrees of reliability. Metals can be deposited by electroplating, evaporation, and sputtering processes. Commonly used metals include gold, nickel, aluminium, copper, chromium, titanium, tungsten, platinum, and silver.


=== Ceramics ===

The nitrides of silicon, aluminium and titanium as well as silicon carbide and other ceramics are increasingly applied in MEMS fabrication due to advantageous combinations of material properties. AlN crystallizes in the wurtzite structure and thus shows pyroelectric and piezoelectric properties enabling sensors, for instance, with sensitivity to normal and shear forces. TiN, on the other hand, exhibits a high electrical conductivity and large elastic modulus, making it possible to implement electrostatic MEMS actuation schemes with ultrathin beams. Moreover, the high resistance of TiN against biocorrosion qualifies the material for applications in biogenic environments. The figure shows an electron-microscopic picture of a MEMS biosensor with a 50 nm thin bendable TiN beam above a TiN ground plate. Both can be driven as opposite electrodes of a capacitor, since the beam is fixed in electrically isolating side walls. When a fluid is suspended in the cavity its viscosity may be derived from bending the beam by electrical attraction to the ground plate and measuring the bending velocity.


== MEMS basic processes ==


=== Deposition processes ===
One of the basic building blocks in MEMS processing is the ability to deposit thin films of material with a thickness anywhere between one micrometre, to about 100 micrometres. The NEMS process is the same, although the measurement of film deposition ranges from a few nanometres to one micrometre. There are two types of deposition processes, as follows.


==== Physical deposition ====
Physical vapor deposition (""PVD"") consists of a process in which a material is removed from a target, and deposited on a surface. Techniques to do this include the process of sputtering, in which an ion beam liberates atoms from a target, allowing them to move through the intervening space and deposit on the desired substrate, and evaporation, in which a material is evaporated from a target using either heat (thermal evaporation) or an electron beam (e-beam evaporation) in a vacuum system.


==== Chemical deposition ====
Chemical deposition techniques include chemical vapor deposition (CVD), in which a stream of source gas reacts on the substrate to grow the material desired. This can be further divided into categories depending on the details of the technique, for example LPCVD (low-pressure chemical vapor deposition) and PECVD (plasma-enhanced chemical vapor deposition).
Oxide films can also be grown by the technique of thermal oxidation, in which the (typically silicon) wafer is exposed to oxygen and/or steam, to grow a thin surface layer of silicon dioxide.


=== Patterning ===
Patterning in MEMS is the transfer of a pattern into a material.


=== Lithography ===
Lithography in MEMS context is typically the transfer of a pattern into a photosensitive material by selective exposure to a radiation source such as light. A photosensitive material is a material that experiences a change in its physical properties when exposed to a radiation source. If a photosensitive material is selectively exposed to radiation (e.g. by masking some of the radiation) the pattern of the radiation on the material is transferred to the material exposed, as the properties of the exposed and unexposed regions differs.
This exposed region can then be removed or treated providing a mask for the underlying substrate. Photolithography is typically used with metal or other thin film deposition, wet and dry etching. Sometimes, photolithography is used to create structure without any kind of post etching. One example is SU8 based lens where SU8 based square blocks are generated. Then the photoresist is melted to form a semi-sphere which acts as a lens.


==== Electron beam lithography ====

Electron beam lithography (often abbreviated as e-beam lithography) is the practice of scanning a beam of electrons in a patterned fashion across a surface covered with a film (called the resist), (""exposing"" the resist) and of selectively removing either exposed or non-exposed regions of the resist (""developing""). The purpose, as with photolithography, is to create very small structures in the resist that can subsequently be transferred to the substrate material, often by etching. It was developed for manufacturing integrated circuits, and is also used for creating nanotechnology architectures.
The primary advantage of electron beam lithography is that it is one of the ways to beat the diffraction limit of light and make features in the nanometer range. This form of maskless lithography has found wide usage in photomask-making used in photolithography, low-volume production of semiconductor components, and research & development.
The key limitation of electron beam lithography is throughput, i.e., the very long time it takes to expose an entire silicon wafer or glass substrate. A long exposure time leaves the user vulnerable to beam drift or instability which may occur during the exposure. Also, the turn-around time for reworking or re-design is lengthened unnecessarily if the pattern is not being changed the second time.


==== Ion beam lithography ====
It is known that focused-ion beam lithography has the capability of writing extremely fine lines (less than 50 nm line and space has been achieved) without proximity effect. However, because the writing field in ion-beam lithography is quite small, large area patterns must be created by stitching together the small fields.


==== Ion track technology ====
Ion track technology is a deep cutting tool with a resolution limit around 8 nm applicable to radiation resistant minerals, glasses and polymers. It is capable of generating holes in thin films without any development process. Structural depth can be defined either by ion range or by material thickness. Aspect ratios up to several 104 can be reached. The technique can shape and texture materials at a defined inclination angle. Random pattern, single-ion track structures and aimed pattern consisting of individual single tracks can be generated.


==== X-ray lithography ====
X-ray lithography is a process used in electronic industry to selectively remove parts of a thin film. It uses X-rays to transfer a geometric pattern from a mask to a light-sensitive chemical photoresist, or simply ""resist"", on the substrate. A series of chemical treatments then engraves the produced pattern into the material underneath the photoresist.


==== Diamond patterning ====
A simple way to carve or create patterns on the surface of nanodiamonds without damaging them could lead to a new photonic devices.Diamond patterning is a method of forming diamond MEMS. It is achieved by the lithographic application of diamond films to a substrate such as silicon. The patterns can be formed by selective deposition through a silicon dioxide mask, or by deposition followed by micromachining or focused ion beam milling.


=== Etching processes ===
There are two basic categories of etching processes: wet etching and dry etching. In the former, the material is dissolved when immersed in a chemical solution. In the latter, the material is sputtered or dissolved using reactive ions or a vapor phase etchant.


==== Wet etching ====

Wet chemical etching consists in selective removal of material by dipping a substrate into a solution that dissolves it. The chemical nature of this etching process provides a good selectivity, which means the etching rate of the target material is considerably higher than the mask material if selected carefully.


===== Isotropic etching =====
Etching progresses at the same speed in all directions. Long and narrow holes in a mask will produce v-shaped grooves in the silicon. The surface of these grooves can be atomically smooth if the etch is carried out correctly, with dimensions and angles being extremely accurate.


===== Anisotropic etching =====
Some single crystal materials, such as silicon, will have different etching rates depending on the crystallographic orientation of the substrate. This is known as anisotropic etching and one of the most common examples is the etching of silicon in KOH (potassium hydroxide), where Si <111> planes etch approximately 100 times slower than other planes (crystallographic orientations). Therefore, etching a rectangular hole in a (100)-Si wafer results in a pyramid shaped etch pit with 54.7° walls, instead of a hole with curved sidewalls as with isotropic etching.


===== HF etching =====
Hydrofluoric acid is commonly used as an aqueous etchant for silicon dioxide (SiO2, also known as BOX for SOI), usually in 49% concentrated form, 5:1, 10:1 or 20:1 BOE (buffered oxide etchant) or BHF (Buffered HF). They were first used in medieval times for glass etching. It was used in IC fabrication for patterning the gate oxide until the process step was replaced by RIE.
Hydrofluoric acid is considered one of the more dangerous acids in the cleanroom. It penetrates the skin upon contact and it diffuses straight to the bone. Therefore, the damage is not felt until it is too late.


===== Electrochemical etching =====
Electrochemical etching (ECE) for dopant-selective removal of silicon is a common method to automate and to selectively control etching. An active p-n diode junction is required, and either type of dopant can be the etch-resistant (""etch-stop"") material. Boron is the most common etch-stop dopant. In combination with wet anisotropic etching as described above, ECE has been used successfully for controlling silicon diaphragm thickness in commercial piezoresistive silicon pressure sensors. Selectively doped regions can be created either by implantation, diffusion, or epitaxial deposition of silicon.


==== Dry etching ====


===== Vapor etching =====


====== Xenon difluoride ======
Xenon difluoride (XeF2) is a dry vapor phase isotropic etch for silicon originally applied for MEMS in 1995 at University of California, Los Angeles. Primarily used for releasing metal and dielectric structures by undercutting silicon, XeF2 has the advantage of a stiction-free release unlike wet etchants. Its etch selectivity to silicon is very high, allowing it to work with photoresist, SiO2, silicon nitride, and various metals for masking. Its reaction to silicon is ""plasmaless"", is purely chemical and spontaneous and is often operated in pulsed mode. Models of the etching action are available, and university laboratories and various commercial tools offer solutions using this approach.


===== Plasma etching =====
Modern VLSI processes avoid wet etching, and use plasma etching instead. Plasma etchers can operate in several modes by adjusting the parameters of the plasma. Ordinary plasma etching operates between 0.1 and 5 Torr. (This unit of pressure, commonly used in vacuum engineering, equals approximately 133.3 pascals.) The plasma produces energetic free radicals, neutrally charged, that react at the surface of the wafer. Since neutral particles attack the wafer from all angles, this process is isotropic.
Plasma etching can be isotropic, i.e., exhibiting a lateral undercut rate on a patterned surface approximately the same as its downward etch rate, or can be anisotropic, i.e., exhibiting a smaller lateral undercut rate than its downward etch rate. Such anisotropy is maximized in deep reactive ion etching. The use of the term anisotropy for plasma etching should not be conflated with the use of the same term when referring to orientation-dependent etching.
The source gas for the plasma usually contains small molecules rich in chlorine or fluorine. For instance, carbon tetrachloride (CCl4) etches silicon and aluminium, and trifluoromethane etches silicon dioxide and silicon nitride. A plasma containing oxygen is used to oxidize (""ash"") photoresist and facilitate its removal.
Ion milling, or sputter etching, uses lower pressures, often as low as 10−4 Torr (10 mPa). It bombards the wafer with energetic ions of noble gases, often Ar+, which knock atoms from the substrate by transferring momentum. Because the etching is performed by ions, which approach the wafer approximately from one direction, this process is highly anisotropic. On the other hand, it tends to display poor selectivity. Reactive-ion etching (RIE) operates under conditions intermediate between sputter and plasma etching (between 10–3 and 10−1 Torr). Deep reactive-ion etching (DRIE) modifies the RIE technique to produce deep, narrow features.


====== Sputtering ======


====== Reactive ion etching (RIE) ======

In reactive-ion etching (RIE), the substrate is placed inside a reactor, and several gases are introduced. A plasma is struck in the gas mixture using an RF power source, which breaks the gas molecules into ions. The ions accelerate towards, and react with, the surface of the material being etched, forming another gaseous material. This is known as the chemical part of reactive ion etching. There is also a physical part, which is similar to the sputtering deposition process. If the ions have high enough energy, they can knock atoms out of the material to be etched without a chemical reaction. It is a very complex task to develop dry etch processes that balance chemical and physical etching, since there are many parameters to adjust. By changing the balance it is possible to influence the anisotropy of the etching, since the chemical part is isotropic and the physical part highly anisotropic the combination can form sidewalls that have shapes from rounded to vertical.

Deep RIE (DRIE) is a special subclass of RIE that is growing in popularity. In this process, etch depths of hundreds of micrometres are achieved with almost vertical sidewalls. The primary technology is based on the so-called ""Bosch process"", named after the German company Robert Bosch, which filed the original patent, where two different gas compositions alternate in the reactor. Currently there are two variations of the DRIE. The first variation consists of three distinct steps (the original Bosch process) while the second variation only consists of two steps.
In the first variation, the etch cycle is as follows:
(i) SF6 isotropic etch;
(ii) C4F8 passivation;
(iii) SF6 anisoptropic etch for floor cleaning.
In the 2nd variation, steps (i) and (iii) are combined.
Both variations operate similarly. The C4F8 creates a polymer on the surface of the substrate, and the second gas composition (SF6 and O2) etches the substrate. The polymer is immediately sputtered away by the physical part of the etching, but only on the horizontal surfaces and not the sidewalls. Since the polymer only dissolves very slowly in the chemical part of the etching, it builds up on the sidewalls and protects them from etching. As a result, etching aspect ratios of 50 to 1 can be achieved. The process can easily be used to etch completely through a silicon substrate, and etch rates are 3–6 times higher than wet etching.


=== Die preparation ===
After preparing a large number of MEMS devices on a silicon wafer, individual dies have to be separated, which is called die preparation in semiconductor technology. For some applications, the separation is preceded by wafer backgrinding in order to reduce the wafer thickness. Wafer dicing may then be performed either by sawing using a cooling liquid or a dry laser process called stealth dicing.


== MEMS manufacturing technologies ==


=== Bulk micromachining ===

Bulk micromachining is the oldest paradigm of silicon-based MEMS. The whole thickness of a silicon wafer is used for building the micro-mechanical structures. Silicon is machined using various etching processes. Anodic bonding of glass plates or additional silicon wafers is used for adding features in the third dimension and for hermetic encapsulation. Bulk micromachining has been essential in enabling high performance pressure sensors and accelerometers that changed the sensor industry in the 1980s and 90's.


=== Surface micromachining ===

Surface micromachining uses layers deposited on the surface of a substrate as the structural materials, rather than using the substrate itself. Surface micromachining was created in the late 1980s to render micromachining of silicon more compatible with planar integrated circuit technology, with the goal of combining MEMS and integrated circuits on the same silicon wafer. The original surface micromachining concept was based on thin polycrystalline silicon layers patterned as movable mechanical structures and released by sacrificial etching of the underlying oxide layer. Interdigital comb electrodes were used to produce in-plane forces and to detect in-plane movement capacitively. This MEMS paradigm has enabled the manufacturing of low cost accelerometers for e.g. automotive air-bag systems and other applications where low performance and/or high g-ranges are sufficient. Analog Devices has pioneered the industrialization of surface micromachining and has realized the co-integration of MEMS and integrated circuits.


=== Thermal oxidation ===

To control the size of micro and nano-scale components, the use of so-called etchless processes is often applied. This approach to MEMS fabrication relies mostly on the oxidation of silicon, as described by the Deal-Grove model. Thermal oxidation processes are used to produced diverse silicon structures with highly precise dimensional control. Devices including optical frequency combs, and silicon MEMS pressure sensors, have been produced through the use of thermal oxidation processes to fine-tune silicon structures in one or two dimensions. Thermal oxidation is of particular value in the fabrication of silicon nanowires, which are widely employed in MEMS systems as both mechanical and electrical components.


=== High aspect ratio (HAR) silicon micromachining ===
Both bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. But in many cases the distinction between these two has diminished. A new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. While it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in HAR silicon micromachining the thickness can be from 10 to 100 µm. The materials commonly used in HAR silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (SOI) wafers although processes for bulk silicon wafer also have been created (SCREAM). Bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the MEMS structures. Integrated circuits are typically not combined with HAR silicon micromachining.


== Applications ==

Some common commercial applications of MEMS include:

Inkjet printers, which use piezoelectrics or thermal bubble ejection to deposit ink on paper.
Accelerometers in modern cars for a large number of purposes including airbag deployment and electronic stability control.
Inertial measurement units (IMUs): MEMS accelerometers and MEMS gyroscopes in remote controlled, or autonomous, helicopters, planes and multirotors (also known as drones), used for automatically sensing and balancing flying characteristics of roll, pitch and yaw. MEMS magnetic field sensor (magnetometer) may also be incorporated in such devices to provide directional heading. MEMS are also used in Inertial navigation systems (INSs) of modern cars, airplanes, submarines and other vehicles to detect yaw, pitch, and roll; for example, the autopilot of an airplane.
Accelerometers in consumer electronics devices such as game controllers (Nintendo Wii), personal media players / cell phones (virtually all smartphones, various HTC PDA models) and a number of Digital Cameras (various Canon Digital IXUS models). Also used in PCs to park the hard disk head when free-fall is detected, to prevent damage and data loss.
MEMS barometers
MEMS microphones in portable devices, e.g., mobile phones, head sets and laptops. The market for smart microphones includes smartphones, wearable devices, smart home and automotive applications.
Precision temperature-compensated resonators in real-time clocks.
Silicon pressure sensors e.g., car tire pressure sensors, and disposable blood pressure sensors
Displays e.g., the digital micromirror device (DMD) chip in a projector based on DLP technology, which has a surface with several hundred thousand micromirrors or single micro-scanning-mirrors also called microscanners
Optical switching technology, which is used for switching technology and alignment for data communications
Bio-MEMS applications in medical and health related technologies from Lab-On-Chip to MicroTotalAnalysis (biosensor, chemosensor), or embedded in medical devices e.g. stents.
Interferometric modulator display (IMOD) applications in consumer electronics (primarily displays for mobile devices), used to create interferometric modulation − reflective display technology as found in mirasol displays
Fluid acceleration, such as for micro-cooling
Micro-scale energy harvesting including piezoelectric, electrostatic and electromagnetic micro harvesters.
Micromachined ultrasound transducers.
MEMS-based loudspeakers focusing on applications such as in-ear headphones and hearing aids
MEMS oscillators
MEMS-based scanning probe microscopes including atomic force microscopes


== Industry structure ==
The global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to Global MEMS/Microsystems Markets and Opportunities, a research report from SEMI and Yole Development and is forecasted to reach $72 billion by 2011.Companies with strong MEMS programs come in many sizes. Larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. Smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. Both large and small companies typically invest in R&D to explore new MEMS technology.
The market for materials and equipment used to manufacture MEMS devices topped $1 billion worldwide in 2006. Materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (CMP). While MEMS manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain MEMS applications.


== See also ==


== References ==


== Further reading ==
Journal of Micro and Nanotechnique
Microsystem Technologies, published by Springer Publishing, Journal homepage
Geschke, O.; Klank, H.; Telleman, P., eds. (2004). Microsystem Engineering of Lab-on-a-chip Devices. Wiley. ISBN 3-527-30733-8.


== External links ==
Chollet, F.; Liu, HB. (10 August 2018). A (not so) short introduction to MEMS. ISBN 9782954201504. 5.4.","pandas(index=231, _1=231, text='microelectromechanical systems (mems), also written as micro-electro-mechanical systems (or microelectronic and microelectromechanical systems) and the related micromechatronics and microsystems constitute the technology of microscopic devices, particularly those with moving parts. they merge at the nanoscale into nanoelectromechanical systems (nems) and nanotechnology. mems are also referred to as micromachines in japan and microsystem technology (mst) in europe. mems are made up of components between 1 and 100 micrometers in size (i.e., 0.001 to 0.1 mm), and mems devices generally range in size from 20 micrometres to a millimetre (i.e., 0.02 to 1.0 mm), although components arranged in arrays (e.g., digital micromirror devices) can be more than 1000 mm2. they usually consist of a central unit that processes data (an integrated circuit chip such as microprocessor) and several components that interact with the surroundings (such as microsensors). because of the large surface area to volume ratio of mems, forces produced by ambient electromagnetism (e.g., electrostatic charges and magnetic moments), and fluid dynamics (e.g., surface tension and viscosity) are more important design considerations than with larger scale mechanical devices. mems technology is distinguished from molecular nanotechnology or molecular electronics in that the latter must also consider surface chemistry. the potential of very small machines was appreciated before the technology existed that could make them (see, for example, richard feynman\'s famous 1959 lecture there\'s plenty of room at the bottom). mems became practical once they could be fabricated using modified semiconductor device fabrication technologies, normally used to make electronics. these include molding and plating, wet etching (koh, tmah) and dry etching (rie and drie), electrical discharge machining (edm), and other technologies capable of manufacturing small devices.   == history == mems technology has roots in the silicon revolution, which can be traced back to two important silicon semiconductor inventions from 1959: the monolithic integrated circuit (ic) chip by robert noyce at fairchild semiconductor, and the mosfet (metal-oxide-semiconductor field-effect transistor, or mos transistor) by mohamed m. atalla and dawon kahng at bell labs. mosfet scaling, the miniaturisation of mosfets on ic chips, led to the miniaturisation of electronics (as predicted by moore\'s law and dennard scaling). this laid the foundations for the miniaturisation of mechanical systems, with the development of micromachining technology based on silicon semiconductor technology, as engineers began realizing that silicon chips and mosfets could interact and communicate with the surroundings and process things such as chemicals, motions and light. one of the first silicon pressure sensors was isotropically micromachined by honeywell in 1962.an early example of a mems device is the resonant-gate transistor, an adaptation of the mosfet, developed by harvey c. nathanson in 1965. another early example is the resonistor, an electromechanical monolithic resonator patented by raymond j. wilfinger between 1966 and 1971. during the 1970s to early 1980s, a number of mosfet microsensors were developed for measuring physical, chemical, biological and environmental parameters.   == types == there are two basic types of mems switch technology: capacitive and ohmic.  a capacitive mems switch is developed using a moving plate or sensing element, which changes the capacitance. ohmic switches are controlled by electrostatically controlled cantilevers.  ohmic mems switches can fail from metal fatigue of the mems actuator (cantilever) and contact wear, since cantilevers can deform over time.   == materials for mems manufacturing == the fabrication of mems evolved from the process technology in semiconductor device fabrication, i.e. the basic techniques are deposition of material layers, patterning by photolithography and etching to produce the required shapes. both bulk and surface silicon micromachining are used in the industrial production of sensors, ink-jet nozzles, and other devices. but in many cases the distinction between these two has diminished. a new etching technology, deep reactive-ion etching, has made it possible to combine good performance typical of bulk micromachining with comb structures and in-plane operation typical of surface micromachining. while it is common in surface micromachining to have structural layer thickness in the range of 2 µm, in har silicon micromachining the thickness can be from 10 to 100 µm. the materials commonly used in har silicon micromachining are thick polycrystalline silicon, known as epi-poly, and bonded silicon-on-insulator (soi) wafers although processes for bulk silicon wafer also have been created (scream). bonding a second wafer by glass frit bonding, anodic bonding or alloy bonding is used to protect the mems structures. integrated circuits are typically not combined with har silicon micromachining.   == applications ==  some common commercial applications of mems include:  inkjet printers, which use piezoelectrics or thermal bubble ejection to deposit ink on paper. accelerometers in modern cars for a large number of purposes including airbag deployment and electronic stability control. inertial measurement units (imus): mems accelerometers and mems gyroscopes in remote controlled, or autonomous, helicopters, planes and multirotors (also known as drones), used for automatically sensing and balancing flying characteristics of roll, pitch and yaw. mems magnetic field sensor (magnetometer) may also be incorporated in such devices to provide directional heading. mems are also used in inertial navigation systems (inss) of modern cars, airplanes, submarines and other vehicles to detect yaw, pitch, and roll; for example, the autopilot of an airplane. accelerometers in consumer electronics devices such as game controllers (nintendo wii), personal media players / cell phones (virtually all smartphones, various htc pda models) and a number of digital cameras (various canon digital ixus models). also used in pcs to park the hard disk head when free-fall is detected, to prevent damage and data loss. mems barometers mems microphones in portable devices, e.g., mobile phones, head sets and laptops. the market for smart microphones includes smartphones, wearable devices, smart home and automotive applications. precision temperature-compensated resonators in real-time clocks. silicon pressure sensors e.g., car tire pressure sensors, and disposable blood pressure sensors displays e.g., the digital micromirror device (dmd) chip in a projector based on dlp technology, which has a surface with several hundred thousand micromirrors or single micro-scanning-mirrors also called microscanners optical switching technology, which is used for switching technology and alignment for data communications bio-mems applications in medical and health related technologies from lab-on-chip to micrototalanalysis (biosensor, chemosensor), or embedded in medical devices e.g. stents. interferometric modulator display (imod) applications in consumer electronics (primarily displays for mobile devices), used to create interferometric modulation − reflective display technology as found in mirasol displays fluid acceleration, such as for micro-cooling micro-scale energy harvesting including piezoelectric, electrostatic and electromagnetic micro harvesters. micromachined ultrasound transducers. mems-based loudspeakers focusing on applications such as in-ear headphones and hearing aids mems oscillators mems-based scanning probe microscopes including atomic force microscopes   == industry structure == the global market for micro-electromechanical systems, which includes products such as automobile airbag systems, display systems and inkjet cartridges totaled $40 billion in 2006 according to global mems/microsystems markets and opportunities, a research report from semi and yole development and is forecasted to reach $72 billion by 2011.companies with strong mems programs come in many sizes. larger firms specialize in manufacturing high volume inexpensive components or packaged solutions for end markets such as automobiles, biomedical, and electronics. smaller firms provide value in innovative solutions and absorb the expense of custom fabrication with high sales margins. both large and small companies typically invest in r&d to explore new mems technology. the market for materials and equipment used to manufacture mems devices topped $1 billion worldwide in 2006. materials demand is driven by substrates, making up over 70 percent of the market, packaging coatings and increasing use of chemical mechanical planarization (cmp). while mems manufacturing continues to be dominated by used semiconductor equipment, there is a migration to 200 mm lines and select new tools, including etch and bonding for certain mems applications.   == see also ==   == references ==   == further reading == journal of micro and nanotechnique microsystem technologies, published by springer publishing, journal homepage geschke, o.; klank, h.; telleman, p., eds. (2004). microsystem engineering of lab-on-a-chip devices. wiley. isbn 3-527-30733-8.   == external links == chollet, f.; liu, hb. (10 august 2018). a (not so) short introduction to mems. isbn 9782954201504. 5.4.')"
232,"Mass transfer is the net movement of mass from one location, usually meaning  stream, phase, fraction or component, to another. Mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. Mass transfer is used by different scientific disciplines for different processes and mechanisms. The phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems.
Some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. In industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers or stripping, adsorbers such as activated carbon beds, and liquid-liquid extraction. Mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. These towers couple heat transfer to mass transfer by allowing hot water to flow in contact with air. The water is cooled by expelling some of its content in the form of water vapour.


== Astrophysics ==
In astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its Roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. It is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.


== Chemical engineering ==
Mass transfer finds extensive application in chemical engineering problems. It is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering like electrochemical engineering.The driving force for mass transfer is usually a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. A chemical species moves from areas of high chemical potential to areas of low chemical potential. Thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. For single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction.
While thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. This rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. These mass transfer coefficients are typically published in terms of dimensionless numbers, often including Péclet numbers, Reynolds numbers, Sherwood numbers and Schmidt numbers, among others.


== Analogies between heat, mass, and momentum transfer ==

There are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. The molecular transfer equations of Newton's law for fluid momentum at low Reynolds number (Stokes flow), Fourier's law for heat, and Fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. 
At higher Reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the Navier-Stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. A great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.


== References ==


== See also ==
Crystal growth
Heat transfer
Fick's laws of diffusion
Distillation column
McCabe-Thiele method
Vapor-Liquid Equilibrium
Liquid-liquid extraction
Separation process
Binary star
Type Ia supernova
Thermodiffusion
Accretion (astrophysics)","pandas(index=232, _1=232, text=""mass transfer is the net movement of mass from one location, usually meaning  stream, phase, fraction or component, to another. mass transfer occurs in many processes, such as absorption, evaporation, drying, precipitation, membrane filtration, and distillation. mass transfer is used by different scientific disciplines for different processes and mechanisms. the phrase is commonly used in engineering for physical processes that involve diffusive and convective transport of chemical species within physical systems. some common examples of mass transfer processes are the evaporation of water from a pond to the atmosphere, the purification of blood in the kidneys and liver, and the distillation of alcohol. in industrial processes, mass transfer operations include separation of chemical components in distillation columns, absorbers such as scrubbers or stripping, adsorbers such as activated carbon beds, and liquid-liquid extraction. mass transfer is often coupled to additional transport processes, for instance in industrial cooling towers. these towers couple heat transfer to mass transfer by allowing hot water to flow in contact with air. the water is cooled by expelling some of its content in the form of water vapour.   == astrophysics == in astrophysics, mass transfer is the process by which matter gravitationally bound to a body, usually a star, fills its roche lobe and becomes gravitationally bound to a second body, usually a compact object (white dwarf, neutron star or black hole), and is eventually accreted onto it. it is a common phenomenon in binary systems, and may play an important role in some types of supernovae and pulsars.   == chemical engineering == mass transfer finds extensive application in chemical engineering problems. it is used in reaction engineering, separations engineering, heat transfer engineering, and many other sub-disciplines of chemical engineering like electrochemical engineering.the driving force for mass transfer is usually a difference in chemical potential, when it can be defined, though other thermodynamic gradients may couple to the flow of mass and drive it as well. a chemical species moves from areas of high chemical potential to areas of low chemical potential. thus, the maximum theoretical extent of a given mass transfer is typically determined by the point at which the chemical potential is uniform. for single phase-systems, this usually translates to uniform concentration throughout the phase, while for multiphase systems chemical species will often prefer one phase over the others and reach a uniform chemical potential only when most of the chemical species has been absorbed into the preferred phase, as in liquid-liquid extraction. while thermodynamic equilibrium determines the theoretical extent of a given mass transfer operation, the actual rate of mass transfer will depend on additional factors including the flow patterns within the system and the diffusivities of the species in each phase. this rate can be quantified through the calculation and application of mass transfer coefficients for an overall process. these mass transfer coefficients are typically published in terms of dimensionless numbers, often including péclet numbers, reynolds numbers, sherwood numbers and schmidt numbers, among others.   == analogies between heat, mass, and momentum transfer ==  there are notable similarities in the commonly used approximate differential equations for momentum, heat, and mass transfer. the molecular transfer equations of newton's law for fluid momentum at low reynolds number (stokes flow), fourier's law for heat, and fick's law for mass are very similar, since they are all linear approximations to transport of conserved quantities in a flow field. at higher reynolds number, the analogy between mass and heat transfer and momentum transfer becomes less useful due to the nonlinearity of the navier-stokes equation (or more fundamentally, the general momentum conservation equation), but the analogy between heat and mass transfer remains good. a great deal of effort has been devoted to developing analogies among these three transport processes so as to allow prediction of one from any of the others.   == references ==   == see also == crystal growth heat transfer fick's laws of diffusion distillation column mccabe-thiele method vapor-liquid equilibrium liquid-liquid extraction separation process binary star type ia supernova thermodiffusion accretion (astrophysics)"")"
233,"A  simple machine is a  mechanical device that changes the direction or magnitude of a force.  In general, they can be defined as the simplest mechanisms that use mechanical advantage (also called leverage) to multiply force. Usually the term refers to the six classical simple machines that were defined by Renaissance scientists:
Lever
Wheel and axle
Pulley
Inclined plane
Wedge
ScrewA simple machine uses a single applied force to do work against a single load force.   Ignoring friction losses, the work done on the load is equal to the work done by the applied force.  The machine can increase the amount of the output force, at the cost of a proportional decrease in the distance moved by the load.  The ratio of the output to the applied force is called the mechanical advantage.
Simple machines can be regarded as the elementary ""building blocks"" of which all more complicated machines (sometimes called ""compound machines"") are composed.  For example, wheels, levers, and pulleys are all used in the mechanism of a bicycle.  The mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed.
Although they continue to be of great importance in mechanics and applied science, modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed, which arose in the Renaissance as a neoclassical amplification of ancient Greek texts.  The great variety and sophistication of modern machine linkages, which arose during the Industrial Revolution, is inadequately described by these six simple categories.   Various post-Renaissance authors have compiled expanded lists of ""simple machines"", often using terms like basic machines, compound machines, or machine elements to distinguish them from the classical simple machines above.  By the late 1800s, Franz Reuleaux had identified hundreds of machine elements, calling them simple machines.   Modern machine theory analyzes machines as kinematic chains composed of elementary linkages called kinematic pairs.


== History ==
The idea of a simple machine originated with the Greek philosopher Archimedes around the 3rd century BC, who studied the Archimedean simple machines: lever, pulley, and  screw.  He discovered the principle of mechanical advantage in the lever.  Archimedes' famous remark with regard to the lever: ""Give me a place to stand on, and I will move the Earth,"" (Greek: δῶς μοι πᾶ στῶ καὶ τὰν γᾶν κινάσω) expresses his realization that there was no limit to the amount of force amplification that could be achieved by using mechanical advantage.  Later Greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to calculate their (ideal) mechanical advantage.  For example, Heron of Alexandria (c. 10–75 AD) in his work Mechanics lists five mechanisms that can ""set a load in motion""; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses.  However the Greeks' understanding was limited to the statics of simple machines (the balance of forces), and did not include dynamics, the tradeoff between force and distance, or the concept of work.
During the Renaissance the dynamics of the Mechanical Powers, as the simple machines were called, began to be studied from the standpoint of how far they could lift a load, in addition to the force they could apply, leading eventually to the new concept of mechanical work.  In 1586 Flemish engineer Simon Stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. The complete dynamic theory of simple machines was worked out by Italian scientist Galileo Galilei in 1600 in Le Meccaniche (On Mechanics), in which he showed the underlying mathematical similarity of the machines as force amplifiers.    He was the first to explain that simple machines do not create energy, only transform it.The classic rules of sliding friction in machines were discovered by Leonardo da Vinci (1452–1519), but were unpublished and merely documented in his notebooks, and were based on pre-Newtonian science such as believing friction was an ethereal fluid.  They were rediscovered by Guillaume Amontons (1699) and were further developed by Charles-Augustin de Coulomb (1785).


== Ideal simple machine ==
If a simple machine does not dissipate energy through friction, wear or deformation, then energy is conserved and it is called an ideal simple machine.  In this case, the power into the machine equals the power out, and the mechanical advantage can be calculated from its geometric dimensions.
Although each machine works differently mechanically, the way they function is similar mathematically.  In each machine, a force 
  
    
      
        
          F
          
            in
          
        
        
      
    
    {\displaystyle F_{\text{in}}\,}
   is applied to the device at one point, and it does work moving a load, 
  
    
      
        
          F
          
            out
          
        
        
      
    
    {\displaystyle F_{\text{out}}\,}
   at another point.  Although some machines only change the direction of the force, such as a stationary pulley, most machines multiply the magnitude of the force by a factor, the mechanical advantage

  
    
      
        
          M
          A
        
        =
        
          F
          
            out
          
        
        
          /
        
        
          F
          
            in
          
        
        
      
    
    {\displaystyle \mathrm {MA} =F_{\text{out}}/F_{\text{in}}\,}
  that can be calculated from the machine's geometry and friction.
Simple machines do not contain a source of energy, so they cannot do more work than they receive from the input force.  A simple machine with no friction or elasticity is called an ideal machine.  Due to conservation of energy, in an ideal simple machine, the power output (rate of energy output) at any time  
  
    
      
        
          P
          
            out
          
        
        
      
    
    {\displaystyle P_{\text{out}}\,}
   is equal to the power input 
  
    
      
        
          P
          
            in
          
        
        
      
    
    {\displaystyle P_{\text{in}}\,}
  

  
    
      
        
          P
          
            out
          
        
        =
        
          P
          
            in
          
        
        
      
    
    {\displaystyle P_{\text{out}}=P_{\text{in}}\!}
  The power output equals the velocity of the load 
  
    
      
        
          v
          
            out
          
        
        
      
    
    {\displaystyle v_{\text{out}}\,}
   multiplied by the load force 
  
    
      
        
          P
          
            out
          
        
        =
        
          F
          
            out
          
        
        
          v
          
            out
          
        
        
      
    
    {\displaystyle P_{\text{out}}=F_{\text{out}}v_{\text{out}}\!}
  .  Similarly the power input from the applied force is equal to the velocity of the input point 
  
    
      
        
          v
          
            in
          
        
        
      
    
    {\displaystyle v_{\text{in}}\,}
   multiplied by the applied force 
  
    
      
        
          P
          
            in
          
        
        =
        
          F
          
            in
          
        
        
          v
          
            in
          
        
        
      
    
    {\displaystyle P_{\text{in}}=F_{\text{in}}v_{\text{in}}\!}
  .
Therefore,

  
    
      
        
          F
          
            out
          
        
        
          v
          
            out
          
        
        =
        
          F
          
            in
          
        
        
          v
          
            in
          
        
        
      
    
    {\displaystyle F_{\text{out}}v_{\text{out}}=F_{\text{in}}v_{\text{in}}\,}
  So the mechanical advantage of an ideal machine 
  
    
      
        
          
            M
            A
          
          
            ideal
          
        
        
      
    
    {\displaystyle \mathrm {MA} _{\text{ideal}}\,}
   is equal to the velocity ratio, the ratio of input velocity to output velocity

  
    
      
        
          
            M
            A
          
          
            ideal
          
        
        =
        
          
            
              F
              
                out
              
            
            
              F
              
                in
              
            
          
        
        =
        
          
            
              v
              
                in
              
            
            
              v
              
                out
              
            
          
        
        
      
    
    {\displaystyle \mathrm {MA} _{\text{ideal}}={F_{\text{out}} \over F_{\text{in}}}={v_{\text{in}} \over v_{\text{out}}}\,}
  The velocity ratio is also equal to the ratio of the distances covered in any given period of time

  
    
      
        
          
            
              v
              
                out
              
            
            
              v
              
                in
              
            
          
        
        =
        
          
            
              d
              
                out
              
            
            
              d
              
                in
              
            
          
        
        
      
    
    {\displaystyle {v_{\text{out}} \over v_{\text{in}}}={d_{\text{out}} \over d_{\text{in}}}\,}
  Therefore the mechanical advantage of an ideal machine is also equal to the distance ratio, the ratio of input distance moved to output distance moved

This can be calculated from the geometry of the machine.  For example, the mechanical advantage and distance ratio of the lever is equal to the ratio of its lever arms.
The mechanical advantage can be greater or less than one:

If 
  
    
      
        
          M
          A
        
        >
        1
        
      
    
    {\displaystyle \mathrm {MA} >1\,}
   the output force is greater than the input, the machine acts as a force amplifier, but the distance moved by the load 
  
    
      
        
          d
          
            out
          
        
        
      
    
    {\displaystyle d_{\text{out}}\,}
   is less than the distance moved by the input force 
  
    
      
        
          d
          
            in
          
        
        
      
    
    {\displaystyle d_{\text{in}}\,}
  .
If 
  
    
      
        
          M
          A
        
        <
        1
        
      
    
    {\displaystyle \mathrm {MA} <1\,}
   the output force is less than the input, but the distance moved by the load is greater than the distance moved by the input force.In the screw, which uses rotational motion, the input force should be replaced by the torque, and the velocity by the angular velocity the shaft is turned.


== Friction and efficiency ==
All real machines have friction, which causes some of the input power to be dissipated as heat.    If 
  
    
      
        
          P
          
            fric
          
        
        
      
    
    {\displaystyle P_{\text{fric}}\,}
   is the power lost to friction, from conservation of energy

  
    
      
        
          P
          
            in
          
        
        =
        
          P
          
            out
          
        
        +
        
          P
          
            fric
          
        
        
      
    
    {\displaystyle P_{\text{in}}=P_{\text{out}}+P_{\text{fric}}\,}
  The mechanical efficiency 
  
    
      
        η
        
      
    
    {\displaystyle \eta \,}
   of a machine (where 
  
    
      
        0
        <
        η
         
        <
        1
      
    
    {\displaystyle 0<\eta \ <1}
  ) is defined as the ratio of power out to the power in, and is a measure of the frictional energy losses

  
    
      
        η
        ≡
        
          
            
              P
              
                out
              
            
            
              P
              
                in
              
            
          
        
        
      
    
    {\displaystyle \eta \equiv {P_{\text{out}} \over P_{\text{in}}}\,}
  

  
    
      
        
          P
          
            out
          
        
        =
        η
        
          P
          
            in
          
        
        
      
    
    {\displaystyle P_{\text{out}}=\eta P_{\text{in}}\,}
  As above, the power is equal to the product of force and velocity, so

  
    
      
        
          F
          
            out
          
        
        
          v
          
            out
          
        
        =
        η
        
          F
          
            in
          
        
        
          v
          
            in
          
        
        
      
    
    {\displaystyle F_{\text{out}}v_{\text{out}}=\eta F_{\text{in}}v_{\text{in}}\,}
  Therefore,

So in non-ideal machines, the mechanical advantage is always less than the velocity ratio by the product with the efficiency η.  So a machine that includes friction will not be able to move as large a load as a corresponding ideal machine using the same input force.


== Compound machines ==
A compound machine is a machine formed from a set of simple machines connected in series with the output force of one providing the input force to the next.  For example, a bench vise consists of a lever (the vise's handle) in series with a screw, and a simple gear train consists of a number of gears (wheels and axles) connected in series.
The mechanical advantage of a compound machine is the ratio of the output force exerted by the last machine in the series divided by the input force applied to the first machine, that is

  
    
      
        
          
            M
            A
          
          
            compound
          
        
        =
        
          
            
              F
              
                outN
              
            
            
              F
              
                in1
              
            
          
        
        
      
    
    {\displaystyle \mathrm {MA} _{\text{compound}}={F_{\text{outN}} \over F_{\text{in1}}}\,}
  Because the output force of each machine is the input of the next, 
  
    
      
        
          F
          
            out1
          
        
        =
        
          F
          
            in2
          
        
        ,
        
        
          F
          
            out2
          
        
        =
        
          F
          
            in3
          
        
        ,
        …
        
        
          F
          
            outK
          
        
        =
        
          F
          
            inK+1
          
        
      
    
    {\displaystyle F_{\text{out1}}=F_{\text{in2}},\;F_{\text{out2}}=F_{\text{in3}},\ldots \;F_{\text{outK}}=F_{\text{inK+1}}}
  , this mechanical advantage is also given by

  
    
      
        
          
            M
            A
          
          
            compound
          
        
        =
        
          
            
              F
              
                out1
              
            
            
              F
              
                in1
              
            
          
        
        
          
            
              F
              
                out2
              
            
            
              F
              
                in2
              
            
          
        
        
          
            
              F
              
                out3
              
            
            
              F
              
                in3
              
            
          
        
        …
        
          
            
              F
              
                outN
              
            
            
              F
              
                inN
              
            
          
        
        
      
    
    {\displaystyle \mathrm {MA} _{\text{compound}}={F_{\text{out1}} \over F_{\text{in1}}}{F_{\text{out2}} \over F_{\text{in2}}}{F_{\text{out3}} \over F_{\text{in3}}}\ldots {F_{\text{outN}} \over F_{\text{inN}}}\,}
  Thus, the mechanical advantage of the compound machine is equal to the product of the mechanical advantages of the series of simple machines that form it

  
    
      
        
          
            M
            A
          
          
            compound
          
        
        =
        
          
            M
            A
          
          
            1
          
        
        
          
            M
            A
          
          
            2
          
        
        …
        
          
            M
            A
          
          
            N
          
        
        
      
    
    {\displaystyle \mathrm {MA} _{\text{compound}}=\mathrm {MA} _{1}\mathrm {MA} _{2}\ldots \mathrm {MA} _{\text{N}}\,}
  Similarly, the efficiency of a compound machine is also the product of the efficiencies of the series of simple machines that form it

  
    
      
        
          η
          
            compound
          
        
        =
        
          η
          
            1
          
        
        
          η
          
            2
          
        
        …
        
        
          η
          
            N
          
        
        .
        
      
    
    {\displaystyle \eta _{\text{compound}}=\eta _{1}\eta _{2}\ldots \;\eta _{\text{N}}.\,}
  


== Self-locking machines ==

In many simple machines, if the load force Fout on the machine is high enough in relation to the input force  Fin, the machine will move backwards, with the load force doing work on the input force.  So these machines can be used in either direction, with the driving force applied to either input point.   For example, if the load force on a lever is high enough, the lever will move backwards, moving the input arm backwards against the input force.  These are called ""reversible"", ""non-locking"" or ""overhauling""  machines, and the backward motion is called ""overhauling"".   
However, in some machines, if the frictional forces are high enough, no amount of load force can move it backwards, even if the input force is zero.   This is called a ""self-locking"", ""nonreversible"", or ""non-overhauling"" machine.  These machines can only be set in motion by a force at the input, and when the input force is removed will remain motionless, ""locked"" by friction at whatever position they were left.
Self-locking occurs mainly in those machines with large areas of sliding contact between moving parts: the screw,  inclined plane, and wedge:

The most common example is a screw.  In most screws, applying torque to the shaft can cause it to turn, moving the shaft linearly to do work against a load, but no amount of axial load force against the shaft will cause it to turn backwards.
In an inclined plane, a load can be pulled up the plane by a sideways input force, but if the plane is not too steep and there is enough friction between load and plane, when the input force is removed the load will remain motionless and will not slide down the plane, regardless of its weight.
A wedge can be driven into a block of wood by force on the end, such as from hitting it with a sledge hammer, forcing the sides apart, but no amount of compression force from the wood walls will cause it to pop back out of the block.A machine will be self-locking if and only if its efficiency η is below 50%:

  
    
      
        η
        ≡
        
          
            
              
                F
                
                  o
                  u
                  t
                
              
              
                /
              
              
                F
                
                  i
                  n
                
              
            
            
              
                d
                
                  i
                  n
                
              
              
                /
              
              
                d
                
                  o
                  u
                  t
                
              
            
          
        
        <
        0.50
        
      
    
    {\displaystyle \eta \equiv {\frac {F_{out}/F_{in}}{d_{in}/d_{out}}}<0.50\,}
  Whether a machine is self-locking depends on both the friction forces (coefficient of static friction) between its parts, and the distance ratio din/dout (ideal mechanical advantage).  If both the friction and ideal mechanical advantage are high enough, it will self-lock.


=== Proof ===
When a machine moves in the forward direction from point 1 to point 2, with the input force doing work on a load force,  from conservation of energy  the input work 
  
    
      
        
          W
          
            1,2
          
        
        
      
    
    {\displaystyle W_{\text{1,2}}\,}
   is equal to the sum of the work done on the load force 
  
    
      
        
          W
          
            load
          
        
        
      
    
    {\displaystyle W_{\text{load}}\,}
   and the work lost to friction 
  
    
      
        
          W
          
            fric
          
        
        
      
    
    {\displaystyle W_{\text{fric}}\,}
  

If the efficiency is below 50%

  
    
      
        η
        =
        
          W
          
            load
          
        
        
          /
        
        
          W
          
            1,2
          
        
        <
        1
        
          /
        
        2
        
      
    
    {\displaystyle \eta =W_{\text{load}}/W_{\text{1,2}}<1/2\,}
  

  
    
      
        2
        
          W
          
            load
          
        
        <
        
          W
          
            1,2
          
        
        
      
    
    {\displaystyle 2W_{\text{load}}<W_{\text{1,2}}\,}
  From Eq. 1

  
    
      
        2
        
          W
          
            load
          
        
        <
        
          W
          
            load
          
        
        +
        
          W
          
            fric
          
        
        
      
    
    {\displaystyle 2W_{\text{load}}<W_{\text{load}}+W_{\text{fric}}\,}
  

  
    
      
        
          W
          
            load
          
        
        <
        
          W
          
            fric
          
        
        
      
    
    {\displaystyle W_{\text{load}}<W_{\text{fric}}\,}
  When the machine moves backward from point 2 to point 1 with the load force doing work on the input force, the work lost to friction 
  
    
      
        
          W
          
            fric
          
        
        
      
    
    {\displaystyle W_{\text{fric}}\,}
   is the same

  
    
      
        
          W
          
            load
          
        
        =
        
          W
          
            2,1
          
        
        +
        
          W
          
            fric
          
        
        
      
    
    {\displaystyle W_{\text{load}}=W_{\text{2,1}}+W_{\text{fric}}\,}
  So the output work is

  
    
      
        
          W
          
            2,1
          
        
        =
        
          W
          
            load
          
        
        −
        
          W
          
            fric
          
        
        <
        0
        
      
    
    {\displaystyle W_{\text{2,1}}=W_{\text{load}}-W_{\text{fric}}<0\,}
  Thus the machine self-locks, because the work dissipated in friction is greater than the work done by the load force moving it backwards even with no input force


== Modern machine theory ==
Machines are studied as mechanical systems consisting of actuators and mechanisms that transmit forces and movement, monitored by sensors and controllers.  The components of actuators and mechanisms consist of links and joints that form kinematic chains.


=== Kinematic chains ===
Simple machines are elementary examples of kinematic chains that are used to model mechanical systems ranging from the steam engine to robot manipulators. The bearings that form the fulcrum of a lever and that allow the wheel and axle and pulleys to rotate are examples of a kinematic pair called a hinged joint. Similarly, the flat surface of an inclined plane and wedge are examples of the kinematic pair called a sliding joint. The screw is usually identified as its own kinematic pair called a helical joint.
Two levers, or cranks, are combined into a planar four-bar linkage by attaching a link that connects the output of one crank to the input of another.  Additional links can be attached to form a six-bar linkage or in series to form a robot.


=== Classification of machines ===
The identification of simple machines arises from a desire for a systematic method to invent new machines.  Therefore, an important concern is how simple machines are combined to make more complex machines.  One approach is to attach simple machines in series to obtain compound machines.
However, a more successful strategy was identified by Franz Reuleaux, who collected and studied over 800 elementary machines.  He realized that a lever, pulley, and wheel and axle are in essence the same device: a body rotating about a hinge. Similarly, an inclined plane, wedge, and screw are a block sliding on a flat surface.This realization shows that it is the joints, or the connections that provide movement, that are the primary elements of a machine.  Starting with four types of joints, the revolute joint, sliding joint, cam joint and gear joint, and related connections such as cables and belts, it is possible to understand a machine as an assembly of solid parts that connect these joints.


=== Kinematic synthesis ===
The design of mechanisms to perform required movement and force transmission is known as kinematic synthesis.  This is a collection of geometric techniques for the mechanical design of linkages, cam and follower mechanisms and gears and gear trains.


== See also ==
Linkage (mechanical)
Cam and follower mechanisms
Gears and gear trains
Mechanism (engineering)
Machine


== References ==","pandas(index=233, _1=233, text='a  simple machine is a  mechanical device that changes the direction or magnitude of a force.  in general, they can be defined as the simplest mechanisms that use mechanical advantage (also called leverage) to multiply force. usually the term refers to the six classical simple machines that were defined by renaissance scientists: lever wheel and axle pulley inclined plane wedge screwa simple machine uses a single applied force to do work against a single load force.   ignoring friction losses, the work done on the load is equal to the work done by the applied force.  the machine can increase the amount of the output force, at the cost of a proportional decrease in the distance moved by the load.  the ratio of the output to the applied force is called the mechanical advantage. simple machines can be regarded as the elementary ""building blocks"" of which all more complicated machines (sometimes called ""compound machines"") are composed.  for example, wheels, levers, and pulleys are all used in the mechanism of a bicycle.  the mechanical advantage of a compound machine is just the product of the mechanical advantages of the simple machines of which it is composed. although they continue to be of great importance in mechanics and applied science, modern mechanics has moved beyond the view of the simple machines as the ultimate building blocks of which all machines are composed, which arose in the renaissance as a neoclassical amplification of ancient greek texts.  the great variety and sophistication of modern machine linkages, which arose during the industrial revolution, is inadequately described by these six simple categories.   various post-renaissance authors have compiled expanded lists of ""simple machines"", often using terms like basic machines, compound machines, or machine elements to distinguish them from the classical simple machines above.  by the late 1800s, franz reuleaux had identified hundreds of machine elements, calling them simple machines.   modern machine theory analyzes machines as kinematic chains composed of elementary linkages called kinematic pairs.   == history == the idea of a simple machine originated with the greek philosopher archimedes around the 3rd century bc, who studied the archimedean simple machines: lever, pulley, and  screw.  he discovered the principle of mechanical advantage in the lever.  archimedes\' famous remark with regard to the lever: ""give me a place to stand on, and i will move the earth,"" (greek: δῶς μοι πᾶ στῶ καὶ τὰν γᾶν κινάσω) expresses his realization that there was no limit to the amount of force amplification that could be achieved by using mechanical advantage.  later greek philosophers defined the classic five simple machines (excluding the inclined plane) and were able to calculate their (ideal) mechanical advantage.  for example, heron of alexandria (c. 10–75 ad) in his work mechanics lists five mechanisms that can ""set a load in motion""; lever, windlass, pulley, wedge, and screw, and describes their fabrication and uses.  however the greeks\' understanding was limited to the statics of simple machines (the balance of forces), and did not include dynamics, the tradeoff between force and distance, or the concept of work. during the renaissance the dynamics of the mechanical powers, as the simple machines were called, began to be studied from the standpoint of how far they could lift a load, in addition to the force they could apply, leading eventually to the new concept of mechanical work.  in 1586 flemish engineer simon stevin derived the mechanical advantage of the inclined plane, and it was included with the other simple machines. the complete dynamic theory of simple machines was worked out by italian scientist galileo galilei in 1600 in le meccaniche (on mechanics), in which he showed the underlying mathematical similarity of the machines as force amplifiers.    he was the first to explain that simple machines do not create energy, only transform it.the classic rules of sliding friction in machines were discovered by leonardo da vinci (1452–1519), but were unpublished and merely documented in his notebooks, and were based on pre-newtonian science such as believing friction was an ethereal fluid.  they were rediscovered by guillaume amontons (1699) and were further developed by charles-augustin de coulomb (1785).   == ideal simple machine == if a simple machine does not dissipate energy through friction, wear or deformation, then energy is conserved and it is called an ideal simple machine.  in this case, the power into the machine equals the power out, and the mechanical advantage can be calculated from its geometric dimensions. although each machine works differently mechanically, the way they function is similar mathematically.  in each machine, a force     f  in       thus the machine self-locks, because the work dissipated in friction is greater than the work done by the load force moving it backwards even with no input force   == modern machine theory == machines are studied as mechanical systems consisting of actuators and mechanisms that transmit forces and movement, monitored by sensors and controllers.  the components of actuators and mechanisms consist of links and joints that form kinematic chains. the design of mechanisms to perform required movement and force transmission is known as kinematic synthesis.  this is a collection of geometric techniques for the mechanical design of linkages, cam and follower mechanisms and gears and gear trains.   == see also == linkage (mechanical) cam and follower mechanisms gears and gear trains mechanism (engineering) machine   == references ==')"
234,"A duty cycle or power cycle is the fraction of one period in which a signal or system is active. Duty cycle is commonly expressed as a percentage or a ratio.  A period is the time it takes for a signal to complete an on-and-off cycle. As a formula, a duty cycle (%) may be expressed as:

  
    
      
        D
        =
        
          
            
              P
              W
            
            T
          
        
        ×
        100
        %
      
    
    {\displaystyle D={\frac {PW}{T}}\times 100\%}
  Equally, a duty cycle (ratio) may be expressed as:

  
    
      
        D
        =
        
          
            
              P
              W
            
            T
          
        
      
    
    {\displaystyle D={\frac {PW}{T}}}
  where 
  
    
      
        D
      
    
    {\displaystyle D}
   is the duty cycle, 
  
    
      
        P
        W
      
    
    {\displaystyle PW}
   is the pulse width (pulse active time), and 
  
    
      
        T
      
    
    {\displaystyle T}
   is the total period of the signal. Thus, a 60% duty cycle means the signal is on 60% of the time but off 40% of the time. The ""on time"" for a 60% duty cycle could be a fraction of a second, a day, or even a week, depending on the length of the period.
Duty cycles can be used to describe the percent time of an active signal in an electrical device such as the power switch in a switching power supply or the firing of action potentials by a living system such as a neuron.The duty factor for periodic signal expresses the same notion, but is usually scaled to a maximum of one rather than 100%.The duty cycle can also be notated as 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  .


== Applications ==


=== Electrical and electronics ===
In electronics, duty cycle is the percentage of the ratio of pulse duration, or pulse width (PW) to the total period (T) of the waveform. It is generally used to represent time duration of a pulse when it is high (1). In digital electronics, signals are used in rectangular waveform which are represented by logic 1 and logic 0. Logic 1 stands for presence of an electric pulse and 0 for absence of an electric pulse. For example, a signal (10101010) has 50% duty cycle, because the pulse remains high for 1/2 of the period or low for 1/2 of the period. Similarly, for pulse (10001000) the duty cycle will be 25% because the pulse remains high only for 1/4 of the period and remains low for 3/4 of the period.
Electrical motors typically use less than a 100% duty cycle.  For example, if a motor runs for one out of 100 seconds, or 1/100 of the time, then, its duty cycle is 1/100, or 1 percent.Pulse-width modulation (PWM) is used in a variety of electronic situations, such as power delivery and voltage regulation.
In electronic music, music synthesizers vary the duty cycle of their audio-frequency oscillators to obtain a subtle effect on the tone colors.  This technique is known as pulse-width modulation.
In the printer / copier industry, the duty cycle specification refers to the rated throughput (that is, printed pages) of a device per month.
In a welding power supply, the maximum duty cycle is defined as the percentage of time in a 10-minute period that it can be operated continuously before overheating.


=== Biological systems ===
The concept of duty cycles is also used to describe the activity of neurons and muscle fibers. In neural circuits for example, a duty cycle specifically refers to the proportion of a cycle period in which a neuron remains active.


=== Generation ===
One way to generate fairly accurate square wave signals with 1/n duty factor, where n is an integer, is to vary the duty cycle until the nth-harmonic is significantly suppressed. For audio-band signals, this can even be done ""by ear""; for example, a -40dB reduction in the 3rd harmonic corresponds to setting the duty factor to 1/3 with a precision of 1% and -60 dB reduction corresponds to a precision of 0.1%.


=== Mark-Space ratio ===
 Mark-Space ratio, or mark-to-space ratio, is another term for the same concept, to describe the temporal relationship between two alternating periods of a waveform. However, whereas the duty cycle relates the duration of one period to the duration of the entire cycle, the mark-space ratio relates the durations of the two individual periods:

  
    
      
        
          Mark Space Ratio
        
        =
        
          
            
              P
              
                W
                
                  o
                  n
                
              
            
            
              P
              
                W
                
                  o
                  f
                  f
                
              
            
          
        
      
    
    {\displaystyle {\text{Mark Space Ratio}}={\frac {PW_{on}}{PW_{off}}}}
  where 
  
    
      
        P
        
          W
          
            o
            n
          
        
      
    
    {\displaystyle PW_{on}}
   and 
  
    
      
        P
        
          W
          
            o
            f
            f
          
        
      
    
    {\displaystyle PW_{off}}
   are the durations of the two alternating periods.


== References ==","pandas(index=234, _1=234, text='a duty cycle or power cycle is the fraction of one period in which a signal or system is active. duty cycle is commonly expressed as a percentage or a ratio.  a period is the time it takes for a signal to complete an on-and-off cycle. as a formula, a duty cycle (%) may be expressed as:     d =    p w  t   × 100 %    are the durations of the two alternating periods.   == references ==')"
235,"A wedding is a ceremony where two people are united in marriage. Wedding traditions and customs vary greatly between cultures, ethnic groups, religions, countries, and social classes. Most wedding ceremonies involve an exchange of marriage vows by a couple, presentation of a gift (offering, rings, symbolic item, flowers, money, dress), and a public proclamation of marriage by an authority figure or celebrant.  Special wedding garments are often worn, and the ceremony is sometimes followed by a wedding reception. Music, poetry, prayers, or readings from religious texts or literature are also commonly incorporated into the ceremony, as well as superstitious customs originating in Ancient Rome.


== Common elements across cultures ==

Some cultures have adopted the traditional Western custom of the white wedding, in which a bride wears a white wedding dress and veil. This tradition was popularized through the marriage of Queen Victoria. Some say Victoria's choice of a white gown may have simply been a sign of extravagance, but may have also been influenced by the values she held which emphasized sexual purity. Within the modern 'white wedding' tradition, a white dress and veil are unusual choices for a woman's second or subsequent wedding.
The use of a wedding ring has long been part of religious weddings in Europe and America, but the origin of the tradition is unclear. One possibility is the Roman belief in the Vena amoris, which was believed to be a blood vessel that ran from the fourth finger (ring finger) directly to the heart. Thus, when a couple wore rings on this finger, their hearts were connected. Historian Vicki Howard points out that the belief in the ""ancient"" quality of the practice is most likely a modern invention. ""Double ring"" ceremonies are also a modern practice, a groom's wedding band not appearing in the United States until the early 20th century.The exit from the wedding ceremony is also called the ""send off"", and often includes traditional practices, such as the newlyweds and the wedding party bowing and kissing the knees of the elders in Ethiopian weddings. The send off often includes throwing rice (a symbol of prosperity and fertility) or other seeds at the newlyweds in most of the Western world, as well as for example India and Malaysia. Despite fears of the opposite, the use of uncooked rice for this purpose is not harmful to birds. Shoe tossing in place of rice has also been used in several cultures.The wedding ceremony is often followed by wedding reception or a wedding breakfast, in which the rituals may include speeches from the groom, best man, father of the bride and possibly the bride, the newlyweds' first dance as a couple, and the cutting of an elegant wedding cake. In recent years traditions have changed to include a father-daughter dance for the bride and her father, and sometimes also a mother-son dance for the groom and his mother.


== Traditional wedding attire ==

Ao dai, traditional garments of Vietnam
Barong Tagalog, an embroidered, formal men's garment of the Philippines
Batik and Kebaya, a garment worn by the Javanese people of Indonesia and also by the Malay people of Malaysia
Dashiki, the traditional West African wedding attire
Dhoti, male garment in South India
Hanbok, the traditional garment of Korea
Kilt, male garment particular to Scottish culture
Kittel, a white robe worn by the groom at an Orthodox Jewish wedding. The kittel is worn only under the chuppah, and is removed before the reception.
Kua (or 裙褂 [kwàhn kwáa]), Chinese traditional formal wear
Ribbon shirt, often worn by American Indian men on auspicious occasions, such as weddings, another common custom is to wrap bride and groom in a blanket
Sampot, traditional dress in Cambodia
Sari/Lehenga, Indian popular and traditional dress in India
Seshweshe, a female dress worn by the Basotho women during special ceremonies. Although it has recently been adopted to men attire as well.
Sherwani, a long coat-like garment worn in South Asia
Shiromuku Kimono, a traditional wedding garment in Japan
Tiara, or wedding crown, worn by Syrian and Greek couples (which are called ""τα στέφανα,"" which literally means ""wreaths"") and Scandinavian brides
Topor, a type of conical headgear traditionally worn by grooms as part of the Bengali Hindu wedding ceremony
Western dress code
Morning dress, western daytime formal dress
Stroller
White tie (""evening dress"" in the U.K; very formal evening attire)
Black tie or Evening Suit (""dinner jacket"" in the U.K; often referred to as a ""tuxedo"" in the U.S; traditionally appropriate only for use after 6:00 p.m.
Non-traditional ""tuxedo"" variants (colored jackets/ties, ""wedding suits"")
Lounge suit
Wedding veil, popularized by Queen Victoria, was a long-held custom in which the 'purity' and 'innocence' of the bride could thwart away evil spirits.
Wedding dress (or bridal gown), a special dress worn by the bride.
Different wedding clothing around the world
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		
		


== Wedding music ==


=== Western weddings ===
Music played at Western weddings includes a processional song for walking down the aisle (ex: wedding march) either before or after the marriage service. An example of such use is reported in the wedding of Nora Robinson and Alexander Kirkman Finlay in 1878.The ""Bridal Chorus"" from Lohengrin by Richard Wagner, commonly known as ""Here Comes the Bride"", is often used as the processional. Wagner is said to have been anti-Semitic, and as a result, the Bridal Chorus is normally not used at Jewish weddings. UK law forbids music with any religious connotations to be used in a civil ceremony.Johann Pachelbel's Canon in D is an alternative processional. Other alternatives include various contemporary melodies, such as Bob Marley's One Love, which is sometimes performed by a steel drum band. 

Caribbean Music for your wedding
In the United States, approximately 2 million people get married each year and close to 70 million people attend a wedding and spend more than $100 on a gift.


== Religious aspects ==
Most religions recognize a lifelong union with established ceremonies and rituals. Some religions permit polygamous marriages or same-sex marriages.
Many Christian faiths emphasize the raising of children as a priority in a marriage. In Judaism, marriage is so important that remaining unmarried is deemed unnatural. Islam also recommends marriage highly; among other things, it helps in the pursuit of spiritual perfection. The Baháʼí Faith believes that marriage is a foundation of the structure of society, and considers it both a physical and spiritual bond that endures into the afterlife. Hinduism sees marriage as a sacred duty that entails both religious and social obligations. By contrast, Buddhism does not encourage or discourage marriage, although it does teach how one might live a happily married life and emphasizes that marital vows are not to be taken lightly.Different religions have different beliefs as regards the breakup of marriage. For example, the Roman Catholic Church believes that marriage is a sacrament and a valid marriage between two baptized persons cannot be broken by any other means than death. This means that civil divorcés cannot remarry in a Catholic marriage while their spouse is alive. In the area of nullity, religions and the state often apply different rules. A couple, for example, may begin the process to have their marriage annulled by the Catholic Church only after they are no longer married in the eyes of the civil authority.


=== Customs associated with various religions and cultures ===


==== Christian customs ====

Most Christian churches give some form of blessing to a marriage, which is seen as a sacred institution in some sense, although terminology and associated theological meanings vary widely from one denomination to another:  e.g., ""holy matrimony,"" ""sacrament of marriage,"" ""holy ordinance of marriage,"" ""holy union,"" and so forth.
In some Western countries, a separate and secular civil wedding ceremony is required for recognition by the state, while in other Western countries, couples must merely obtain a marriage license from a local government authority and can be married by Christian or other clergy authorized by law to do so.
Since the beginning of the 21st century, same-sex couples have been allowed to marry civilly in many countries, and some Christian churches in those countries allow religious marriages of same-sex couples, though some forbid it.  See the article Same-sex marriage.
A Christian wedding ceremony typically includes mutual vows or solemn promises of lifelong love and fidelity by the couple, and may include some sort of pledge by the community to support the couple's relationship. A church wedding is a ceremony held in a church building and presided over by a Christian priest, minister, or pastor; weddings may also take place outdoors or in secular buildings if permitted by the rules of a particular denomination.
Wedding ceremonies typically contain prayers and readings from the Holy Bible and reflect the church's teachings about the spiritual significance of marriage, as well as its purpose and obligations.  The wedding service is sometimes combined with a Mass or Holy Communion.Customs may vary widely among denominations.  Pre-marital counseling may be urged or required for the engaged couple.  In some countries or denominations, the reading of banns of marriage may also be required before the wedding date.In the Roman Catholic Church, Holy Matrimony is considered to be one of the seven sacraments, in this case, one that the spouses bestow upon each other in front of a priest and members of the community as witnesses. As with all sacraments, it is seen as having been instituted by Jesus himself (see Gospel of Matthew 19:1–2, Catechism of the Catholic Church §1614–1615). In the Eastern Orthodox Church, it is one of the Mysteries and is seen as an ordination and a martyrdom. The wedding ceremony of Saint Thomas Christians, an ethnoreligious group of Christians in India, incorporates elements from Hindu, Jewish, and Christian weddings.
Protestant weddings may be elaborate or simple.  For example, in the United Methodist Church, the Service of Christian Marriage (Rite I) includes the elements found in a typical Sunday service, such as hymns, prayers, and readings from the Bible, as well as other elements unique to a wedding, including the exchange of marriage vows and wedding rings, and a special benediction for the couple.  Holy Communion may be part of the wedding service in liturgical Protestant churches (e.g., Anglican, Lutheran, or high-church Methodist), but is rarely, if ever, found in weddings of other low-church Protestant denominations.
A Quaker wedding ceremony in a Friends meeting is similar to any other meeting for worship, and therefore often very different from the experience expected by non-Friends.


==== Hindu customs ====

Hindu ceremonies are usually conducted totally or at least partially in Sanskrit, the language of the Hindu scriptures. The wedding celebrations may last for several days and they can be extremely diverse, depending upon the region, denomination, and caste. Mehendi ceremony is a traditional ritual in Hindu weddings, where Henna application takes place on the bride's hands and legs, before the wedding. On the wedding day, the bride and the bridegroom garland each other in front of the guests. Most guests witness only this short ceremony and then socialize, have food, and leave. The religious part (if applicable) comes hours later, witnessed by close friends and relatives. In cases where a religious ceremony is present, a Brahmin (Hindu priest) arranges a sacred yajna (fire-sacrifice), and the sacred fire (Agni) is considered the prime witness (sākshī) of the marriage. He chants mantras from the Vedas and subsidiary texts while the couple is seated before the fire. The most important step is saptapadi or saat phere, wherein the bride and the groom, hand-in-hand, encircle the sacred fire seven times, each circle representing a matrimonial vow. Then the groom marks the bride's hair parting with vermilion (sindoor) and puts a gold necklace (mangalsutra) around her neck. Or a yellow thread applied with turmeric is knotted around the bride's neck 3 times at marriage. The first knot represents her obedience and respect to her husband, the second one to his parents and the third represents her respect to God. Several other rituals may precede or follow these afore-mentioned rites. Then the bride formally departs from her blood-relatives to join the groom's family.


==== Jewish customs ====

A traditional Jewish wedding usually follows this format:
Before the ceremony, the couple formalize a written ketubah (marriage contract), specifying the obligations of husband to the wife and contingencies in case of divorce. The ketubah is signed by two witnesses and later read under the chuppah (wedding canopy).
The couple is married under the chuppah signifying their new home together. The chuppah can be made from a piece of cloth or other material attached to four poles, or a giant tallit (prayer shawl) held over the couple by four family members or friends.
The wedding couple is accompanied by both sets of parents and they join the wedding couple under the chuppah.
In Orthodox Jewish weddings, the bride is accompanied to the chuppah by both mothers, and the groom is accompanied to the chuppah by both fathers.
Seven blessings are recited, blessing the bride and groom and their new home.
The couple sip a glass of wine from a Kiddush cup.
The groom will smash a wine glass with his right foot, ostensibly in remembrance of the destruction of the Second Temple.
In Reform Jewish weddings, the bride and groom can smash the wine glass together.
At some weddings, the couple may declare that each is sanctified to the other, repeat other vows and exchange rings.
In Orthodox Jewish weddings, the bride does not speak under the chuppah and only she receives a ring. The groom recites ""Harei at mekudeshet li k'dat Moshe V'Yisrael""- ""behold you are [thus] sanctified to me by the law of Moses and Israel"" as he places the ring on the bride's right index finger. The bride's silence and acceptance of the ring signify her agreement to the marriage. This part of the ceremony is called kiddushin. The groom's giving an object of value to the bride is necessary for the wedding to be valid.
In more egalitarian weddings, the bride responds verbally, often giving the groom a ring in return. A common response is ""ani l'dodi, v'dodi li"" (I am my beloved's, my beloved is mine)
In some Orthodox weddings, the groom then says:""If I forget you, O Jerusalem, may my right hand forget its skill.
May my tongue cling to the roof of my mouth.
If I do not remember you,
if I do not consider Jerusalem in my highest joy.""The ceremony ends with the groom breaking a glass underfoot.
The couple spend their first moments as man and wife in seclusion (apart from the wedding guests, and with no other person present). This cheder yichud – ""the room of seclusion (or 'oneness')"" halachically strengthens the marriage bond since Orthodox Jews are forbidden to be secluded with an unrelated person of the opposite sex.
The ceremony is followed by a seudat mitzvah, the wedding meal, as well as music and dancing.
At the conclusion of the wedding meal, Birkat Hamazon (Grace After Meals) is recited, as well as the seven wedding blessings.In more observant communities, the couple will celebrate for seven more days, called the Sheva Brachot (seven blessings) during which the seven wedding blessings are recited at every large gathering during this time.


==== Islamic customs ====

A wedding is typically a happy time for families to celebrate. In the Muslim world, there are colorful, cultural variations from place to place.Two male witnesses who are the members of the family in most cases are required for Nikah. According to the Quran, a married Muslim couple, both husband and wife act as each other's protector and comforter and therefore only meant ""for each other"".
All Muslim marriages have to be declared publicly and are never to be undertaken in secret. For many Muslims, it is the ceremony that counts as the actual wedding alongside a confirmation of that wedding in a registry office according to fiqh, in Islam a wedding is also viewed as a legal contract particularly in Islamic jurisprudences. However, most Muslim cultures separate both the institutions of the mosque and marriage, no religious official is necessary, but very often an Imam presides and performs the ceremony, he may deliver a short sermon. Celebrations may differ from country to country depending on their culture but the main ceremony is followed by a Walima (the marriage banquet).
In Islam, polygyny is allowed with certain religious restrictions, despite that, an overwhelming majority of Muslims traditionally practice monogamy.
It is forbidden in Islam for parents or anyone else: to force, coerce, or trick either man or woman into a marriage that is contrary to the individual will of any one of the couples. It is also necessary for all marriages to commence with the best of intentions.


==== Chinese customs ====

At traditional Chinese weddings, the tea ceremony is the equivalent of an exchange of vows at a Western wedding ceremony. This ritual is still practiced widely among rural Chinese; however, young people in larger cities, as well as in Taiwan, Hong Kong, Malaysia, and Singapore, tend to practice a combination of Western style of marriage together with the tea ceremony.
When the bride leaves her home with the groom to his house, a ""Good Luck Woman"" will hold a red umbrella over her head, meaning, ""Raise the bark, spread the leaves."" This ""Good Luck Woman"" should be someone who is blessed with a good marriage, healthy children, and husband and living parents. Other relatives will scatter rice, red beans, and green beans in front of her. The red umbrella protects the bride from evil spirits, and the rice and beans are to attract the attention of the gold chicken.The newlyweds kneel in front of parents presenting tea. A Good Luck Woman making the tea says auspicious phrases to bless the newlyweds and their families. The newlyweds also present tea to each other, raising the tea cups high to show respect before presenting the tea to each other.
The attendants receiving the tea usually give the bride gifts such as jewelry or a red envelope.
The tea ceremony is an official ritual to introduce the newlyweds to each other's family, and a way for newlyweds to show respect and appreciation to their parents. The newlyweds kneel in front of their parents, serving tea to both sides of parents, as well as elder close relatives. Parents give their words of blessing and gifts to the newlyweds.


== Humanist weddings ==
While many wedding traditions and rituals have origins in religions and are still performed by religious leaders, some marriage traditions are cultural and predate the prevalent religions in those regions. Non-religious people will often want to have a wedding that is secular (not religious) in content. In order to meet this demand, secular ceremonies by carried out by humanist celebrants first developed in the 19th century. Humanists UK members pioneered humanist weddings in the 1890s, and its weddings continue to be popular with couples across England, Wales, and Northern Ireland. In Scotland, Humanist Society Scotland (HSS) has carried out secular ceremonies in the country since the 1980s. These have been legally recognised since 2005, and became more numerous than church weddings in 2018.Humanist wedding ceremonies are carried out in a variety of countries like the U.S., Canada and recently Brazil, having legal status in only a few of these countries. Humanist celebrants are able to perform valid civil marriages and civil partnerships in the Republic of Ireland. Secular weddings are becoming more popular in Ireland due to a declining influence of the Catholic Church. Since 2015, Irish humanists have conducted more weddings than the Church of Ireland.


== Types ==
There are many ways to categorize weddings, such as by the size or cultural traditions.  A wedding may fall into several categories, such as a destination microwedding, or a civil elopement.


=== Civil wedding ===
A civil wedding is a ceremony presided over by a local civil authority, such as an elected or appointed judge, Justice of the peace or the mayor of a locality. Civil wedding ceremonies may use references to God or a deity (except in U.K law where readings and music are also restricted), but generally no references to a particular religion or denomination. 
Civil weddings allow partners of different faiths to marry without one partner converting to the other partner's religion.
They can be either elaborate or simple. Many civil wedding ceremonies take place in local town or city halls or courthouses in judges' chambers.
The relevance of civil weddings varies greatly from country to country. Some countries do not provide any form of civil wedding at all (Israel and many Islamic countries), while in others it is the only legally recognized form of marriage (most countries in Latin America, Europe, and Asia). In this case civil weddings are typically either a mandatory prerequisite for any religious ceremony or religious weddings have no legal significance at all. See Civil Marriage


=== Open-air weddings by lawyers ===
Some countries and parts of countries allow open-air weddings officiated by authorised lawyers: an example is Hong Kong.


=== Destination wedding ===

Not to be confused with an elopement, a destination wedding is one in which a wedding is hosted, often in a vacation-like setting, at a location to which most of the invited guests must travel and often stay for several days. This could be a beach ceremony in the tropics, a lavish event in a metropolitan resort, or a simple ceremony at the home of a geographically distant friend or relative. During the recession of 2009, destination weddings continued to see growth compared to traditional weddings, as the typically smaller size results in lower costs.Weddings held at prestigious venues such as castles or stately homes have become increasingly popular in the 21st century particularly in European countries such as the UK, France and Germany. From 2010 onwards, there has been an increase in destination weddings that are hosted in exotic places like Indonesia, Maldives, India, and Pakistan.


=== Double wedding ===
A double wedding is a double ceremony where two affianced couples rendezvous for two simultaneous or consecutive weddings. Typically, a fiancé with a sibling who is also engaged, or four close friends in which both couples within the friendship are engaged might plan a double wedding where both couples legally marry.


=== Elopement ===
Elopement is the act of getting married, often unexpectedly, without inviting guests to the wedding. In some cases, a small group of family or friends may be present, while in others, the engaged couple may marry without the consent or knowledge of parents or others. While the couple may or may not be widely known to be engaged prior to the elopement, the wedding itself is generally a surprise to those who are later informed of its occurrence.


=== Handfasting ===
A handfasting is an old pagan custom, dating back to the time of the ancient Celts. A handfasting was originally more like an engagement period, where two people would declare a binding union between themselves for a year and a day. The original handfasting was a trial marriage.


=== Highland or Scottish wedding ===

A Highland or Scottish wedding has the groom, with some or all of the groom's men wear a kilt. The bride may wear a sash or other tartan clothing. The Scottish basket sword is used for any Saber Arch.


=== Mass wedding ===
A collective or mass wedding is a single ceremony where numerous couples are married simultaneously.


=== Microwedding ===
A microwedding is defined by the small number of friends and family members present.  The number of guests is usually understood to be no more than 10 or 15 people including family members, although some sources will use this label for a small wedding with up to 50 guests.  Compared to an elopement or a civil wedding with no guests, a microwedding is planned and announced in advance and may incorporate whatever traditions and activities the family wants to maintain, such as a wedding cake, photographs, or religious ceremonies.  Although the cost per guest may be higher, the overall cost of a microwedding is usually significantly less than a large wedding.  Microweddings gained attention during the COVID-19 pandemic as a way to have a wedding event in compliance with public health restrictions.


=== Military wedding ===

A military wedding is a ceremony conducted in a military chapel and may involve a Saber Arch. In most military weddings the bride, groom, or both will wear a military dress uniform in lieu of civilian formal wear. Some retired military personnel who marry after their service has ended may opt for a military wedding.


=== Peasant wedding ===
A peasant wedding is a Dutch carnival custom. 
Not everywhere in Limburg and Brabant is a boerenbruiloft (peasant's wedding) part of the carnival. Especially in the northern and central part of Limburg and eastern part of North Brabant is the boerenbruiloft very often held during the carnival and is an important part of the carnival culture. Each carnival association has its own tradition concerning choosing the spouse for a wedding. Often the bride and groom are chosen by the council of eleven or by the couple that was married the year before. It is not necessary that the newlyweds are a couple in real life. It is also not necessary that the bride and groom are single. Both the bride and groom, however, should be in love during the carnival and they need to transfer their love to all the people who celebrate their wedding along with them. The highlight of the festival of the peasant wedding is the wedding and feast of the onecht (not-marriage) of the bride and groom. There are many aspects that can be found in a real-life marriage. First the engagement will be announced just as if it would be an official marriage. And both the families should learn to know each other very well in organizing the party and the ceremony, like a normal wedding. The two families prepare a piece of entertainment for the wedding. And just like a real wedding, a reception and a feast is organized where guests are asked to wear appropriate clothing. The bride and groom will often dress in wedding clothing from before 1940. The bride, for example, will often wear a poffer, which is a traditional Brabantian headdress.


=== Same-sex wedding ===
A marriage between two people of the same sex.


=== Shotgun wedding ===
A shotgun wedding is a wedding in which the groom is reluctant to marry the bride, however, is strongly encouraged to do so to avoid family, social or legal repercussions. In many cases, the bride is pregnant before the wedding and the family of the bride, most commonly the bride's father insists that the groom marry the bride before the pregnancy becomes obvious.


=== Vow renewal wedding ===
A wedding vow renewal is a ceremony in which a married couple renews or reaffirms their wedding vows. Typically, this ceremony is held to commemorate a milestone wedding anniversary. It may also be held to recreate the marriage ceremony in the presence of family and friends, especially in the case of an earlier elopement.


=== Weekend wedding ===
A weekend wedding is a wedding in which couples and their guests celebrate over the course of an entire weekend. Special activities, such as spa treatments and golf tournaments may be scheduled into the wedding itinerary. Lodging usually is at the same facility as the wedding and couples often host a Sunday brunch for the weekend's finale.


=== White wedding ===

A white wedding is a term for a traditional formal or semi-formal Western wedding. This term refers to the color of the wedding dress, which became popular after Queen Victoria wore a pure white gown when she married Prince Albert and many were quick to copy her choice. At the time, the color white symbolized both extravagance and virginal purity to many and had become the color for use by young women being formally presented to the royal court.


== Wedding ceremony participants ==

Wedding ceremony participants also referred to as the wedding party, are the people that participate directly in the wedding ceremony itself.
Depending on the location, religion, and style of the wedding, this group may include only the individual people that are marrying, or it may include one or more brides, grooms (or bridegrooms), persons of honor,  bridespersons, best persons, groomsmen, flower girls,  pages, and ring bearers.
A ""bride's party"" consists of those on her side, while a ""groom's party"" consists of those on his side.

Bride: The woman about to be married.
Bridegroom or Groom: The man about to be married.
Marriage officiant: The person who officiates at the wedding, validating the wedding from a legal and/or religious standpoint. This person may be a judge, justice of the peace, or a member of the clergy. In Hindu marriages, the marriage officiant is called a pandit or Brahmin.
Best Man, Woman, or Person: The chief assistant to a bridegroom at a wedding, typically a sibling or friend of special significance in his life. Often holds the wedding rings until their exchange.
Mothers of the Bride and Groom
Fathers of the Bride and Groom
Maid, Matron or Man of Honor: the title and position held by a bride's chief attendant, typically her closest friend or sibling.
Bridesmaids: the female attendants to a bride. Males in this role may be called honor attendants or sometimes bridesmen, but that term has a different traditional meaning.
Groomsmen or Ushers: The attendants, usually male, to a bridegroom in a wedding ceremony.  Female attendants, such as a sister of the groom, are typically called honor attendants.
Pages: Young attendants may carry the bride's train. In a formal wedding, the ring bearer is a special page that carries the rings down the aisle. The coin bearer is a similar page that marches on the wedding aisle to bring the wedding coins.
Flower girls: In some traditions, one or more children carry bouquets or drop flower petals in front of the bride in the wedding procession.


== Wedding industry ==
The global wedding industry was worth $300 billion as of 2016. The United States wedding industry alone was estimated to be worth $60  billion as of the same year. In the United States, the wedding industry employs over one million people throughout 600,000 businesses and grows 2% each year. The industry has undergone a transition due to the increased use of technology. Bridal websites, blogs, and social media accounts have driven spending up and created new trends and traditions.In 2016 alone the average cost of a wedding in the U.S. was estimated to be at $35,329, though the average American spent around $14,399 that year.  According to one scholarly study of American couples, extravagant spending on weddings is associated with debt stress and short-lived marriages that end in divorce.  Couples who spent less than US$10,000 on all wedding-related expenses, and who had a relatively large number of guests in attendance, were the least likely to divorce.


== See also ==
Elopement
Wedding cake
Wedding reception
White wedding
Wedding customs by country


== References ==


== External links ==
 The dictionary definition of wedding at Wiktionary","pandas(index=235, _1=235, text='a wedding is a ceremony where two people are united in marriage. wedding traditions and customs vary greatly between cultures, ethnic groups, religions, countries, and social classes. most wedding ceremonies involve an exchange of marriage vows by a couple, presentation of a gift (offering, rings, symbolic item, flowers, money, dress), and a public proclamation of marriage by an authority figure or celebrant.  special wedding garments are often worn, and the ceremony is sometimes followed by a wedding reception. music, poetry, prayers, or readings from religious texts or literature are also commonly incorporated into the ceremony, as well as superstitious customs originating in ancient rome.   == common elements across cultures ==  some cultures have adopted the traditional western custom of the white wedding, in which a bride wears a white wedding dress and veil. this tradition was popularized through the marriage of queen victoria. some say victoria\'s choice of a white gown may have simply been a sign of extravagance, but may have also been influenced by the values she held which emphasized sexual purity. within the modern \'white wedding\' tradition, a white dress and veil are unusual choices for a woman\'s second or subsequent wedding. the use of a wedding ring has long been part of religious weddings in europe and america, but the origin of the tradition is unclear. one possibility is the roman belief in the vena amoris, which was believed to be a blood vessel that ran from the fourth finger (ring finger) directly to the heart. thus, when a couple wore rings on this finger, their hearts were connected. historian vicki howard points out that the belief in the ""ancient"" quality of the practice is most likely a modern invention. ""double ring"" ceremonies are also a modern practice, a groom\'s wedding band not appearing in the united states until the early 20th century.the exit from the wedding ceremony is also called the ""send off"", and often includes traditional practices, such as the newlyweds and the wedding party bowing and kissing the knees of the elders in ethiopian weddings. the send off often includes throwing rice (a symbol of prosperity and fertility) or other seeds at the newlyweds in most of the western world, as well as for example india and malaysia. despite fears of the opposite, the use of uncooked rice for this purpose is not harmful to birds. shoe tossing in place of rice has also been used in several cultures.the wedding ceremony is often followed by wedding reception or a wedding breakfast, in which the rituals may include speeches from the groom, best man, father of the bride and possibly the bride, the newlyweds\' first dance as a couple, and the cutting of an elegant wedding cake. in recent years traditions have changed to include a father-daughter dance for the bride and her father, and sometimes also a mother-son dance for the groom and his mother.   == traditional wedding attire ==  ao dai, traditional garments of vietnam barong tagalog, an embroidered, formal men\'s garment of the philippines batik and kebaya, a garment worn by the javanese people of indonesia and also by the malay people of malaysia dashiki, the traditional west african wedding attire dhoti, male garment in south india hanbok, the traditional garment of korea kilt, male garment particular to scottish culture kittel, a white robe worn by the groom at an orthodox jewish wedding. the kittel is worn only under the chuppah, and is removed before the reception. kua (or 裙褂 [kwàhn kwáa]), chinese traditional formal wear ribbon shirt, often worn by american indian men on auspicious occasions, such as weddings, another common custom is to wrap bride and groom in a blanket sampot, traditional dress in cambodia sari/lehenga, indian popular and traditional dress in india seshweshe, a female dress worn by the basotho women during special ceremonies. although it has recently been adopted to men attire as well. sherwani, a long coat-like garment worn in south asia shiromuku kimono, a traditional wedding garment in japan tiara, or wedding crown, worn by syrian and greek couples (which are called ""τα στέφανα,"" which literally means ""wreaths"") and scandinavian brides topor, a type of conical headgear traditionally worn by grooms as part of the bengali hindu wedding ceremony western dress code morning dress, western daytime formal dress stroller white tie (""evening dress"" in the u.k; very formal evening attire) black tie or evening suit (""dinner jacket"" in the u.k; often referred to as a ""tuxedo"" in the u.s; traditionally appropriate only for use after 6:00 p.m. non-traditional ""tuxedo"" variants (colored jackets/ties, ""wedding suits"") lounge suit wedding veil, popularized by queen victoria, was a long-held custom in which the \'purity\' and \'innocence\' of the bride could thwart away evil spirits. wedding dress (or bridal gown), a special dress worn by the bride. different wedding clothing around the world                                                                  == wedding music == a white wedding is a term for a traditional formal or semi-formal western wedding. this term refers to the color of the wedding dress, which became popular after queen victoria wore a pure white gown when she married prince albert and many were quick to copy her choice. at the time, the color white symbolized both extravagance and virginal purity to many and had become the color for use by young women being formally presented to the royal court.   == wedding ceremony participants ==  wedding ceremony participants also referred to as the wedding party, are the people that participate directly in the wedding ceremony itself. depending on the location, religion, and style of the wedding, this group may include only the individual people that are marrying, or it may include one or more brides, grooms (or bridegrooms), persons of honor,  bridespersons, best persons, groomsmen, flower girls,  pages, and ring bearers. a ""bride\'s party"" consists of those on her side, while a ""groom\'s party"" consists of those on his side.  bride: the woman about to be married. bridegroom or groom: the man about to be married. marriage officiant: the person who officiates at the wedding, validating the wedding from a legal and/or religious standpoint. this person may be a judge, justice of the peace, or a member of the clergy. in hindu marriages, the marriage officiant is called a pandit or brahmin. best man, woman, or person: the chief assistant to a bridegroom at a wedding, typically a sibling or friend of special significance in his life. often holds the wedding rings until their exchange. mothers of the bride and groom fathers of the bride and groom maid, matron or man of honor: the title and position held by a bride\'s chief attendant, typically her closest friend or sibling. bridesmaids: the female attendants to a bride. males in this role may be called honor attendants or sometimes bridesmen, but that term has a different traditional meaning. groomsmen or ushers: the attendants, usually male, to a bridegroom in a wedding ceremony.  female attendants, such as a sister of the groom, are typically called honor attendants. pages: young attendants may carry the bride\'s train. in a formal wedding, the ring bearer is a special page that carries the rings down the aisle. the coin bearer is a similar page that marches on the wedding aisle to bring the wedding coins. flower girls: in some traditions, one or more children carry bouquets or drop flower petals in front of the bride in the wedding procession.   == wedding industry == the global wedding industry was worth $300 billion as of 2016. the united states wedding industry alone was estimated to be worth $60  billion as of the same year. in the united states, the wedding industry employs over one million people throughout 600,000 businesses and grows 2% each year. the industry has undergone a transition due to the increased use of technology. bridal websites, blogs, and social media accounts have driven spending up and created new trends and traditions.in 2016 alone the average cost of a wedding in the u.s. was estimated to be at $35,329, though the average american spent around $14,399 that year.  according to one scholarly study of american couples, extravagant spending on weddings is associated with debt stress and short-lived marriages that end in divorce.  couples who spent less than us$10,000 on all wedding-related expenses, and who had a relatively large number of guests in attendance, were the least likely to divorce.   == see also == elopement wedding cake wedding reception white wedding wedding customs by country   == references ==   == external links == the dictionary definition of wedding at wiktionary')"
236,"A flange is a protruded ridge, lip or rim, either external or internal, that serves to increase strength (as the flange of an iron beam such as an I-beam or a T-beam); for easy attachment/transfer of contact force with another object (as the flange on the end of a pipe, steam cylinder, etc., or on the lens mount of a camera); or for stabilizing and guiding the movements of a machine or its parts (as the inside flange of a rail car or tram wheel, which keep the wheels from running off the rails). The term ""flange"" is also used for a kind of tool used to form flanges.


== Plumbing or piping ==

A flange can also be a plate or ring to form a rim at the end of a pipe when fastened to the pipe (for example, a closet flange). A blind flange is a plate for covering or closing the end of a pipe. A flange joint is a connection of pipes, where the connecting pieces have flanges by which the parts are bolted together.
Although the word flange generally refers to the actual raised rim or lip of a fitting, many flanged plumbing fittings are themselves known as 'flanges':
Common flanges used in plumbing are the Surrey flange or Danzey flange, York flange, Sussex flange and Essex flange.
Surrey and York flanges fit to the top of the hot water tank allowing all the water to be taken without disturbance to the tank. They are often used to ensure an even flow of water to showers. An Essex flange requires a hole to be drilled in the side of the tank.
There is also a Warix flange which is the same as a York flange but the shower output is on the top of the flange and the vent on the side. The York and Warix flange have female adapters so that they fit onto a male tank, whereas the Surrey flange connects to a female tank.
A closet flange provides the mount for a toilet.


== Pipe flanges ==
Flanges are piping components bolted together using gasket in between two flanges as a sealing material. Flanges are used to connect pipes with each other, connect pipes to flanged valves, connect pipes to flanged fittings, connect pipes to flanged piping specialty items such as strainers and to isolate piping sections using a blind flange.A flange is designed to connect sections of pipe to join them to an assembly such as a pressure vessel, valve or any equipment. Flanges are joined by bolting, and sealing is completed with the use of gaskets and other sealing methods like spray guards or specific spray flanges. If it’s not properly fit then it means there is a gap through which a fraction of undesirable substance may flow out of the system.
Industries where flammable, volatile, toxic or corrosive substances are being processed then there is greater need of special protection to prevent any kind of leakage because in case of failure that leak can cause harm to human and environment both. There should be detection so industry could be safe to face that hazards.
Flange Guards are used to provide that added level of protection to ensure safety during operation of the plant.
There are many different flange standards to be found worldwide. To allow easy functionality and interchangeability, these are designed to have standardised dimensions. Common world standards include ASA/ASME (USA), PN/DIN (European), BS10 (British/Australian), and JIS/KS (Japanese/Korean). In the USA, ANSI stopped publishing B16.5 in 1996, and the standard is ASME B16.5. ASME B16.5 covers flanges up to 24 inches size and up to pressure rating of Class 2500. Flanges larger than 24 inches are covered in ASME B16.47.
In most cases these are interchangeable as most local standards have been aligned to ISO standards, however, some local standards still differ (e.g. an ASME flange will not mate against an ISO flange). Further, many of the flanges in each standard are divided into ""pressure classes"", allowing flanges to be capable of taking different pressure ratings. Again these are not generally interchangeable (e.g. an ASME 150 will not mate with an ASME 300).These pressure classes also have differing pressure and temperature ratings for different materials. Unique pressure classes for piping can also  be developed for a process plant or power generating station; these may be specific to the corporation, engineering procurement and construction (EPC) contractor, or the process plant owner.  The ASME pressure classes for Flat-Face flanges are Class 125 and Class 250.  The classes for Ring-Joint, Tongue & Groove, and Raised-Face flanges are Class 150, Class 300, (Class 400 - unusual), Class 600, Class 900, Class 1500, and Class 2500.The flange faces are also made to standardized dimensions and are typically ""flat face"", ""raised face"", ""tongue and groove"", or ""ring joint"" styles, although other obscure styles are possible.
Flange designs are available as ""weld neck"", ""slip-on"", ""lap joint"", ""socket weld"", ""threaded"", and also ""blind"".


=== ASME standards (U.S.) ===

Pipe flanges that are made to standards called out by ASME B16.5 or ASME B16.47, and MSS SP-44. They are typically made from forged materials and have machined surfaces. ASME B16.5 refers to nominal pipe sizes (NPS) from ½"" to 24"". B16.47 covers NPSs from 26"" to 60"". Each specification further delineates flanges into pressure classes: 150, 300, 400, 600, 900, 1500 and 2500 for B16.5, and  B16.47 delineates its flanges into pressure classes 75, 150, 300, 400, 600, 900.  However these classes do not correspond to maximum pressures in psi.  Instead, the maximum pressure depends on the material of the flange and the temperature. For example, the maximum pressure for a Class 150 flange is 285 psi, and for a Class 300 Flange it is 740 psi (both are for ASTM A105 Carbon Steel and temperatures below 100F).
The gasket type and bolt type are generally specified by the standard(s); however, sometimes the standards refer to the ASME Boiler and Pressure Vessel Code (B&PVC) for details (see ASME Code Section VIII Division 1 – Appendix 2). These flanges are recognized by ASME Pipe Codes such as ASME B31.1 Power Piping, and ASME B31.3 Process Piping.
Materials for flanges are usually under ASME designation: SA-105 (Specification for Carbon Steel Forgings for Piping Applications), SA-266 (Specification for Carbon Steel Forgings for Pressure Vessel Components), or SA-182 (Specification for Forged or Rolled Alloy-Steel Pipe Flanges, Forged Fittings, and Valves and Parts for High-Temperature Service). In addition, there are many ""industry standard"" flanges that in some circumstance may be used on ASME work.
The product range includes SORF, SOFF, BLRF, BLFF, WNRF (XS, XXS, STD & Schedule 20, 40, 80), WNFF (XS, XXS, STD & Schedule 20, 40, 80), SWRF (XS & STD), SWFF (XS & STD), Threaded RF, Threaded FF & LJ, with sizes from 1/2"" to 16"". The bolting material used for flange connection is stud bolts mated with two nut (washer when required). In Petrochemical industries, ASTM A193 B7 STUD & ASTM A193 B16 Stud Bolts are used as these have high tensile strength.


=== European Dimensions (EN / DIN) ===
Most countries in Europe mainly install flanges according to standard DIN EN 1092-1 (forged Stainless or Steel Flanges). Similar to the ASME flange standard, the EN 1092-1 standard has the basic flange forms, such as weld neck flange, blind flange, lapped flange, threaded Flange (Thread ISO7-1 instead of NPT), weld on collar, pressed collars, and adapter flange such as flange coupling GD press fittings. The different forms of flanges within the EN 1092-1 (European Norm Euronorm) is indicated within the flange name through the type. 

Similar to ASME flanges, EN1092-1 steel and stainless flanges, have several different versions of raised or none raised faces. According to the European form the seals are indicated by different form: 


=== Other countries ===
Flanges in the rest of the world are manufactured according to the ISO standards for materials, pressure ratings, etc. to which local standards including DIN, BS, and others, have been aligned.


== Compact flanges ==
As the Compact flange size increase it becomes relatively increasingly heavy and complex resulting in high procurement, installation and maintenance costs.
Large flange diameters in particular are difficult to work with, and inevitably require more space and have a more challenging handling and installation procedure, particularly on remote installations such as oil rigs.
The design of the flange face includes two independent seals. The first seal is created by application of seal seating stress at the flange heel, but it is not straight forward to ensure the function of this seal.
Theoretically, the heel contact will be maintained for pressure values up to 1,8 times the flange rating at room temperature.
Theoretically, the flange also remains in contact along its outer circumference at the flange faces for all allowable load levels that it is designed for.
The main seal is the IX seal ring. The seal ring force is provided by the elastic stored energy in the stressed seal ring. Any heel leakage will give internal pressure acting on the seal ring inside intensifying the sealing action. This however requires the IX ring to be retained in the theoretical location in the ring groove which is difficult to ensure and verify during installation.
The design aims at preventing exposure to oxygen and other corrosive agents. Thus, this prevents corrosion of the flange faces, the stressed length of the bolts and the seal ring. This however depends on the outer dust rim to remain in satisfactory contact and that the inside fluid is not corrosive in case of leaking into the bolt circle void.


=== Applications of compact flanges ===
The initial cost of the theoretical higher performance compact flange is inevitably higher than a regular flange due to the closer tolerances and significantly more sophisticated design and installation requirements.
By way of example, compact flanges are often used across the following applications: subsea oil and gas or riser, cold work and cryogenics, gas injection, high temperature, and nuclear applications.


== Train wheels ==

Trains. and trams, stay on their tracks primarily due to the conical geometry of their wheels. They also have a flange on one side to keep the wheels, and hence the train, running on the rails, when the limits of the geometry based alignment are reached, e.g. due to some emergency or defect.


== Vacuum flanges ==

A vacuum flange is a flange at the end of a tube used to connect vacuum chambers, tubing and vacuum pumps to each other.


== Microwave ==

In microwave telecommunications, a flange is a type of cable joint which allows different types of waveguide to connect.
Several different microwave RF flange types exist, such as CAR, CBR, OPC, PAR, PBJ, PBR, PDR, UAR, UBR, UDR, icp and UPX.


== Ski boots ==

Ski boots use flanges at the toe or heel to connect to the binding of the ski.  The size and shape for flanges on alpine skiing boots is standardized in ISO 5355. Traditional telemark and cross country boots use the 75 mm Nordic Norm, but the toe flange is informally known as the ""duckbill"". New cross country bindings eliminate the flange entirely and use a steel bar embedded within the sole instead.


== See also ==
Casing head
Closet flange
Victaulic
SwivelA flange is a method of connecting pipes, valves, pumps and other equipment to form a piping system. It also provides easy access for cleaning, inspection or modification. Flanges are usually welded or screwed. Flanged joints are made by bolting together two flanges with a gasket between them to provide a seal.


== References ==


== Further reading ==
ASME B16.5: Standard Pipe Flanges up to and including 24 inches nominal
ASME B16.47: Standard Pipe Flanges above 24 inches
ASME Section II (Materials), Part A – Ferrous Material Specifications
Nayyar, Mohinder (1999). Piping Handbook, Seventh Edition. New York: McGraw-Hill. ISBN 0-07-047106-1.
ASME B16.47 Standard Pipe Flanges Yaang Pipe Industry
ANSI Flange Torque Lookup ToolA flange is a method of connecting pipes, valves, pumps and other equipment to form a piping system. It also provides easy access for cleaning, inspection or modification. Flanges are usually welded or screwed. Flanged joints are made by bolting together two flanges with a gasket between them to provide a seal.","pandas(index=236, _1=236, text='a flange is a protruded ridge, lip or rim, either external or internal, that serves to increase strength (as the flange of an iron beam such as an i-beam or a t-beam); for easy attachment/transfer of contact force with another object (as the flange on the end of a pipe, steam cylinder, etc., or on the lens mount of a camera); or for stabilizing and guiding the movements of a machine or its parts (as the inside flange of a rail car or tram wheel, which keep the wheels from running off the rails). the term ""flange"" is also used for a kind of tool used to form flanges.   == plumbing or piping ==  a flange can also be a plate or ring to form a rim at the end of a pipe when fastened to the pipe (for example, a closet flange). a blind flange is a plate for covering or closing the end of a pipe. a flange joint is a connection of pipes, where the connecting pieces have flanges by which the parts are bolted together. although the word flange generally refers to the actual raised rim or lip of a fitting, many flanged plumbing fittings are themselves known as \'flanges\': common flanges used in plumbing are the surrey flange or danzey flange, york flange, sussex flange and essex flange. surrey and york flanges fit to the top of the hot water tank allowing all the water to be taken without disturbance to the tank. they are often used to ensure an even flow of water to showers. an essex flange requires a hole to be drilled in the side of the tank. there is also a warix flange which is the same as a york flange but the shower output is on the top of the flange and the vent on the side. the york and warix flange have female adapters so that they fit onto a male tank, whereas the surrey flange connects to a female tank. a closet flange provides the mount for a toilet.   == pipe flanges == flanges are piping components bolted together using gasket in between two flanges as a sealing material. flanges are used to connect pipes with each other, connect pipes to flanged valves, connect pipes to flanged fittings, connect pipes to flanged piping specialty items such as strainers and to isolate piping sections using a blind flange.a flange is designed to connect sections of pipe to join them to an assembly such as a pressure vessel, valve or any equipment. flanges are joined by bolting, and sealing is completed with the use of gaskets and other sealing methods like spray guards or specific spray flanges. if it’s not properly fit then it means there is a gap through which a fraction of undesirable substance may flow out of the system. industries where flammable, volatile, toxic or corrosive substances are being processed then there is greater need of special protection to prevent any kind of leakage because in case of failure that leak can cause harm to human and environment both. there should be detection so industry could be safe to face that hazards. flange guards are used to provide that added level of protection to ensure safety during operation of the plant. there are many different flange standards to be found worldwide. to allow easy functionality and interchangeability, these are designed to have standardised dimensions. common world standards include asa/asme (usa), pn/din (european), bs10 (british/australian), and jis/ks (japanese/korean). in the usa, ansi stopped publishing b16.5 in 1996, and the standard is asme b16.5. asme b16.5 covers flanges up to 24 inches size and up to pressure rating of class 2500. flanges larger than 24 inches are covered in asme b16.47. in most cases these are interchangeable as most local standards have been aligned to iso standards, however, some local standards still differ (e.g. an asme flange will not mate against an iso flange). further, many of the flanges in each standard are divided into ""pressure classes"", allowing flanges to be capable of taking different pressure ratings. again these are not generally interchangeable (e.g. an asme 150 will not mate with an asme 300).these pressure classes also have differing pressure and temperature ratings for different materials. unique pressure classes for piping can also  be developed for a process plant or power generating station; these may be specific to the corporation, engineering procurement and construction (epc) contractor, or the process plant owner.  the asme pressure classes for flat-face flanges are class 125 and class 250.  the classes for ring-joint, tongue & groove, and raised-face flanges are class 150, class 300, (class 400 - unusual), class 600, class 900, class 1500, and class 2500.the flange faces are also made to standardized dimensions and are typically ""flat face"", ""raised face"", ""tongue and groove"", or ""ring joint"" styles, although other obscure styles are possible. flange designs are available as ""weld neck"", ""slip-on"", ""lap joint"", ""socket weld"", ""threaded"", and also ""blind"". the initial cost of the theoretical higher performance compact flange is inevitably higher than a regular flange due to the closer tolerances and significantly more sophisticated design and installation requirements. by way of example, compact flanges are often used across the following applications: subsea oil and gas or riser, cold work and cryogenics, gas injection, high temperature, and nuclear applications.   == train wheels ==  trains. and trams, stay on their tracks primarily due to the conical geometry of their wheels. they also have a flange on one side to keep the wheels, and hence the train, running on the rails, when the limits of the geometry based alignment are reached, e.g. due to some emergency or defect.   == vacuum flanges ==  a vacuum flange is a flange at the end of a tube used to connect vacuum chambers, tubing and vacuum pumps to each other.   == microwave ==  in microwave telecommunications, a flange is a type of cable joint which allows different types of waveguide to connect. several different microwave rf flange types exist, such as car, cbr, opc, par, pbj, pbr, pdr, uar, ubr, udr, icp and upx.   == ski boots ==  ski boots use flanges at the toe or heel to connect to the binding of the ski.  the size and shape for flanges on alpine skiing boots is standardized in iso 5355. traditional telemark and cross country boots use the 75 mm nordic norm, but the toe flange is informally known as the ""duckbill"". new cross country bindings eliminate the flange entirely and use a steel bar embedded within the sole instead.   == see also == casing head closet flange victaulic swivela flange is a method of connecting pipes, valves, pumps and other equipment to form a piping system. it also provides easy access for cleaning, inspection or modification. flanges are usually welded or screwed. flanged joints are made by bolting together two flanges with a gasket between them to provide a seal.   == references ==   == further reading == asme b16.5: standard pipe flanges up to and including 24 inches nominal asme b16.47: standard pipe flanges above 24 inches asme section ii (materials), part a – ferrous material specifications nayyar, mohinder (1999). piping handbook, seventh edition. new york: mcgraw-hill. isbn 0-07-047106-1. asme b16.47 standard pipe flanges yaang pipe industry ansi flange torque lookup toola flange is a method of connecting pipes, valves, pumps and other equipment to form a piping system. it also provides easy access for cleaning, inspection or modification. flanges are usually welded or screwed. flanged joints are made by bolting together two flanges with a gasket between them to provide a seal.')"
237,"Heating, ventilation, and air conditioning (HVAC) is the technology of indoor and vehicular environmental comfort. Its goal is to provide thermal comfort and acceptable indoor air quality. HVAC system design is a subdiscipline of mechanical engineering, based on the principles of thermodynamics, fluid mechanics and heat transfer. ""Refrigeration"" is sometimes added to the field's abbreviation, as HVAC&R or HVACR or ""ventilation"" is dropped, as in HACR (as in the designation of HACR-rated circuit breakers).
HVAC is an important part of residential structures such as single family homes, apartment buildings, hotels and senior living facilities, medium to large industrial and office buildings such as skyscrapers and hospitals, vehicles such as cars, trains, airplanes, ships and submarines, and in marine environments, where safe and healthy building conditions are regulated with respect to temperature and humidity, using fresh air from outdoors.
Ventilating or ventilation (the ""V"" in HVAC)  is the process of exchanging or replacing air in any space to provide high indoor air quality which involves temperature control, oxygen replenishment, and removal of moisture, odors, smoke, heat, dust, airborne bacteria, carbon dioxide, and other gases. Ventilation removes unpleasant smells and excessive moisture, introduces outside air, keeps interior building air circulating, and prevents stagnation of the interior air.
Ventilation often refers to the intentional delivery of the outside air to the building indoor environment. It is one of the most important factors for maintaining acceptable indoor air quality in buildings. Methods for ventilating a building are divided into mechanical/forced and natural types.


== Overview ==
The three major functions of heating, ventilation, and air conditioning are interrelated, especially with the need to provide thermal comfort and acceptable indoor air quality within reasonable installation, operation, and maintenance costs. HVAC systems can be used in both domestic and commercial environments. HVAC systems can provide ventilation, and maintain pressure relationships between spaces. The means of air delivery and removal from spaces is known as room air distribution.


=== Individual systems ===

In modern buildings, the design, installation, and control systems of these functions are integrated into one or more HVAC systems. For very small buildings, contractors normally estimate the capacity and type of system needed and then design the system, selecting the appropriate refrigerant and various components needed. For larger buildings, building service designers, mechanical engineers, or building services engineers analyze, design, and specify the HVAC systems. Specialty mechanical contractors and suppliers then fabricate, install and commission the systems. Building permits and code-compliance inspections of the installations are normally required for all sizes of building.


=== District networks ===
Although HVAC is executed in individual buildings or other enclosed spaces (like NORAD's underground headquarters), the equipment involved is in some cases an extension of a larger district heating (DH) or district cooling (DC) network, or a combined DHC network. In such cases, the operating and maintenance aspects are simplified and metering becomes necessary to bill for the energy that is consumed, and in some cases energy that is returned to the larger system. For example, at a given time one building may be utilizing chilled water for air conditioning and the warm water it returns may be used in another building for heating, or for the overall heating-portion of the DHC network (likely with energy added to boost the temperature).Basing HVAC on a larger network helps provide an economy of scale that is often not possible for individual buildings, for utilizing renewable energy sources such as solar heat, winter's cold, the cooling potential in some places of lakes or seawater for free cooling, and the enabling function of seasonal thermal energy storage. By utilizing natural sources that can be used for HVAC systems it can make a huge difference for the environment and help expand the knowledge of using different methods. 


== History ==

HVAC is based on inventions and discoveries made by Nikolay Lvov, Michael Faraday, Rolla C. Carpenter, Willis Carrier, Edwin Ruud, Reuben Trane, James Joule, William Rankine, Sadi Carnot, and many others.Multiple inventions within this time frame preceded the beginnings of first comfort air conditioning system, which was designed in 1902 by Alfred Wolff (Cooper, 2003) for the New York Stock Exchange, while Willis Carrier equipped the Sacketts-Wilhems Printing Company with the process AC unit the same year. Coyne College was the first school to offer HVAC training in 1899.The invention of the components of HVAC systems went hand-in-hand with the industrial revolution, and new methods of modernization, higher efficiency, and system control are constantly being introduced by companies and inventors worldwide.


== Heating ==

Heaters are appliances whose purpose is to generate heat (i.e. warmth) for the building. This can be done via central heating. Such a system contains a boiler, furnace, or heat pump to heat water, steam, or air in a central location such as a furnace room in a home, or a mechanical room in a large building. The heat can be transferred by convection, conduction, or radiation. Space heaters are used to heat single rooms and only consist of a single unit.


=== Generation ===

Heaters exist for various types of fuel, including solid fuels, liquids, and gases. Another type of heat source is electricity, normally heating ribbons composed of high resistance wire (see Nichrome). This principle is also used for baseboard heaters and portable heaters. Electrical heaters are often used as backup or supplemental heat for heat pump systems.
The heat pump gained popularity in the 1950s in Japan and the United States. Heat pumps can extract heat from various sources, such as environmental air, exhaust air from a building, or from the ground. Heat pumps transfer heat from outside the structure into the air inside. Initially, heat pump HVAC systems were only used in moderate climates, but with improvements in low temperature operation and reduced loads due to more efficient homes, they are increasing in popularity in cooler climates.


=== Distribution ===


==== Water/steam ====
In the case of heated water or steam, piping is used to transport the heat to the rooms. Most modern hot water boiler heating systems have a circulator, which is a pump, to move hot water through the distribution system (as opposed to older gravity-fed systems). The heat can be transferred to the surrounding air using radiators, hot water coils (hydro-air), or other heat exchangers. The radiators may be mounted on walls or installed within the floor to produce floor heat.
The use of water as the heat transfer medium is known as hydronics. The heated water can also supply an auxiliary heat exchanger to supply hot water for bathing and washing.


==== Air ====
Warm air systems distribute heated air through duct work systems of supply and return air through metal or fiberglass ducts. Many systems use the same ducts to distribute air cooled by an evaporator coil for air conditioning. The air supply is normally filtered through air cleaners to remove dust and pollen particles.


=== Dangers ===
The use of furnaces, space heaters, and boilers as a method of indoor heating could result in incomplete combustion and the emission of carbon monoxide, nitrogen oxides, formaldehyde, volatile organic compounds, and other combustion byproducts. Incomplete combustion occurs when there is insufficient oxygen; the inputs are fuels containing various contaminants and the outputs are harmful byproducts, most dangerously carbon monoxide, which is a tasteless and odorless gas with serious adverse health effects.Without proper ventilation, carbon monoxide can be lethal at concentrations of 1000 ppm (0.1%). However, at several hundred ppm, carbon monoxide exposure induces headaches, fatigue, nausea, and vomiting. Carbon monoxide binds with hemoglobin in the blood, forming carboxyhemoglobin, reducing the blood's ability to transport oxygen. The primary health concerns associated with carbon monoxide exposure are its cardiovascular and neurobehavioral effects. Carbon monoxide can cause atherosclerosis (the hardening of arteries) and can also trigger heart attacks. Neurologically, carbon monoxide exposure reduces hand to eye coordination, vigilance, and continuous performance. It can also affect time discrimination.


== Ventilation ==

Ventilation is the process of changing or replacing air in any space to control temperature or remove any combination of moisture, odors, smoke, heat, dust, airborne bacteria, or carbon dioxide, and to replenish oxygen. Ventilation often refers to the intentional delivery of the outside air to the building indoor space. It is one of the most important factors for maintaining acceptable indoor air quality in buildings. Methods for ventilating a building may be divided into mechanical/forced and natural types.


=== Mechanical or forced ===

Mechanical, or forced, ventilation is provided by an air handler (AHU) and used to control indoor air quality. Excess humidity, odors, and contaminants can often be controlled via dilution or replacement with outside air. However, in humid climates more energy is required to remove excess moisture from ventilation air.
Kitchens and bathrooms typically have mechanical exhausts to control odors and sometimes humidity. Factors in the design of such systems include the flow rate (which is a function of the fan speed and exhaust vent size) and noise level. Direct drive fans are available for many applications, and can reduce maintenance needs.
In summer, ceiling fans and table/floor fans circulate air within a room for the purpose of reducing the perceived temperature by increasing evaporation of perspiration on the skin of the occupants. Because hot air rises, ceiling fans may be used to keep a room warmer in the winter by circulating the warm stratified air from the ceiling to the floor.


=== Passive ===

Natural ventilation is the ventilation of a building with outside air without using fans or other mechanical systems. It can be via operable windows, louvers, or trickle vents when spaces are small and the architecture permits. ASHRAE defined Natural ventilation as the flow of air through open windows, doors, grilles, and other planned building envelope penetrations, and as being driven by natural and/or artificially produced pressure differentials. In more complex schemes, warm air is allowed to rise and flow out high building openings to the outside (stack effect), causing cool outside air to be drawn into low building openings. Natural ventilation schemes can use very little energy, but care must be taken to ensure comfort. In warm or humid climates, maintaining thermal comfort solely via natural ventilation might not be possible. Air conditioning systems are used, either as backups or supplements. Air-side economizers also use outside air to condition spaces, but do so using fans, ducts, dampers, and control systems to introduce and distribute cool outdoor air when appropriate.
An important component of natural ventilation is air change rate or air changes per hour: the hourly rate of ventilation divided by the volume of the space. For example, six air changes per hour means an amount of new air, equal to the volume of the space, is added every ten minutes. For human comfort, a minimum of four air changes per hour is typical, though warehouses might have only two. Too high of an air change rate may be uncomfortable, akin to a wind tunnel which have thousands of changes per hour. The highest air change rates are for crowded spaces, bars, night clubs, commercial kitchens at around 30 to 50 air changes per hour.Room pressure can be either positive or negative with respect to outside the room. Positive pressure occurs when there is more air being supplied than exhausted, and is common to reduce the infiltration of outside contaminants.


==== Airborne diseases ====
Natural ventilation is a key factor in reducing the spread of airborne illnesses such as tuberculosis, the common cold, influenza and meningitis. Opening doors and windows are good ways to maximize natural ventilation, which would make the risk of airborne contagion much lower than with costly and maintenance-requiring mechanical systems. Old-fashioned clinical areas with high ceilings and large windows provide greatest protection. Natural ventilation costs little and is maintenance free, and is particularly suited to limited-resource settings and tropical climates, where the burden of TB and institutional TB transmission is highest. In settings where respiratory isolation is difficult and climate permits, windows and doors should be opened to reduce the risk of airborne contagion. Natural ventilation requires little maintenance and is inexpensive.


== Air conditioning ==

An air conditioning system, or a standalone air conditioner, provides cooling and/or humidity control for all or part of a building. Air conditioned buildings often have sealed windows, because open windows would work against the system intended to maintain constant indoor air conditions. Outside, fresh air is generally drawn into the system by a vent into a mix air chamber for mixing with the space return air.  Then the mixture air enters an  indoor or outdoor heat exchanger section where the air is to be cooled down, then be guided to the space creating positive air pressure. The percentage of return air made up of fresh air can usually be manipulated by adjusting the opening of this vent. Typical fresh air intake is about 10% of the total supply air.Air conditioning and refrigeration are provided through the removal of heat. Heat can be removed through radiation, convection, or conduction. The heat transfer medium is a refrigeration system, such as water, air, ice, and chemicals are referred to as refrigerants. A refrigerant is employed either in a heat pump system in which a compressor is used to drive thermodynamic refrigeration cycle, or in a free cooling system which uses pumps to circulate a cool refrigerant (typically water or a glycol mix).
It is imperative that the air conditioning horsepower is sufficient for the area being cooled. Underpowered air conditioning system will lead to power wastage and inefficient usage. Adequate horsepower is required for any air conditioner installed.


=== Refrigeration cycle ===

The refrigeration cycle uses four essential elements to cool, which are compressor, condenser, metering device and evaporator.

At the inlet of a compressor, the refrigerant  inside the system is in a low pressure, low temperature, gaseous state. The compressor pumps the refrigerant gas up to a high pressure and temperature.
From there it enters a heat exchanger (sometimes called a condensing coil or condenser) where it loses heat to the outside, cools, and condenses into its liquid phase.
An expansion valve (also called metering device) regulates the refrigerant liquid to flow at the proper rate.
The liquid refrigerant is returned to another heat exchanger where it is allowed to evaporate, hence the heat exchanger is often called an evaporating coil or evaporator. As the liquid refrigerant evaporates it absorbs heat from the inside air, returns to the compressor, and repeats the cycle. In the process, heat is absorbed from indoors and transferred outdoors, resulting in cooling of the building.In variable climates, the system may include a reversing valve that switches from heating in winter to cooling in summer. By reversing the flow of refrigerant, the heat pump refrigeration cycle is changed from cooling to heating or vice versa. This allows a facility to be heated and cooled by a single piece of equipment by the same means, and with the same hardware.


=== Free cooling ===

Free cooling systems can have very high efficiencies, and are sometimes combined with seasonal thermal energy storage so that the cold of winter can be used for summer air conditioning. Common storage mediums are deep aquifers or a natural underground rock mass accessed via a cluster of small-diameter, heat-exchanger-equipped boreholes. Some systems with small storages are hybrids, using free cooling early in the cooling season, and later employing a heat pump to chill the circulation coming from the storage. The heat pump is added-in because the storage acts as a heat sink when the system is in cooling (as opposed to charging) mode, causing the temperature to gradually increase during the cooling season.
Some systems include an ""economizer mode"", which is sometimes called a ""free-cooling mode"". When economizing, the control system will open (fully or partially) the outside air damper and close (fully or partially) the return air damper. This will cause fresh, outside air to be supplied to the system. When the outside air is cooler than the demanded cool air, this will allow the demand to be met without using the mechanical supply of cooling (typically chilled water or a direct expansion ""DX"" unit), thus saving energy. The control system can compare the temperature of the outside air vs. return air, or it can compare the enthalpy of the air, as is frequently done in climates where humidity is more of an issue. In both cases, the outside air must be less energetic than the return air for the system to enter the economizer mode.


=== Packaged vis-à-vis split system ===
Central, ""all-air"" air-conditioning systems (or package systems) with a combined outdoor condenser/evaporator unit are often installed in North American residences, offices, and public buildings, but are difficult to retrofit (install in a building that was not designed to receive it) because of the bulky air ducts required. (Minisplit ductless systems are used in these situations.) Outside of North America, packaged systems are only used in limited applications involving large indoor space such as stadiums, theatres or exhibition halls.
An alternative to packaged systems is the use of separate indoor and outdoor coils in split systems. Split systems are preferred and widely used worldwide except in North America. In North America, split systems are most often seen in residential applications, but they are gaining popularity in small commercial buildings.
The split systems are a great choice for small buildings where ductwork is not feasible or where the space conditioning efficiency is of prime concern. The benefits of ductless air conditioning systems include easy installation, no ductwork, greater zonal control, flexibility of control and quiet operation. In space conditioning, the duct losses can account for 30% of energy consumption. The use of minisplit can result in energy savings in space conditioning as there are no losses associated with ducting.
With the split system, the evaporator coil is connected to a remote condenser unit using refrigerant piping between an indoor and outdoor unit instead of ducting air directly from the outdoor unit. Indoor units with directional vents mount onto walls, suspended from ceilings, or fit into the ceiling. Other indoor units mount inside the ceiling cavity, so that short lengths of duct handle air from the indoor unit to vents or diffusers around the rooms.
Split systems are more efficient and the footprint is typically smaller than the package systems. On the other hand, package systems tend to have slightly lower indoor noise level compared to split system since the fan motor is located outside.


=== Dehumidification ===
Dehumidification (air drying) in an air conditioning system is provided by the evaporator. Since the evaporator operates at a temperature below the dew point, moisture in the air condenses on the evaporator coil tubes. This moisture is collected at the bottom of the evaporator in a pan and removed by piping to a central drain or onto the ground outside.
A dehumidifier is an air-conditioner-like device that controls the humidity of a room or building. It is often employed in basements which have a higher relative humidity because of their lower temperature (and propensity for damp floors and walls). In food retailing establishments, large open chiller cabinets are highly effective at dehumidifying the internal air. Conversely, a humidifier increases the humidity of a building.


=== Maintenance ===
All modern air conditioning systems, even small window package units, are equipped with internal air filters. These are generally of a lightweight gauze-like material, and must be replaced or washed as conditions warrant. For example, a building in a high dust environment, or a home with furry pets, will need to have the filters changed more often than buildings without these dirt loads. Failure to replace these filters as needed will contribute to a lower heat exchange rate, resulting in wasted energy, shortened equipment life, and higher energy bills; low air flow can result in iced-over evaporator coils, which can completely stop air flow. Additionally, very dirty or plugged filters can cause overheating during a heating cycle, and can result in damage to the system or even fire.
Because an air conditioner moves heat between the indoor coil and the outdoor coil, both must be kept clean. This means that, in addition to replacing the air filter at the evaporator coil, it is also necessary to regularly clean the condenser coil. Failure to keep the condenser clean will eventually result in harm to the compressor, because the condenser coil is responsible for discharging both the indoor heat (as picked up by the evaporator) and the heat generated by the electric motor driving the compressor.


== Energy efficiency ==
Since the 1980s, manufacturers of HVAC equipment  have been making an effort to make the systems they manufacture more efficient. This was originally driven by rising energy costs, and has more recently been driven by increased awareness of environmental issues. Additionally, improvements to the HVAC system efficiency can also help increase occupant health and productivity. In the US, the EPA has imposed tighter restrictions over the years. There are several methods for making HVAC systems more efficient.


=== Heating energy ===
In the past, water heating was more efficient for heating buildings and was the standard in the United States. Today, forced air systems can double for air conditioning and are more popular.
Some benefits of forced air systems, which are now widely used in churches, schools and high-end residences, are

Better air conditioning effects
Energy savings of up to 15-20%
Even conditioningA drawback is the installation cost, which can be slightly higher than traditional HVAC systems.
Energy efficiency can be improved even more in central heating systems by introducing zoned heating. This allows a more granular application of heat, similar to non-central heating systems. Zones are controlled by multiple thermostats. In water heating systems the thermostats control zone valves, and in forced air systems they control zone dampers inside the vents which selectively block the flow of air. In this case, the control system is very critical to maintaining a proper temperature.
Forecasting is another method of controlling building heating by calculating demand for heating energy that should be supplied to the building in each time unit.


=== Ground source heat pump ===

Ground source, or geothermal, heat pumps are similar to ordinary heat pumps, but instead of transferring heat to or from outside air, they rely on the stable, even temperature of the earth to provide heating and air conditioning. Many regions experience seasonal temperature extremes, which would require large-capacity heating and cooling equipment to heat or cool buildings. For example, a conventional heat pump system used to heat a building in Montana's −57 °C (−70 °F) low temperature or cool a building in the highest temperature ever recorded in the US—57 °C (134 °F) in Death Valley, California, in 1913 would require a large amount of energy due to the extreme difference between inside and outside air temperatures. A metre below the earth's surface, however, the ground remains at a relatively constant temperature. Utilizing this large source of relatively moderate temperature earth, a heating or cooling system's capacity can often be significantly reduced. Although ground temperatures vary according to latitude, at 1.8 metres (6 ft) underground, temperatures generally only range from 7 to 24 °C (45 to 75 °F).


=== Ventilation energy recovery ===
Energy recovery systems sometimes utilize heat recovery ventilation or energy recovery ventilation systems that employ heat exchangers or enthalpy wheels to recover sensible or latent heat from exhausted air. This is done by transfer of energy to the incoming outside fresh air.


=== Air conditioning energy ===
The performance of vapor compression refrigeration cycles is limited by thermodynamics. These air conditioning and heat pump devices move heat rather than convert it from one form to another, so thermal efficiencies do not appropriately describe the performance of these devices. The Coefficient-of-Performance (COP) measures performance, but this dimensionless measure has not been adopted. Instead, the Energy Efficiency Ratio (EER) has traditionally been used to characterize the performance of many HVAC systems. EER is the Energy Efficiency Ratio based on a 35 °C (95 °F) outdoor temperature. To more accurately describe the performance of air conditioning equipment over a typical cooling season a modified version of the EER, the Seasonal Energy Efficiency Ratio (SEER), or in Europe the ESEER, is used. SEER ratings are based on seasonal temperature averages instead of a constant 35 °C (95 °F) outdoor temperature. The current industry minimum SEER rating is 14 SEER.
Engineers have pointed out some areas where efficiency of the existing hardware could be improved. For example, the fan blades used to move the air are usually stamped from sheet metal, an economical method of manufacture, but as a result they are not aerodynamically efficient. A well-designed blade could reduce electrical power required to move the air by a third.


=== Demand controlled kitchen ventilation ===

Demand controlled kitchen ventilation (DCKV) is a building controls approach of controlling the volume of kitchen exhaust and supply air in response to the actual cooking loads in a commercial kitchen. Traditional commercial kitchen ventilation systems operate at 100% fan speed independent of the volume of cooking activity and DCKV technology changes that to provide significant fan energy and conditioned air savings. By deploying smart sensing technology, both the exhaust and supply fans can be controlled to capitalize on the affinity laws for motor energy savings, reduce makeup air heating and cooling energy, increasing safety and reducing ambient kitchen noise levels.


== Air filtration and cleaning ==

Air cleaning and filtration removes particles, contaminants, vapors and gases from the air. The filtered and cleaned air then is used in heating, ventilation and air conditioning. Air cleaning and filtration should be taken in account when protecting our building environments.Clean air delivery rate (CADR) is the amount of clean air an air cleaner provides to a room or space. When determining CADR, the amount of airflow in a space is taken into account. For example, an air cleaner with a flow rate of 30 cubic metres (1,000 cu ft) per minute and an efficiency of 50% has a CADR of 15 cubic metres (500 cu ft) per minute. Along with CADR, filtration performance is very important when it comes to the air in our indoor environment. This depends on the size of the particle or fiber, the filter packing density and depth and the air flow rate.


== Industry and standards ==
The HVAC industry is a worldwide enterprise, with roles including operation and maintenance, system design and construction, equipment manufacturing and sales, and in education and research. The HVAC industry was historically regulated by the manufacturers of HVAC equipment, but regulating and standards organizations such as HARDI, ASHRAE, SMACNA, ACCA, Uniform Mechanical Code, International Mechanical Code, and AMCA have been established to support the industry and encourage high standards and achievement. (UL as an omnibus agency is not specific to the HVAC industry.)
The starting point in carrying out an estimate both for cooling and heating depends on the exterior climate and interior specified conditions. However, before taking up the heat load calculation, it is necessary to find fresh air requirements for each area in detail, as pressurization is an important consideration.


=== International ===
ISO 16813:2006 is one of the  ISO building environment standards. It establishes the general principles of building environment design. It takes into account the need to provide a healthy indoor environment for the occupants as well as the need to protect the environment for future generations and promote collaboration among the various parties involved in building environmental design for sustainability. ISO16813 is applicable to new construction and the retrofit of existing buildings.The building environmental design standard aims to:
provide the constraints concerning sustainability issues from the initial stage of the design process, with building and plant life cycle to be considered together with owning and operating costs from the beginning of the design process;
assess the proposed design with rational criteria for indoor air quality, thermal comfort, acoustical comfort, visual comfort, energy efficiency and HVAC system controls at every stage of the design process;
iterate decisions and evaluations of the design throughout the design process.


=== United States ===

In the United States, HVAC engineers generally are members of the American Society of Heating, Refrigerating, and Air-Conditioning Engineers (ASHRAE), EPA Universal CFC certified (for installation and service of CFC HVAC devices), or locally engineer certified such as a Special to Chief Boilers License issued by the state or, in some jurisdictions, the city. ASHRAE is an international technical society for all individuals and organizations interested in HVAC. The Society, organized into regions, chapters, and student branches, allows exchange of HVAC knowledge and experiences for the benefit of the field's practitioners and the public. ASHRAE provides many opportunities to participate in the development of new knowledge via, for example, research and its many technical committees. These committees typically meet twice per year at the ASHRAE Annual and Winter Meetings. A popular product show, the AHR Expo, has been held in conjunction with each winter ASHRAE meeting. The Society has approximately 50,000 members and has headquarters in Atlanta, Georgia.
The most recognized standards for HVAC design are based on ASHRAE data. The four volumes of most popular ASHRAE Handbooks are Fundamentals, Refrigeration, HVAC Applications and HVAC Systems and Equipment. The current versions of the four handbooks are shown below:
2020 ASHRAE Handbook—HVAC Systems and Equipment
2019 ASHRAE Handbook—HVAC Applications
2018 ASHRAE Handbook—Refrigeration
2017 ASHRAE Handbook—FundamentalsEach volume of the ASHRAE Handbook is updated every four years. The Fundamentals Handbook includes heating and cooling calculations. The design professional must consult ASHRAE data for the standards of design and care as the typical building codes provide little to no information on HVAC design practices; codes such as the UMC and IMC do include much detail on installation requirements, however. Other useful reference materials include items from SMACNA, ACGIH, and technical trade journals.
American design standards are legislated in the Uniform Mechanical Code or International Mechanical Code. In certain states, counties, or cities, either of these codes may be adopted and amended via various legislative processes. These codes are updated and published by the International Association of Plumbing and Mechanical Officials (IAPMO) or the International Code Council (ICC) respectively, on a 3-year code development cycle. Typically, local building permit departments are charged with enforcement of these standards on private and certain public properties=.


==== Technicians ====
An HVAC technician is a tradesman who specializes in heating, ventilation, air conditioning, and refrigeration. HVAC technicians in the US can receive training through formal training institutions, where most earn associate degrees. Training for HVAC technicians includes classroom lectures and hands-on tasks, and can be followed by an apprenticeship wherein the recent graduate works alongside a professional HVAC technician for a temporary period. HVAC techs who have been trained can also be certified in areas such as air conditioning, heat pumps, gas heating, and commercial refrigeration.


=== United Kingdom ===
The Chartered Institution of Building Services Engineers is a body that covers the essential Service (systems architecture) that allow buildings to operate. It includes the electrotechnical, heating, ventilating, air conditioning, refrigeration and plumbing industries. To train as a building services engineer, the academic requirements are GCSEs (A-C) / Standard Grades (1-3) in Maths and Science, which are important in measurements, planning and theory. Employers will often want a degree in a branch of engineering, such as building environment engineering, electrical engineering or mechanical engineering. To become a full member of CIBSE, and so also to be registered by the Engineering Council UK as a chartered engineer, engineers must also attain an Honours Degree and a master's degree in a relevant engineering subject.
CIBSE publishes several guides to HVAC design relevant to the UK market, and also the Republic of Ireland, Australia, New Zealand and Hong Kong. These guides include various recommended design criteria and standards, some of which are cited within the UK building regulations, and therefore form a legislative requirement for major building services works. The main guides are:

Guide A: Environmental Design
Guide B: Heating, Ventilating, Air Conditioning and Refrigeration
Guide C: Reference Data
Guide D: Transportation systems in Buildings
Guide E: Fire Safety Engineering
Guide F: Energy Efficiency in Buildings
Guide G: Public Health Engineering
Guide H: Building Control Systems
Guide J: Weather, Solar and Illuminance Data
Guide K: Electricity in Buildings
Guide L: Sustainability
Guide M: Maintenance Engineering and ManagementWithin the construction sector, it is the job of the building services engineer to design and oversee the installation and maintenance of the essential services such as gas, electricity, water, heating and lighting, as well as many others. These all help to make buildings comfortable and healthy places to live and work in. Building Services is part of a sector that has over 51,000 businesses and employs represents 2%-3% of the GDP.


=== Australia ===
The Air Conditioning and Mechanical Contractors Association of Australia (AMCA), Australian Institute of Refrigeration, Air Conditioning and Heating (AIRAH), Australian Refrigeration Mechanical Association and CIBSE are responsible.


=== Asia ===
Asian architectural temperature-control have different priorities than European methods. For example, Asian heating traditionally focuses on maintaining temperatures of objects such as the floor or furnishings such as Kotatsu tables and directly warming people, as opposed to the Western focus, in modern periods, on designing air systems.


==== Philippines ====
The Philippine Society of Ventilating, Air Conditioning and Refrigerating Engineers (PSVARE) along with Philippine Society of Mechanical Engineers (PSME) govern on the codes and standards for HVAC / MVAC (MVAC means ""mechanical ventilation and air conditioning"") in the Philippines.


==== India ====
The Indian Society of Heating, Refrigerating and Air Conditioning Engineers (ISHRAE) was established to promote the HVAC industry in India. ISHRAE is an associate of ASHRAE. ISHRAE was started at Delhi in 1981 and a chapter was started in Bangalore in 1989. Between 1989 & 1993, ISHRAE chapters were formed in all major cities in India.


== See also ==


== References ==


== Further reading ==
Mechanical system (building service) at the Encyclopædia Britannica
International Mechanical Code (2012 (Second Printing)) by the International Code Council, Thomson Delmar Learning.
Modern Refrigeration and Air Conditioning (August 2003) by Althouse, Turnquist, and Bracciano, Goodheart-Wilcox Publisher; 18th edition.
The Cost of Cool.


== External links ==
 Media related to Climate control at Wikimedia Commons","pandas(index=237, _1=237, text='heating, ventilation, and air conditioning (hvac) is the technology of indoor and vehicular environmental comfort. its goal is to provide thermal comfort and acceptable indoor air quality. hvac system design is a subdiscipline of mechanical engineering, based on the principles of thermodynamics, fluid mechanics and heat transfer. ""refrigeration"" is sometimes added to the field\'s abbreviation, as hvac&r or hvacr or ""ventilation"" is dropped, as in hacr (as in the designation of hacr-rated circuit breakers). hvac is an important part of residential structures such as single family homes, apartment buildings, hotels and senior living facilities, medium to large industrial and office buildings such as skyscrapers and hospitals, vehicles such as cars, trains, airplanes, ships and submarines, and in marine environments, where safe and healthy building conditions are regulated with respect to temperature and humidity, using fresh air from outdoors. ventilating or ventilation (the ""v"" in hvac)  is the process of exchanging or replacing air in any space to provide high indoor air quality which involves temperature control, oxygen replenishment, and removal of moisture, odors, smoke, heat, dust, airborne bacteria, carbon dioxide, and other gases. ventilation removes unpleasant smells and excessive moisture, introduces outside air, keeps interior building air circulating, and prevents stagnation of the interior air. ventilation often refers to the intentional delivery of the outside air to the building indoor environment. it is one of the most important factors for maintaining acceptable indoor air quality in buildings. methods for ventilating a building are divided into mechanical/forced and natural types.   == overview == the three major functions of heating, ventilation, and air conditioning are interrelated, especially with the need to provide thermal comfort and acceptable indoor air quality within reasonable installation, operation, and maintenance costs. hvac systems can be used in both domestic and commercial environments. hvac systems can provide ventilation, and maintain pressure relationships between spaces. the means of air delivery and removal from spaces is known as room air distribution. the indian society of heating, refrigerating and air conditioning engineers (ishrae) was established to promote the hvac industry in india. ishrae is an associate of ashrae. ishrae was started at delhi in 1981 and a chapter was started in bangalore in 1989. between 1989 & 1993, ishrae chapters were formed in all major cities in india.   == see also ==   == references ==   == further reading == mechanical system (building service) at the encyclopædia britannica international mechanical code (2012 (second printing)) by the international code council, thomson delmar learning. modern refrigeration and air conditioning (august 2003) by althouse, turnquist, and bracciano, goodheart-wilcox publisher; 18th edition. the cost of cool.   == external links == media related to climate control at wikimedia commons')"
238,"Hydraulics (from Greek: Υδραυλική) is a technology and applied science using engineering, chemistry, and other sciences involving the mechanical properties and use of liquids. At a very basic level, hydraulics is the liquid counterpart of pneumatics, which concerns gases. Fluid mechanics provides the theoretical foundation for hydraulics, which focuses on the applied engineering using the properties of fluids. In its fluid power applications, hydraulics is used for the generation, control, and transmission of power by the use of pressurized liquids. Hydraulic topics range through some parts of science and most of engineering modules, and cover concepts such as pipe flow, dam design, fluidics and fluid control circuitry. The principles of hydraulics are in use naturally in the human body within the vascular system and erectile tissue.
Free surface hydraulics is the branch of hydraulics dealing with free surface flow, such as occurring in rivers, canals, lakes, estuaries and seas. Its sub-field open-channel flow studies the flow in open channels.
The word ""hydraulics"" originates from the Greek word ὑδραυλικός (hydraulikos) which in turn originates from ὕδωρ (hydor, Greek for water) and αὐλός (aulos, meaning pipe).


== Ancient and medieval eras ==

Early uses of water power date back to Mesopotamia and ancient Egypt, where irrigation has been used since the 6th millennium BC and water clocks had been used since the early 2nd millennium BC. Other early examples of water power include the Qanat system in ancient Persia and the Turpan water system in ancient Central Asia.


=== Persian Empire ===
In the Persian Empire, the Persians constructed an intricate system of water mills, canals and dams known as the Shushtar Historical Hydraulic System. The project, commenced by Achaemenid king Darius the Great and finished by a group of Roman engineers captured by Sassanian king Shapur I, has been referred to by UNESCO as ""a masterpiece of creative genius"". They were also the inventors of the Qanat, an underground aqueduct. Several of Iran's large, ancient gardens were irrigated thanks to QanatsThe earliest evidence of water wheels and watermills date back to the ancient Near East in the 4th century BC, specifically in the Persian Empire before 350 BCE, in the regions of Iraq, Iran, and Egypt.


=== China ===
In ancient China there was Sunshu Ao (6th century BC), Ximen Bao (5th century BC), Du Shi (circa 31 AD), Zhang Heng (78 – 139 AD), and Ma Jun (200 – 265 AD), while medieval China had Su Song (1020 – 1101 AD) and Shen Kuo (1031–1095). Du Shi employed a waterwheel to power the bellows of a blast furnace producing cast iron. Zhang Heng was the first to employ hydraulics to provide motive power in rotating an armillary sphere for astronomical observation.


=== Sri Lanka ===

In ancient Sri Lanka, hydraulics were widely used in the ancient kingdoms of Anuradhapura and Polonnaruwa. The discovery of the principle of the valve tower, or valve pit, (Bisokotuwa in Sinhalese) for regulating the escape of water is credited to ingenuity more than 2,000 years ago. By the first century AD, several large-scale irrigation works had been completed. Macro- and micro-hydraulics to provide for domestic horticultural and agricultural needs, surface drainage and erosion control, ornamental and recreational water courses and retaining structures and also cooling systems were in place in Sigiriya, Sri Lanka. The coral on the massive rock at the site includes cisterns for collecting water.  Large ancient reservoirs of Sri Lanka are Kalawewa (King Dhatusena), Parakrama Samudra (King Parakrama Bahu), Tisa Wewa (King Dutugamunu), Minneriya (King Mahasen)


=== Greco-Roman world ===
In Ancient Greece, the Greeks constructed sophisticated water and hydraulic power systems. An example is a construction by Eupalinos, under a public contract, of a watering channel for Samos, the Tunnel of Eupalinos. An early example of the usage of hydraulic wheel, probably the earliest in Europe, is the Perachora wheel (3rd century BC).In Greco-Roman Egypt, the construction of the first hydraulic machine automata by Ctesibius (flourished c. 270 BC) and Hero of Alexandria (c. 10 – 80 AD) is notable. Hero describes several working machines using hydraulic power, such as the force pump, which is known from many Roman sites as having been used for raising water and in fire engines.

In the Roman Empire, different hydraulic applications were developed, including public water supplies, innumerable aqueducts, power using watermills and hydraulic mining. They were among the first to make use of the siphon to carry water across valleys, and used hushing on a large scale to prospect for and then extract metal ores. They used lead widely in plumbing systems for domestic and public supply, such as feeding thermae.Hydraulic mining was used in the gold-fields of northern Spain, which was conquered by Augustus in 25 BC. The alluvial gold-mine of Las Medulas was one of the largest of their mines. At least seven long aqueducts worked it, and the water streams were used to erode the soft deposits, and then wash the tailings for the valuable gold content.


=== Arabic-Islamic world ===
In the Muslim world during the Islamic Golden Age and Arab Agricultural Revolution (8th–13th centuries), engineers made wide use of hydropower as well as early uses of tidal power, and large hydraulic factory complexes. A variety of water-powered industrial mills were used in the Islamic world, including fulling mills, gristmills, paper mills, hullers, sawmills, ship mills, stamp mills, steel mills, sugar mills, and tide mills. By the 11th century, every province throughout the Islamic world had these industrial mills in operation, from Al-Andalus and North Africa to the Middle East and Central Asia. Muslim engineers also used water turbines, employed gears in watermills and water-raising machines, and pioneered the use of dams as a source of water power, used to provide additional power to watermills and water-raising machines.Al-Jazari (1136–1206) described designs for 50 devices, many of them water-powered, in his book, The Book of Knowledge of Ingenious Mechanical Devices, including water clocks, a device to serve wine, and five devices to lift water from rivers or pools. These include an endless belt with jugs attached and a reciprocating device with hinged valves.The earliest programmable machines were water-powered devices developed in the Muslim world. A music sequencer, a programmable musical instrument, was the earliest type of programmable machine. The first music sequencer was an automated water-powered flute player invented by the Banu Musa brothers, described in their Book of Ingenious Devices, in the 9th century. In 1206, Al-Jazari invented water-powered programmable automata/robots. He described four automaton musicians, including drummers operated by a programmable drum machine, where they could be made to play different rhythms and different drum patterns. The castle clock, a hydro-powered mechanical astronomical clock invented by Al-Jazari, was the first programmable analog computer.


== Modern era (c. 1600 – 1870) ==


=== Benedetto Castelli ===
In 1619 Benedetto Castelli, a student of Galileo Galilei, published the book Della Misura dell'Acque Correnti or ""On the Measurement of Running Waters,"" one of the foundations of modern hydrodynamics. He served as a chief consultant to the Pope on hydraulic projects, i.e., management of rivers in the Papal States, beginning in 1626.


=== Blaise Pascal ===
Blaise Pascal (1623–1662) studied fluid hydrodynamics and hydrostatics, centered on the principles of hydraulic fluids. His discovery on the theory behind hydraulics led to his invention of the hydraulic press, which multiplied a smaller force acting on a smaller area into the application of a larger force totaled over a larger area, transmitted through the same pressure (or exact change of pressure) at both locations. Pascal's law or principle states that for an incompressible fluid at rest, the difference in pressure is proportional to the difference in height, and this difference remains the same whether or not the overall pressure of the fluid is changed by applying an external force. This implies that by increasing the pressure at any point in a confined fluid, there is an equal increase at every other end in the container, i.e., any change in pressure applied at any point of the liquid is transmitted undiminished throughout the fluids.


=== Jean Léonard Marie Poiseuille ===
A French physician, Poiseuille (1797–1869) researched the flow of blood through the body and discovered an important law governing the rate of flow with the diameter of the tube in which flow occurred.


=== In the UK ===
Several cities developed citywide hydraulic power networks in the 19th century, to operate machinery such as lifts, cranes, capstans and the like. Joseph Bramah (1748–1814) was an early innovator and William Armstrong  (1810–1900) perfected the apparatus for power delivery on an industrial scale.  In London, the London Hydraulic Power Company was a major supplier its pipes serving large parts of the West End of London, City and the Docks, but there were schemes restricted to single enterprises such as docks and railway goods yards.


=== Hydraulic models ===
After students understand the basic principles of hydraulics, some teachers use a hydraulic analogy to help students learn other things.
For example:

The MONIAC Computer uses water flowing through hydraulic components to help students learn about economics.
The thermal-hydraulic analogy uses hydraulic principles to help students learn about thermal circuits.
The electronic–hydraulic analogy uses hydraulic principles to help students learn about electronics.The conservation of mass requirement combined with fluid compressibility yields a fundamental relationship between pressure, fluid flow, and volumetric expansion, as shown below:

  
    
      
        
          
            
              d
              p
            
            
              d
              t
            
          
        
        =
        
          
            β
            V
          
        
        
          (
          
            
              ∑
              
                in
              
            
            Q
            −
            
              
                
                  d
                  V
                
                
                  d
                  t
                
              
            
          
          )
        
      
    
    {\displaystyle {\frac {dp}{dt}}={\frac {\beta }{V}}\left(\sum _{\text{in}}Q-{\frac {dV}{dt}}\right)}
  Assuming an incompressible fluid or a ""very large"" ratio of compressibility to contained fluid volume, a finite rate of pressure rise requires that any net flow into the collected fluid volume create a volumetric change.


== See also ==


== Notes ==


== References ==
Rāshid, Rushdī; Morelon, Régis (1996), Encyclopedia of the history of Arabic science, London: Routledge, ISBN 978-0-415-12410-2.


== External links ==
Pascal's Principle and Hydraulics
The principle of hydraulics
IAHR media library Web resource of photos, animation & video
Basic hydraulic equations
MIT hydraulics course notes","pandas(index=238, _1=238, text='hydraulics (from greek: υδραυλική) is a technology and applied science using engineering, chemistry, and other sciences involving the mechanical properties and use of liquids. at a very basic level, hydraulics is the liquid counterpart of pneumatics, which concerns gases. fluid mechanics provides the theoretical foundation for hydraulics, which focuses on the applied engineering using the properties of fluids. in its fluid power applications, hydraulics is used for the generation, control, and transmission of power by the use of pressurized liquids. hydraulic topics range through some parts of science and most of engineering modules, and cover concepts such as pipe flow, dam design, fluidics and fluid control circuitry. the principles of hydraulics are in use naturally in the human body within the vascular system and erectile tissue. free surface hydraulics is the branch of hydraulics dealing with free surface flow, such as occurring in rivers, canals, lakes, estuaries and seas. its sub-field open-channel flow studies the flow in open channels. the word ""hydraulics"" originates from the greek word ὑδραυλικός (hydraulikos) which in turn originates from ὕδωρ (hydor, greek for water) and αὐλός (aulos, meaning pipe).   == ancient and medieval eras ==  early uses of water power date back to mesopotamia and ancient egypt, where irrigation has been used since the 6th millennium bc and water clocks had been used since the early 2nd millennium bc. other early examples of water power include the qanat system in ancient persia and the turpan water system in ancient central asia. after students understand the basic principles of hydraulics, some teachers use a hydraulic analogy to help students learn other things. for example:  the moniac computer uses water flowing through hydraulic components to help students learn about economics. the thermal-hydraulic analogy uses hydraulic principles to help students learn about thermal circuits. the electronic–hydraulic analogy uses hydraulic principles to help students learn about electronics.the conservation of mass requirement combined with fluid compressibility yields a fundamental relationship between pressure, fluid flow, and volumetric expansion, as shown below:        d p   d t    =   β v    (   ∑  in   q −    d v   d t     )     assuming an incompressible fluid or a ""very large"" ratio of compressibility to contained fluid volume, a finite rate of pressure rise requires that any net flow into the collected fluid volume create a volumetric change.   == see also ==   == notes ==   == references == rāshid, rushdī; morelon, régis (1996), encyclopedia of the history of arabic science, london: routledge, isbn 978-0-415-12410-2.   == external links == pascal\'s principle and hydraulics the principle of hydraulics iahr media library web resource of photos, animation & video basic hydraulic equations mit hydraulics course notes')"
239,"The technical meaning of maintenance involves functional checks, servicing, repairing or replacing of necessary devices, equipment, machinery, building infrastructure, and supporting utilities in industrial, business, and residential installations. Over time, this has come to include multiple wordings that describe various cost-effective practices to keep equipment operational; these activities occur  either before or after a failure.


== Definitions ==
Maintenance functions are often referred to as maintenance, repair and overhaul (MRO), and MRO is also used for maintenance, repair and operations. Over time, the terminology of maintenance and MRO has begun to become standardized. The United States Department of Defense uses the following definitions:
Any activity—such as tests, measurements, replacements, adjustments, and repairs—intended to retain or restore a functional unit in or to a specified state in which the unit can perform its required functions.
All action taken to retain material in a serviceable condition or to restore it to serviceability. It includes inspections, testing, servicing, classification as to serviceability, repair,  rebuilding, and reclamation.
All supply and repair action taken to keep a force in condition to carry out its mission.
The routine recurring work required to keep a facility (plant, building, structure, ground facility, utility system, or other real property) in such condition that it may be continuously used, at its original or designed capacity and efficiency for its intended purpose.Maintenance is strictly connected to the utilization stage of the product or technical system, in which the concept of maintainability must be included. In this scenario, maintainability is considered as the ability of an item, under stated conditions of use, to be retained in or restored to a state in which it can perform its required functions, using prescribed procedures and resources.In some domains like aircraft maintenance, terms maintenance, repair and overhaul also include inspection, rebuilding, alteration and the supply of spare parts, accessories, raw materials, adhesives, sealants, coatings and consumables for aircraft maintenance at the utilization stage. In international civil aviation maintenance means: 

The performance of tasks required to ensure the continuing airworthiness of an aircraft, including any one or combination of overhaul, inspection, replacement, defect rectification, and the embodiment of a modification or a repair.
This definition covers all activities for which aviation regulations require issuance of a maintenance release document (aircraft certificate of return to service – CRS).


== Types ==
The marine and air transportation, offshore structures, industrial plant and facility management industries depend on maintenance, repair and overhaul (MRO) including scheduled or preventive paint maintenance programmes to maintain and restore coatings applied to steel in environments subject to attack from erosion, corrosion and environmental pollution.The basic types of maintenance falling under MRO include:

Preventive maintenance, also known as PM
Corrective maintenance, where equipment is repaired or replaced after wear, malfunction or break down
Predictive maintenance, which uses sensor data to monitor a system, then continuously evaluates it against historical trends to predict failure before it occurs
ReinforcementArchitectural conservation employs MRO to preserve, rehabilitate, restore, or reconstruct historical structures with stone, brick, glass, metal, and wood which match the original constituent materials where possible, or with suitable polymer technologies when not.


=== Preventive maintenance ===

Preventive maintenance (PM) is ""a routine for periodically inspecting"" with the goal of ""noticing small problems and fixing them before major ones develop."" Ideally, ""nothing breaks down.""The main goal behind PM is for the equipment to make it from one planned service to the next planned service without any failures caused by fatigue, neglect, or normal wear (preventable items), which Planned Maintenance and Condition Based Maintenance help to achieve by replacing worn components before they actually fail. Maintenance activities include partial or complete overhauls at specified periods, oil changes, lubrication, minor adjustments, and so on. In addition, workers can record equipment deterioration so they know to replace or repair worn parts before they cause system failure.
The New York Times gave an example of ""machinery that is not lubricated on schedule"" that functions ""until a bearing burns out."" Preventive maintenance contracts are generally a fixed cost, whereas improper maintenance introduces a variable cost: replacement of major equipment.Main objective of PM are:

Enhance capital equipment productive life.
Reduce critical equipment breakdown.
Minimize production loss due to equipment failures.Preventive maintenance or preventative maintenance (PM) has the following meanings:

The care and servicing by personnel for the purpose of maintaining equipment in satisfactory operating condition by providing for systematic inspection, detection, and correction of incipient failures either before they occur or before they develop into major defects.
The work carried out on equipment in order to avoid its breakdown or malfunction. It is a regular and routine action taken on equipment in order to prevent its breakdown.
Maintenance, including tests, measurements, adjustments, parts replacement, and cleaning, performed specifically to prevent faults from occurring.Other terms and abbreviations related to PM are:

scheduled maintenance
planned maintenance, which may include scheduled downtime for equipment replacement
planned preventive maintenance (PPM) is another name for PM
breakdown maintenance: fixing things only when they break. This is also known as ""a reactive maintenance strategy"" and may involve ""consequential damage.""


==== Planned maintenance ====
Planned preventive maintenance (PPM), more commonly referred to as simply planned maintenance (PM) or scheduled maintenance, is any variety of scheduled maintenance to an object or item of equipment. Specifically, planned maintenance is a scheduled service visit carried out by a competent and suitable agent, to ensure that an item of equipment is operating correctly and to therefore avoid any unscheduled breakdown and downtime.The key factor as to when and why this work is being done is timing, and involves a service, resource or facility being unavailable. By contrast, condition-based maintenance is not directly based on equipment age.
Planned maintenance is preplanned, and can be date-based, based on equipment running hours, or on distance travelled.
Parts that have scheduled maintenance at fixed intervals, usually due to wearout or a fixed shelf life, are sometimes known as time-change interval, or TCI items.


==== Predictive maintenance ====

Predictive maintenance techniques are designed to help determine the condition of in-service equipment in order to estimate when maintenance should be performed. This approach promises cost savings over routine or time-based preventive maintenance, because tasks are performed only when warranted. Thus, it is regarded as condition-based maintenance carried out as suggested by estimations of the degradation state of an item.
The main promise of predictive maintenance is to allow convenient scheduling of corrective maintenance, and to prevent unexpected equipment failures. 
Predictive replacement is the replacement of an item that is still functioning properly. Usually it's a tax-benefit based replacement policy whereby expensive equipment or batches of individually inexpensive supply items are removed  and donated on a predicted/fixed shelf life schedule. These items are given to tax-exempt institutions.


==== Condition-based maintenance ====
Condition-based maintenance (CBM), shortly described, is maintenance when need arises. Albeit chronologically much older, It is considered one section or practice inside the broader and newer predictive maintenance field, where new AI technologies and connectivity abilities are put to action and where the acronym CBM is more often used to describe 'condition Based Monitoring' rather than the maintenance itself. CBM maintenance is performed after one or more indicators show that equipment is going to fail or that equipment performance is deteriorating.
This concept is applicable to mission-critical systems that incorporate active redundancy and fault reporting. It is also applicable to non-mission critical systems that lack redundancy and fault reporting.
Condition-based maintenance was introduced to try to maintain the correct equipment at the right time. CBM is based on using real-time data to prioritize and optimize maintenance resources. Observing the state of the system is known as condition monitoring. Such a system will determine the equipment's health, and act only when maintenance is actually necessary. Developments in recent years have allowed extensive instrumentation of equipment, and together with better tools for analyzing condition data, the maintenance personnel of today is more than ever able to decide what is the right time to perform maintenance on some piece of equipment. Ideally, condition-based maintenance will allow the maintenance personnel to do only the right things, minimizing spare parts cost, system downtime and time spent on maintenance.


===== Challenges =====
Despite its usefulness, there are several challenges to the use of CBM. First and most important of all, the initial cost of CBM can be high. It requires improved instrumentation of the equipment. Often the cost of sufficient instruments can be quite large, especially on equipment that is already installed.  Wireless systems have reduced the initial cost.  Therefore, it is important for the installer to decide the importance of the investment before adding CBM to all equipment. A result of this cost is that the first generation of CBM in the oil and gas industry has only focused on vibration in heavy rotating equipment.
Secondly, introducing CBM will invoke a major change in how maintenance is performed, and potentially to the whole maintenance organization in a company. Organizational changes are in general difficult.
Also, the technical side of it is not always as simple. Even if some types of equipment can easily be observed by measuring simple values such as vibration (displacement, velocity or acceleration), temperature or pressure, it is not trivial to turn this measured data into actionable knowledge about the health of the equipment.


===== Value potential =====
As systems get more costly, and instrumentation and information systems tend to become cheaper and more reliable, CBM becomes an important tool for running a plant or factory in an optimal manner. Better operations will lead to lower production cost and lower use of resources. And lower use of resources may be one of the most important differentiators in a future where environmental issues become more important by the day.
A more down to earth scenario where value can be created is by monitoring the health of your car motor. Rather than changing parts at predefined intervals, the car itself can tell you when something needs to be changed based on cheap and simple instrumentation.
It is Department of Defense policy that condition-based maintenance (CBM) be ""implemented to improve maintenance agility and responsiveness, increase operational availability, and reduce life cycle total ownership costs"".


===== Advantages and disadvantages =====
CBM has some advantages over planned maintenance:

Improved system reliability
Decreased maintenance costs
Decreased number of maintenance operations causes a reduction of human error influencesIts disadvantages are:

High installation costs, for minor equipment items often more than the value of the equipment
Unpredictable maintenance periods cause costs to be divided unequally.
Increased number of parts (the CBM installation itself) that need maintenance and checking.Today, due to its costs, CBM is not used for less important parts of machinery despite obvious advantages. However it can be found everywhere where increased reliability and safety is required, and in future will be applied even more widely.


=== Corrective ===

Corrective maintenance is a type of maintenance used for equipment after equipment break down or malfunction is often most expensive – not only can worn equipment damage other parts and cause multiple damage, but consequential repair and replacement costs and loss of revenues due to down time during overhaul can be significant. Rebuilding and resurfacing of equipment and infrastructure damaged by erosion and corrosion as part of corrective or preventive maintenance programmes involves conventional processes such as welding and metal flame spraying, as well as engineered solutions with thermoset polymeric materials.


=== Predictive ===

More recently, advances in sensing and computing technology have given rise to predictive maintenance (PdM). This maintenance strategy uses sensors to monitor key parameters within a machine or system, and uses this data in conjunction with analysed historical trends to continuously evaluate the system health and predict a breakdown before it happens. This strategy allows maintenance to be performed more efficiently, since more up-to-date data is obtained about how close the product is to failure.


== See also ==


== References ==

 This article incorporates public domain material from the General Services Administration document: ""Federal Standard 1037C"". (in support of MIL-STD-188)


== Bibliography ==
Maintenance Planning, Coordination & Scheduling, by Don Nyman & Joel Levitt Maintenance ISBN 978-0831134181


== Further reading ==
Wu, S.; Zuo, M.J. (2010). ""Linear and nonlinear preventive maintenance"" (PDF). IEEE Transactions on Reliability. 59 (1): 242–249. doi:10.1109/TR.2010.2041972.


== Sources ==","pandas(index=239, _1=239, text='the technical meaning of maintenance involves functional checks, servicing, repairing or replacing of necessary devices, equipment, machinery, building infrastructure, and supporting utilities in industrial, business, and residential installations. over time, this has come to include multiple wordings that describe various cost-effective practices to keep equipment operational; these activities occur  either before or after a failure.   == definitions == maintenance functions are often referred to as maintenance, repair and overhaul (mro), and mro is also used for maintenance, repair and operations. over time, the terminology of maintenance and mro has begun to become standardized. the united states department of defense uses the following definitions: any activity—such as tests, measurements, replacements, adjustments, and repairs—intended to retain or restore a functional unit in or to a specified state in which the unit can perform its required functions. all action taken to retain material in a serviceable condition or to restore it to serviceability. it includes inspections, testing, servicing, classification as to serviceability, repair,  rebuilding, and reclamation. all supply and repair action taken to keep a force in condition to carry out its mission. the routine recurring work required to keep a facility (plant, building, structure, ground facility, utility system, or other real property) in such condition that it may be continuously used, at its original or designed capacity and efficiency for its intended purpose.maintenance is strictly connected to the utilization stage of the product or technical system, in which the concept of maintainability must be included. in this scenario, maintainability is considered as the ability of an item, under stated conditions of use, to be retained in or restored to a state in which it can perform its required functions, using prescribed procedures and resources.in some domains like aircraft maintenance, terms maintenance, repair and overhaul also include inspection, rebuilding, alteration and the supply of spare parts, accessories, raw materials, adhesives, sealants, coatings and consumables for aircraft maintenance at the utilization stage. in international civil aviation maintenance means:  the performance of tasks required to ensure the continuing airworthiness of an aircraft, including any one or combination of overhaul, inspection, replacement, defect rectification, and the embodiment of a modification or a repair. this definition covers all activities for which aviation regulations require issuance of a maintenance release document (aircraft certificate of return to service – crs).   == types == the marine and air transportation, offshore structures, industrial plant and facility management industries depend on maintenance, repair and overhaul (mro) including scheduled or preventive paint maintenance programmes to maintain and restore coatings applied to steel in environments subject to attack from erosion, corrosion and environmental pollution.the basic types of maintenance falling under mro include:  preventive maintenance, also known as pm corrective maintenance, where equipment is repaired or replaced after wear, malfunction or break down predictive maintenance, which uses sensor data to monitor a system, then continuously evaluates it against historical trends to predict failure before it occurs reinforcementarchitectural conservation employs mro to preserve, rehabilitate, restore, or reconstruct historical structures with stone, brick, glass, metal, and wood which match the original constituent materials where possible, or with suitable polymer technologies when not. more recently, advances in sensing and computing technology have given rise to predictive maintenance (pdm). this maintenance strategy uses sensors to monitor key parameters within a machine or system, and uses this data in conjunction with analysed historical trends to continuously evaluate the system health and predict a breakdown before it happens. this strategy allows maintenance to be performed more efficiently, since more up-to-date data is obtained about how close the product is to failure.   == see also ==   == references ==  this article incorporates public domain material from the general services administration document: ""federal standard 1037c"". (in support of mil-std-188)   == bibliography == maintenance planning, coordination & scheduling, by don nyman & joel levitt maintenance isbn 978-0831134181   == further reading == wu, s.; zuo, m.j. (2010). ""linear and nonlinear preventive maintenance"" (pdf). ieee transactions on reliability. 59 (1): 242–249. doi:10.1109/tr.2010.2041972.   == sources ==')"
240,"In physics, work is the energy transferred to or from an object via the application of force along a displacement. In its simplest form, it is often represented as the product of force and displacement. A force is said to do positive work if (when applied) it has a component in the direction of the displacement of the point of application. A force does negative work if it has a component opposite to the direction of the displacement at the point of application of the force.
For example, when a ball is held above the ground and then dropped, the work done by the gravitational force on the ball as it falls is equal to the weight of the ball (a force) multiplied by the distance to the ground (a displacement). When the force F is constant and the angle between the force and the displacement s is θ, then the work done is given by: 

  
    
      
        W
        =
        F
        s
        cos
        ⁡
        
          θ
        
      
    
    {\displaystyle W=Fs\cos {\theta }}
  Work is a scalar quantity, so it has only magnitude and no direction. Work transfers energy from one place to another, or one form to another. The SI unit of work is the joule (J), the same unit as for energy.


== Etymology ==
According to Jammer, the term work was introduced in 1826 by the French mathematician Gaspard-Gustave Coriolis as ""weight lifted through a height"", which is based on the use of early steam engines to lift buckets of water out of flooded ore mines. According to Rene Dugas, French engineer and historian, it is to Solomon of Caux ""that we owe the term work in the sense that it is used in mechanics now"".


== Units ==
The SI unit of work is the joule (J), named after the 19th-century English physicist James Prescott Joule, which is defined as the work required to exert a force of one newton through a displacement of one metre.
The dimensionally equivalent newton-metre (N⋅m) is sometimes used as the measuring unit for work, but this can be confused with the measurement unit of torque. Usage of N⋅m is discouraged by the SI authority, since it can lead to confusion as to whether the quantity expressed in newton metres is a torque measurement, or a measurement of work.Non-SI units of work include the newton-metre, erg, the foot-pound, the foot-poundal, the kilowatt hour, the litre-atmosphere, and the horsepower-hour.  Due to work having the same physical dimension as heat, occasionally measurement units typically reserved for heat or energy content, such as therm, BTU and calorie, are utilized as a measuring unit.


== Work and energy ==
The work W done by a constant force of magnitude F on a point that moves a displacement s in a straight line in the direction of the force is the product

  
    
      
        W
        =
        F
        s
      
    
    {\displaystyle W=Fs}
  .For example, if a force of 10 newtons (F = 10 N) acts along a point that travels 2 metres (s = 2 m), then W = Fs = (10 N) (2 m) = 20 J. This is approximately the work done lifting a 1 kg object from ground level to over a person's head against the force of gravity.
The work is doubled either by lifting twice the weight the same distance or by lifting the same weight twice the distance.
Work is closely related to energy. The work-energy principle states that an increase in the kinetic energy of a rigid body is caused by an equal amount of positive work done on the body by the resultant force acting on that body. Conversely, a decrease in kinetic energy is caused by an equal amount of negative work done by the resultant force. Thus, if the net work is positive, then the particle’s kinetic energy increases by the amount of the work. If the net work done is negative, then the particle’s kinetic energy decreases by the amount of the work.From Newton's second law, it can be shown that work on a free (no fields), rigid (no internal degrees of freedom) body, is equal to the change in kinetic energy KE corresponding to the linear velocity and angular velocity of that body,

  
    
      
        W
        =
        Δ
        K
        E
        .
      
    
    {\displaystyle W=\Delta KE.}
  The work of forces generated by a potential function is known as potential energy and the forces are said to be conservative. Therefore, work on an object that is merely displaced in a conservative force field, without change in velocity or rotation, is equal to minus the change of potential energy PE of the object,

  
    
      
        W
        =
        −
        Δ
        P
        E
        .
      
    
    {\displaystyle W=-\Delta PE.}
  These formulas show that work is the energy associated with the action of a force, so work subsequently possesses the physical dimensions, and units, of energy.
The work/energy principles discussed here are identical to electric work/energy principles.


== Constraint forces ==
Constraint forces determine the object's displacement in the system, limiting it within a range. For example, in the case of a slope plus gravity, the object is stuck to the slope and, when attached to a taut string, it cannot move in an outwards direction to make the string any 'tauter'. It eliminates all displacements in that direction, that is, the velocity in the direction of the constraint is limited to 0, so that the constraint forces do not perform work on the system.
For a mechanical system, constraint forces eliminate movement in directions that characterize the constraint. Thus the virtual work done by the forces of constraint is zero, a result which is only true if friction forces are excluded.Fixed, frictionless constraint forces do not perform work on the system, as the angle between the motion and the constraint forces is always 90°. Examples of workless constraints are: rigid interconnections between particles, sliding motion on a frictionless surface, and rolling contact without slipping.For example, in a pulley system like the Atwood machine, the internal forces on the rope and at the supporting pulley do no work on the system. Therefore work need only be computed for the gravitational forces acting on the bodies. Another example is the centripetal force exerted inwards by a string on a ball in uniform circular motion sideways constrains the ball to circular motion restricting its movement away from the centre of the circle.  This force does zero work because it is perpendicular to the velocity of the ball.
The magnetic force on a charged particle is F = qv × B, where q is the charge, v is the velocity of the particle, and B is the magnetic field. The result of a cross product is always perpendicular to both of the original vectors, so F ⊥ v. The dot product of two perpendicular vectors is always zero, so the work W = F ⋅ v = 0, and the magnetic force does not do work. It can change the direction of motion but never change the speed.


== Mathematical calculation ==
For moving objects, the quantity of work/time (power) is integrated along the trajectory of the point of application of the force. Thus, at any instant, the rate of the work done by a force (measured in joules/second, or watts) is the scalar product of the force (a vector), and the velocity vector of the point of application. This scalar product of force and velocity is known as instantaneous power. Just as velocities may be integrated over time to obtain a total distance, by the fundamental theorem of calculus, the total work along a path is similarly the time-integral of instantaneous power applied along the trajectory of the point of application.Work is the result of a force on a point that follows a curve X, with a velocity v, at each instant.  The small amount of work δW that occurs over an instant of time dt is calculated as

  
    
      
        δ
        W
        =
        
          F
        
        ⋅
        d
        
          s
        
        =
        
          F
        
        ⋅
        
          v
        
        d
        t
      
    
    {\displaystyle \delta W=\mathbf {F} \cdot d\mathbf {s} =\mathbf {F} \cdot \mathbf {v} dt}
  where the F ⋅ v is the power over the instant dt.  The sum of these small amounts of work over the trajectory of the point yields the work,

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          v
        
        d
        t
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          
            
              
                d
                
                  s
                
              
              
                d
                t
              
            
          
        
        d
        t
        =
        
          ∫
          
            C
          
        
        
          F
        
        ⋅
        d
        
          s
        
        ,
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot \mathbf {v} dt=\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot {\tfrac {d\mathbf {s} }{dt}}dt=\int _{C}\mathbf {F} \cdot d\mathbf {s} ,}
  where C is the trajectory from x(t1) to x(t2).  This integral is computed along the trajectory of the particle, and is therefore said to be path dependent.
If the force is always directed along this line, and the magnitude of the force is F, then this integral simplifies to

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        F
        
        d
        s
      
    
    {\displaystyle W=\int _{C}F\,ds}
  where s is displacement along the line. If F is constant, in addition to being directed along the line, then the integral simplifies further to

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        F
        
        d
        s
        =
        F
        
          ∫
          
            C
          
        
        d
        s
        =
        F
        s
      
    
    {\displaystyle W=\int _{C}F\,ds=F\int _{C}ds=Fs}
  where s is the displacement of the point along the line.
This calculation can be generalized for a constant force that is not directed along the line, followed by the particle.  In this case the dot product F ⋅ ds = F cos θ ds, where θ is the angle between the force vector and the direction of movement, that is

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        
          F
        
        ⋅
        d
        
          s
        
        =
        F
        s
        cos
        ⁡
        θ
        .
      
    
    {\displaystyle W=\int _{C}\mathbf {F} \cdot d\mathbf {s} =Fs\cos \theta .}
  When a force component is perpendicular to the displacement of the object (such as when a body moves in a circular path under a central force), no work is done, since the cosine of 90° is zero. Thus, no work can be performed by gravity on a planet with a circular orbit (this is ideal, as all orbits are slightly elliptical). Also, no work is done on a body moving circularly at a constant speed while constrained by mechanical force, such as moving at constant speed in a frictionless ideal centrifuge.


=== Work done by a variable force ===
Calculating the work as ""force times straight path segment"" would only apply in the most simple of circumstances, as noted above. If force is changing, or if the body is moving along a curved path, possibly rotating and not necessarily rigid, then only the path of the application point of the force is relevant for the work done, and only the component of the force parallel to the application point velocity is doing work (positive work when in the same direction, and negative when in the opposite direction of the velocity). This component of force can be described by the scalar quantity called scalar tangential component (F cos(θ), where θ is the angle between the force and the velocity). And then the most general definition of work can be formulated as follows:

Work of a force is the line integral of its scalar tangential component along the path of its application point.
If the force varies (e.g. compressing a spring) we need to use calculus to find the work done.  If the force is given by F(x) (a function of x) then the work done by the force along the x-axis from a to b is:
  
    
      
        W
        =
        
          ∫
          
            a
          
          
            b
          
        
        
          F
          (
          s
          )
        
        ⋅
        d
        
          s
        
      
    
    {\displaystyle W=\int _{a}^{b}\mathbf {F(s)} \cdot d\mathbf {s} }
  


=== Torque and rotation ===
A force couple results from equal and opposite forces, acting on two different points of a rigid body.  The sum (resultant) of these forces may cancel, but their effect on the body is the couple or torque T.  The work of the torque is calculated as

  
    
      
        d
        W
        =
        
          T
        
        ⋅
        
          
            
              ω
              →
            
          
        
        d
        t
        ,
      
    
    {\displaystyle dW=\mathbf {T} \cdot {\vec {\omega }}dt,}
  where the T ⋅ ω is the power over the instant δt.  The sum of these small amounts of work over the trajectory of the rigid body yields the work,

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          T
        
        ⋅
        
          
            
              ω
              →
            
          
        
        d
        t
        .
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {T} \cdot {\vec {\omega }}dt.}
  This integral is computed along the trajectory of the rigid body with an angular velocity ω that varies with time, and is therefore said to be path dependent.
If the angular velocity vector maintains a constant direction, then it takes the form,

  
    
      
        
          
            
              ω
              →
            
          
        
        =
        
          
            
              ϕ
              ˙
            
          
        
        
          S
        
        ,
      
    
    {\displaystyle {\vec {\omega }}={\dot {\phi }}\mathbf {S} ,}
  where φ is the angle of rotation about the constant unit vector S.  In this case, the work of the torque becomes,

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          T
        
        ⋅
        
          
            
              ω
              →
            
          
        
        d
        t
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          T
        
        ⋅
        
          S
        
        
          
            
              d
              ϕ
            
            
              d
              t
            
          
        
        d
        t
        =
        
          ∫
          
            C
          
        
        
          T
        
        ⋅
        
          S
        
        d
        ϕ
        ,
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {T} \cdot {\vec {\omega }}dt=\int _{t_{1}}^{t_{2}}\mathbf {T} \cdot \mathbf {S} {\frac {d\phi }{dt}}dt=\int _{C}\mathbf {T} \cdot \mathbf {S} d\phi ,}
  where C is the trajectory from φ(t1) to φ(t2).  This integral depends on the rotational trajectory φ(t), and is therefore path-dependent.
If the torque T is aligned with the angular velocity vector so that,

  
    
      
        
          T
        
        =
        τ
        
          S
        
        ,
      
    
    {\displaystyle \mathbf {T} =\tau \mathbf {S} ,}
  and both the torque and angular velocity are constant, then the work takes the form,

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        τ
        
          
            
              ϕ
              ˙
            
          
        
        d
        t
        =
        τ
        (
        
          ϕ
          
            2
          
        
        −
        
          ϕ
          
            1
          
        
        )
        .
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\tau {\dot {\phi }}dt=\tau (\phi _{2}-\phi _{1}).}
  
This result can be understood more simply by considering the torque as arising from a force of constant magnitude F, being applied perpendicularly to a lever arm at a distance r, as shown in the figure.  This force will act through the distance along the circular arc s = rφ, so the work done is

  
    
      
        W
        =
        F
        s
        =
        F
        r
        ϕ
        .
      
    
    {\displaystyle W=Fs=Fr\phi .}
  Introduce the torque τ = Fr, to obtain

  
    
      
        W
        =
        F
        r
        ϕ
        =
        τ
        ϕ
        ,
      
    
    {\displaystyle W=Fr\phi =\tau \phi ,}
  as presented above.
Notice that only the component of torque in the direction of the angular velocity vector contributes to the work.


== Work and potential energy ==
The scalar product of a force F and the velocity v of its point of application defines the power input to a system at an instant of time.  Integration of this power over the trajectory of the point of application, C = x(t), defines the work input to the system by the force.


=== Path dependence ===
Therefore, the work done by a force F on an object that travels along a curve C is given by the line integral:

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        
          F
        
        ⋅
        d
        
          x
        
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          v
        
        d
        t
        ,
      
    
    {\displaystyle W=\int _{C}\mathbf {F} \cdot d\mathbf {x} =\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot \mathbf {v} dt,}
  where dx(t) defines the trajectory C and v is the velocity along this trajectory.  In general this integral requires the path along which the velocity is defined, so the evaluation of work is said to be path dependent.
The time derivative of the integral for work yields the instantaneous power,

  
    
      
        
          
            
              d
              W
            
            
              d
              t
            
          
        
        =
        P
        (
        t
        )
        =
        
          F
        
        ⋅
        
          v
        
        .
      
    
    {\displaystyle {\frac {dW}{dt}}=P(t)=\mathbf {F} \cdot \mathbf {v} .}
  


=== Path independence ===
If the work for an applied force is independent of the path, then the work done by the force, by the gradient theorem, defines a potential function which is evaluated at the start and end of the trajectory of the point of application. This means that there is a potential function U(x), that can be evaluated at the two points x(t1) and x(t2) to obtain the work over any trajectory between these two points. It is tradition to define this function with a negative sign so that positive work is a reduction in the potential, that is

  
    
      
        W
        =
        
          ∫
          
            C
          
        
        
          F
        
        ⋅
        
          d
        
        
          x
        
        =
        
          ∫
          
            
              x
            
            (
            
              t
              
                1
              
            
            )
          
          
            
              x
            
            (
            
              t
              
                2
              
            
            )
          
        
        
          F
        
        ⋅
        
          d
        
        
          x
        
        =
        U
        (
        
          x
        
        (
        
          t
          
            1
          
        
        )
        )
        −
        U
        (
        
          x
        
        (
        
          t
          
            2
          
        
        )
        )
        .
      
    
    {\displaystyle W=\int _{C}\mathbf {F} \cdot \mathrm {d} \mathbf {x} =\int _{\mathbf {x} (t_{1})}^{\mathbf {x} (t_{2})}\mathbf {F} \cdot \mathrm {d} \mathbf {x} =U(\mathbf {x} (t_{1}))-U(\mathbf {x} (t_{2})).}
  The function U(x) is called the potential energy associated with the applied force.  The force derived from such a potential function is said to be conservative. Examples of forces that have potential energies are gravity and spring forces.
In this case, the gradient of work yields

  
    
      
        ∇
        W
        =
        −
        ∇
        U
        =
        −
        
          (
          
            
              
                
                  ∂
                  U
                
                
                  ∂
                  x
                
              
            
            ,
            
              
                
                  ∂
                  U
                
                
                  ∂
                  y
                
              
            
            ,
            
              
                
                  ∂
                  U
                
                
                  ∂
                  z
                
              
            
          
          )
        
        =
        
          F
        
        ,
      
    
    {\displaystyle \nabla W=-\nabla U=-\left({\frac {\partial U}{\partial x}},{\frac {\partial U}{\partial y}},{\frac {\partial U}{\partial z}}\right)=\mathbf {F} ,}
  and the force F is said to be ""derivable from a potential.""Because the potential U defines a force F at every point x in space, the set of forces is called  a force field.  The power applied to a body by a force field is obtained from the gradient of the work, or potential, in the direction of the velocity V of the body, that is

  
    
      
        P
        (
        t
        )
        =
        −
        ∇
        U
        ⋅
        
          v
        
        =
        
          F
        
        ⋅
        
          v
        
        .
      
    
    {\displaystyle P(t)=-\nabla U\cdot \mathbf {v} =\mathbf {F} \cdot \mathbf {v} .}
  


=== Work by gravity ===

In the absence of other forces, gravity results in a constant downward acceleration of every freely moving object. Near Earth's surface the acceleration due to gravity is g = 9.8 m⋅s−2 and the gravitational force on an object of mass m is Fg = mg. It is convenient to imagine this gravitational force concentrated at the center of mass of the object.
If an object is displaced upwards or downwards a vertical distance y2 − y1, the work W done on the object by its weight mg is:

  
    
      
        W
        =
        
          F
          
            g
          
        
        (
        
          y
          
            2
          
        
        −
        
          y
          
            1
          
        
        )
        =
        
          F
          
            g
          
        
        Δ
        y
        =
        −
        m
        g
        Δ
        y
      
    
    {\displaystyle W=F_{g}(y_{2}-y_{1})=F_{g}\Delta y=-mg\Delta y}
  where Fg is weight (pounds in imperial units, and newtons in SI units), and Δy is the change in height y. Notice that the work done by gravity depends only on the vertical movement of the object. The presence of friction does not affect the work done on the object by its weight.


=== Work by gravity in space ===
The force of gravity exerted by a mass M on another mass m is given by

  
    
      
        
          F
        
        =
        −
        
          
            
              G
              M
              m
            
            
              r
              
                3
              
            
          
        
        
          r
        
        ,
      
    
    {\displaystyle \mathbf {F} =-{\frac {GMm}{r^{3}}}\mathbf {r} ,}
  where r is the position vector from M to m.
Let the mass m move at the velocity v; then the work of gravity on this mass as it moves from position r(t1) to  r(t2) is given by

  
    
      
        W
        =
        −
        
          ∫
          
            
              r
            
            (
            
              t
              
                1
              
            
            )
          
          
            
              r
            
            (
            
              t
              
                2
              
            
            )
          
        
        
          
            
              G
              M
              m
            
            
              r
              
                3
              
            
          
        
        
          r
        
        ⋅
        d
        
          r
        
        =
        −
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              G
              M
              m
            
            
              r
              
                3
              
            
          
        
        
          r
        
        ⋅
        
          v
        
        d
        t
        .
      
    
    {\displaystyle W=-\int _{\mathbf {r} (t_{1})}^{\mathbf {r} (t_{2})}{\frac {GMm}{r^{3}}}\mathbf {r} \cdot d\mathbf {r} =-\int _{t_{1}}^{t_{2}}{\frac {GMm}{r^{3}}}\mathbf {r} \cdot \mathbf {v} dt.}
  Notice that the position and velocity of the mass m are given by

  
    
      
        
          r
        
        =
        r
        
          
            e
          
          
            r
          
        
        ,
        
        
          v
        
        =
        
          
            
              d
              
                r
              
            
            
              d
              t
            
          
        
        =
        
          
            
              r
              ˙
            
          
        
        
          
            e
          
          
            r
          
        
        +
        r
        
          
            
              θ
              ˙
            
          
        
        
          
            e
          
          
            t
          
        
        ,
      
    
    {\displaystyle \mathbf {r} =r\mathbf {e} _{r},\qquad \mathbf {v} ={\frac {d\mathbf {r} }{dt}}={\dot {r}}\mathbf {e} _{r}+r{\dot {\theta }}\mathbf {e} _{t},}
  where er and et are the radial and tangential unit vectors directed relative to the vector from M to m, and we use the fact that 
  
    
      
        d
        
          
            e
          
          
            r
          
        
        
          /
        
        d
        t
        =
        
          
            
              θ
              ˙
            
          
        
        
          
            e
          
          
            t
          
        
        .
      
    
    {\displaystyle d\mathbf {e} _{r}/dt={\dot {\theta }}\mathbf {e} _{t}.}
    Use this to simplify the formula for work of gravity to,

  
    
      
        W
        =
        −
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              G
              m
              M
            
            
              r
              
                3
              
            
          
        
        (
        r
        
          
            e
          
          
            r
          
        
        )
        ⋅
        (
        
          
            
              r
              ˙
            
          
        
        
          
            e
          
          
            r
          
        
        +
        r
        
          
            
              θ
              ˙
            
          
        
        
          
            e
          
          
            t
          
        
        )
        d
        t
        =
        −
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              G
              m
              M
            
            
              r
              
                3
              
            
          
        
        r
        
          
            
              r
              ˙
            
          
        
        d
        t
        =
        
          
            
              G
              M
              m
            
            
              r
              (
              
                t
                
                  2
                
              
              )
            
          
        
        −
        
          
            
              G
              M
              m
            
            
              r
              (
              
                t
                
                  1
                
              
              )
            
          
        
        .
      
    
    {\displaystyle W=-\int _{t_{1}}^{t_{2}}{\frac {GmM}{r^{3}}}(r\mathbf {e} _{r})\cdot ({\dot {r}}\mathbf {e} _{r}+r{\dot {\theta }}\mathbf {e} _{t})dt=-\int _{t_{1}}^{t_{2}}{\frac {GmM}{r^{3}}}r{\dot {r}}dt={\frac {GMm}{r(t_{2})}}-{\frac {GMm}{r(t_{1})}}.}
  This calculation uses the fact that

  
    
      
        
          
            d
            
              d
              t
            
          
        
        
          r
          
            −
            1
          
        
        =
        −
        
          r
          
            −
            2
          
        
        
          
            
              r
              ˙
            
          
        
        =
        −
        
          
            
              
                r
                ˙
              
            
            
              r
              
                2
              
            
          
        
        .
      
    
    {\displaystyle {\frac {d}{dt}}r^{-1}=-r^{-2}{\dot {r}}=-{\frac {\dot {r}}{r^{2}}}.}
  The function

  
    
      
        U
        =
        −
        
          
            
              G
              M
              m
            
            r
          
        
        ,
      
    
    {\displaystyle U=-{\frac {GMm}{r}},}
  is the gravitational potential function, also known as gravitational potential energy.  The negative sign follows the convention that work is gained from a loss of potential energy.


=== Work by a spring ===

Consider a spring that exerts a horizontal force F = (−kx, 0, 0) that is proportional to its deflection in the x direction independent of how a body moves.  The work of this spring on a body moving along the space with the curve X(t) = (x(t), y(t), z(t)), is calculated using its velocity, v = (vx, vy, vz), to obtain

  
    
      
        W
        =
        
          ∫
          
            0
          
          
            t
          
        
        
          F
        
        ⋅
        
          v
        
        d
        t
        =
        −
        
          ∫
          
            0
          
          
            t
          
        
        k
        x
        
          v
          
            x
          
        
        d
        t
        =
        −
        
          
            1
            2
          
        
        k
        
          x
          
            2
          
        
        .
      
    
    {\displaystyle W=\int _{0}^{t}\mathbf {F} \cdot \mathbf {v} dt=-\int _{0}^{t}kxv_{x}dt=-{\frac {1}{2}}kx^{2}.}
  For convenience, consider contact with the spring occurs at t = 0, then the integral of the product of the distance x and the x-velocity, xvx, is (1/2)x2.
The velocity is not a factor here. The work is the product of the distance times the spring force, which is also dependent on distance; hence the x2 result.


=== Work by a gas ===

  
    
      
        W
        =
        
          ∫
          
            a
          
          
            b
          
        
        
          P
        
        d
        V
      
    
    {\displaystyle W=\int _{a}^{b}{P}dV}
  Where P is pressure, V is volume, and a and b are initial and final volumes.


== Work–energy principle ==
The principle of work and kinetic energy (also known as the work–energy principle) states that the work done by all forces acting on a particle (the work of the resultant force) equals the change in the kinetic energy of the particle.  That is, the work W done by the resultant force on a particle equals the change in the particle's kinetic energy 
  
    
      
        
          E
          
            k
          
        
      
    
    {\displaystyle E_{k}}
  ,

  
    
      
        W
        =
        Δ
        
          E
          
            k
          
        
        =
        
          
            
              1
              2
            
          
        
        m
        
          v
          
            2
          
          
            2
          
        
        −
        
          
            
              1
              2
            
          
        
        m
        
          v
          
            1
          
          
            2
          
        
      
    
    {\displaystyle W=\Delta E_{k}={\tfrac {1}{2}}mv_{2}^{2}-{\tfrac {1}{2}}mv_{1}^{2}}
  ,where 
  
    
      
        
          v
          
            1
          
        
      
    
    {\displaystyle v_{1}}
   and 
  
    
      
        
          v
          
            2
          
        
      
    
    {\displaystyle v_{2}}
   are the speeds of the particle before and after the work is done, and m is its mass.
The derivation of the work–energy principle begins with Newton’s second law of motion and the resultant force on a particle.  Computation of the scalar product of the forces with the velocity of the particle evaluates the instantaneous power added to the system.Constraints define the direction of movement of the particle by ensuring there is no component of velocity in the direction of the constraint force. This also means the constraint forces do not add to the instantaneous power.   The time integral of this scalar equation yields work from the instantaneous power, and kinetic energy from the scalar product of velocity and acceleration.  The fact that the work–energy principle eliminates the constraint forces underlies Lagrangian mechanics.This section focuses on the work–energy principle as it applies to particle dynamics.  In more general systems work can change the potential energy of a mechanical device, the thermal energy in a thermal system, or the electrical energy in an electrical device.  Work transfers energy from one place to another or one form to another.


=== Derivation for a particle moving along a straight line ===
In the case the resultant force F is constant in both magnitude and direction, and parallel to the velocity of the particle, the particle is moving with constant acceleration a along a straight line. The relation between the net force and the acceleration is given by the equation F = ma (Newton's second law), and the particle displacement s can be expressed by the equation

  
    
      
        s
        =
        
          
            
              
                v
                
                  2
                
                
                  2
                
              
              −
              
                v
                
                  1
                
                
                  2
                
              
            
            
              2
              a
            
          
        
      
    
    {\displaystyle s={\frac {v_{2}^{2}-v_{1}^{2}}{2a}}}
  which follows from 
  
    
      
        
          v
          
            2
          
          
            2
          
        
        =
        
          v
          
            1
          
          
            2
          
        
        +
        2
        a
        s
      
    
    {\displaystyle v_{2}^{2}=v_{1}^{2}+2as}
   (see Equations of motion).
The work of the net force is calculated as the product of its magnitude and the particle displacement. Substituting the above equations, one obtains:

  
    
      
        W
        =
        F
        s
        =
        m
        a
        s
        =
        m
        a
        
          (
          
            
              
                
                  v
                  
                    2
                  
                  
                    2
                  
                
                −
                
                  v
                  
                    1
                  
                  
                    2
                  
                
              
              
                2
                a
              
            
          
          )
        
        =
        
          
            
              m
              
                v
                
                  2
                
                
                  2
                
              
            
            2
          
        
        −
        
          
            
              m
              
                v
                
                  1
                
                
                  2
                
              
            
            2
          
        
        =
        Δ
        
          
            E
            
              
                k
              
            
          
        
      
    
    {\displaystyle W=Fs=mas=ma\left({\frac {v_{2}^{2}-v_{1}^{2}}{2a}}\right)={\frac {mv_{2}^{2}}{2}}-{\frac {mv_{1}^{2}}{2}}=\Delta {E_{\mathrm {k} }}}
  Other derivation:

  
    
      
        W
        =
        F
        s
        =
        m
        a
        s
        =
        m
        
          (
          
            
              
                
                  v
                  
                    2
                  
                  
                    2
                  
                
                −
                
                  v
                  
                    1
                  
                  
                    2
                  
                
              
              
                2
                s
              
            
          
          )
        
        s
        =
        
          
            1
            2
          
        
        m
        
          v
          
            2
          
          
            2
          
        
        −
        
          
            1
            2
          
        
        m
        
          v
          
            1
          
          
            2
          
        
        =
        Δ
        
          
            E
            
              
                k
              
            
          
        
      
    
    {\displaystyle W=Fs=mas=m\left({\frac {v_{2}^{2}-v_{1}^{2}}{2s}}\right)s={\frac {1}{2}}mv_{2}^{2}-{\frac {1}{2}}mv_{1}^{2}=\Delta {E_{\mathrm {k} }}}
  In the general case of rectilinear motion, when the net force F is not constant in magnitude, but is constant in direction, and parallel to the velocity of the particle, the work must be integrated along the path of the particle:

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          v
        
        d
        t
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        F
        
        v
        d
        t
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        m
        a
        
        v
        d
        t
        =
        m
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        v
        
        
          
            
              d
              v
            
            
              d
              t
            
          
        
        
        d
        t
        =
        m
        
          ∫
          
            
              v
              
                1
              
            
          
          
            
              v
              
                2
              
            
          
        
        v
        
        d
        v
        =
        
          
            
              1
              2
            
          
        
        m
        (
        
          v
          
            2
          
          
            2
          
        
        −
        
          v
          
            1
          
          
            2
          
        
        )
        .
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot \mathbf {v} dt=\int _{t_{1}}^{t_{2}}F\,vdt=\int _{t_{1}}^{t_{2}}ma\,vdt=m\int _{t_{1}}^{t_{2}}v\,{dv \over dt}\,dt=m\int _{v_{1}}^{v_{2}}v\,dv={\tfrac {1}{2}}m(v_{2}^{2}-v_{1}^{2}).}
  


=== General derivation of the work–energy theorem for a particle ===
For any net force acting on a particle moving along any curvilinear path, it can be demonstrated that its work equals the change in the kinetic energy of the particle by a simple derivation analogous to the equation above. Some authors call this result work–energy principle, but it is more widely known as the work–energy theorem:

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          v
        
        d
        t
        =
        m
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          a
        
        ⋅
        
          v
        
        d
        t
        =
        
          
            m
            2
          
        
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              d
              
                v
                
                  2
                
              
            
            
              d
              t
            
          
        
        
        d
        t
        =
        
          
            m
            2
          
        
        
          ∫
          
            
              v
              
                1
              
              
                2
              
            
          
          
            
              v
              
                2
              
              
                2
              
            
          
        
        d
        
          v
          
            2
          
        
        =
        
          
            
              m
              
                v
                
                  2
                
                
                  2
                
              
            
            2
          
        
        −
        
          
            
              m
              
                v
                
                  1
                
                
                  2
                
              
            
            2
          
        
        =
        Δ
        
          
            E
            
              k
            
          
        
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot \mathbf {v} dt=m\int _{t_{1}}^{t_{2}}\mathbf {a} \cdot \mathbf {v} dt={\frac {m}{2}}\int _{t_{1}}^{t_{2}}{\frac {dv^{2}}{dt}}\,dt={\frac {m}{2}}\int _{v_{1}^{2}}^{v_{2}^{2}}dv^{2}={\frac {mv_{2}^{2}}{2}}-{\frac {mv_{1}^{2}}{2}}=\Delta {E_{k}}}
  The identity 
  
    
      
        
          
            a
          
          ⋅
          
            v
          
          =
          
            
              1
              2
            
          
          
            
              
                d
                
                  v
                  
                    2
                  
                
              
              
                d
                t
              
            
          
        
      
    
    {\displaystyle \textstyle \mathbf {a} \cdot \mathbf {v} ={\frac {1}{2}}{\frac {dv^{2}}{dt}}}
   requires some algebra.
From the identity 
  
    
      
        
          
            v
            
              2
            
          
          =
          
            v
          
          ⋅
          
            v
          
        
      
    
    {\displaystyle \textstyle v^{2}=\mathbf {v} \cdot \mathbf {v} }
   and definition 
  
    
      
        
          
            a
          
          =
          
            
              
                d
                
                  v
                
              
              
                d
                t
              
            
          
        
      
    
    {\displaystyle \textstyle \mathbf {a} ={\frac {d\mathbf {v} }{dt}}}
  
it follows

  
    
      
        
          
            
              d
              
                v
                
                  2
                
              
            
            
              d
              t
            
          
        
        =
        
          
            
              d
              (
              
                v
              
              ⋅
              
                v
              
              )
            
            
              d
              t
            
          
        
        =
        
          
            
              d
              
                v
              
            
            
              d
              t
            
          
        
        ⋅
        
          v
        
        +
        
          v
        
        ⋅
        
          
            
              d
              
                v
              
            
            
              d
              t
            
          
        
        =
        2
        
          
            
              d
              
                v
              
            
            
              d
              t
            
          
        
        ⋅
        
          v
        
        =
        2
        
          a
        
        ⋅
        
          v
        
      
    
    {\displaystyle {\frac {dv^{2}}{dt}}={\frac {d(\mathbf {v} \cdot \mathbf {v} )}{dt}}={\frac {d\mathbf {v} }{dt}}\cdot \mathbf {v} +\mathbf {v} \cdot {\frac {d\mathbf {v} }{dt}}=2{\frac {d\mathbf {v} }{dt}}\cdot \mathbf {v} =2\mathbf {a} \cdot \mathbf {v} }
  .The remaining part of the above derivation is just simple calculus, same as in the preceding rectilinear case.


=== Derivation for a particle in constrained movement ===
In particle dynamics, a formula equating work applied to a system to its change in kinetic energy is obtained as a first integral of Newton's second law of motion.  It is useful to notice that the resultant force used in Newton's laws can be separated into forces that are applied to the particle and forces imposed by constraints on the movement of the particle.  Remarkably, the work of a constraint force is zero, therefore only the work of the applied forces need be considered in the work–energy principle.
To see this, consider a particle P that follows the trajectory X(t) with a force F acting on it.  Isolate the particle from its environment to expose constraint forces R, then Newton's Law takes the form

  
    
      
        
          F
        
        +
        
          R
        
        =
        m
        
          
            
              
                X
              
              ¨
            
          
        
        ,
      
    
    {\displaystyle \mathbf {F} +\mathbf {R} =m{\ddot {\mathbf {X} }},}
  where m is the mass of the particle.


==== Vector formulation ====
Note that n dots above a vector indicates its nth time derivative.
The scalar product of each side of Newton's law with the velocity vector yields

  
    
      
        
          F
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        =
        m
        
          
            
              
                X
              
              ¨
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        ,
      
    
    {\displaystyle \mathbf {F} \cdot {\dot {\mathbf {X} }}=m{\ddot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }},}
  because the constraint forces are perpendicular to the particle velocity.  Integrate this equation along its trajectory from the point X(t1) to the point X(t2)  to obtain

  
    
      
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        d
        t
        =
        m
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              
                X
              
              ¨
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        d
        t
        .
      
    
    {\displaystyle \int _{t_{1}}^{t_{2}}\mathbf {F} \cdot {\dot {\mathbf {X} }}dt=m\int _{t_{1}}^{t_{2}}{\ddot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}dt.}
  The left side of this equation is the work of the applied force as it acts on the particle along the trajectory from time t1 to time t2.  This can also be written as

  
    
      
        W
        =
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        d
        t
        =
        
          ∫
          
            
              X
            
            (
            
              t
              
                1
              
            
            )
          
          
            
              X
            
            (
            
              t
              
                2
              
            
            )
          
        
        
          F
        
        ⋅
        d
        
          X
        
        .
      
    
    {\displaystyle W=\int _{t_{1}}^{t_{2}}\mathbf {F} \cdot {\dot {\mathbf {X} }}dt=\int _{\mathbf {X} (t_{1})}^{\mathbf {X} (t_{2})}\mathbf {F} \cdot d\mathbf {X} .}
  This integral is computed along the trajectory X(t) of the particle and is therefore path dependent.
The right side of the first integral of Newton's equations can be simplified using the following identity

  
    
      
        
          
            1
            2
          
        
        
          
            d
            
              d
              t
            
          
        
        (
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        )
        =
        
          
            
              
                X
              
              ¨
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        ,
      
    
    {\displaystyle {\frac {1}{2}}{\frac {d}{dt}}({\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }})={\ddot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }},}
  (see product rule for derivation). Now it is integrated explicitly to obtain the change in kinetic energy,

  
    
      
        Δ
        K
        =
        m
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              
                X
              
              ¨
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        d
        t
        =
        
          
            m
            2
          
        
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            d
            
              d
              t
            
          
        
        (
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        )
        d
        t
        =
        
          
            m
            2
          
        
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        (
        
          t
          
            2
          
        
        )
        −
        
          
            m
            2
          
        
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        (
        
          t
          
            1
          
        
        )
        =
        
          
            1
            2
          
        
        m
        Δ
        
          
            v
            
              2
            
          
        
        ,
      
    
    {\displaystyle \Delta K=m\int _{t_{1}}^{t_{2}}{\ddot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}dt={\frac {m}{2}}\int _{t_{1}}^{t_{2}}{\frac {d}{dt}}({\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }})dt={\frac {m}{2}}{\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}(t_{2})-{\frac {m}{2}}{\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}(t_{1})={\frac {1}{2}}m\Delta \mathbf {v^{2}} ,}
  where the kinetic energy of the particle is defined by the scalar quantity,

  
    
      
        K
        =
        
          
            m
            2
          
        
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        =
        
          
            1
            2
          
        
        m
        
          
            
              v
              
                2
              
            
          
        
      
    
    {\displaystyle K={\frac {m}{2}}{\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}={\frac {1}{2}}m{\mathbf {v^{2}} }}
  


==== Tangential and normal components ====
It is useful to resolve the velocity and acceleration vectors into tangential and normal components along the trajectory X(t), such that

  
    
      
        
          
            
              
                X
              
              ˙
            
          
        
        =
        v
        
          T
        
        
        
          
            and
          
        
        
        
          
            
              
                X
              
              ¨
            
          
        
        =
        
          
            
              v
              ˙
            
          
        
        
          T
        
        +
        
          v
          
            2
          
        
        κ
        
          N
        
        ,
      
    
    {\displaystyle {\dot {\mathbf {X} }}=v\mathbf {T} \quad {\mbox{and}}\quad {\ddot {\mathbf {X} }}={\dot {v}}\mathbf {T} +v^{2}\kappa \mathbf {N} ,}
  where

  
    
      
        v
        =
        
          |
        
        
          
            
              
                X
              
              ˙
            
          
        
        
          |
        
        =
        
          
            
              
                
                  
                    X
                  
                  ˙
                
              
            
            ⋅
            
              
                
                  
                    X
                  
                  ˙
                
              
            
          
        
        .
      
    
    {\displaystyle v=|{\dot {\mathbf {X} }}|={\sqrt {{\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}}}.}
  Then, the scalar product of velocity with acceleration in Newton's second law takes the form

  
    
      
        Δ
        K
        =
        m
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            
              v
              ˙
            
          
        
        v
        d
        t
        =
        
          
            m
            2
          
        
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          
            d
            
              d
              t
            
          
        
        
          v
          
            2
          
        
        d
        t
        =
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            2
          
        
        )
        −
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            1
          
        
        )
        ,
      
    
    {\displaystyle \Delta K=m\int _{t_{1}}^{t_{2}}{\dot {v}}vdt={\frac {m}{2}}\int _{t_{1}}^{t_{2}}{\frac {d}{dt}}v^{2}dt={\frac {m}{2}}v^{2}(t_{2})-{\frac {m}{2}}v^{2}(t_{1}),}
  where the kinetic energy of the particle is defined by the scalar quantity,

  
    
      
        K
        =
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        =
        
          
            m
            2
          
        
        
          
            
              
                X
              
              ˙
            
          
        
        ⋅
        
          
            
              
                X
              
              ˙
            
          
        
        .
      
    
    {\displaystyle K={\frac {m}{2}}v^{2}={\frac {m}{2}}{\dot {\mathbf {X} }}\cdot {\dot {\mathbf {X} }}.}
  The result is the work–energy principle for particle dynamics,

  
    
      
        W
        =
        Δ
        K
        .
        
      
    
    {\displaystyle W=\Delta K.\!}
  This derivation can be generalized to arbitrary rigid body systems.


=== Moving in a straight line (skid to a stop) ===
Consider the case of a vehicle moving along a straight horizontal trajectory under the action of a driving force and gravity that sum to F.  The constraint forces between the vehicle and the road define R, and we have

  
    
      
        
          F
        
        +
        
          R
        
        =
        m
        
          
            
              
                X
              
              ¨
            
          
        
        .
      
    
    {\displaystyle \mathbf {F} +\mathbf {R} =m{\ddot {\mathbf {X} }}.}
  For convenience let the trajectory be along the X-axis, so X = (d, 0) and the velocity is V = (v, 0), then R ⋅ V = 0, and F ⋅ V = Fxv, where Fx is the component of F along the X-axis, so

  
    
      
        
          F
          
            x
          
        
        v
        =
        m
        
          
            
              v
              ˙
            
          
        
        v
        .
      
    
    {\displaystyle F_{x}v=m{\dot {v}}v.}
  Integration of both sides yields

  
    
      
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        
          F
          
            x
          
        
        v
        d
        t
        =
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            2
          
        
        )
        −
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            1
          
        
        )
        .
      
    
    {\displaystyle \int _{t_{1}}^{t_{2}}F_{x}vdt={\frac {m}{2}}v^{2}(t_{2})-{\frac {m}{2}}v^{2}(t_{1}).}
  If Fx is constant along the trajectory, then the integral of velocity is distance, so

  
    
      
        
          F
          
            x
          
        
        (
        d
        (
        
          t
          
            2
          
        
        )
        −
        d
        (
        
          t
          
            1
          
        
        )
        )
        =
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            2
          
        
        )
        −
        
          
            m
            2
          
        
        
          v
          
            2
          
        
        (
        
          t
          
            1
          
        
        )
        .
      
    
    {\displaystyle F_{x}(d(t_{2})-d(t_{1}))={\frac {m}{2}}v^{2}(t_{2})-{\frac {m}{2}}v^{2}(t_{1}).}
  As an example consider a car skidding to a stop, where k is the coefficient of friction and W is the weight of the car.  Then the force along the trajectory is Fx = −kW.  The velocity v of the car can be determined from the length s of the skid using the work–energy principle,

  
    
      
        k
        W
        s
        =
        
          
            W
            
              2
              g
            
          
        
        
          v
          
            2
          
        
        ,
        
        
          
            or
          
        
        
        v
        =
        
          
            2
            k
            s
            g
          
        
        .
      
    
    {\displaystyle kWs={\frac {W}{2g}}v^{2},\quad {\mbox{or}}\quad v={\sqrt {2ksg}}.}
  Notice that this formula uses the fact that the mass of the vehicle is m = W/g.


=== Coasting down a mountain road (gravity racing) ===
Consider the case of a vehicle that starts at rest and coasts down a mountain road, the work-energy principle helps compute the minimum distance that the vehicle travels to reach a velocity V, of say 60 mph (88 fps).  Rolling resistance and air drag will slow the vehicle down so the actual distance will be greater than if these forces are neglected.
Let the trajectory of the vehicle following the road be X(t) which is a curve in three-dimensional space.  The force acting on the vehicle that pushes it down the road is the constant force of gravity F = (0, 0, W), while the force of the road on the vehicle is the constraint force R.  Newton's second law yields,

  
    
      
        
          F
        
        +
        
          R
        
        =
        m
        
          
            
              
                X
              
              ¨
            
          
        
        .
      
    
    {\displaystyle \mathbf {F} +\mathbf {R} =m{\ddot {\mathbf {X} }}.}
  The scalar product of this equation with the velocity, V = (vx, vy, vz), yields

  
    
      
        W
        
          v
          
            z
          
        
        =
        m
        
          
            
              V
              ˙
            
          
        
        V
        ,
      
    
    {\displaystyle Wv_{z}=m{\dot {V}}V,}
  where V is the magnitude of V.  The constraint forces between the vehicle and the road cancel from this equation because R ⋅ V = 0, which means they do no work.
Integrate both sides to obtain

  
    
      
        
          ∫
          
            
              t
              
                1
              
            
          
          
            
              t
              
                2
              
            
          
        
        W
        
          v
          
            z
          
        
        d
        t
        =
        
          
            m
            2
          
        
        
          V
          
            2
          
        
        (
        
          t
          
            2
          
        
        )
        −
        
          
            m
            2
          
        
        
          V
          
            2
          
        
        (
        
          t
          
            1
          
        
        )
        .
      
    
    {\displaystyle \int _{t_{1}}^{t_{2}}Wv_{z}dt={\frac {m}{2}}V^{2}(t_{2})-{\frac {m}{2}}V^{2}(t_{1}).}
  The weight force W is constant along the trajectory and the integral of the vertical velocity is the vertical distance, therefore,

  
    
      
        W
        Δ
        z
        =
        
          
            m
            2
          
        
        
          V
          
            2
          
        
        .
      
    
    {\displaystyle W\Delta z={\frac {m}{2}}V^{2}.}
  Recall that V(t1)=0. Notice that this result does not depend on the shape of the road followed by the vehicle.
In order to determine the distance along the road assume the downgrade is 6%, which is a steep road.  This means the altitude decreases 6 feet for every 100 feet traveled—for angles this small the sin and tan functions are approximately equal.  Therefore, the distance s in feet down a 6% grade to reach the velocity V is at least

  
    
      
        s
        =
        
          
            
              Δ
              z
            
            0.06
          
        
        =
        8.3
        
          
            
              V
              
                2
              
            
            g
          
        
        ,
        
        
          
            or
          
        
        
        s
        =
        8.3
        
          
            
              88
              
                2
              
            
            32.2
          
        
        ≈
        2000
        
          
            ft
          
        
        .
      
    
    {\displaystyle s={\frac {\Delta z}{0.06}}=8.3{\frac {V^{2}}{g}},\quad {\mbox{or}}\quad s=8.3{\frac {88^{2}}{32.2}}\approx 2000{\mbox{ft}}.}
  This formula uses the fact that the weight of the vehicle is W = mg.


== Work of forces acting on a rigid body ==
The work of forces acting at various points on a single rigid body can be calculated from the work of a resultant force and torque.  To see this, let the forces F1, F2 ... Fn act on the points X1, X2 ... Xn in a rigid body.
The trajectories of Xi, i = 1, ..., n  are defined by the movement of the rigid body.  This movement is given by the set of rotations [A(t)] and the trajectory d(t) of a reference point in the body.  Let the coordinates xi i = 1, ..., n  define these points in the moving rigid body's  reference frame M, so that the trajectories traced in the fixed frame F are given by

  
    
      
        
          
            X
          
          
            i
          
        
        (
        t
        )
        =
        [
        A
        (
        t
        )
        ]
        
          
            x
          
          
            i
          
        
        +
        
          d
        
        (
        t
        )
        
        i
        =
        1
        ,
        …
        ,
        n
        .
      
    
    {\displaystyle \mathbf {X} _{i}(t)=[A(t)]\mathbf {x} _{i}+\mathbf {d} (t)\quad i=1,\ldots ,n.}
  The velocity of the points Xi along their trajectories  are

  
    
      
        
          
            V
          
          
            i
          
        
        =
        
          
            
              ω
              →
            
          
        
        ×
        (
        
          
            X
          
          
            i
          
        
        −
        
          d
        
        )
        +
        
          
            
              
                d
              
              ˙
            
          
        
        ,
      
    
    {\displaystyle \mathbf {V} _{i}={\vec {\omega }}\times (\mathbf {X} _{i}-\mathbf {d} )+{\dot {\mathbf {d} }},}
  where ω is the angular velocity vector obtained from the skew symmetric matrix

  
    
      
        [
        Ω
        ]
        =
        
          
            
              A
              ˙
            
          
        
        
          A
          
            
              T
            
          
        
        ,
      
    
    {\displaystyle [\Omega ]={\dot {A}}A^{\mathrm {T} },}
  known as the angular velocity matrix.
The small amount of work by the forces over the small displacements δri can be determined by approximating the displacement by δr = vδt so

  
    
      
        δ
        W
        =
        
          
            F
          
          
            1
          
        
        ⋅
        
          
            V
          
          
            1
          
        
        δ
        t
        +
        
          
            F
          
          
            2
          
        
        ⋅
        
          
            V
          
          
            2
          
        
        δ
        t
        +
        …
        +
        
          
            F
          
          
            n
          
        
        ⋅
        
          
            V
          
          
            n
          
        
        δ
        t
      
    
    {\displaystyle \delta W=\mathbf {F} _{1}\cdot \mathbf {V} _{1}\delta t+\mathbf {F} _{2}\cdot \mathbf {V} _{2}\delta t+\ldots +\mathbf {F} _{n}\cdot \mathbf {V} _{n}\delta t}
  or

  
    
      
        δ
        W
        =
        
          ∑
          
            i
            =
            1
          
          
            n
          
        
        
          
            F
          
          
            i
          
        
        ⋅
        (
        
          
            
              ω
              →
            
          
        
        ×
        (
        
          
            X
          
          
            i
          
        
        −
        
          d
        
        )
        +
        
          
            
              
                d
              
              ˙
            
          
        
        )
        δ
        t
        .
      
    
    {\displaystyle \delta W=\sum _{i=1}^{n}\mathbf {F} _{i}\cdot ({\vec {\omega }}\times (\mathbf {X} _{i}-\mathbf {d} )+{\dot {\mathbf {d} }})\delta t.}
  This formula can be rewritten to obtain

  
    
      
        δ
        W
        =
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              
                F
              
              
                i
              
            
          
          )
        
        ⋅
        
          
            
              
                d
              
              ˙
            
          
        
        δ
        t
        +
        
          (
          
            
              ∑
              
                i
                =
                1
              
              
                n
              
            
            
              (
              
                
                  
                    X
                  
                  
                    i
                  
                
                −
                
                  d
                
              
              )
            
            ×
            
              
                F
              
              
                i
              
            
          
          )
        
        ⋅
        
          
            
              ω
              →
            
          
        
        δ
        t
        =
        
          (
          
            
              F
            
            ⋅
            
              
                
                  
                    d
                  
                  ˙
                
              
            
            +
            
              T
            
            ⋅
            
              
                
                  ω
                  →
                
              
            
          
          )
        
        δ
        t
        ,
      
    
    {\displaystyle \delta W=\left(\sum _{i=1}^{n}\mathbf {F} _{i}\right)\cdot {\dot {\mathbf {d} }}\delta t+\left(\sum _{i=1}^{n}\left(\mathbf {X} _{i}-\mathbf {d} \right)\times \mathbf {F} _{i}\right)\cdot {\vec {\omega }}\delta t=\left(\mathbf {F} \cdot {\dot {\mathbf {d} }}+\mathbf {T} \cdot {\vec {\omega }}\right)\delta t,}
  where F and T are the resultant force and torque applied at the reference point d of the moving frame M in the rigid body.


== References ==


== Bibliography ==
Serway, Raymond A.; Jewett, John W. (2004). Physics for Scientists and Engineers (6th ed.). Brooks/Cole. ISBN 0-534-40842-7.
Tipler, Paul (1991). Physics for Scientists and Engineers: Mechanics (3rd ed., extended version ed.). W. H. Freeman. ISBN 0-87901-432-6.


== External links ==
Work–energy principle","pandas(index=240, _1=240, text='in physics, work is the energy transferred to or from an object via the application of force along a displacement. in its simplest form, it is often represented as the product of force and displacement. a force is said to do positive work if (when applied) it has a component in the direction of the displacement of the point of application. a force does negative work if it has a component opposite to the direction of the displacement at the point of application of the force. for example, when a ball is held above the ground and then dropped, the work done by the gravitational force on the ball as it falls is equal to the weight of the ball (a force) multiplied by the distance to the ground (a displacement). when the force f is constant and the angle between the force and the displacement s is θ, then the work done is given by:     w = f s cos \u2061  θ     where f and t are the resultant force and torque applied at the reference point d of the moving frame m in the rigid body.   == references ==   == bibliography == serway, raymond a.; jewett, john w. (2004). physics for scientists and engineers (6th ed.). brooks/cole. isbn 0-534-40842-7. tipler, paul (1991). physics for scientists and engineers: mechanics (3rd ed., extended version ed.). w. h. freeman. isbn 0-87901-432-6.   == external links == work–energy principle')"
