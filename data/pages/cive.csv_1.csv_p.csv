Unnamed: 0,Text,Text_1
0,"Aerodynamics, from Greek ἀήρ aero (air) + δυναμική (dynamics), is the study of motion of air, particularly when affected by a solid object, such as an airplane wing. It is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.  The term aerodynamics is often used synonymously with gas dynamics, the difference being that ""gas dynamics"" applies to the study of the motion of all gases, and is not limited to air. 
The formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier.  Most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by Otto Lilienthal in 1891.  Since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies.  Recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.


== History ==

Modern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the Ancient Greek legend of Icarus and Daedalus.  Fundamental concepts of continuum, drag, and pressure gradients appear in the work of Aristotle and Archimedes.In 1726, Sir Isaac Newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. Dutch-Swiss mathematician Daniel Bernoulli followed in 1738 with Hydrodynamica in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as Bernoulli's principle, which provides one method for calculating aerodynamic lift. In 1757, Leonhard Euler published the more general Euler equations which could be applied to both compressible and incompressible flows. The Euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the Navier–Stokes equations.  The Navier-Stokes equations are the most general governing equations of fluid flow and but are difficult to solve for the flow around all but the simplest of shapes.

In 1799, Sir George Cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century.  In 1871, Francis Herbert Wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces.  Drag theories were developed by Jean le Rond d'Alembert, Gustav Kirchhoff, and Lord Rayleigh. In 1889, Charles Renard, a French aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. Otto Lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag.  Building on these developments as well as research carried out in their own wind tunnel, the Wright brothers flew the first powered airplane on December 17, 1903.
During the time of the first flights, Frederick W. Lanchester, Martin Kutta, and Nikolai Zhukovsky independently created theories that connected circulation of a fluid flow to lift. Kutta and Zhukovsky went on to develop a two-dimensional wing theory. Expanding upon the work of Lanchester, Ludwig Prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers.
As aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near the speed of sound. The differences in airflow under such conditions lead to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. The ratio of the flow speed to the speed of sound was named the Mach number after Ernst Mach who was one of the first to investigate the properties of the supersonic flow. Macquorn Rankine and Pierre Henri Hugoniot independently developed the theory for flow properties before and after a shock wave, while Jakob Ackeret led the initial work of calculating the lift and drag of supersonic airfoils. Theodore von Kármán and Hugh Latimer Dryden introduced the term transonic to describe flow speeds between the critical Mach number and Mach 1 where drag increases rapidly. This rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken in 1947 using the Bell X-1 aircraft.
By the time the sound barrier was broken, aerodynamicists' understanding of the subsonic and low supersonic flow had matured. The Cold War prompted the design of an ever-evolving line of high-performance aircraft. Computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions.  Understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behaviour of fluid flow to the engineering of a vehicle such that it interacts predictably with the fluid flow. Designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the Navier-Stokes equations.


== Fundamental concepts ==

Understanding the motion of air around an object (often called a flow field)  enables the calculation of forces and moments acting on the object.  In many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight.  Of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body.  Calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. Continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. These properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows.  Density, flow velocity, and an additional property, viscosity, are used to classify flow fields. 


=== Flow classification ===
Flow velocity is used to classify flows according to speed regime. Subsonic flows are flow fields in which the air speed field is always below the local speed of sound. Transonic flows include both regions of subsonic flow and regions in which the local flow speed is greater than the local speed of sound. Supersonic flows are defined to be flows in which the flow speed is greater than the speed of sound everywhere. A fourth classification, hypersonic flow, refers to flows where the flow speed is much greater than the speed of sound.  Aerodynamicists disagree on the precise definition of hypersonic flow.
Compressible flow accounts for varying density within the flow. Subsonic flows are often idealized as incompressible, i.e. the density is assumed to be constant. Transonic and supersonic flows are compressible, and calculations that neglect the changes of density in these flow fields will yield inaccurate results.
Viscosity is associated with the frictional forces in a flow. In some flow fields, viscous effects are very small, and approximate solutions may safely neglect viscous effects. These approximations are called inviscid flows. Flows for which viscosity is not neglected are called viscous flows. Finally, aerodynamic problems may also be classified by the flow environment. External aerodynamics is the study of flow around solid objects of various shapes (e.g. around an airplane wing), while internal aerodynamics is the study of flow through passages inside solid objects (e.g. through a jet engine).


==== Continuum assumption ====
Unlike liquids and solids, gases are composed of discrete molecules which occupy only a small fraction of the volume filled by the gas. On a molecular level, flow fields are made up of the collisions of many individual of gas molecules between themselves and with solid surfaces. However, in most aerodynamics applications, the discrete molecular nature of gases is ignored, and the flow field is assumed to behave as a continuum. This assumption allows fluid properties such as density and flow velocity to be defined everywhere within the flow.
The validity of the continuum assumption is dependent on the density of the gas and the application in question. For the continuum assumption to be valid, the mean free path length must be much smaller than the length scale of the application in question. For example, many aerodynamics applications deal with aircraft flying in atmospheric conditions, where the mean free path length is on the order of micrometers and where the body is orders of magnitude larger. In these cases, the length scale of the aircraft ranges from a few meters to a few tens of meters, which is much larger than the mean free path length. For such applications, the continuum assumption is reasonable. The continuum assumption is less valid for extremely low-density flows, such as those encountered by vehicles at very high altitudes (e.g. 300,000 ft/90 km) or satellites in Low Earth orbit. In those cases, statistical mechanics is a more accurate method of solving the problem than is continuum aerodynamics. The Knudsen number can be used to guide the choice between statistical mechanics and the continuous formulation of aerodynamics.


=== Conservation laws ===
The assumption of  a fluid continuum allows problems in aerodynamics to be solved using fluid dynamics conservation laws. Three conservation principles are used: 

Conservation of mass
Conservation of mass requires that mass is neither created nor destroyed within a flow; the mathematical formulation of this principle is known as the mass continuity equation.
Conservation of momentum
The mathematical formulation of this principle can be considered an application of Newton's Second Law. Momentum within a flow is only changed by external forces, which may include both surface forces, such as viscous (frictional) forces, and body forces, such as weight.  The momentum conservation principle may be expressed as either a vector equation or separated into a set of three scalar equations (x,y,z components).
Conservation of energy
The energy conservation equation states that energy is neither created nor destroyed within a flow, and that any addition or subtraction of energy to a volume in the flow is caused by heat transfer, or by work into and out of the region of interest.Together, these  equations are known as the Navier-Stokes equations, although some authors define the term to only include the momentum equation(s).  The Navier-Stokes equations have no known analytical solution and are solved in modern aerodynamics using computational techniques. Because computational methods using high speed computers were not historically available and the high computational cost of solving these complex equations now that they are available, simplifications of the Navier-Stokes equations have been and continue to be employed. The Euler equations are a set of similar conservation equations which neglect viscosity and may be used in cases where the effect of viscosity is expected to be small. Further simplifications lead to Laplace's equation and potential flow theory.  Additionally, Bernoulli's equation is a solution in one dimension to both the momentum and energy conservation equations.
The ideal gas law or another such equation of state is often used in conjunction with these equations to form a determined system that allows the solution for the unknown variables.


== Branches of aerodynamics ==

Aerodynamic problems are classified by the flow environment or properties of the flow, including flow speed, compressibility, and viscosity. External aerodynamics is the study of flow around solid objects of various shapes. Evaluating the lift and drag on an airplane or the shock waves that form in front of the nose of a rocket are examples of external aerodynamics. Internal aerodynamics is the study of flow through passages in solid objects. For instance, internal aerodynamics encompasses the study of the airflow through a jet engine or through an air conditioning pipe.
Aerodynamic problems can also be classified according to whether the flow speed is below, near or above the speed of sound. A problem is called subsonic if all the speeds in the problem are less than the speed of sound, transonic if speeds both below and above the speed of sound are present (normally when the characteristic speed is approximately the speed of sound), supersonic when the characteristic flow speed is greater than the speed of sound, and hypersonic when the flow speed is much greater than the speed of sound. Aerodynamicists disagree over the precise definition of hypersonic flow; a rough definition considers flows with Mach numbers above 5 to be hypersonic.The influence of viscosity on the flow dictates a third classification. Some problems may encounter only very small viscous effects, in which case viscosity can be considered to be negligible. The approximations to these problems are called inviscid flows. Flows for which viscosity cannot be neglected are called viscous flows.


=== Incompressible aerodynamics ===

An incompressible flow is a flow in which density is constant in both time and space.  Although all real fluids are compressible, a flow is often approximated as incompressible if the effect of the density changes cause only small changes to the calculated results. This is more likely to be true when the flow speeds are significantly lower than the speed of sound. Effects of compressibility are more significant at speeds close to or above the speed of sound.  The Mach number is used to evaluate whether the incompressibility can be assumed, otherwise the effects of compressibility must be included.


==== Subsonic flow ====
Subsonic (or low-speed) aerodynamics describes fluid motion in flows which are much lower than the speed of sound everywhere in the flow. There are several branches of subsonic flow but one special case arises when the flow is inviscid, incompressible and irrotational. This case is called potential flow and allows the differential equations that describe the flow to be a simplified version of the equations of fluid dynamics, thus making available to the aerodynamicist a range of quick and easy solutions.In solving a subsonic problem, one decision to be made by the aerodynamicist is whether to incorporate the effects of compressibility. Compressibility is a description of the amount of change of density in the flow. When the effects of compressibility on the solution are small, the assumption that density is constant may be made. The problem is then an incompressible low-speed aerodynamics problem. When the density is allowed to vary, the flow is called compressible. In air, compressibility effects are usually ignored when the Mach number in the flow does not exceed 0.3 (about 335 feet (102 m) per second or 228 miles (366 km) per hour at 60 °F (16 °C)). Above Mach 0.3, the problem flow should be described using compressible aerodynamics.


=== Compressible aerodynamics ===

According to the theory of aerodynamics, a flow is considered to be compressible if the density changes along a streamline. This means that – unlike incompressible flow – changes in density are considered. In general, this is the case where the Mach number in part or all of the flow exceeds 0.3. The Mach 0.3 value is rather arbitrary, but it is used because gas flows with a Mach number below that value demonstrate changes in density of less than 5%. Furthermore, that maximum 5% density change occurs at the stagnation point (the point on the object where flow speed is zero), while the density changes around the rest of the object will be significantly lower. Transonic, supersonic, and hypersonic flows are all compressible flows.


==== Transonic flow ====

The term Transonic refers to a range of flow velocities just below and above the local speed of sound (generally taken as Mach 0.8–1.2). It is defined as the range of speeds between the critical Mach number, when some parts of the airflow over an aircraft become supersonic, and a higher speed, typically near Mach 1.2, when all of the airflow is supersonic. Between these speeds, some of the airflow is supersonic, while some of the airflow is not supersonic.


==== Supersonic flow ====

Supersonic aerodynamic problems are those involving flow speeds greater than the speed of sound. Calculating the lift on the Concorde during cruise can be an example of a supersonic aerodynamic problem.
Supersonic flow behaves very differently from subsonic flow. Fluids react to differences in pressure; pressure changes are how a fluid is ""told"" to respond to its environment. Therefore, since sound is, in fact, an infinitesimal pressure difference propagating through a fluid, the speed of sound in that fluid can be considered the fastest speed that ""information"" can travel in the flow. This difference most obviously manifests itself in the case of a fluid striking an object. In front of that object, the fluid builds up a stagnation pressure as impact with the object brings the moving fluid to rest. In fluid traveling at subsonic speed, this pressure disturbance can propagate upstream, changing the flow pattern ahead of the object and giving the impression that the fluid ""knows"" the object is there by seemingly adjusting its movement and is flowing around it. In a supersonic flow, however, the pressure disturbance cannot propagate upstream. Thus, when the fluid finally reaches the object it strikes it and the fluid is forced to change its properties – temperature, density, pressure, and Mach number—in an extremely violent and irreversible fashion called a shock wave. The presence of shock waves, along with the compressibility effects of high-flow velocity (see Reynolds number) fluids, is the central difference between the supersonic and subsonic aerodynamics regimes.


==== Hypersonic flow ====

In aerodynamics, hypersonic speeds are speeds that are highly supersonic. In the 1970s, the term generally came to refer to speeds of Mach 5 (5 times the speed of sound) and above. The hypersonic regime is a subset of the supersonic regime. Hypersonic flow is characterized by high temperature flow behind a shock wave, viscous interaction, and chemical dissociation of gas.


== Associated terminology ==

The incompressible and compressible flow regimes produce many associated phenomena, such as boundary layers and turbulence.


=== Boundary layers ===

The concept of a boundary layer is important in many problems in aerodynamics. The viscosity and fluid friction in the air is approximated as being significant only in this thin layer. This assumption makes the description of such aerodynamics much more tractable mathematically.


=== Turbulence ===

In aerodynamics, turbulence is characterized by chaotic property changes in the flow. These include low momentum diffusion, high momentum convection, and rapid variation of pressure and flow velocity in space and time. Flow that is not turbulent is called laminar flow.


== Aerodynamics in other fields ==


=== Engineering design ===

Aerodynamics is a significant element of vehicle design, including road cars and trucks where the main goal is to reduce the vehicle drag coefficient, and racing cars, where in addition to reducing drag the goal is also to increase the overall level of downforce.  Aerodynamics is also important in the prediction of forces and moments acting on sailing vessels. It is used in the design of mechanical components such as hard drive heads. Structural engineers resort to aerodynamics, and particularly aeroelasticity, when calculating wind loads in the design of large buildings, bridges, and wind turbines 
The aerodynamics of internal passages is important in heating/ventilation, gas piping, and in automotive engines where detailed flow patterns strongly affect the performance of the engine.


=== Environmental design ===
Urban aerodynamics are studied by town planners and designers seeking to improve amenity in outdoor spaces, or in creating urban microclimates to reduce the effects of urban pollution. The field of environmental aerodynamics describes ways in which atmospheric circulation and flight mechanics affect ecosystems. 
Aerodynamic equations are used in numerical weather prediction.


=== Ball-control in sports ===
Sports in which aerodynamics are of crucial importance include soccer, table tennis, cricket, baseball,  and golf, in which expert players can control the trajectory of the ball using the ""Magnus effect"".


== See also ==
Aeronautics
Aerostatics
Aviation
Insect flight – how bugs fly
List of aerospace engineering topics
List of engineering topics
Nose cone design


== References ==


== Further reading ==


== External links ==
NASA Beginner's Guide to Aerodynamics
Smithsonian National Air and Space Museum's How Things Fly website
Aerodynamics for Students
Aerodynamics for Pilots
Aerodynamics and Race Car Tuning
Aerodynamic Related Projects
eFluids Bicycle Aerodynamics
Application of Aerodynamics in Formula One (F1)
Aerodynamics in Car Racing
Aerodynamics of Birds","pandas(index=0, _1=0, text='aerodynamics, from greek ἀήρ aero (air)δυναμική (dynamics), is the study of motion of air, particularly when affected by a solid object, such as an airplane wing. it is a sub-field of fluid dynamics and gas dynamics, and many aspects of aerodynamics theory are common to these fields.  the term aerodynamics is often used synonymously with gas dynamics, the difference being that ""gas dynamics"" applies to the study of the motion of all gases, and is not limited to air. the formal study of aerodynamics began in the modern sense in the eighteenth century, although observations of fundamental concepts such as aerodynamic drag were recorded much earlier.  most of the early efforts in aerodynamics were directed toward achieving heavier-than-air flight, which was first demonstrated by otto lilienthal in 1891.  since then, the use of aerodynamics through mathematical analysis, empirical approximations, wind tunnel experimentation, and computer simulations has formed a rational basis for the development of heavier-than-air flight and a number of other technologies.  recent work in aerodynamics has focused on issues related to compressible flow, turbulence, and boundary layers and has become increasingly computational in nature.   == history ==  modern aerodynamics only dates back to the seventeenth century, but aerodynamic forces have been harnessed by humans for thousands of years in sailboats and windmills, and images and stories of flight appear throughout recorded history, such as the ancient greek legend of icarus and daedalus.  fundamental concepts of continuum, drag, and pressure gradients appear in the work of aristotle and archimedes.in 1726, sir isaac newton became the first person to develop a theory of air resistance, making him one of the first aerodynamicists. dutch-swiss mathematician daniel bernoulli followed in 1738 with hydrodynamica in which he described a fundamental relationship between pressure, density, and flow velocity for incompressible flow known today as bernoulli\'s principle, which provides one method for calculating aerodynamic lift. in 1757, leonhard euler published the more general euler equations which could be applied to both compressible and incompressible flows. the euler equations were extended to incorporate the effects of viscosity in the first half of the 1800s, resulting in the navier–stokes equations.  the navier-stokes equations are the most general governing equations of fluid flow and but are difficult to solve for the flow around all but the simplest of shapes.  in 1799, sir george cayley became the first person to identify the four aerodynamic forces of flight (weight, lift, drag, and thrust), as well as the relationships between them, and in doing so outlined the path toward achieving heavier-than-air flight for the next century.  in 1871, francis herbert wenham constructed the first wind tunnel, allowing precise measurements of aerodynamic forces.  drag theories were developed by jean le rond d\'alembert, gustav kirchhoff, and lord rayleigh. in 1889, charles renard, a french aeronautical engineer, became the first person to reasonably predict the power needed for sustained flight. otto lilienthal, the first person to become highly successful with glider flights, was also the first to propose thin, curved airfoils that would produce high lift and low drag.  building on these developments as well as research carried out in their own wind tunnel, the wright brothers flew the first powered airplane on december 17, 1903. during the time of the first flights, frederick w. lanchester, martin kutta, and nikolai zhukovsky independently created theories that connected circulation of a fluid flow to lift. kutta and zhukovsky went on to develop a two-dimensional wing theory. expanding upon the work of lanchester, ludwig prandtl is credited with developing the mathematics behind thin-airfoil and lifting-line theories as well as work with boundary layers. as aircraft speed increased, designers began to encounter challenges associated with air compressibility at speeds near the speed of sound. the differences in airflow under such conditions lead to problems in aircraft control, increased drag due to shock waves, and the threat of structural failure due to aeroelastic flutter. the ratio of the flow speed to the speed of sound was named the mach number after ernst mach who was one of the first to investigate the properties of the supersonic flow. macquorn rankine and pierre henri hugoniot independently developed the theory for flow properties before and after a shock wave, while jakob ackeret led the initial work of calculating the lift and drag of supersonic airfoils. theodore von kármán and hugh latimer dryden introduced the term transonic to describe flow speeds between the critical mach number and mach 1 where drag increases rapidly. this rapid increase in drag led aerodynamicists and aviators to disagree on whether supersonic flight was achievable until the sound barrier was broken in 1947 using the bell x-1 aircraft. by the time the sound barrier was broken, aerodynamicists\' understanding of the subsonic and low supersonic flow had matured. the cold war prompted the design of an ever-evolving line of high-performance aircraft. computational fluid dynamics began as an effort to solve for flow properties around complex objects and has rapidly grown to the point where entire aircraft can be designed using computer software, with wind-tunnel tests followed by flight tests to confirm the computer predictions.  understanding of supersonic and hypersonic aerodynamics has matured since the 1960s, and the goals of aerodynamicists have shifted from the behaviour of fluid flow to the engineering of a vehicle such that it interacts predictably with the fluid flow. designing aircraft for supersonic and hypersonic conditions, as well as the desire to improve the aerodynamic efficiency of current aircraft and propulsion systems, continues to motivate new research in aerodynamics, while work continues to be done on important problems in basic aerodynamic theory related to flow turbulence and the existence and uniqueness of analytical solutions to the navier-stokes equations.   == fundamental concepts ==  understanding the motion of air around an object (often called a flow field)  enables the calculation of forces and moments acting on the object.  in many aerodynamics problems, the forces of interest are the fundamental forces of flight: lift, drag, thrust, and weight.  of these, lift and drag are aerodynamic forces, i.e. forces due to air flow over a solid body.  calculation of these quantities is often founded upon the assumption that the flow field behaves as a continuum. continuum flow fields are characterized by properties such as flow velocity, pressure, density, and temperature, which may be functions of position and time. these properties may be directly or indirectly measured in aerodynamics experiments or calculated starting with the equations for conservation of mass, momentum, and energy in air flows.  density, flow velocity, and an additional property, viscosity, are used to classify flow fields. sports in which aerodynamics are of crucial importance include soccer, table tennis, cricket, baseball,  and golf, in which expert players can control the trajectory of the ball using the ""magnus effect"".   == see also == aeronautics aerostatics aviation insect flight – how bugs fly list of aerospace engineering topics list of engineering topics nose cone design   == references ==   == further reading ==   == external links == nasa beginner\'s guide to aerodynamics smithsonian national air and space museum\'s how things fly website aerodynamics for students aerodynamics for pilots aerodynamics and race car tuning aerodynamic related projects efluids bicycle aerodynamics application of aerodynamics in formula one (f1) aerodynamics in car racing aerodynamics of birds')"
1,"Atmospheric entry is the movement of an object from outer space into and through the gases of an atmosphere of a planet, dwarf planet, or natural satellite. There are two main types of atmospheric entry: uncontrolled entry, such as the entry of astronomical objects, space debris, or bolides; and controlled entry (or reentry) of a spacecraft capable of being navigated or following a predetermined course. Technologies and procedures allowing the controlled atmospheric entry, descent, and landing of spacecraft are collectively termed as EDL.

Objects entering an atmosphere experience atmospheric drag, which puts mechanical stress on the object, and aerodynamic heating—caused mostly by compression of the air in front of the object, but also by drag. These forces can cause loss of mass (ablation) or even complete disintegration of smaller objects, and objects with lower compressive strength can explode.
Crewed space vehicles must be slowed to subsonic speeds before parachutes or air brakes may be deployed. Such vehicles have kinetic energies typically between 50 and 1,800 megajoules, and atmospheric dissipation is the only way of expending the kinetic energy. The amount of rocket fuel required to slow the vehicle would be nearly equal to the amount used to accelerate it initially, and it is thus highly impractical to use retro rockets for the entire Earth reentry procedure. While the high temperature generated at the surface of the heat shield is due to adiabatic compression, the vehicle's kinetic energy is ultimately lost to gas friction (viscosity) after the vehicle has passed by. Other smaller energy losses include black-body radiation directly from the hot gases and chemical reactions between ionized gases.
Ballistic warheads and expendable vehicles do not require slowing at reentry, and in fact, are made streamlined so as to maintain their speed. Furthermore, slow-speed returns to Earth from near-space such as parachute jumps from balloons do not require heat shielding because the gravitational acceleration of an object starting at relative rest from within the atmosphere itself (or not far above it) cannot create enough velocity to cause significant atmospheric heating.
For Earth, atmospheric entry occurs by convention at the Kármán line at an altitude of 100 km (62 miles; 54 nautical miles) above the surface, while at Venus atmospheric entry occurs at 250 km (160 mi; 130 nmi) and at Mars atmospheric entry at about 80 km (50 mi; 43 nmi). Uncontrolled objects reach high velocities while accelerating through space toward the Earth under the influence of Earth's gravity, and are slowed by friction upon encountering Earth's atmosphere. Meteors are also often travelling quite fast relative to the Earth simply because their own orbital path is different from that of the Earth before they encounter Earth's gravity well. Most controlled objects enter at hypersonic speeds due to their sub-orbital (e.g., intercontinental ballistic missile reentry vehicles), orbital (e.g., the Soyuz), or unbounded (e.g., meteors) trajectories. Various advanced technologies have been developed to enable atmospheric reentry and flight at extreme velocities. An alternative low-velocity method of controlled atmospheric entry is buoyancy which is suitable for planetary entry where thick atmospheres, strong gravity, or both factors complicate high-velocity hyperbolic entry, such as the atmospheres of Venus, Titan and the gas giants.


== History ==

The concept of the ablative heat shield was described as early as 1920 by Robert Goddard: ""In the case of meteors, which enter the atmosphere with speeds as high as 30 miles (48 km) per second, the interior of the meteors remains cold, and the erosion is due, to a large extent, to chipping or cracking of the suddenly heated surface. For this reason, if the outer surface of the apparatus were to consist of layers of a very infusible hard substance with layers of a poor heat conductor between, the surface would not be eroded to any considerable extent, especially as the velocity of the apparatus would not be nearly so great as that of the average meteor.""Practical development of reentry systems began as the range, and reentry velocity of ballistic missiles increased. For early short-range missiles, like the V-2, stabilization and aerodynamic stress were important issues (many V-2s broke apart during reentry), but heating was not a serious problem. Medium-range missiles like the Soviet R-5, with a 1,200-kilometer (650-nautical-mile) range, required ceramic composite heat shielding on separable reentry vehicles (it was no longer possible for the entire rocket structure to survive reentry). The first ICBMs, with ranges of 8,000 to 12,000 km (4,300 to 6,500 nmi), were only possible with the development of modern ablative heat shields and blunt-shaped vehicles.
In the United States, this technology was pioneered by H. Julian Allen and A. J. Eggers Jr. of the National Advisory Committee for Aeronautics (NACA) at Ames Research Center. In 1951, they made the counterintuitive discovery that a blunt shape (high drag) made the most effective heat shield. From simple engineering principles, Allen and Eggers showed that the heat load experienced by an entry vehicle was inversely proportional to the drag coefficient; i.e., the greater the drag, the less the heat load. If the reentry vehicle is made blunt, air cannot ""get out of the way"" quickly enough, and acts as an air cushion to push the shock wave and heated shock layer forward (away from the vehicle). Since most of the hot gases are no longer in direct contact with the vehicle, the heat energy would stay in the shocked gas and simply move around the vehicle to later dissipate into the atmosphere.
The Allen and Eggers discovery, though initially treated as a military secret, was eventually published in 1958.


== Terminology, definitions and jargon ==
Over the decades since the 1950s, a rich technical jargon has grown around the engineering of vehicles designed to enter planetary atmospheres. It is recommended that the reader review the jargon glossary before continuing with this article on atmospheric reentry.
When atmospheric entry is part of a spacecraft landing or recovery, particularly on a planetary body other than Earth, entry is part of a phase referred to as entry, descent, and landing, or EDL. When the atmospheric entry returns to the same body that the vehicle had launched from, the event is referred to as reentry (almost always referring to Earth entry).
The fundamental design objective in atmospheric entry of a spacecraft is to dissipate the energy of a spacecraft that is traveling at hypersonic speed as it enters an atmosphere such that equipment, cargo, and any passengers are slowed and land near a specific destination on the surface at zero velocity while keeping stresses on the spacecraft and any passengers within acceptable limits. This may be accomplished by propulsive or aerodynamic (vehicle characteristics or parachute) means, or by some combination.


== Entry vehicle shapes ==

There are several basic shapes used in designing entry vehicles:


=== Sphere or spherical section ===

The simplest axisymmetric shape is the sphere or spherical section. This can either be a complete sphere or a spherical section forebody with a converging conical afterbody. The aerodynamics of a sphere or spherical section are easy to model analytically using Newtonian impact theory. Likewise, the spherical section's heat flux can be accurately modeled with the Fay-Riddell equation. The static stability of a spherical section is assured if the vehicle's center of mass is upstream from the center of curvature (dynamic stability is more problematic). Pure spheres have no lift. However, by flying at an angle of attack, a spherical section has modest aerodynamic lift thus providing some cross-range capability and widening its entry corridor. In the late 1950s and early 1960s, high-speed computers were not yet available and computational fluid dynamics was still embryonic. Because the spherical section was amenable to closed-form analysis, that geometry became the default for conservative design. Consequently, manned capsules of that era were based upon the spherical section.
Pure spherical entry vehicles were used in the early Soviet Vostok and Voskhod capsules and in Soviet Mars and Venera descent vehicles. The Apollo command module used a spherical section forebody heat shield with a converging conical afterbody. It flew a lifting entry with a hypersonic trim angle of attack of −27° (0° is blunt-end first) to yield an average L/D (lift-to-drag ratio) of 0.368. The resultant lift achieved a measure of cross-range control by offsetting the vehicle's center of mass from its axis of symmetry, allowing the lift force to be directed left or right by rolling the capsule on its longitudinal axis. Other examples of the spherical section geometry in manned capsules are Soyuz/Zond, Gemini, and Mercury. Even these small amounts of lift allow trajectories that have very significant effects on peak g-force, reducing it from 8–9 g for a purely ballistic (slowed only by drag) trajectory to 4–5 g, as well as greatly reducing the peak reentry heat.


=== Sphere-cone ===
The sphere-cone is a spherical section with a frustum or blunted cone attached. The sphere-cone's dynamic stability is typically better than that of a spherical section. The vehicle enters sphere-first. With a sufficiently small half-angle and properly placed center of mass, a sphere-cone can provide aerodynamic stability from Keplerian entry to surface impact. (The half-angle is the angle between the cone's axis of rotational symmetry and its outer surface, and thus half the angle made by the cone's surface edges.)

The original American sphere-cone aeroshell was the Mk-2 RV (reentry vehicle), which was developed in 1955 by the General Electric Corp. The Mk-2's design was derived from blunt-body theory and used a radiatively cooled thermal protection system (TPS) based upon a metallic heat shield (the different TPS types are later described in this article). The Mk-2 had significant defects as a weapon delivery system, i.e., it loitered too long in the upper atmosphere due to its lower ballistic coefficient and also trailed a stream of vaporized metal making it very visible to radar. These defects made the Mk-2 overly susceptible to anti-ballistic missile (ABM) systems. Consequently, an alternative sphere-cone RV to the Mk-2 was developed by General Electric.

This new RV was the Mk-6 which used a non-metallic ablative TPS, a nylon phenolic. This new TPS was so effective as a reentry heat shield that significantly reduced bluntness was possible. However, the Mk-6 was a huge RV with an entry mass of 3,360 kg, a length of 3.1 m and a half-angle of 12.5°. Subsequent advances in nuclear weapon and ablative TPS design allowed RVs to become significantly smaller with a further reduced bluntness ratio compared to the Mk-6. Since the 1960s, the sphere-cone has become the preferred geometry for modern ICBM RVs with typical half-angles being between 10° to 11°.

Reconnaissance satellite RVs (recovery vehicles) also used a sphere-cone shape and were the first American example of a non-munition entry vehicle (Discoverer-I, launched on 28 February 1959). The sphere-cone was later used for space exploration missions to other celestial bodies or for return from open space; e.g., Stardust probe. Unlike with military RVs, the advantage of the blunt body's lower TPS mass remained with space exploration entry vehicles like the Galileo Probe with a half-angle of 45° or the Viking aeroshell with a half-angle of 70°. Space exploration sphere-cone entry vehicles have landed on the surface or entered the atmospheres of Mars, Venus, Jupiter, and Titan.


=== Biconic ===

The biconic is a sphere-cone with an additional frustum attached. The biconic offers a significantly improved L/D ratio. A biconic designed for Mars aerocapture typically has an L/D of approximately 1.0 compared to an L/D of 0.368 for the Apollo-CM. The higher L/D makes a biconic shape better suited for transporting people to Mars due to the lower peak deceleration. Arguably, the most significant biconic ever flown was the Advanced Maneuverable Reentry Vehicle (AMaRV). Four AMaRVs were made by the McDonnell Douglas Corp. and represented a significant leap in RV sophistication. Three AMaRVs were launched by Minuteman-1 ICBMs on 20 December 1979, 8 October 1980 and 4 October 1981. AMaRV had an entry mass of approximately 470 kg, a nose radius of 2.34 cm, a forward-frustum half-angle of 10.4°, an inter-frustum radius of 14.6 cm, aft-frustum half-angle of 6°, and an axial length of 2.079 meters. No accurate diagram or picture of AMaRV has ever appeared in the open literature. However, a schematic sketch of an AMaRV-like vehicle along with trajectory plots showing hairpin turns has been published.AMaRV's attitude was controlled through a split body flap (also called a split-windward flap) along with two yaw flaps mounted on the vehicle's sides. Hydraulic actuation was used for controlling the flaps. AMaRV was guided by a fully autonomous navigation system designed for evading anti-ballistic missile (ABM) interception. The McDonnell Douglas DC-X (also a biconic) was essentially a scaled-up version of AMaRV. AMaRV and the DC-X also served as the basis for an unsuccessful proposal for what eventually became the Lockheed Martin X-33.


=== Non-axisymmetric shapes ===
Non-axisymmetric shapes have been used for manned entry vehicles. One example is the winged orbit vehicle that uses a delta wing for maneuvering during descent much like a conventional glider. This approach has been used by the American Space Shuttle and the Soviet Buran. The lifting body is another entry vehicle geometry and was used with the X-23 PRIME (Precision Recovery Including Maneuvering Entry) vehicle.


== Reentry heating ==

Objects entering an atmosphere from space at high velocities relative to the atmosphere will cause very high levels of heating. Reentry heating comes principally from two sources:
convective heating, of two types:
hot gas flow past the surface of the body and
catalytic chemical recombination reactions between the object surface and the atmospheric gases
radiative heating, from the energetic shock layer that forms in front and to the sides of the objectAs velocity increases, both convective and radiative heating increase. At very high speeds, radiative heating will come to quickly dominate the convective heat fluxes, as convective heating is proportional to the velocity cubed, while radiative heating is proportional to the eighth power of velocity. Radiative heating—which is highly wavelength dependent—thus predominates very early in atmospheric entry while convection predominates in the later phases.


=== Shock layer gas physics ===
At typical reentry temperatures, the air in the shock layer is both ionized and dissociated. This chemical dissociation necessitates various physical models to describe the shock layer's thermal and chemical properties. There are four basic physical models of a gas that are important to aeronautical engineers who design heat shields:


==== Perfect gas model ====
Almost all aeronautical engineers are taught the perfect (ideal) gas model during their undergraduate education. Most of the important perfect gas equations along with their corresponding tables and graphs are shown in NACA Report 1135. Excerpts from NACA Report 1135 often appear in the appendices of thermodynamics textbooks and are familiar to most aeronautical engineers who design supersonic aircraft.
The perfect gas theory is elegant and extremely useful for designing aircraft but assumes that the gas is chemically inert. From the standpoint of aircraft design, air can be assumed to be inert for temperatures less than 550 K at one atmosphere pressure. The perfect gas theory begins to break down at 550 K and is not usable at temperatures greater than 2,000 K. For temperatures greater than 2,000 K, a heat shield designer must use a real gas model.


==== Real (equilibrium) gas model ====
An entry vehicle's pitching moment can be significantly influenced by real-gas effects. Both the Apollo command module and the Space Shuttle were designed using incorrect pitching moments determined through inaccurate real-gas modelling. The Apollo-CM's trim-angle angle of attack was higher than originally estimated, resulting in a narrower lunar return entry corridor. The actual aerodynamic center of the Columbia was upstream from the calculated value due to real-gas effects. On Columbia's maiden flight (STS-1), astronauts John W. Young and Robert Crippen had some anxious moments during reentry when there was concern about losing control of the vehicle.An equilibrium real-gas model assumes that a gas is chemically reactive, but also assumes all chemical reactions have had time to complete and all components of the gas have the same temperature (this is called thermodynamic equilibrium). When air is processed by a shock wave, it is superheated by compression and chemically dissociates through many different reactions. Direct friction upon the reentry object is not the main cause of shock-layer heating. It is caused mainly from isentropic heating of the air molecules within the compression wave. Friction based entropy increases of the molecules within the wave also account for some heating. The distance from the shock wave to the stagnation point on the entry vehicle's leading edge is called shock wave stand off. An approximate rule of thumb for shock wave standoff distance is 0.14 times the nose radius. One can estimate the time of travel for a gas molecule from the shock wave to the stagnation point by assuming a free stream velocity of 7.8 km/s and a nose radius of 1 meter, i.e., time of travel is about 18 microseconds. This is roughly the time required for shock-wave-initiated chemical dissociation to approach chemical equilibrium in a shock layer for a 7.8 km/s entry into air during peak heat flux. Consequently, as air approaches the entry vehicle's stagnation point, the air effectively reaches chemical equilibrium thus enabling an equilibrium model to be usable. For this case, most of the shock layer between the shock wave and leading edge of an entry vehicle is chemically reacting and not in a state of equilibrium. The Fay-Riddell equation, which is of extreme importance towards modeling heat flux, owes its validity to the stagnation point being in chemical equilibrium. The time required for the shock layer gas to reach equilibrium is strongly dependent upon the shock layer's pressure. For example, in the case of the Galileo probe's entry into Jupiter's atmosphere, the shock layer was mostly in equilibrium during peak heat flux due to the very high pressures experienced (this is counterintuitive given the free stream velocity was 39 km/s during peak heat flux).
Determining the thermodynamic state of the stagnation point is more difficult under an equilibrium gas model than a perfect gas model. Under a perfect gas model, the ratio of specific heats (also called isentropic exponent, adiabatic index, gamma, or kappa) is assumed to be constant along with the gas constant. For a real gas, the ratio of specific heats can wildly oscillate as a function of temperature. Under a perfect gas model there is an elegant set of equations for determining thermodynamic state along a constant entropy stream line called the isentropic chain. For a real gas, the isentropic chain is unusable and a Mollier diagram would be used instead for manual calculation. However, graphical solution with a Mollier diagram is now considered obsolete with modern heat shield designers using computer programs based upon a digital lookup table (another form of Mollier diagram) or a chemistry based thermodynamics program. The chemical composition of a gas in equilibrium with fixed pressure and temperature can be determined through the Gibbs free energy method. Gibbs free energy is simply the total enthalpy of the gas minus its total entropy times temperature. A chemical equilibrium program normally does not require chemical formulas or reaction-rate equations. The program works by preserving the original elemental abundances specified for the gas and varying the different molecular combinations of the elements through numerical iteration until the lowest possible Gibbs free energy is calculated (a Newton-Raphson method is the usual numerical scheme). The data base for a Gibbs free energy program comes from spectroscopic data used in defining partition functions. Among the best equilibrium codes in existence is the program Chemical Equilibrium with Applications (CEA) which was written by Bonnie J. McBride and Sanford Gordon at NASA Lewis (now renamed ""NASA Glenn Research Center""). Other names for CEA are the ""Gordon and McBride Code"" and the ""Lewis Code"". CEA is quite accurate up to 10,000 K for planetary atmospheric gases, but unusable beyond 20,000 K (double ionization is not modelled). CEA can be downloaded from the Internet along with full documentation and will compile on Linux under the G77 Fortran compiler.


==== Real (non-equilibrium) gas model ====
A non-equilibrium real gas model is the most accurate model of a shock layer's gas physics, but is more difficult to solve than an equilibrium model. As of 1958, the simplest non-equilibrium model was the Lighthill-Freeman model. The Lighthill-Freeman model initially assumes a gas made up of a single diatomic species susceptible to only one chemical formula and its reverse; e.g., N2 ? N + N and N + N ? N2 (dissociation and recombination). Because of its simplicity, the Lighthill-Freeman model is a useful pedagogical tool, but is unfortunately too simple for modelling non-equilibrium air. Air is typically assumed to have a mole fraction composition of 0.7812 molecular nitrogen, 0.2095 molecular oxygen and 0.0093 argon. The simplest real gas model for air is the five species model, which is based upon N2, O2, NO, N, and O. The five species model assumes no ionization and ignores trace species like carbon dioxide.
When running a Gibbs free energy equilibrium program, the iterative process from the originally specified molecular composition to the final calculated equilibrium composition is essentially random and not time accurate. With a non-equilibrium program, the computation process is time accurate and follows a solution path dictated by chemical and reaction rate formulas. The five species model has 17 chemical formulas (34 when counting reverse formulas). The Lighthill-Freeman model is based upon a single ordinary differential equation and one algebraic equation. The five species model is based upon 5 ordinary differential equations and 17 algebraic equations. Because the 5 ordinary differential equations are tightly coupled, the system is numerically ""stiff"" and difficult to solve. The five species model is only usable for entry from low Earth orbit where entry velocity is approximately 7.8 km/s (28,000 km/h; 17,000 mph). For lunar return entry of 11 km/s, the shock layer contains a significant amount of ionized nitrogen and oxygen. The five-species model is no longer accurate and a twelve-species model must be used instead.
Atmospheric entry interface velocities on a Mars–Earth trajectory are on the order of 12 km/s (43,000 km/h; 27,000 mph).
Modeling high-speed Mars atmospheric entry—which involves a carbon dioxide, nitrogen and argon atmosphere—is even more complex requiring a 19-species model.An important aspect of modelling non-equilibrium real gas effects is radiative heat flux. If a vehicle is entering an atmosphere at very high speed (hyperbolic trajectory, lunar return) and has a large nose radius then radiative heat flux can dominate TPS heating. Radiative heat flux during entry into an air or carbon dioxide atmosphere typically comes from asymmetric diatomic molecules; e.g., cyanogen (CN), carbon monoxide, nitric oxide (NO), single ionized molecular nitrogen etc. These molecules are formed by the shock wave dissociating ambient atmospheric gas followed by recombination within the shock layer into new molecular species. The newly formed diatomic molecules initially have a very high vibrational temperature that efficiently transforms the vibrational energy into radiant energy; i.e., radiative heat flux. The whole process takes place in less than a millisecond which makes modelling a challenge. The experimental measurement of radiative heat flux (typically done with shock tubes) along with theoretical calculation through the unsteady Schrödinger equation are among the more esoteric aspects of aerospace engineering. Most of the aerospace research work related to understanding radiative heat flux was done in the 1960s, but largely discontinued after conclusion of the Apollo Program. Radiative heat flux in air was just sufficiently understood to ensure Apollo's success. However, radiative heat flux in carbon dioxide (Mars entry) is still barely understood and will require major research.


==== Frozen gas model ====
The frozen gas model describes a special case of a gas that is not in equilibrium. The name ""frozen gas"" can be misleading. A frozen gas is not ""frozen"" like ice is frozen water. Rather a frozen gas is ""frozen"" in time (all chemical reactions are assumed to have stopped). Chemical reactions are normally driven by collisions between molecules. If gas pressure is slowly reduced such that chemical reactions can continue then the gas can remain in equilibrium. However, it is possible for gas pressure to be so suddenly reduced that almost all chemical reactions stop. For that situation the gas is considered frozen.The distinction between equilibrium and frozen is important because it is possible for a gas such as air to have significantly different properties (speed-of-sound, viscosity etc.) for the same thermodynamic state; e.g., pressure and temperature. Frozen gas can be a significant issue in the wake behind an entry vehicle. During reentry, free stream air is compressed to high temperature and pressure by the entry vehicle's shock wave. Non-equilibrium air in the shock layer is then transported past the entry vehicle's leading side into a region of rapidly expanding flow that causes freezing. The frozen air can then be entrained into a trailing vortex behind the entry vehicle. Correctly modelling the flow in the wake of an entry vehicle is very difficult. Thermal protection shield (TPS) heating in the vehicle's afterbody is usually not very high, but the geometry and unsteadiness of the vehicle's wake can significantly influence aerodynamics (pitching moment) and particularly dynamic stability.


== Thermal protection systems ==

A thermal protection system, or TPS, is the barrier that protects a spacecraft during the searing heat of atmospheric reentry. A secondary goal may be to protect the spacecraft from the heat and cold of space while in orbit. Multiple approaches for the thermal protection of spacecraft are in use, among them ablative heat shields, passive cooling, and active cooling of spacecraft surfaces.


=== Ablative ===

The ablative heat shield functions by lifting the hot shock layer gas away from the heat shield's outer wall (creating a cooler boundary layer). The boundary layer comes from blowing of gaseous reaction products from the heat shield material and provides protection against all forms of heat flux. The overall process of reducing the heat flux experienced by the heat shield's outer wall by way of a boundary layer is called blockage. Ablation occurs at two levels in an ablative TPS: the outer surface of the TPS material chars, melts, and sublimes, while the bulk of the TPS material undergoes pyrolysis and expels product gases. The gas produced by pyrolysis is what drives blowing and causes blockage of convective and catalytic heat flux. Pyrolysis can be measured in real time using thermogravimetric analysis, so that the ablative performance can be evaluated. Ablation can also provide blockage against radiative heat flux by introducing carbon into the shock layer thus making it optically opaque. Radiative heat flux blockage was the primary thermal protection mechanism of the Galileo Probe TPS material (carbon phenolic). Carbon phenolic was originally developed as a rocket nozzle throat material (used in the Space Shuttle Solid Rocket Booster) and for reentry-vehicle nose tips.
Early research on ablation technology in the USA was centered at NASA's Ames Research Center located at Moffett Field, California. Ames Research Center was ideal, since it had numerous wind tunnels capable of generating varying wind velocities. Initial experiments typically mounted a mock-up of the ablative material to be analyzed within a hypersonic wind tunnel. Testing of ablative materials occurs at the Ames Arc Jet Complex. Many spacecraft thermal protection systems have been tested in this facility, including the Apollo, space shuttle, and Orion heat shield materials.

The thermal conductivity of a particular TPS material is usually proportional to the material's density. Carbon phenolic is a very effective ablative material, but also has high density which is undesirable. If the heat flux experienced by an entry vehicle is insufficient to cause pyrolysis then the TPS material's conductivity could allow heat flux conduction into the TPS bondline material thus leading to TPS failure. Consequently, for entry trajectories causing lower heat flux, carbon phenolic is sometimes inappropriate and lower-density TPS materials such as the following examples can be better design choices:


==== Super light-weight ablator ====
SLA in SLA-561V stands for super light-weight ablator. SLA-561V is a proprietary ablative made by Lockheed Martin that has been used as the primary TPS material on all of the 70° sphere-cone entry vehicles sent by NASA to Mars other than the Mars Science Laboratory (MSL). SLA-561V begins significant ablation at a heat flux of approximately 110 W/cm2, but will fail for heat fluxes greater than 300 W/cm2. The MSL aeroshell TPS is currently designed to withstand a peak heat flux of 234 W/cm2. The peak heat flux experienced by the Viking 1 aeroshell which landed on Mars was 21 W/cm2. For Viking 1, the TPS acted as a charred thermal insulator and never experienced significant ablation. Viking 1 was the first Mars lander and based upon a very conservative design. The Viking aeroshell had a base diameter of 3.54 meters (the largest used on Mars until Mars Science Laboratory). SLA-561V is applied by packing the ablative material into a honeycomb core that is pre-bonded to the aeroshell's structure thus enabling construction of a large heat shield.


==== Phenolic-impregnated carbon ablator ====

Phenolic-impregnated carbon ablator (PICA), a carbon fiber preform impregnated in phenolic resin, is a modern TPS material and has the advantages of low density (much lighter than carbon phenolic) coupled with efficient ablative ability at high heat flux. It is a good choice for ablative applications such as high-peak-heating conditions found on sample-return missions or lunar-return missions. PICA's thermal conductivity is lower than other high-heat-flux-ablative materials, such as conventional carbon phenolics.PICA was patented by NASA Ames Research Center in the 1990s and was the primary TPS material for the Stardust aeroshell. The Stardust sample-return capsule was the fastest man-made object ever to reenter Earth's atmosphere (12.4 km/s (28,000 mph) at 135 km altitude). This was faster than the Apollo mission capsules and 70% faster than the Shuttle. PICA was critical for the viability of the Stardust mission, which returned to Earth in 2006. Stardust's heat shield (0.81 m base diameter) was made of one monolithic piece sized to withstand a nominal peak heating rate of 1.2 kW/cm2. A PICA heat shield was also used for the Mars Science Laboratory entry into the Martian atmosphere.


===== PICA-X =====
An improved and easier to produce version called PICA-X was developed by SpaceX in 2006–2010 for the Dragon space capsule. The first reentry test of a PICA-X heat shield was on the Dragon C1 mission on 8 December 2010. The PICA-X heat shield was designed, developed and fully qualified by a small team of a dozen engineers and technicians in less than four years.
PICA-X is ten times less expensive to manufacture than the NASA PICA heat shield material.


===== PICA-3 =====
A second enhanced version of PICA—called PICA-3—was developed by SpaceX during the mid-2010s.  It was first flight tested on the Crew Dragon spacecraft in 2019 during the flight demonstration mission, in April 2019, and put into regular service on that spacecraft in 2020.


==== SIRCA ====

Silicone-impregnated reusable ceramic ablator (SIRCA) was also developed at NASA Ames Research Center and was used on the Backshell Interface Plate (BIP) of the Mars Pathfinder and Mars Exploration Rover (MER) aeroshells. The BIP was at the attachment points between the aeroshell's backshell (also called the afterbody or aft cover) and the cruise ring (also called the cruise stage). SIRCA was also the primary TPS material for the unsuccessful Deep Space 2 (DS/2) Mars impactor probes with their 0.35-meter-base-diameter (1.1 ft) aeroshells. SIRCA is a monolithic, insulating material that can provide thermal protection through ablation. It is the only TPS material that can be machined to custom shapes and then applied directly to the spacecraft. There is no post-processing, heat treating, or additional coatings required (unlike Space Shuttle tiles). Since SIRCA can be machined to precise shapes, it can be applied as tiles, leading edge sections, full nose caps, or in any number of custom shapes or sizes. As of 1996, SIRCA had been demonstrated in backshell interface applications, but not yet as a forebody TPS material.


==== AVCOAT ====
AVCOAT is a NASA-specified ablative heat shield, a glass-filled epoxy–novolac system.NASA originally used it for the Apollo capsule in the 1960s, and then utilized the material for its next-generation beyond low-Earth-orbit Orion spacecraft, slated to fly in the late 2010s. The Avcoat to be used on Orion has been reformulated to meet environmental legislation that has been passed since the end of Apollo.


=== Thermal soak ===

Thermal soak is a part of almost all TPS schemes. For example, an ablative heat shield loses most of its thermal protection effectiveness when the outer wall temperature drops below the minimum necessary for pyrolysis. From that time to the end of the heat pulse, heat from the shock layer convects into the heat shield's outer wall and would eventually conduct to the payload. This outcome is prevented by ejecting the heat shield (with its heat soak) prior to the heat conducting to the inner wall.
Typical Space Shuttle TPS tiles (LI-900) have remarkable thermal protection properties. An LI-900 tile exposed to a temperature of 1,000 K on one side will remain merely warm to the touch on the other side. However, they are relatively brittle and break easily, and cannot survive in-flight rain.


=== Passively cooled ===
In some early ballistic missile RVs (e.g., the Mk-2 and the sub-orbital Mercury spacecraft), radiatively cooled TPS were used to initially absorb heat flux during the heat pulse, and, then, after the heat pulse, radiate and convect the stored heat back into the atmosphere. However, the earlier version of this technique required a considerable quantity of metal TPS (e.g., titanium, beryllium, copper, etc.). Modern designers prefer to avoid this added mass by using ablative and thermal-soak TPS instead.
Thermal protection systems relying on emissivity use high emissivity coatings (HECs) to facilitate radiative cooling, while an underlying porous ceramic layer serves to protect the structure from high surface temperatures. High thermally stable emissivity values coupled with low thermal conductivity are key to the functionality of such systems.Radiatively cooled TPS can be found on modern entry vehicles, but reinforced carbon–carbon (RCC) (also called carbon–carbon) is normally used instead of metal. RCC was the TPS material on the Space Shuttle's nose cone and wing leading edges, and was also proposed as the leading-edge material for the X-33. Carbon is the most refractory material known, with a one-atmosphere sublimation temperature of 3,825 °C (6,917 °F) for graphite. This high temperature made carbon an obvious choice as a radiatively cooled TPS material. Disadvantages of RCC are that it is currently expensive to manufacture, is heavy, and lacks robust impact resistance.Some high-velocity aircraft, such as the SR-71 Blackbird and Concorde, deal with heating similar to that experienced by spacecraft, but at much lower intensity, and for hours at a time. Studies of the SR-71's titanium skin revealed that the metal structure was restored to its original strength through annealing due to aerodynamic heating. In the case of the Concorde, the aluminium nose was permitted to reach a maximum operating temperature of 127 °C (261 °F) (approximately 180 °C (324 °F) warmer than the normally sub-zero, ambient air); the metallurgical implications (loss of temper) that would be associated with a higher peak temperature were the most significant factors determining the top speed of the aircraft.
A radiatively cooled TPS for an entry vehicle is often called a hot-metal TPS. Early TPS designs for the Space Shuttle called for a hot-metal TPS based upon a nickel superalloy (dubbed René 41) and titanium shingles. This Shuttle TPS concept was rejected, because it was believed a silica tile-based TPS would involve lower development and manufacturing costs. A nickel superalloy-shingle TPS was again proposed for the unsuccessful X-33 single-stage-to-orbit (SSTO) prototype.Recently, newer radiatively cooled TPS materials have been developed that could be superior to RCC. Known as Ultra-High Temperature Ceramics, they were developed for the prototype vehicle Slender Hypervelocity Aerothermodynamic Research Probe (SHARP). These TPS materials are based on zirconium diboride and hafnium diboride. SHARP TPS have suggested performance improvements allowing for sustained Mach 7 flight at sea level, Mach 11 flight at 100,000-foot (30,000 m) altitudes, and significant improvements for vehicles designed for continuous hypersonic flight. SHARP TPS materials enable sharp leading edges and nose cones to greatly reduce drag for airbreathing combined-cycle-propelled spaceplanes and lifting bodies. SHARP materials have exhibited effective TPS characteristics from zero to more than 2,000 °C (3,630 °F), with melting points over 3,500 °C (6,330 °F). They are structurally stronger than RCC, and, thus, do not require structural reinforcement with materials such as Inconel. SHARP materials are extremely efficient at reradiating absorbed heat, thus eliminating the need for additional TPS behind and between the SHARP materials and conventional vehicle structure. NASA initially funded (and discontinued) a multi-phase R&D program through the University of Montana in 2001 to test SHARP materials on test vehicles.


=== Actively cooled ===
Various advanced reusable spacecraft and hypersonic aircraft designs have been proposed to employ heat shields made from temperature-resistant metal alloys that incorporate a refrigerant or cryogenic fuel circulating through them, and one such spacecraft design is currently under development.
Such a TPS concept was proposed for the X-30 National Aerospace Plane (NASP). The NASP was supposed to have been a scramjet powered hypersonic aircraft, but failed in development.
SpaceX is currently developing an actively cooled heat shield for its Starship spacecraft where a part of the thermal protection system will be a transpirationally cooled outer-skin design for the reentering spaceship.In the early 1960s various TPS systems were proposed to use water or other cooling liquid sprayed into the shock layer, or passed through channels in the heat shield. Advantages included the possibility of more all-metal designs which would be cheaper to develop, be more rugged, and eliminate the need for classified technology. The disadvantages are increased weight and complexity, and lower reliability. The concept has never been flown, but a similar technology (the plug nozzle) did undergo extensive ground testing.


== Feathered reentry ==
In 2004, aircraft designer Burt Rutan demonstrated the feasibility of a shape-changing airfoil for reentry with the sub-orbital SpaceShipOne. The wings on this craft rotate upward into the feathered configuration that provides a shuttlecock effect. Thus SpaceShipOne achieves much more aerodynamic drag on reentry while not experiencing significant thermal loads.
The configuration increases drag, as the craft is now less streamlined and results in more atmospheric gas particles hitting the spacecraft at higher altitudes than otherwise. The aircraft thus slows down more in higher atmospheric layers which is the key to efficient reentry. Secondly, the aircraft will automatically orient itself in this state to a high drag attitude.However, the velocity attained by SpaceShipOne prior to reentry is much lower than that of an orbital spacecraft, and engineers, including Rutan, recognize that a feathered reentry technique is not suitable for return from orbit.
On 4 May 2011, the first test on the SpaceShipTwo of the feathering mechanism was made during a glideflight after release
from the White Knight Two. Premature deployment of the feathering system was responsible for the 2014 VSS Enterprise crash, in which the aircraft disintegrated, killing the co-pilot.
The feathered reentry was first described by Dean Chapman of NACA in 1958. In the section of his report on Composite Entry, Chapman described a solution to the problem using a high-drag device:

It may be desirable to combine lifting and nonlifting entry in order to achieve some advantages... For landing maneuverability it obviously is advantageous to employ a lifting vehicle. The total heat absorbed by a lifting vehicle, however, is much higher than for a nonlifting vehicle... Nonlifting vehicles can more easily be constructed... by employing, for example, a large, light drag device... The larger the device, the smaller is the heating rate.
Nonlifting vehicles with shuttlecock stability are advantageous also from the viewpoint of minimum control requirements during entry.
... an evident composite type of entry, which combines some of the desirable features of lifting and nonlifting trajectories, would be to enter first without lift but with a... drag device; then, when the velocity is reduced to a certain value... the device is jettisoned or retracted, leaving a lifting vehicle... for the remainder of the descent.

The North American X-15 used a similar mechanism.


== Inflatable heat shield reentry ==
Deceleration for atmospheric reentry, especially for higher-speed Mars-return missions, benefits from maximizing ""the drag area of the entry system. The larger the diameter of the aeroshell, the bigger the payload can be."" An inflatable aeroshell provides one alternative for enlarging the drag area with a low-mass design.


=== Non-US ===
Such an inflatable shield/aerobrake was designed for the penetrators of Mars 96 mission. Since the mission failed due to the launcher malfunction, the NPO Lavochkin and DASA/ESA have designed a mission for Earth orbit. The Inflatable Reentry and Descent Technology (IRDT) demonstrator was launched on Soyuz-Fregat on 8 February 2000. The inflatable shield was designed as a cone with two stages of inflation. Although the second stage of the shield failed to inflate, the demonstrator survived the orbital reentry and was recovered. The subsequent missions flown on the Volna rocket failed due to launcher failure.


=== NASA IRVE ===
NASA launched an inflatable heat shield experimental spacecraft on 17 August 2009 with the successful first test flight of the Inflatable Re-entry Vehicle Experiment (IRVE). The heat shield had been vacuum-packed into a 15-inch-diameter (38 cm) payload shroud and launched on a Black Brant 9 sounding rocket from NASA's Wallops Flight Facility on Wallops Island, Virginia. ""Nitrogen inflated the 10-foot-diameter (3.0 m) heat shield, made of several layers of silicone-coated [Kevlar] fabric, to a mushroom shape in space several minutes after liftoff."" The rocket apogee was at an altitude of 131 miles (211 km) where it began its descent to supersonic speed. Less than a minute later the shield was released from its cover to inflate at an altitude of 124 miles (200 km). The inflation of the shield took less than 90 seconds.


=== NASA HIAD ===
Following the success of the initial IRVE experiments, NASA developed the concept into the more ambitious Hypersonic Inflatable Aerodynamic Decelerator (HIAD). The current design is shaped like a shallow cone, with the structure built up as a stack of circular inflated tubes of gradually increasing major diameter. The forward (convex) face of the cone is covered with a flexible thermal protection system robust enough to withstand the stresses of atmospheric entry (or reentry).In 2012, a HIAD was tested as Inflatable Reentry Vehicle Experiment 3 (IRVE-3) using a sub-orbital sounding rocket, and worked.In 2020 there were plans to launch in 2022 a 6 m inflatable as Low-Earth Orbit Flight Test of an Inflatable Decelerator (LOFTID).See also Low-Density Supersonic Decelerator, a NASA project with tests in 2014 & 2015.


== Entry vehicle design considerations ==
There are four critical parameters considered when designing a vehicle for atmospheric entry:
Peak heat flux
Heat load
Peak deceleration
Peak dynamic pressurePeak heat flux and dynamic pressure selects the TPS material. Heat load selects the thickness of the TPS material stack. Peak deceleration is of major importance for manned missions. The upper limit for manned return to Earth from low Earth orbit (LEO) or lunar return is 10g. For Martian atmospheric entry after long exposure to zero gravity, the upper limit is 4g. Peak dynamic pressure can also influence the selection of the outermost TPS material if spallation is an issue.
Starting from the principle of conservative design, the engineer typically considers two worst-case trajectories, the undershoot and overshoot trajectories. The overshoot trajectory is typically defined as the shallowest-allowable entry velocity angle prior to atmospheric skip-off. The overshoot trajectory has the highest heat load and sets the TPS thickness. The undershoot trajectory is defined by the steepest allowable trajectory. For manned missions the steepest entry angle is limited by the peak deceleration. The undershoot trajectory also has the highest peak heat flux and dynamic pressure. Consequently, the undershoot trajectory is the basis for selecting the TPS material. There is no ""one size fits all"" TPS material. A TPS material that is ideal for high heat flux may be too conductive (too dense) for a long duration heat load. A low-density TPS material might lack the tensile strength to resist spallation if the dynamic pressure is too high. A TPS material can perform well for a specific peak heat flux, but fail catastrophically for the same peak heat flux if the wall pressure is significantly increased (this happened with NASA's R-4 test spacecraft). Older TPS materials tend to be more labor-intensive and expensive to manufacture compared to modern materials. However, modern TPS materials often lack the flight history of the older materials (an important consideration for a risk-averse designer).
Based upon Allen and Eggers discovery, maximum aeroshell bluntness (maximum drag) yields minimum TPS mass. Maximum bluntness (minimum ballistic coefficient) also yields a minimal terminal velocity at maximum altitude (very important for Mars EDL, but detrimental for military RVs). However, there is an upper limit to bluntness imposed by aerodynamic stability considerations based upon shock wave detachment. A shock wave will remain attached to the tip of a sharp cone if the cone's half-angle is below a critical value. This critical half-angle can be estimated using perfect gas theory (this specific aerodynamic instability occurs below hypersonic speeds). For a nitrogen atmosphere (Earth or Titan), the maximum allowed half-angle is approximately 60°. For a carbon dioxide atmosphere (Mars or Venus), the maximum-allowed half-angle is approximately 70°. After shock wave detachment, an entry vehicle must carry significantly more shocklayer gas around the leading edge stagnation point (the subsonic cap). Consequently, the aerodynamic center moves upstream thus causing aerodynamic instability. It is incorrect to reapply an aeroshell design intended for Titan entry (Huygens probe in a nitrogen atmosphere) for Mars entry (Beagle 2 in a carbon dioxide atmosphere). Prior to being abandoned, the Soviet Mars lander program achieved one successful landing (Mars 3), on the second of three entry attempts (the others were Mars 2 and Mars 6). The Soviet Mars landers were based upon a 60° half-angle aeroshell design.
A 45° half-angle sphere-cone is typically used for atmospheric probes (surface landing not intended) even though TPS mass is not minimized. The rationale for a 45° half-angle is to have either aerodynamic stability from entry-to-impact (the heat shield is not jettisoned) or a short-and-sharp heat pulse followed by prompt heat shield jettison. A 45° sphere-cone design was used with the DS/2 Mars impactor and Pioneer Venus probes.


== Notable atmospheric entry accidents ==

Not all atmospheric reentries have been successful and some have resulted in significant disasters.

Voskhod 2 – The service module failed to detach for some time, but the crew survived.
Soyuz 1 – The attitude control system failed while still in orbit and later parachutes got entangled during the emergency landing sequence (entry, descent, and landing (EDL) failure). Lone cosmonaut Vladimir Mikhailovich Komarov died.
Soyuz 5 – The service module failed to detach, but the crew survived.
Soyuz 11 – After tri-module separation, a valve was weakened by the blast and failed on reentry. The cabin depressurized killing all three crew members.
Mars Polar Lander – Failed during EDL. The failure was believed to be the consequence of a software error. The precise cause is unknown for lack of real-time telemetry.
Space Shuttle Columbia
STS-1 – a combination of launch damage, protruding gap filler, and tile installation error resulted in serious damage to the orbiter, only some of which the crew was privy to. Had the crew known the true extent of the damage before attempting reentry, they would have flown the shuttle to a safe altitude and then bailed out. Nevertheless, reentry was successful, and the orbiter proceeded to a normal landing.
STS-107 – The failure of an RCC panel on a wing leading edge caused by debris impact at launch led to breakup of the orbiter on reentry resulting in the deaths of all seven crew members.
Genesis – The parachute failed to deploy due to a G-switch having been installed backwards (a similar error delayed parachute deployment for the Galileo Probe). Consequently, the Genesis entry vehicle crashed into the desert floor. The payload was damaged, but most scientific data were recoverable.
Soyuz TMA-11 – The Soyuz propulsion module failed to separate properly; fallback ballistic reentry was executed that subjected the crew to accelerations of about 8 standard gravities (78 m/s2). The crew survived.


== Uncontrolled and unprotected reentries ==
Of satellites that reenter, approximately 10–40% of the mass of the object is likely to reach the surface of the Earth. On average, about one catalogued object reenters per day.Due to the Earth's surface being primarily water, most objects that survive reentry land in one of the world's oceans. The estimated chances that a given person will get hit and injured during his/her lifetime is around 1 in a trillion.On January 24, 1978, the Soviet Kosmos 954 (3,800 kilograms [8,400 lb]) reentered and crashed near Great Slave Lake in the Northwest Territories of Canada. The satellite was nuclear-powered and left radioactive debris near its impact site.On July 11, 1979, the US Skylab space station (77,100 kilograms [170,000 lb]) reentered and spread debris across the Australian Outback. The reentry was a major media event largely due to the Cosmos 954 incident, but not viewed as much as a potential disaster since it did not carry toxic nuclear or hydrazine fuel. NASA had originally hoped to use a Space Shuttle mission to either extend its life or enable a controlled reentry, but delays in the Shuttle program, plus unexpectedly high solar activity, made this impossible.On February 7, 1991, the Soviet Salyut 7 space station (19,820 kilograms [43,700 lb]), with the Kosmos 1686 module (20,000 kilograms [44,000 lb]) attached, reentered and scattered debris over the town of Capitán Bermúdez, Argentina. The station had been boosted to a higher orbit in August 1986 in an attempt to keep it up until 1994, but in a scenario similar to Skylab, the planned Buran shuttle was cancelled and high solar activity caused it to come down sooner than expected.
On September 7, 2011, NASA announced the impending uncontrolled reentry of the Upper Atmosphere Research Satellite (6,540 kilograms [14,420 lb]) and noted that there was a small risk to the public. The decommissioned satellite reentered the atmosphere on September 24, 2011, and some pieces are presumed to have crashed into the South Pacific Ocean over a debris field 500 miles (800 km) long.On April 1, 2018, the Chinese Tiangong-1 space station (8,510 kilograms [18,760 lb]) reentered over the Pacific Ocean, halfway between Australia and South America. The China Manned Space Engineering Office had intended to control the reentry, but lost telemetry and control in March 2017.On May 11, 2020, the core stage of Chinese Long March 5B (COSPAR ID 2020-027C) weighing roughly 20,000 kilograms [44,000 lb]) made an uncontrolled reentry over the Atlantic Ocean, near West African coast. Few pieces of rocket debris reportedly survived reentry and fell over at least two villages in Ivory Coast.It is expected that the Cruise Mass Balance Devices (CMBDs) from the Mars 2020 mission, which are ejected prior to the spacecraft entering the atmosphere, will survive re-entry and impact the surface on Thursday 18 February, 2021. The CMBDs are 77 kg tungsten blocks used to adjust the spacecraft's trajectory prior to entry. The Science Team of another NASA mission, InSight, announced in early 2021 that they would attempt to detect the seismic waves from this impact event.  


=== Deorbit disposal ===
Salyut 1, the world's first space station, was deliberately de-orbited into the Pacific Ocean in 1971 following the Soyuz 11 accident. Its successor, Salyut 6, was de-orbited in a controlled manner as well.
On June 4, 2000 the Compton Gamma Ray Observatory was deliberately de-orbited after one of its gyroscopes failed. The debris that did not burn up fell harmlessly into the Pacific Ocean. The observatory was still operational, but the failure of another gyroscope would have made de-orbiting much more difficult and dangerous. With some controversy, NASA decided in the interest of public safety that a controlled crash was preferable to letting the craft come down at random.
In 2001, the Russian Mir space station was deliberately de-orbited, and broke apart in the fashion expected by the command center during atmospheric reentry. Mir entered the Earth's atmosphere on March 23, 2001, near Nadi, Fiji, and fell into the South Pacific Ocean.
On February 21, 2008, a disabled U.S. spy satellite, USA-193, was hit at an altitude of approximately 246 kilometers (153 mi) with an SM-3 missile fired from the U.S. Navy cruiser Lake Erie off the coast of Hawaii. The satellite was inoperative, having failed to reach its intended orbit when it was launched in 2006. Due to its rapidly deteriorating orbit it was destined for uncontrolled reentry within a month. U.S. Department of Defense expressed concern that the 1,000-pound (450 kg) fuel tank containing highly toxic hydrazine might survive reentry to reach the Earth's surface intact. Several governments including those of Russia, China, and Belarus protested the action as a thinly-veiled demonstration of US anti-satellite capabilities. China had previously caused an international incident when it tested an anti-satellite missile in 2007.

		


== Successful atmospheric reentries from orbital velocities ==
Manned orbital reentry, by country/governmental entity

 China – Shenzhou
 Soviet Union/ Russia – Vostok, Voskhod, Soyuz
 United States – Mercury, Gemini, Apollo, Space ShuttleManned orbital reentry, by commercial entity

SpaceX – Dragon 2Unmanned orbital reentry, by country/governmental entity

 China
 European Space Agency
 India / Indian Space Research Organisation
 Japan
 Soviet Union/ Russia
 United StatesUnmanned orbital reentry, by commercial entity

SpaceX – Dragon


== Selected atmospheric reentries ==
This list includes some notable atmospheric entries in which the spacecraft was not intended to be recovered, but was destroyed in the atmosphere.


== See also ==


== Notes and references ==


== Further reading ==
Launius, Roger D.; Jenkins, Dennis R. (October 10, 2012). Coming Home: Reentry and Recovery from Space. NASA. ISBN 9780160910647. OCLC 802182873. Retrieved August 21, 2014.
Martin, John J. (1966). Atmospheric Entry – An Introduction to Its Science and Engineering. Old Tappan, New Jersey: Prentice-Hall.
Regan, Frank J. (1984). Re-Entry Vehicle Dynamics (AIAA Education Series). New York: American Institute of Aeronautics and Astronautics, Inc. ISBN 978-0-915928-78-1.
Etkin, Bernard (1972). Dynamics of Atmospheric Flight. New York: John Wiley & Sons, Inc. ISBN 978-0-471-24620-6.
Vincenti, Walter G.; Kruger Jr, Charles H. (1986). Introduction to Physical Gas Dynamics. Malabar, Florida: Robert E. Krieger Publishing Co. ISBN 978-0-88275-309-6.
Hansen, C. Frederick (1976). Molecular Physics of Equilibrium Gases, A Handbook for Engineers. NASA. Bibcode:1976mpeg.book.....H. NASA SP-3096.
Hayes, Wallace D.; Probstein, Ronald F. (1959). Hypersonic Flow Theory. New York and London: Academic Press. A revised version of this classic text has been reissued as an inexpensive paperback: Hayes, Wallace D. (1966). Hypersonic Inviscid Flow. Mineola, New York: Dover Publications. ISBN 978-0-486-43281-6. reissued in 2004
Anderson, John D. Jr. (1989). Hypersonic and High Temperature Gas Dynamics. New York: McGraw-Hill, Inc. ISBN 978-0-07-001671-2.


== External links ==
Aerocapture Mission Analysis Tool (AMAT) provides preliminary mission analysis and simulation capability for atmospheric entry vehicles at various Solar System destinations.
Center for Orbital and Reentry Debris Studies (The Aerospace Corporation)
Apollo Atmospheric Entry Phase, 1968, NASA Mission Planning and Analysis Division, Project Apollo. video (25:14).
Buran's heat shield
Encyclopedia Astronautica article on the history of space rescue crafts, including some reentry craft designs.","pandas(index=1, _1=1, text='atmospheric entry is the movement of an object from outer space into and through the gases of an atmosphere of a planet, dwarf planet, or natural satellite. there are two main types of atmospheric entry: uncontrolled entry, such as the entry of astronomical objects, space debris, or bolides; and controlled entry (or reentry) of a spacecraft capable of being navigated or following a predetermined course. technologies and procedures allowing the controlled atmospheric entry, descent, and landing of spacecraft are collectively termed as edl.  objects entering an atmosphere experience atmospheric drag, which puts mechanical stress on the object, and aerodynamic heating—caused mostly by compression of the air in front of the object, but also by drag. these forces can cause loss of mass (ablation) or even complete disintegration of smaller objects, and objects with lower compressive strength can explode. crewed space vehicles must be slowed to subsonic speeds before parachutes or air brakes may be deployed. such vehicles have kinetic energies typically between 50 and 1,800 megajoules, and atmospheric dissipation is the only way of expending the kinetic energy. the amount of rocket fuel required to slow the vehicle would be nearly equal to the amount used to accelerate it initially, and it is thus highly impractical to use retro rockets for the entire earth reentry procedure. while the high temperature generated at the surface of the heat shield is due to adiabatic compression, the vehicle\'s kinetic energy is ultimately lost to gas friction (viscosity) after the vehicle has passed by. other smaller energy losses include black-body radiation directly from the hot gases and chemical reactions between ionized gases. ballistic warheads and expendable vehicles do not require slowing at reentry, and in fact, are made streamlined so as to maintain their speed. furthermore, slow-speed returns to earth from near-space such as parachute jumps from balloons do not require heat shielding because the gravitational acceleration of an object starting at relative rest from within the atmosphere itself (or not far above it) cannot create enough velocity to cause significant atmospheric heating. for earth, atmospheric entry occurs by convention at the kármán line at an altitude of 100 km (62 miles; 54 nautical miles) above the surface, while at venus atmospheric entry occurs at 250 km (160 mi; 130 nmi) and at mars atmospheric entry at about 80 km (50 mi; 43 nmi). uncontrolled objects reach high velocities while accelerating through space toward the earth under the influence of earth\'s gravity, and are slowed by friction upon encountering earth\'s atmosphere. meteors are also often travelling quite fast relative to the earth simply because their own orbital path is different from that of the earth before they encounter earth\'s gravity well. most controlled objects enter at hypersonic speeds due to their sub-orbital (e.g., intercontinental ballistic missile reentry vehicles), orbital (e.g., the soyuz), or unbounded (e.g., meteors) trajectories. various advanced technologies have been developed to enable atmospheric reentry and flight at extreme velocities. an alternative low-velocity method of controlled atmospheric entry is buoyancy which is suitable for planetary entry where thick atmospheres, strong gravity, or both factors complicate high-velocity hyperbolic entry, such as the atmospheres of venus, titan and the gas giants.   == history ==  the concept of the ablative heat shield was described as early as 1920 by robert goddard: ""in the case of meteors, which enter the atmosphere with speeds as high as 30 miles (48 km) per second, the interior of the meteors remains cold, and the erosion is due, to a large extent, to chipping or cracking of the suddenly heated surface. for this reason, if the outer surface of the apparatus were to consist of layers of a very infusible hard substance with layers of a poor heat conductor between, the surface would not be eroded to any considerable extent, especially as the velocity of the apparatus would not be nearly so great as that of the average meteor.""practical development of reentry systems began as the range, and reentry velocity of ballistic missiles increased. for early short-range missiles, like the v-2, stabilization and aerodynamic stress were important issues (many v-2s broke apart during reentry), but heating was not a serious problem. medium-range missiles like the soviet r-5, with a 1,200-kilometer (650-nautical-mile) range, required ceramic composite heat shielding on separable reentry vehicles (it was no longer possible for the entire rocket structure to survive reentry). the first icbms, with ranges of 8,000 to 12,000 km (4,300 to 6,500 nmi), were only possible with the development of modern ablative heat shields and blunt-shaped vehicles. in the united states, this technology was pioneered by h. julian allen and a. j. eggers jr. of the national advisory committee for aeronautics (naca) at ames research center. in 1951, they made the counterintuitive discovery that a blunt shape (high drag) made the most effective heat shield. from simple engineering principles, allen and eggers showed that the heat load experienced by an entry vehicle was inversely proportional to the drag coefficient; i.e., the greater the drag, the less the heat load. if the reentry vehicle is made blunt, air cannot ""get out of the way"" quickly enough, and acts as an air cushion to push the shock wave and heated shock layer forward (away from the vehicle). since most of the hot gases are no longer in direct contact with the vehicle, the heat energy would stay in the shocked gas and simply move around the vehicle to later dissipate into the atmosphere. the allen and eggers discovery, though initially treated as a military secret, was eventually published in 1958.   == terminology, definitions and jargon == over the decades since the 1950s, a rich technical jargon has grown around the engineering of vehicles designed to enter planetary atmospheres. it is recommended that the reader review the jargon glossary before continuing with this article on atmospheric reentry. when atmospheric entry is part of a spacecraft landing or recovery, particularly on a planetary body other than earth, entry is part of a phase referred to as entry, descent, and landing, or edl. when the atmospheric entry returns to the same body that the vehicle had launched from, the event is referred to as reentry (almost always referring to earth entry). the fundamental design objective in atmospheric entry of a spacecraft is to dissipate the energy of a spacecraft that is traveling at hypersonic speed as it enters an atmosphere such that equipment, cargo, and any passengers are slowed and land near a specific destination on the surface at zero velocity while keeping stresses on the spacecraft and any passengers within acceptable limits. this may be accomplished by propulsive or aerodynamic (vehicle characteristics or parachute) means, or by some combination.   == entry vehicle shapes ==  there are several basic shapes used in designing entry vehicles: salyut 1, the world\'s first space station, was deliberately de-orbited into the pacific ocean in 1971 following the soyuz 11 accident. its successor, salyut 6, was de-orbited in a controlled manner as well. on june 4, 2000 the compton gamma ray observatory was deliberately de-orbited after one of its gyroscopes failed. the debris that did not burn up fell harmlessly into the pacific ocean. the observatory was still operational, but the failure of another gyroscope would have made de-orbiting much more difficult and dangerous. with some controversy, nasa decided in the interest of public safety that a controlled crash was preferable to letting the craft come down at random. in 2001, the russian mir space station was deliberately de-orbited, and broke apart in the fashion expected by the command center during atmospheric reentry. mir entered the earth\'s atmosphere on march 23, 2001, near nadi, fiji, and fell into the south pacific ocean. on february 21, 2008, a disabled u.s. spy satellite, usa-193, was hit at an altitude of approximately 246 kilometers (153 mi) with an sm-3 missile fired from the u.s. navy cruiser lake erie off the coast of hawaii. the satellite was inoperative, having failed to reach its intended orbit when it was launched in 2006. due to its rapidly deteriorating orbit it was destined for uncontrolled reentry within a month. u.s. department of defense expressed concern that the 1,000-pound (450 kg) fuel tank containing highly toxic hydrazine might survive reentry to reach the earth\'s surface intact. several governments including those of russia, china, and belarus protested the action as a thinly-veiled demonstration of us anti-satellite capabilities. china had previously caused an international incident when it tested an anti-satellite missile in 2007.       == successful atmospheric reentries from orbital velocities == manned orbital reentry, by country/governmental entity  china – shenzhou soviet union/ russia – vostok, voskhod, soyuz united states – mercury, gemini, apollo, space shuttlemanned orbital reentry, by commercial entity  spacex – dragon 2unmanned orbital reentry, by country/governmental entity  china european space agency india / indian space research organisation japan soviet union/ russia united statesunmanned orbital reentry, by commercial entity  spacex – dragon   == selected atmospheric reentries == this list includes some notable atmospheric entries in which the spacecraft was not intended to be recovered, but was destroyed in the atmosphere.   == see also ==   == notes and references ==   == further reading == launius, roger d.; jenkins, dennis r. (october 10, 2012). coming home: reentry and recovery from space. nasa. isbn 9780160910647. oclc 802182873. retrieved august 21, 2014. martin, john j. (1966). atmospheric entry – an introduction to its science and engineering. old tappan, new jersey: prentice-hall. regan, frank j. (1984). re-entry vehicle dynamics (aiaa education series). new york: american institute of aeronautics and astronautics, inc. isbn 978-0-915928-78-1. etkin, bernard (1972). dynamics of atmospheric flight. new york: john wiley & sons, inc. isbn 978-0-471-24620-6. vincenti, walter g.; kruger jr, charles h. (1986). introduction to physical gas dynamics. malabar, florida: robert e. krieger publishing co. isbn 978-0-88275-309-6. hansen, c. frederick (1976). molecular physics of equilibrium gases, a handbook for engineers. nasa. bibcode:1976mpeg.book.....h. nasa sp-3096. hayes, wallace d.; probstein, ronald f. (1959). hypersonic flow theory. new york and london: academic press. a revised version of this classic text has been reissued as an inexpensive paperback: hayes, wallace d. (1966). hypersonic inviscid flow. mineola, new york: dover publications. isbn 978-0-486-43281-6. reissued in 2004 anderson, john d. jr. (1989). hypersonic and high temperature gas dynamics. new york: mcgraw-hill, inc. isbn 978-0-07-001671-2.   == external links == aerocapture mission analysis tool (amat) provides preliminary mission analysis and simulation capability for atmospheric entry vehicles at various solar system destinations. center for orbital and reentry debris studies (the aerospace corporation) apollo atmospheric entry phase, 1968, nasa mission planning and analysis division, project apollo. video (25:14). buran\'s heat shield encyclopedia astronautica article on the history of space rescue crafts, including some reentry craft designs.')"
2,"In aerodynamics, a hypersonic speed is one that greatly exceeds the speed of sound, often stated as starting at speeds of Mach 5 and above.The precise Mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around Mach 5-10. The hypersonic regime can also be alternatively defined as speeds where specific heat capacity changes with the temperature of the flow as kinetic energy of the moving object is converted into heat.


== Characteristics of flow ==
While the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. The peculiarity in hypersonic flows are as follows:

Shock layer
Aerodynamic heating
Entropy layer
Real gas effects
Low density effects
Independence of aerodynamic coefficients with Mach number.


=== Small shock stand-off distance ===
As a body's Mach number increases, the density behind a bow shock generated by the body also increases, which corresponds to a decrease in volume behind the shock due to conservation of mass. Consequently, the distance between the bow shock and the body decreases at higher Mach numbers.


=== Entropy layer ===
As Mach numbers increase, the entropy change across the shock also increases, which results in a strong entropy gradient and highly vortical flow that mixes with the boundary layer.


=== Viscous interaction ===
A portion of the large kinetic energy associated with flow at high Mach numbers transforms into internal energy in the fluid due to viscous effects. The increase in internal energy is realized as an increase in temperature. Since the pressure gradient normal to the flow within a boundary layer is approximately zero for low to moderate hypersonic Mach numbers, the increase of temperature through the boundary layer coincides with a decrease in density. This causes the bottom of the boundary layer to expand, so that the boundary layer over the body grows thicker and can often merge with the shock wave near the body leading edge.


=== High-temperature flow ===
High temperatures due to a manifestation of viscous dissipation cause non-equilibrium chemical flow properties such as vibrational excitation and dissociation and ionization of molecules resulting in convective and radiative heat-flux.


== Classification of Mach regimes ==
Although ""subsonic"" and ""supersonic"" usually refer to speeds below and above the local speed of sound respectively, aerodynamicists often use these terms to refer to particular ranges of Mach values. This occurs because a ""transonic regime"" exists around M=1 where approximations of the Navier–Stokes equations used for subsonic design no longer apply, partly because the flow locally exceeds M=1 even when the freestream Mach number is below this value.
The ""supersonic regime"" usually refers to the set of Mach numbers for which linearised theory may be used; for example, where the (air) flow is not chemically reacting and where heat transfer between air and vehicle may be reasonably neglected in calculations. Generally, NASA defines ""high"" hypersonic as any Mach number from 10 to 25, and re-entry speeds as anything greater than Mach 25. Among the aircraft operating in this regime are the Space Shuttle and (theoretically) various developing spaceplanes.
In the following table, the ""regimes"" or ""ranges of Mach values"" are referenced instead of the usual meanings of ""subsonic"" and ""supersonic"".


== Similarity parameters ==
The categorization of airflow relies on a number of similarity parameters, which allow the simplification of a nearly infinite number of test cases into groups of similarity. For transonic and compressible flow, the Mach and Reynolds numbers alone allow good categorization of many flow cases.
Hypersonic flows, however, require other similarity parameters. First, the analytic equations for the oblique shock angle become nearly independent of Mach number at high (~>10) Mach numbers. Second, the formation of strong shocks around aerodynamic bodies means that the freestream Reynolds number is less useful as an estimate of the behavior of the boundary layer over a body (although it is still important). Finally, the increased temperature of hypersonic flows mean that real gas effects become important. For this reason, research in hypersonics is often referred to as aerothermodynamics, rather than aerodynamics.
The introduction of real gas effects means that more variables are required to describe the full state of a gas. Whereas a stationary gas can be described by three variables (pressure, temperature, adiabatic index), and a moving gas by four (flow velocity), a hot gas in chemical equilibrium also requires state equations for the chemical components of the gas, and a gas in nonequilibrium solves those state equations using time as an extra variable. This means that for a nonequilibrium flow, something between 10 and 100 variables may be required to describe the state of the gas at any given time. Additionally, rarefied hypersonic flows (usually defined as those with a Knudsen number above 0.1) do not follow the Navier–Stokes equations.
Hypersonic flows are typically categorized by their total energy, expressed as total enthalpy (MJ/kg), total pressure (kPa-MPa), stagnation pressure (kPa-MPa), stagnation temperature (K), or flow velocity (km/s).
Wallace D. Hayes developed a similarity parameter, similar to the Whitcomb area rule, which allowed similar configurations to be compared.


== Regimes ==
Hypersonic flow can be approximately separated into a number of regimes. The selection of these regimes is rough, due to the blurring of the boundaries where a particular effect can be found.


=== Perfect gas ===
In this regime, the gas can be regarded as an ideal gas. Flow in this regime is still Mach number dependent. Simulations start to depend on the use of a constant-temperature wall, rather than the adiabatic wall typically used at lower speeds. The lower border of this region is around Mach 5, where ramjets become inefficient, and the upper border around Mach 10-12.


=== Two-temperature ideal gas ===
This is a subset of the perfect gas regime, where the gas can be considered chemically perfect, but the rotational and vibrational temperatures of the gas must be considered separately, leading to two temperature models. See particularly the modeling of supersonic nozzles, where vibrational freezing becomes important.


=== Dissociated gas ===
In this regime, diatomic or polyatomic gases (the gases found in most atmospheres) begin to dissociate as they come into contact with the bow shock generated by the body. Surface catalysis plays a role in the calculation of surface heating, meaning that the type of surface material also has an effect on the flow. The lower border of this regime is where any component of a gas mixture first begins to dissociate in the stagnation point of a flow (which for nitrogen is around 2000 K). At the upper border of this regime, the effects of ionization start to have an effect on the flow.


=== Ionized gas ===
In this regime the ionized electron population of the stagnated flow becomes significant, and the electrons must be modeled separately. Often the electron temperature is handled separately from the temperature of the remaining gas components. This region occurs for freestream flow velocities around 3-4 km/s. Gases in this region are modeled as non-radiating plasmas.


=== Radiation-dominated regime ===
Above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. The modeling of gases in this regime is split into two classes:

Optically thin: where the gas does not re-absorb radiation emitted from other parts of the gas
Optically thick: where the radiation must be considered a separate source of energy.The modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.


== See also ==

EnginesRocket engine
Ramjet
Scramjet
Reaction Engines SABRE, LAPCAT (design studies)MissilesShaurya (missile) Ballistic Missile -  India (Entered Production)
BrahMos-II Cruise Missile -   (Under Development)
9K720 Iskander Short-range ballistic missile  Russia (Currently In Service)
3M22 Zircon Anti-ship hypersonic cruise missile  (in production)
R-37 (missile) Hypersonic air-to-air missile  (in service)
Kh-47M2 Kinzhal Hypersonic air-launched ballistic missile  (in service)Other flow regimesSubsonic flight
Transonic
Supersonic speed


== References ==


== External links ==
NASA's Guide to Hypersonics
Hypersonics Group at Imperial College
University of Queensland Centre for Hypersonics
High Speed Flow Group at University of New South Wales
Hypersonics Group at the University of Oxford","pandas(index=2, _1=2, text='in aerodynamics, a hypersonic speed is one that greatly exceeds the speed of sound, often stated as starting at speeds of mach 5 and above.the precise mach number at which a craft can be said to be flying at hypersonic speed varies, since individual physical changes in the airflow (like molecular dissociation and ionization) occur at different speeds; these effects collectively become important around mach 5-10. the hypersonic regime can also be alternatively defined as speeds where specific heat capacity changes with the temperature of the flow as kinetic energy of the moving object is converted into heat.   == characteristics of flow == while the definition of hypersonic flow can be quite vague and is generally debatable (especially due to the absence of discontinuity between supersonic and hypersonic flows), a hypersonic flow may be characterized by certain physical phenomena that can no longer be analytically discounted as in supersonic flow. the peculiarity in hypersonic flows are as follows:  shock layer aerodynamic heating entropy layer real gas effects low density effects independence of aerodynamic coefficients with mach number. above around 12 km/s, the heat transfer to a vehicle changes from being conductively dominated to radiatively dominated. the modeling of gases in this regime is split into two classes:  optically thin: where the gas does not re-absorb radiation emitted from other parts of the gas optically thick: where the radiation must be considered a separate source of energy.the modeling of optically thick gases is extremely difficult, since, due to the calculation of the radiation at each point, the computation load theoretically expands exponentially as the number of points considered increases.   == see also ==  enginesrocket engine ramjet scramjet reaction engines sabre, lapcat (design studies)missilesshaurya (missile) ballistic missile -  india (entered production) brahmos-ii cruise missile -   (under development) 9k720 iskander short-range ballistic missile  russia (currently in service) 3m22 zircon anti-ship hypersonic cruise missile  (in production) r-37 (missile) hypersonic air-to-air missile  (in service) kh-47m2 kinzhal hypersonic air-launched ballistic missile  (in service)other flow regimessubsonic flight transonic supersonic speed   == references ==   == external links == nasa\'s guide to hypersonics hypersonics group at imperial college university of queensland centre for hypersonics high speed flow group at university of new south wales hypersonics group at the university of oxford')"
3,"Supersonic speed is the speed of an object that exceeds the speed of sound (Mach 1). For objects traveling in dry air of a temperature of 20 °C (68 °F) at sea level, this speed is approximately 343.2 m/s (1,126 ft/s; 768 mph; 667.1 kn; 1,236 km/h). Speeds greater than five times the speed of sound (Mach 5) are often referred to as hypersonic. Flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. This occurs typically somewhere between Mach 0.8 and Mach 1.2.
Sounds are traveling vibrations in the form of pressure waves in an elastic medium. In gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. Since air temperature and composition varies significantly with altitude, Mach numbers for aircraft may change despite a constant travel speed. In water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). In solids, sound waves can be polarized longitudinally or transversely and have even higher velocities.
Supersonic fracture is crack motion faster than the speed of sound in a brittle material.


== Early meaning ==
At the beginning of the 20th century, the term ""supersonic"" was used as an adjective to describe sound whose frequency is above the range of normal human hearing.  The modern term for this meaning is ""ultrasonic"".
Etymology: The word supersonic comes from two Latin derived words; 1) super: above and 2) sonus: sound, which together mean above sound or in other words faster than sound.


== Supersonic objects ==

The tip of a bullwhip is thought to be the first man-made object to break the sound barrier, resulting in the telltale ""crack"" (actually a small sonic boom). The wave motion traveling through the bullwhip is what makes it capable of achieving supersonic speeds.Most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely Concorde and the Tupolev Tu-144. Both these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. Due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, Concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. Since Concorde's final retirement flight on November 26, 2003, there are no supersonic passenger aircraft left in service. Some large bombers, such as the Tupolev Tu-160 and Rockwell B-1 Lancer are also supersonic-capable.
Most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding Mach 3.
Most spacecraft, most notably the Space Shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. During ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag.
Note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). At even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.When an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.


== Supersonic land vehicles ==
To date, only one land vehicle has officially travelled at supersonic speed. It is ThrustSSC, driven by Andy Green, which holds the world land speed record, having achieved an average speed on its bi-directional run of 1,228 km/h (763 mph) in the Black Rock Desert on 15 October 1997.
The Bloodhound LSR project is planning an attempt on the record in 2020 at Hakskeen Pan in South Africa with a combination jet and hybrid rocket propelled car. The aim is to break the existing record, then make further attempts during which [the members of] the team hope to reach speeds of up to 1,600 km/h (1,000 mph). The effort was originally run by Richard Noble who was the leader of the ThrustSSC project, however following funding issues in 2018, the team was bought by Ian Warhurst and renamed Bloodhound LSR. The new project retains many of the original Bloodhound SSC engineering staff, and Andy Green is still the driver for record attempt, with high speed trials expected to start in October 2019.


== Supersonic flight ==
Supersonic aerodynamics is simpler than subsonic aerodynamics because the airsheets at different points along the plane often cannot affect each other. Supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around Mach 0.85–1.2). At these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. Designers use the Supersonic area rule and the Whitcomb area rule to minimize sudden changes in size.

However, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex.
One problem with sustained supersonic flight is the generation of heat in flight.  At high speeds aerodynamic heating can occur, so an aircraft must be designed to operate and function under very high temperatures. Duralumin, a material traditionally used in aircraft manufacturing, starts to lose strength and deform at relatively low temperatures, and is unsuitable for continuous use at speeds above Mach 2.2 to 2.4. Materials such as titanium and stainless steel allow operations at much higher temperatures. For example, the Lockheed SR-71 Blackbird jet could fly continuously at Mach 3.1 which could lead to temperatures on some parts of the aircraft reaching above 315 °C (600 °F).
Another area of concern for sustained high-speed flight is engine operation. Jet engines create thrust by increasing the temperature of the air they ingest, and as the aircraft speeds up, the compression process in the intake causes a temperature rise before it reaches the engines. The maximum allowable temperature of the exhaust is determined by the materials in the turbine at the rear of the engine, so as the aircraft speeds up, the difference in intake and exhaust temperature that the engine can create, by burning fuel, decreases, as does the thrust. The higher thrust needed for supersonic speeds had to be regained by burning extra fuel in the exhaust.
Intake design was also a major issue. As much of the available energy in the incoming air has to be recovered, known as intake recovery, using shock waves in the supersonic compression process in the intake. At supersonic speeds the intake has to make sure that the air slows down without excessive pressure loss. It has to use the correct type of shock waves, oblique/plane, for the aircraft design speed to compress and slow the air to subsonic speed before it reaches the engine. The shock waves are positioned using a ramp or cone which may need to be adjustable depending on trade-offs between complexity and the required aircraft performance. 
An aircraft able to operate for extended periods at supersonic speeds has a potential range advantage over a similar design operating subsonically. Most of the drag an aircraft sees while speeding up to supersonic speeds occurs just below the speed of sound, due to an aerodynamic effect known as wave drag. An aircraft that can accelerate past this speed sees a significant drag decrease, and can fly supersonically with improved fuel economy. However, due to the way lift is generated supersonically, the lift-to-drag ratio of the aircraft as a whole drops, leading to lower range, offsetting or overturning this advantage.
The key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a ""perfect"" shape, the von Karman ogive or Sears-Haack body. This has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. SR-71, Concorde, etc. Although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use.


=== History of supersonic flight ===

Aviation research during World War II led to the creation of the first rocket- and jet-powered aircraft.  Several claims of breaking the sound barrier during the war subsequently emerged.  However, the first recognized flight exceeding the speed of sound by a manned aircraft in controlled level flight was performed on October 14, 1947 by the experimental Bell X-1 research rocket plane piloted by Charles ""Chuck"" Yeager. The first production plane to break the sound barrier was an F-86 Canadair Sabre with the first 'supersonic' woman pilot, Jacqueline Cochran, at the controls. According to David Masters, the DFS 346 prototype captured in Germany by the Soviets, after being released from a B-29 at 32800 ft (10000 m), reached 683 mph (1100 km/h) late in 1945, which would have exceeded Mach 1 at that height.  The pilot in these flights was the German Wolfgang Ziese.
On August 21, 1961, a Douglas DC-8-43 (registration N9604Z) exceeded Mach 1 in a controlled dive during a test flight at Edwards Air Force Base.  The crew were William Magruder (pilot), Paul Patten (copilot), Joseph Tomich (flight engineer), and Richard H. Edwards (flight test engineer).  This was the first supersonic flight by a civilian airliner other than the Concorde or Tu-144.


== See also ==
Area rule
Hypersonic speed
Transonic speed
Sonic boom
Supersonic aircraft
Supersonic airfoils
Vapor cone
Prandtl–Glauert singularity


== References ==


== External links ==
""Can We Ever Fly Faster Speed of Sound"", October 1944, Popular Science one of the earliest articles on shock waves and flying the speed of sound
""Britain Goes Supersonic"", January 1946, Popular Science 1946 article trying to explain supersonic flight to the general public
MathPages - The Speed of Sound
Supersonic sound pressure levels","pandas(index=3, _1=3, text='supersonic speed is the speed of an object that exceeds the speed of sound (mach 1). for objects traveling in dry air of a temperature of 20 °c (68 °f) at sea level, this speed is approximately 343.2 m/s (1,126 ft/s; 768 mph; 667.1 kn; 1,236 km/h). speeds greater than five times the speed of sound (mach 5) are often referred to as hypersonic. flights during which only some parts of the air surrounding an object, such as the ends of rotor blades, reach supersonic speeds are called transonic. this occurs typically somewhere between mach 0.8 and mach 1.2. sounds are traveling vibrations in the form of pressure waves in an elastic medium. in gases, sound travels longitudinally at different speeds, mostly depending on the molecular mass and temperature of the gas, and pressure has little effect. since air temperature and composition varies significantly with altitude, mach numbers for aircraft may change despite a constant travel speed. in water at room temperature supersonic speed can be considered as any speed greater than 1,440 m/s (4,724 ft/s). in solids, sound waves can be polarized longitudinally or transversely and have even higher velocities. supersonic fracture is crack motion faster than the speed of sound in a brittle material.   == early meaning == at the beginning of the 20th century, the term ""supersonic"" was used as an adjective to describe sound whose frequency is above the range of normal human hearing.  the modern term for this meaning is ""ultrasonic"". etymology: the word supersonic comes from two latin derived words; 1) super: above and 2) sonus: sound, which together mean above sound or in other words faster than sound.   == supersonic objects ==  the tip of a bullwhip is thought to be the first man-made object to break the sound barrier, resulting in the telltale ""crack"" (actually a small sonic boom). the wave motion traveling through the bullwhip is what makes it capable of achieving supersonic speeds.most modern fighter aircraft are supersonic aircraft, but there have been supersonic passenger aircraft, namely concorde and the tupolev tu-144. both these passenger aircraft and some modern fighters are also capable of supercruise, a condition of sustained supersonic flight without the use of an afterburner. due to its ability to supercruise for several hours and the relatively high frequency of flight over several decades, concorde spent more time flying supersonically than all other aircraft combined by a considerable margin. since concorde\'s final retirement flight on november 26, 2003, there are no supersonic passenger aircraft left in service. some large bombers, such as the tupolev tu-160 and rockwell b-1 lancer are also supersonic-capable. most modern firearm bullets are supersonic, with rifle projectiles often travelling at speeds approaching and in some cases well exceeding mach 3. most spacecraft, most notably the space shuttle are supersonic at least during portions of their reentry, though the effects on the spacecraft are reduced by low air densities. during ascent, launch vehicles generally avoid going supersonic below 30 km (~98,400 feet) to reduce air drag. note that the speed of sound decreases somewhat with altitude, due to lower temperatures found there (typically up to 25 km). at even higher altitudes the temperature starts increasing, with the corresponding increase in the speed of sound.when an inflated balloon is burst, the torn pieces of latex contract at supersonic speed, which contributes to the sharp and loud popping noise.   == supersonic land vehicles == to date, only one land vehicle has officially travelled at supersonic speed. it is thrustssc, driven by andy green, which holds the world land speed record, having achieved an average speed on its bi-directional run of 1,228 km/h (763 mph) in the black rock desert on 15 october 1997. the bloodhound lsr project is planning an attempt on the record in 2020 at hakskeen pan in south africa with a combination jet and hybrid rocket propelled car. the aim is to break the existing record, then make further attempts during which [the members of] the team hope to reach speeds of up to 1,600 km/h (1,000 mph). the effort was originally run by richard noble who was the leader of the thrustssc project, however following funding issues in 2018, the team was bought by ian warhurst and renamed bloodhound lsr. the new project retains many of the original bloodhound ssc engineering staff, and andy green is still the driver for record attempt, with high speed trials expected to start in october 2019.   == supersonic flight == supersonic aerodynamics is simpler than subsonic aerodynamics because the airsheets at different points along the plane often cannot affect each other. supersonic jets and rocket vehicles require several times greater thrust to push through the extra aerodynamic drag experienced within the transonic region (around mach 0.85–1.2). at these speeds aerospace engineers can gently guide air around the fuselage of the aircraft without producing new shock waves, but any change in cross area farther down the vehicle leads to shock waves along the body. designers use the supersonic area rule and the whitcomb area rule to minimize sudden changes in size.  however, in practical applications, a supersonic aircraft must operate stably in both subsonic and supersonic profiles, hence aerodynamic design is more complex. one problem with sustained supersonic flight is the generation of heat in flight.  at high speeds aerodynamic heating can occur, so an aircraft must be designed to operate and function under very high temperatures. duralumin, a material traditionally used in aircraft manufacturing, starts to lose strength and deform at relatively low temperatures, and is unsuitable for continuous use at speeds above mach 2.2 to 2.4. materials such as titanium and stainless steel allow operations at much higher temperatures. for example, the lockheed sr-71 blackbird jet could fly continuously at mach 3.1 which could lead to temperatures on some parts of the aircraft reaching above 315 °c (600 °f). another area of concern for sustained high-speed flight is engine operation. jet engines create thrust by increasing the temperature of the air they ingest, and as the aircraft speeds up, the compression process in the intake causes a temperature rise before it reaches the engines. the maximum allowable temperature of the exhaust is determined by the materials in the turbine at the rear of the engine, so as the aircraft speeds up, the difference in intake and exhaust temperature that the engine can create, by burning fuel, decreases, as does the thrust. the higher thrust needed for supersonic speeds had to be regained by burning extra fuel in the exhaust. intake design was also a major issue. as much of the available energy in the incoming air has to be recovered, known as intake recovery, using shock waves in the supersonic compression process in the intake. at supersonic speeds the intake has to make sure that the air slows down without excessive pressure loss. it has to use the correct type of shock waves, oblique/plane, for the aircraft design speed to compress and slow the air to subsonic speed before it reaches the engine. the shock waves are positioned using a ramp or cone which may need to be adjustable depending on trade-offs between complexity and the required aircraft performance. an aircraft able to operate for extended periods at supersonic speeds has a potential range advantage over a similar design operating subsonically. most of the drag an aircraft sees while speeding up to supersonic speeds occurs just below the speed of sound, due to an aerodynamic effect known as wave drag. an aircraft that can accelerate past this speed sees a significant drag decrease, and can fly supersonically with improved fuel economy. however, due to the way lift is generated supersonically, the lift-to-drag ratio of the aircraft as a whole drops, leading to lower range, offsetting or overturning this advantage. the key to having low supersonic drag is to properly shape the overall aircraft to be long and thin, and close to a ""perfect"" shape, the von karman ogive or sears-haack body. this has led to almost every supersonic cruising aircraft looking very similar to every other, with a very long and slender fuselage and large delta wings, cf. sr-71, concorde, etc. although not ideal for passenger aircraft, this shaping is quite adaptable for bomber use. aviation research during world war ii led to the creation of the first rocket- and jet-powered aircraft.  several claims of breaking the sound barrier during the war subsequently emerged.  however, the first recognized flight exceeding the speed of sound by a manned aircraft in controlled level flight was performed on october 14, 1947 by the experimental bell x-1 research rocket plane piloted by charles ""chuck"" yeager. the first production plane to break the sound barrier was an f-86 canadair sabre with the first \'supersonic\' woman pilot, jacqueline cochran, at the controls. according to david masters, the dfs 346 prototype captured in germany by the soviets, after being released from a b-29 at 32800 ft (10000 m), reached 683 mph (1100 km/h) late in 1945, which would have exceeded mach 1 at that height.  the pilot in these flights was the german wolfgang ziese. on august 21, 1961, a douglas dc-8-43 (registration n9604z) exceeded mach 1 in a controlled dive during a test flight at edwards air force base.  the crew were william magruder (pilot), paul patten (copilot), joseph tomich (flight engineer), and richard h. edwards (flight test engineer).  this was the first supersonic flight by a civilian airliner other than the concorde or tu-144.   == see also == area rule hypersonic speed transonic speed sonic boom supersonic aircraft supersonic airfoils vapor cone prandtl–glauert singularity   == references ==   == external links == ""can we ever fly faster speed of sound"", october 1944, popular science one of the earliest articles on shock waves and flying the speed of sound ""britain goes supersonic"", january 1946, popular science 1946 article trying to explain supersonic flight to the general public mathpages - the speed of sound supersonic sound pressure levels')"
4,"Aeroelasticity is the branch of physics and engineering studying the interactions between the inertial, elastic, and aerodynamic forces occurring while an elastic body is exposed to a fluid flow. The study of aeroelasticity may be broadly classified into two fields: static aeroelasticity dealing with the static or steady state response of an elastic body to a fluid flow; and dynamic aeroelasticity dealing with the body's dynamic (typically vibrational) response.
Aircraft are prone to aeroelastic effects because they need to be lightweight and withstand large aerodynamic loads.  Aircraft are designed to avoid the following aeroelastic problems:

divergence where the aerodynamic forces increase the angle of attack of a wing which further increases the force;
control reversal where control activation produces an opposite aerodynamic moment that reduces, or in extreme cases, reverses the control effectiveness; and
flutter which is the uncontained vibration that can lead to the destruction of an aircraft.Aeroelasticity problems can be prevented by adjusting the mass, stiffness or aerodynamics of structures which can be determined and verified through the use of calculations, ground vibration tests and flight flutter trials. Flutter of control surfaces is usually eliminated by the careful placement of mass balances.
The synthesis of aeroelasticity with thermodynamics is known as aerothermoelasticity, and its synthesis with control theory is known as aeroservoelasticity.


== History ==
The second failure of Samuel Langley's prototype plane on the Potomac was attributed to aeroelastic effects (specifically, torsional divergence). An early scientific work on the subject was George Bryan's Theory of the Stability of a Rigid Aeroplane published in 1906. Problems with torsional divergence plagued aircraft in the First World War and were solved largely by trial-and-error and ad-hoc stiffening of the wing. The first recorded and documented case of flutter in an aircraft was that which occurred to a Handley Page O/400 bomber during a flight in 1916, when it suffered a violent tail oscillation, which caused extreme distortion of the rear fuselage and the elevators to move asymmetrically. Although the aircraft landed safely, in the subsequent investigation F. W. Lanchester was consulted. One of his recommendations was that left and right elevators should be rigidly connected by a stiff shaft, which was to subsequently become a design requirement. In addition, the National Physical Laboratory (NPL) was asked to investigate the phenomenon theoretically, which was subsequently carried out by Leonard Bairstow and Arthur Fage.In 1926, Hans Reissner published a theory of wing divergence, leading to much further theoretical research on the subject. The term aeroelasticity itself was coined by Harold Roxbee Cox and Alfred Pugsley at the Royal Aircraft Establishment (RAE), Farnborough in the early 1930s.In the development of aeronautical engineering at Caltech, Theodore von Kármán started a course ""Elasticity applied to Aeronautics"". After teaching the course for one term, Kármán passed it over to Ernest Edwin Sechler, who developed aeroelasticity in that course and in publication of textbooks on the subject.In 1947, Arthur Roderick Collar defined aeroelasticity as ""the study of the mutual interaction that takes place within the triangle of the inertial, elastic, and aerodynamic forces acting on structural members exposed to an airstream, and the influence of this study on design"".


== Static aeroelasticity ==
In an aeroplane, two significant static aeroelastic effects may occur. Divergence is a phenomenon in which the elastic twist of the wing suddenly becomes theoretically infinite, typically causing the wing to fail. Control reversal is a phenomenon occurring only in wings with ailerons or other control surfaces, in which these control surfaces reverse their usual functionality (e.g., the rolling direction associated with a given aileron moment is reversed).


=== Divergence ===
Divergence occurs when a lifting surface deflects under aerodynamic load in a direction which further increases lift in a positive feedback loop. The increased lift deflects the structure further, which eventually brings the structure to the point of divergence. 


=== Control reversal ===

Control surface reversal is the loss (or reversal) of the expected response of a control surface, due to deformation of the main lifting surface. For simple models (e.g. single aileron on an Euler-Bernoulli beam), control reversal speeds can be derived analytically as for torsional divergence. Control reversal can be used to aerodynamic advantage, and forms part of the Kaman servo-flap rotor design.


== Dynamic aeroelasticity ==
Dynamic aeroelasticity studies the interactions among aerodynamic, elastic, and inertial forces. Examples of dynamic aeroelastic phenomena are:


=== Flutter ===
Flutter is a dynamic instability of an elastic structure in a fluid flow, caused by positive feedback between the body's deflection and the force exerted by the fluid flow. In a linear system, ""flutter point"" is the point at which the structure is undergoing simple harmonic motion—zero net damping—and so any further decrease in net damping will result in a self-oscillation and eventual failure. ""Net damping"" can be understood as the sum of the structure's natural positive damping and the negative damping of the aerodynamic force. Flutter can be classified into two types: hard flutter, in which the net damping decreases very suddenly, very close to the flutter point; and soft flutter, in which the net damping decreases gradually.In water the mass ratio of the pitch inertia of the foil to that of the circumscribing cylinder of fluid is generally too low for binary flutter to occur, as shown by explicit solution of the simplest pitch and heave flutter stability determinant.

Structures exposed to aerodynamic forces—including wings and aerofoils, but also chimneys and bridges—are designed carefully within known parameters to avoid flutter. 
Blunt shapes, such as chimneys, can give off a continuous stream of vortices known as a Kármán vortex street, which can induce structural oscillations.  Strakes are typically wrapped around chimneys to stop the formation of these vortices. 
In complex structures where both the aerodynamics and the mechanical properties of the structure are not fully understood, flutter can be discounted only through detailed testing. Even changing the mass distribution of an aircraft or the stiffness of one component can induce flutter in an apparently unrelated aerodynamic component.  At its mildest, this can appear as a ""buzz"" in the aircraft structure, but at its most violent, it can develop uncontrollably with great speed and cause serious damage to or lead to the destruction of the aircraft, as in Braniff Flight 542, or the prototypes for the VL Myrsky fighter aircraft. Famously, the original Tacoma Narrows Bridge was destroyed as a result of aeroelastic fluttering.


==== Aeroservoelasticity ====
In some cases, automatic control systems have been demonstrated to help prevent or limit flutter-related structural vibration.


==== Propeller whirl flutter ====
Propeller whirl flutter is a special case of flutter involving the aerodynamic and inertial effects of a rotating propeller and the stiffness of the supporting nacelle structure. Dynamic instability can occur involving pitch and yaw degrees of freedom of the propeller and the engine supports leading to an unstable precession of the propeller. Failure of the engine supports led to whirl flutter occurring on two Lockheed L-188 Electra in 1959 on Braniff Flight 542 and again in 1960 on Northwest Orient Airlines Flight 710.


==== Transonic aeroelasticity ====
Flow is highly non-linear in the transonic regime, dominated by moving shock waves. It is mission-critical for aircraft that fly through transonic Mach numbers. The role of shock waves was first analyzed by Holt Ashley. A phenomenon that impacts stability of aircraft known as ""transonic dip"", in which the flutter speed can get close to flight speed, was reported in May 1976 by Farmer and Hanson of the Langley Research Center.


=== Buffeting ===

Buffeting is a high-frequency instability, caused by airflow separation or shock wave oscillations from one object striking another. It is caused by a sudden impulse of load increasing. It is a random forced vibration. Generally it affects the tail unit of the aircraft structure due to air flow downstream of the wing.The methods for buffet detection are:

Pressure coefficient diagram
Pressure divergence at trailing edge
Computing separation from trailing edge based on Mach number
Normal force fluctuating divergence


== Prediction and cure ==

In the period 1950–1970, AGARD developed the Manual on Aeroelasticity which details the processes used in solving and verifying aeroelastic problems along with standard examples that can be used to test numerical solutions.Aeroelasticity involves not just the external aerodynamic loads and the way they change but also the structural, damping and mass characteristics of the aircraft. Prediction involves making a mathematical model of the aircraft as a series of masses connected by springs and dampers which are tuned to represent the dynamic characteristics of the aircraft structure. The model also includes details of applied aerodynamic forces and how they vary.
The model can be used to predict the flutter margin and, if necessary, test fixes to potential problems. Small carefully chosen changes to mass distribution and local structural stiffness can be very effective in solving aeroelastic problems.
Methods of predicting flutter in linear structures include the p-method, the k-method and the p-k method.For nonlinear systems, flutter is usually interpreted as a limit cycle oscillation (LCO), and methods from the study of dynamical systems can be used to determine the speed at which flutter will occur.


== Media ==
These videos detail the Active Aeroelastic Wing two-phase NASA-Air Force flight research program to investigate the potential of aerodynamically twisting flexible wings to improve maneuverability of high-performance aircraft at transonic and supersonic speeds, with traditional control surfaces such as ailerons and leading-edge flaps used to induce the twist.

		


== Notable aeroelastic failures ==
The original Tacoma Narrows Bridge was destroyed as a result of aeroelastic fluttering.
Propeller whirl flutter of the Lockheed L-188 Electra on Braniff Flight 542.
1931 Transcontinental & Western Air Fokker F-10 crash.
Body freedom flutter of the GAF Jindivik drone.


== See also ==


== References ==


== Further reading ==
Bisplinghoff, R. L., Ashley, H. and Halfman, H., Aeroelasticity. Dover Science, 1996, ISBN 0-486-69189-6, 880 p.
Dowell, E. H., A Modern Course on Aeroelasticity. ISBN 90-286-0057-4.
Fung, Y. C., An Introduction to the Theory of Aeroelasticity. Dover, 1994, ISBN 978-0-486-67871-9.
Hodges, D. H. and Pierce, A., Introduction to Structural Dynamics and Aeroelasticity, Cambridge, 2002, ISBN 978-0-521-80698-5.
Wright, J. R. and Cooper, J. E., Introduction to Aircraft Aeroelasticity and Loads, Wiley 2007, ISBN 978-0-470-85840-0.
Hoque, M. E., ""Active Flutter Control"", LAP Lambert Academic Publishing, Germany, 2010, ISBN 978-3-8383-6851-1.
Collar, A. R., ""The first fifty years of aeroelasticity"", Aerospace, vol. 5, no. 2, pp. 12–20, 1978.
Garrick, I. E. and Reed W. H., ""Historical development of aircraft flutter"", Journal of Aircraft, vol. 18, pp. 897–912, Nov. 1981.
Patrick R. Veillette (Aug 23, 2018). ""Low-Speed Buffet: High-Altitude, Transonic Training Weakness Continues"". Business & Commercial Aviation. Aviation Week Network.


== External links ==
Aeroelasticity Branch – NASA Langley Research Center
DLR Institute of Aeroelasticity
National Aerospace Laboratory
The Aeroelasticity Group – Texas A&M University
NACA Technical Reports – NASA Langley Research Center
NASA Aeroelasticity Handbook","pandas(index=4, _1=4, text='aeroelasticity is the branch of physics and engineering studying the interactions between the inertial, elastic, and aerodynamic forces occurring while an elastic body is exposed to a fluid flow. the study of aeroelasticity may be broadly classified into two fields: static aeroelasticity dealing with the static or steady state response of an elastic body to a fluid flow; and dynamic aeroelasticity dealing with the body\'s dynamic (typically vibrational) response. aircraft are prone to aeroelastic effects because they need to be lightweight and withstand large aerodynamic loads.  aircraft are designed to avoid the following aeroelastic problems:  divergence where the aerodynamic forces increase the angle of attack of a wing which further increases the force; control reversal where control activation produces an opposite aerodynamic moment that reduces, or in extreme cases, reverses the control effectiveness; and flutter which is the uncontained vibration that can lead to the destruction of an aircraft.aeroelasticity problems can be prevented by adjusting the mass, stiffness or aerodynamics of structures which can be determined and verified through the use of calculations, ground vibration tests and flight flutter trials. flutter of control surfaces is usually eliminated by the careful placement of mass balances. the synthesis of aeroelasticity with thermodynamics is known as aerothermoelasticity, and its synthesis with control theory is known as aeroservoelasticity.   == history == the second failure of samuel langley\'s prototype plane on the potomac was attributed to aeroelastic effects (specifically, torsional divergence). an early scientific work on the subject was george bryan\'s theory of the stability of a rigid aeroplane published in 1906. problems with torsional divergence plagued aircraft in the first world war and were solved largely by trial-and-error and ad-hoc stiffening of the wing. the first recorded and documented case of flutter in an aircraft was that which occurred to a handley page o/400 bomber during a flight in 1916, when it suffered a violent tail oscillation, which caused extreme distortion of the rear fuselage and the elevators to move asymmetrically. although the aircraft landed safely, in the subsequent investigation f. w. lanchester was consulted. one of his recommendations was that left and right elevators should be rigidly connected by a stiff shaft, which was to subsequently become a design requirement. in addition, the national physical laboratory (npl) was asked to investigate the phenomenon theoretically, which was subsequently carried out by leonard bairstow and arthur fage.in 1926, hans reissner published a theory of wing divergence, leading to much further theoretical research on the subject. the term aeroelasticity itself was coined by harold roxbee cox and alfred pugsley at the royal aircraft establishment (rae), farnborough in the early 1930s.in the development of aeronautical engineering at caltech, theodore von kármán started a course ""elasticity applied to aeronautics"". after teaching the course for one term, kármán passed it over to ernest edwin sechler, who developed aeroelasticity in that course and in publication of textbooks on the subject.in 1947, arthur roderick collar defined aeroelasticity as ""the study of the mutual interaction that takes place within the triangle of the inertial, elastic, and aerodynamic forces acting on structural members exposed to an airstream, and the influence of this study on design"".   == static aeroelasticity == in an aeroplane, two significant static aeroelastic effects may occur. divergence is a phenomenon in which the elastic twist of the wing suddenly becomes theoretically infinite, typically causing the wing to fail. control reversal is a phenomenon occurring only in wings with ailerons or other control surfaces, in which these control surfaces reverse their usual functionality (e.g., the rolling direction associated with a given aileron moment is reversed). buffeting is a high-frequency instability, caused by airflow separation or shock wave oscillations from one object striking another. it is caused by a sudden impulse of load increasing. it is a random forced vibration. generally it affects the tail unit of the aircraft structure due to air flow downstream of the wing.the methods for buffet detection are:  pressure coefficient diagram pressure divergence at trailing edge computing separation from trailing edge based on mach number normal force fluctuating divergence   == prediction and cure ==  in the period 1950–1970, agard developed the manual on aeroelasticity which details the processes used in solving and verifying aeroelastic problems along with standard examples that can be used to test numerical solutions.aeroelasticity involves not just the external aerodynamic loads and the way they change but also the structural, damping and mass characteristics of the aircraft. prediction involves making a mathematical model of the aircraft as a series of masses connected by springs and dampers which are tuned to represent the dynamic characteristics of the aircraft structure. the model also includes details of applied aerodynamic forces and how they vary. the model can be used to predict the flutter margin and, if necessary, test fixes to potential problems. small carefully chosen changes to mass distribution and local structural stiffness can be very effective in solving aeroelastic problems. methods of predicting flutter in linear structures include the p-method, the k-method and the p-k method.for nonlinear systems, flutter is usually interpreted as a limit cycle oscillation (lco), and methods from the study of dynamical systems can be used to determine the speed at which flutter will occur.   == media == these videos detail the active aeroelastic wing two-phase nasa-air force flight research program to investigate the potential of aerodynamically twisting flexible wings to improve maneuverability of high-performance aircraft at transonic and supersonic speeds, with traditional control surfaces such as ailerons and leading-edge flaps used to induce the twist.       == notable aeroelastic failures == the original tacoma narrows bridge was destroyed as a result of aeroelastic fluttering. propeller whirl flutter of the lockheed l-188 electra on braniff flight 542. 1931 transcontinental & western air fokker f-10 crash. body freedom flutter of the gaf jindivik drone.   == see also ==   == references ==   == further reading == bisplinghoff, r. l., ashley, h. and halfman, h., aeroelasticity. dover science, 1996, isbn 0-486-69189-6, 880 p. dowell, e. h., a modern course on aeroelasticity. isbn 90-286-0057-4. fung, y. c., an introduction to the theory of aeroelasticity. dover, 1994, isbn 978-0-486-67871-9. hodges, d. h. and pierce, a., introduction to structural dynamics and aeroelasticity, cambridge, 2002, isbn 978-0-521-80698-5. wright, j. r. and cooper, j. e., introduction to aircraft aeroelasticity and loads, wiley 2007, isbn 978-0-470-85840-0. hoque, m. e., ""active flutter control"", lap lambert academic publishing, germany, 2010, isbn 978-3-8383-6851-1. collar, a. r., ""the first fifty years of aeroelasticity"", aerospace, vol. 5, no. 2, pp. 12–20, 1978. garrick, i. e. and reed w. h., ""historical development of aircraft flutter"", journal of aircraft, vol. 18, pp. 897–912, nov. 1981. patrick r. veillette (aug 23, 2018). ""low-speed buffet: high-altitude, transonic training weakness continues"". business & commercial aviation. aviation week network.   == external links == aeroelasticity branch – nasa langley research center dlr institute of aeroelasticity national aerospace laboratory the aeroelasticity group – texas a&m university naca technical reports – nasa langley research center nasa aeroelasticity handbook')"
5,"An aerospace manufacturer is a company or individual involved in the various aspects of designing, building, testing, selling, and maintaining aircraft, aircraft parts, missiles, rockets, or spacecraft. Aerospace is a high technology industry.
The aircraft industry is the industry supporting aviation by building aircraft and manufacturing aircraft parts for their maintenance.  This includes aircraft and parts used for civil aviation and military aviation. Most production is done pursuant to type certificates and Defense Standards issued by a government body. This term has been largely subsumed by the more encompassing term: ""aerospace industry"".


== Market ==
In 2015 the aircraft production was worth US$180.3 Billion: 61% airliners, 14% business and general aviation, 12% Military aircraft, 10% military rotary wing and 3% civil rotary wing; while their MRO was worth $135.1 Bn or $315.4 Bn combined.The global aerospace industry was worth $838 billion in 2017: Aircraft & Engine OEMs represented 28% ($235 Bn), Civil & Military MRO & Upgrades 27% ($226 Bn), Aircraft Systems & Component Manufacturing 26% ($218 Bn), Satellites & Space 7% ($59 Bn), Missiles & UAVs 5% ($42 Bn) and other activity, including flight simulators, defense electronics, public research accounted for 7% ($59 Bn).
The countries with the largest industry were led by the United States with $408.4 Bn (49%) followed by France with $69 Bn (8.2%) then China with $61.2 Bn (7.3%), United Kingdom with $48.8 Bn (5.8%), Germany with $46.2 Bn (5.5%), Russia with $27.1 Bn (3.2%), Canada with $24 Bn (2.9%), Japan with $21 Bn (2.5%), Spain with $14 Bn (1.7%) and India with $11Bn (1.3%). The top 10 countries represent $731 Bn or 87.2% of the whole industry.In 2018, the new commercial aircraft value is projected for $270.4 billion while business aircraft will amount for $18 billion and civil helicopters for $4 billion.


== Largest companies ==


== Geography ==
In September 2018, PwC ranked aerospace manufacturing attractiveness: the most attractive country was the United States, with $240 billion in sales in 2017, due to the sheer size of the industry (#1) and educated workforce (#1), low geopolitical risk (#4, #1 is Japan), strong transportation infrastructure (#5, #1 is Hong Kong), a healthy economy (#10, #1 is China), but high costs (#7, #1 is Denmark) and average tax policy (#36, #1 is Qatar).
Following were Canada, Singapore, Switzerland and United Kingdom.Within the US, the most attractive was Washington state, due to the best Industry (#1), leading Infrastructure (#4, New Jersey is #1) and Economy (#4, Texas is #1), good labor (#9, Massachusetts is #1), average tax policy (#17, Alaska is #1) but is costly (#33, Montana is #1).
Washington is tied to Boeing Commercial Airplanes, earning $10.3 billion, is home to 1,400 aerospace-related businesses, and has the highest aerospace jobs concentration.
Following are Texas, Georgia, Arizona and Colorado.In the European Union, aerospace companies such as Airbus, Safran, BAE Systems, Thales, Dassault, Saab AB, Terma A/S, Patria Plc and Leonardo are participants in the global aerospace industry and research effort.
In Russia, large aerospace companies like Oboronprom and the United Aircraft Corporation (encompassing Mikoyan, Sukhoi, Ilyushin, Tupolev, Yakovlev, and Irkut, which includes Beriev) are among the major global players in this industry.
In the US, the Department of Defense and NASA are the two biggest consumers of aerospace technology and products. The Bureau of Labor Statistics of the United States reported that the aerospace industry employed 444,000 wage and salary jobs in 2004, many of which were in Washington and California, this marked a steep decline from the peak years during the Reagan Administration when total employment exceeded 1,000,000 aerospace industry workers.During that period of recovery a special program to restore U.S. competitiveness across all U.S. industries, Project Socrates, contributed to employment growth as the U.S. aerospace industry captured 72 percent of world aerospace market.  By 1999 U.S. share of the world market fell to 52 percent.


=== Cities ===
Important locations of the civil aerospace industry worldwide include Seattle, Wichita, Kansas, Dayton, Ohio and St. Louis in the United States (Boeing), Montreal and Toronto in Canada (Bombardier, Pratt & Whitney Canada), Toulouse and Bordeaux in France (Airbus, Dassault, ATR), Seville in Spain and Hamburg in Germany (Airbus, EADS), the North-West of England and Bristol in Britain (BAE Systems, Airbus and AgustaWestland), Komsomolsk-on-Amur and Irkutsk in Russia (Sukhoi, Beriev), Kyiv and Kharkiv in Ukraine (Antonov), Nagoya in Japan (Mitsubishi Heavy Industries Aerospace and Kawasaki Heavy Industries Aerospace), as well as São José dos Campos in Brazil where Embraer is based.


== Consolidation ==
Several consolidations took place in the aerospace and defense industries over the last few decades.
BAE Systems is the successor company to numerous British aircraft manufacturers which merged throughout the second half of the 20th century. Many of these mergers followed the 1957 Defence White Paper.Airbus prominently illustrated the European airliner manufacturing consolidation in the late 1960s.Between 1988 and 2010, more than 5,452 mergers and acquisitions with a total known-value of US$579 billion were announced worldwide.In 1993, then United States Secretary of Defense Les Aspin and his deputy William J. Perry held the ""Last Supper"" at the Pentagon with contractors executives who were told that there were twice as many military suppliers as he wanted to see: $55 billion in military-industry mergers took place from 1992 to 1997, leaving mainly Boeing, Lockheed Martin, Northrop Grumman and Raytheon.Boeing bought McDonnell Douglas for US$13.3 billion in 1996.Raytheon acquired Hughes Aircraft Company for $9.5 billion in 1997.Marconi Electronic Systems, a subsidiary of the General Electric Company plc, was acquired by British Aerospace for US$12.3 billion in 1999 merger, to form BAE Systems.
In 2002, when Fairchild Dornier was bankrupt, Airbus, Boeing or Bombardier declined to take the 728JET/928JET large regional jet program as mainline and regional aircraft manufacturers were split and Airbus was digesting its ill-fated Fokker acquisition a decade earlier.On September 4, 2017, United Technologies acquired Rockwell Collins in cash and stock for $23 billion, $30 billion including Rockwell Collins' net debt, for $500+ million of synergies expected by year four.
The Oct. 16, 2017 announcement of the CSeries partnership between Airbus and Bombardier Aerospace could trigger a daisy chain of reactions towards a new order. 
Airbus gets a new, efficient model at the lower end of the narrowbody market which provides the bulk of airliner profits and can abandon the slow selling A319 while Bombardier benefits from the growth in this expanded market even if it holds a smaller residual stake.
Boeing could forge a similar alliance with either Embraer with its E-jet E2 or Mitsubishi Heavy Industries and its MRJ.On 21 December, Boeing and Embraer confirmed to be discussing a potential combination with a transaction subject to Brazilian government regulators, the companies' boards and shareholders approvals.
The weight of Airbus and Boeing could help E2 and CSeries sales but the 100-150 seats market seems slow.
As the CSeries, renamed A220, and E-jet E2 are more capable than their predecessors, they moved closer to the lower end of the narrowbodies.
In 2018, the four Western airframers combined into two within nine months as Boeing acquired 80% of Embraer's airliners for $3.8 billion on July 5.On April 3, 2020, Raytheon and United Technologies Corporation (except Otis Worldwide, leaving Rockwell Collins and engine maker Pratt and Whitney)  merged to form Raytheon Technologies Corporation, with combined sales of $79 billion in 2019.The most prominent unions between 1995 and 2020 include those of Boeing and McDonnell Douglas; the French, German and Spanish parts of EADS; and United Technologies with Rockwell Collins then Raytheon, but many mergers projects did not went through: Textron-Bombardier, EADS-BAE Systems, Hawker Beechcraft-Superior Aviation, GE-Honeywell, BAE Systems-Boeing (or Lockheed Martin), Dassault-Aerospatiale, Safran-Thales, BAE Systems-Rolls-Royce or Lockheed Martin–Northrop Grumman.


== Suppliers ==
The largest aerospace suppliers are United Technologies with $28.2 Billion of revenue, followed by GE Aviation with $24.7 Billion, Safran with $22.5 Billion, Rolls-Royce Holdings with $16.9 Billion, Honeywell Aerospace with $15.2 Billion and Rockwell Collins including B/E Aerospace with $8.1 Billion.
The electric aircraft development could generate large changes for the aerospace suppliers.On 26 November 2018, United Technologies announced the completion of its Rockwell Collins acquisition, renaming systems supplier UTC Aerospace Systems as Collins Aerospace, for $23 billion of sales in 2017 and 70,000 employees, and $39.0 billion of sales in 2017 combined with engine manufacturer Pratt & Whitney.


== Supply chain ==
Before the 1980s/1990s, aircraft and aeroengine manufacturers were vertically integrated.
Then Douglas aircraft outsourced large aerostructures and the Bombardier Global Express pioneered the ""Tier 1"" supply chain model inspired by automotive industry, with 10-12 risk-sharing limited partners funding around half of the development costs.
The Embraer E-Jet followed in the late 1990s with fewer than 40 primary suppliers. 
Tier 1 suppliers were led by Honeywell, Safran, Goodrich Corporation and Hamilton Sundstrand.In the 2000s Rolls-Royce reduced its supplier count after bringing in automotive supply chain executives.
On the Airbus A380, less than 100 major suppliers outsource 60% of its value, even 80% on the A350XWB.
Boeing embraced an aggressive Tier 1 model for the B787 but with its difficulties began to question why it was earning lower margins than its suppliers while it seemed to take all the risk, ensuing its 2011 Partnering for Success initiative, as Airbus initiated its own Scope+ initiative for the A320.
Tier 1 consolidation also affects engine manufacturers : GE Aviation acquired Avio in 2013 and Rolls-Royce plc is taking control of Industria de Turbo Propulsores.


== See also ==
Aerospace
Aviation accidents and incidents
List of aircraft manufacturers
List of spacecraft manufacturers
Military-industrial complex
Aircraft parts industry
Aerospace industry of Russia
Aviation


== References ==


== Further reading ==


== External links ==
""U.S. Aerospace Industries Association"".
""Aerospace, Defense & Government Services – Mergers & Acquisitions (January 1993 - December 2016)"" (PDF). Grundman Advisory. 6 Apr 2017.
Jens Flottau (Feb 22, 2018). ""Opinion: Airframers Should Watch Where They Squeeze Suppliers"". Aviation Week & Space Technology.
Aerospace Craft & Structural Components","pandas(index=5, _1=5, text='an aerospace manufacturer is a company or individual involved in the various aspects of designing, building, testing, selling, and maintaining aircraft, aircraft parts, missiles, rockets, or spacecraft. aerospace is a high technology industry. the aircraft industry is the industry supporting aviation by building aircraft and manufacturing aircraft parts for their maintenance.  this includes aircraft and parts used for civil aviation and military aviation. most production is done pursuant to type certificates and defense standards issued by a government body. this term has been largely subsumed by the more encompassing term: ""aerospace industry"".   == market == in 2015 the aircraft production was worth us$180.3 billion: 61% airliners, 14% business and general aviation, 12% military aircraft, 10% military rotary wing and 3% civil rotary wing; while their mro was worth $135.1 bn or $315.4 bn combined.the global aerospace industry was worth $838 billion in 2017: aircraft & engine oems represented 28% ($235 bn), civil & military mro & upgrades 27% ($226 bn), aircraft systems & component manufacturing 26% ($218 bn), satellites & space 7% ($59 bn), missiles & uavs 5% ($42 bn) and other activity, including flight simulators, defense electronics, public research accounted for 7% ($59 bn). the countries with the largest industry were led by the united states with $408.4 bn (49%) followed by france with $69 bn (8.2%) then china with $61.2 bn (7.3%), united kingdom with $48.8 bn (5.8%), germany with $46.2 bn (5.5%), russia with $27.1 bn (3.2%), canada with $24 bn (2.9%), japan with $21 bn (2.5%), spain with $14 bn (1.7%) and india with $11bn (1.3%). the top 10 countries represent $731 bn or 87.2% of the whole industry.in 2018, the new commercial aircraft value is projected for $270.4 billion while business aircraft will amount for $18 billion and civil helicopters for $4 billion.   == largest companies ==   == geography == in september 2018, pwc ranked aerospace manufacturing attractiveness: the most attractive country was the united states, with $240 billion in sales in 2017, due to the sheer size of the industry (#1) and educated workforce (#1), low geopolitical risk (#4, #1 is japan), strong transportation infrastructure (#5, #1 is hong kong), a healthy economy (#10, #1 is china), but high costs (#7, #1 is denmark) and average tax policy (#36, #1 is qatar). following were canada, singapore, switzerland and united kingdom.within the us, the most attractive was washington state, due to the best industry (#1), leading infrastructure (#4, new jersey is #1) and economy (#4, texas is #1), good labor (#9, massachusetts is #1), average tax policy (#17, alaska is #1) but is costly (#33, montana is #1). washington is tied to boeing commercial airplanes, earning $10.3 billion, is home to 1,400 aerospace-related businesses, and has the highest aerospace jobs concentration. following are texas, georgia, arizona and colorado.in the european union, aerospace companies such as airbus, safran, bae systems, thales, dassault, saab ab, terma a/s, patria plc and leonardo are participants in the global aerospace industry and research effort. in russia, large aerospace companies like oboronprom and the united aircraft corporation (encompassing mikoyan, sukhoi, ilyushin, tupolev, yakovlev, and irkut, which includes beriev) are among the major global players in this industry. in the us, the department of defense and nasa are the two biggest consumers of aerospace technology and products. the bureau of labor statistics of the united states reported that the aerospace industry employed 444,000 wage and salary jobs in 2004, many of which were in washington and california, this marked a steep decline from the peak years during the reagan administration when total employment exceeded 1,000,000 aerospace industry workers.during that period of recovery a special program to restore u.s. competitiveness across all u.s. industries, project socrates, contributed to employment growth as the u.s. aerospace industry captured 72 percent of world aerospace market.  by 1999 u.s. share of the world market fell to 52 percent. important locations of the civil aerospace industry worldwide include seattle, wichita, kansas, dayton, ohio and st. louis in the united states (boeing), montreal and toronto in canada (bombardier, pratt & whitney canada), toulouse and bordeaux in france (airbus, dassault, atr), seville in spain and hamburg in germany (airbus, eads), the north-west of england and bristol in britain (bae systems, airbus and agustawestland), komsomolsk-on-amur and irkutsk in russia (sukhoi, beriev), kyiv and kharkiv in ukraine (antonov), nagoya in japan (mitsubishi heavy industries aerospace and kawasaki heavy industries aerospace), as well as são josé dos campos in brazil where embraer is based.   == consolidation == several consolidations took place in the aerospace and defense industries over the last few decades. bae systems is the successor company to numerous british aircraft manufacturers which merged throughout the second half of the 20th century. many of these mergers followed the 1957 defence white paper.airbus prominently illustrated the european airliner manufacturing consolidation in the late 1960s.between 1988 and 2010, more than 5,452 mergers and acquisitions with a total known-value of us$579 billion were announced worldwide.in 1993, then united states secretary of defense les aspin and his deputy william j. perry held the ""last supper"" at the pentagon with contractors executives who were told that there were twice as many military suppliers as he wanted to see: $55 billion in military-industry mergers took place from 1992 to 1997, leaving mainly boeing, lockheed martin, northrop grumman and raytheon.boeing bought mcdonnell douglas for us$13.3 billion in 1996.raytheon acquired hughes aircraft company for $9.5 billion in 1997.marconi electronic systems, a subsidiary of the general electric company plc, was acquired by british aerospace for us$12.3 billion in 1999 merger, to form bae systems. in 2002, when fairchild dornier was bankrupt, airbus, boeing or bombardier declined to take the 728jet/928jet large regional jet program as mainline and regional aircraft manufacturers were split and airbus was digesting its ill-fated fokker acquisition a decade earlier.on september 4, 2017, united technologies acquired rockwell collins in cash and stock for $23 billion, $30 billion including rockwell collins\' net debt, for $500million of synergies expected by year four. the oct. 16, 2017 announcement of the cseries partnership between airbus and bombardier aerospace could trigger a daisy chain of reactions towards a new order. airbus gets a new, efficient model at the lower end of the narrowbody market which provides the bulk of airliner profits and can abandon the slow selling a319 while bombardier benefits from the growth in this expanded market even if it holds a smaller residual stake. boeing could forge a similar alliance with either embraer with its e-jet e2 or mitsubishi heavy industries and its mrj.on 21 december, boeing and embraer confirmed to be discussing a potential combination with a transaction subject to brazilian government regulators, the companies\' boards and shareholders approvals. the weight of airbus and boeing could help e2 and cseries sales but the 100-150 seats market seems slow. as the cseries, renamed a220, and e-jet e2 are more capable than their predecessors, they moved closer to the lower end of the narrowbodies. in 2018, the four western airframers combined into two within nine months as boeing acquired 80% of embraer\'s airliners for $3.8 billion on july 5.on april 3, 2020, raytheon and united technologies corporation (except otis worldwide, leaving rockwell collins and engine maker pratt and whitney)  merged to form raytheon technologies corporation, with combined sales of $79 billion in 2019.the most prominent unions between 1995 and 2020 include those of boeing and mcdonnell douglas; the french, german and spanish parts of eads; and united technologies with rockwell collins then raytheon, but many mergers projects did not went through: textron-bombardier, eads-bae systems, hawker beechcraft-superior aviation, ge-honeywell, bae systems-boeing (or lockheed martin), dassault-aerospatiale, safran-thales, bae systems-rolls-royce or lockheed martin–northrop grumman.   == suppliers == the largest aerospace suppliers are united technologies with $28.2 billion of revenue, followed by ge aviation with $24.7 billion, safran with $22.5 billion, rolls-royce holdings with $16.9 billion, honeywell aerospace with $15.2 billion and rockwell collins including b/e aerospace with $8.1 billion. the electric aircraft development could generate large changes for the aerospace suppliers.on 26 november 2018, united technologies announced the completion of its rockwell collins acquisition, renaming systems supplier utc aerospace systems as collins aerospace, for $23 billion of sales in 2017 and 70,000 employees, and $39.0 billion of sales in 2017 combined with engine manufacturer pratt & whitney.   == supply chain == before the 1980s/1990s, aircraft and aeroengine manufacturers were vertically integrated. then douglas aircraft outsourced large aerostructures and the bombardier global express pioneered the ""tier 1"" supply chain model inspired by automotive industry, with 10-12 risk-sharing limited partners funding around half of the development costs. the embraer e-jet followed in the late 1990s with fewer than 40 primary suppliers. tier 1 suppliers were led by honeywell, safran, goodrich corporation and hamilton sundstrand.in the 2000s rolls-royce reduced its supplier count after bringing in automotive supply chain executives. on the airbus a380, less than 100 major suppliers outsource 60% of its value, even 80% on the a350xwb. boeing embraced an aggressive tier 1 model for the b787 but with its difficulties began to question why it was earning lower margins than its suppliers while it seemed to take all the risk, ensuing its 2011 partnering for success initiative, as airbus initiated its own scopeinitiative for the a320. tier 1 consolidation also affects engine manufacturers : ge aviation acquired avio in 2013 and rolls-royce plc is taking control of industria de turbo propulsores.   == see also == aerospace aviation accidents and incidents list of aircraft manufacturers list of spacecraft manufacturers military-industrial complex aircraft parts industry aerospace industry of russia aviation   == references ==   == further reading ==   == external links == ""u.s. aerospace industries association"". ""aerospace, defense & government services – mergers & acquisitions (january 1993 - december 2016)"" (pdf). grundman advisory. 6 apr 2017. jens flottau (feb 22, 2018). ""opinion: airframers should watch where they squeeze suppliers"". aviation week & space technology. aerospace craft & structural components')"
6,"In fluid dynamics, a stall is a reduction in the lift coefficient generated by a foil as angle of attack increases. This occurs when the critical angle of attack of the foil is exceeded.  The critical angle of attack is typically about 15 degrees, but it may vary significantly depending on the fluid, foil, and Reynolds number.
Stalls in fixed-wing flight are often experienced as a sudden reduction in lift as the pilot increases the wing's angle of attack and exceeds its critical angle of attack (which may be due to slowing down below stall speed in level flight). A stall does not mean that the engine(s) have stopped working, or that the aircraft has stopped moving—the effect is the same even in an unpowered glider aircraft. Vectored thrust in manned and unmanned aircraft is used to maintain altitude or controlled flight with wings stalled by replacing lost wing lift with engine or propeller thrust, thereby giving rise to post-stall technology.Because stalls are most commonly discussed in connection with aviation, this article discusses stalls as they relate mainly to aircraft, in particular fixed-wing aircraft. The principles of stall discussed here translate to foils in other fluids as well.


== Formal definition ==

A stall is a condition in aerodynamics and aviation such that if the angle of attack increases beyond a certain point, then lift begins to decrease. The angle at which this occurs is called the critical angle of attack. This angle is dependent upon the airfoil section or profile of the wing, its planform, its aspect ratio, and other factors, but is typically in the range of 8 to 20 degrees relative to the incoming wind (""relative wind"") for most subsonic airfoils. The critical angle of attack is the angle of attack on the lift coefficient versus angle-of-attack (Cl~alpha) curve at which the maximum lift coefficient occurs.Stalling is caused by flow separation which, in turn, is caused by the air flowing against a rising pressure. Whitford describes three types of stall: trailing-edge, leading-edge and thin-aerofoil, each with distinctive Cl~alpha features. For the trailing-edge stall, separation begins at small angles of attack near the trailing edge of the wing while the rest of the flow over the wing remains attached. As angle of attack increases, the separated regions on the top of the wing increase in size as the flow separation moves forward, and this hinders the ability of the wing to create lift. This is shown by the reduction in lift-slope on a Cl~alpha curve as the lift nears its maximum value. The separated flow usually causes buffeting. Beyond the critical angle of attack, separated flow is so dominant that additional increases in angle of attack cause the lift to fall from its peak value.
Piston-engined and early jet transports had very good stall behaviour with pre-stall buffet warning and, if ignored, a straight nose-drop for a natural recovery. Wing developments that came with the introduction of turbo-prop engines introduced unacceptable stall behaviour. Leading-edge developments on high-lift wings, and the introduction of rear-mounted engines and high-set tailplanes on the next generation of jet transports, also introduced unacceptable stall behaviour. The probability of achieving the stall speed inadvertently, a potentially hazardous event, had been calculated, in 1965, at about once in every 100,000 flights, often enough to justify the cost of development of warning devices, such as stick shakers, and devices to automatically provide an adequate nose-down pitch, such as stick pushers.When the mean angle of attack of the wings is beyond the stall a spin, which is an autorotation of a stalled wing, may develop. A spin follows departures in roll, yaw and pitch from balanced flight. For example, a roll is naturally damped with an unstalled wing, but with wings stalled the damping moment is replaced with a propelling moment.


== Variation of lift with angle of attack ==

The graph shows that the greatest amount of lift is produced as the critical angle of attack is reached (which in early-20th century aviation was called the ""burble point""). This angle is 17.5 degrees in this case, but it varies from airfoil to airfoil. In particular, for aerodynamically thick airfoils (thickness to chord ratios of around 10%), the critical angle is higher than with a thin airfoil of the same camber. Symmetric airfoils have lower critical angles (but also work efficiently in inverted flight). The graph shows that, as the angle of attack exceeds the critical angle, the lift produced by the airfoil decreases.
The information in a graph of this kind is gathered using a model of the airfoil in a wind tunnel. Because aircraft models are normally used, rather than full-size machines, special care is needed to make sure that data is taken in the same Reynolds number regime (or scale speed) as in free flight. The separation of flow from the upper wing surface at high angles of attack is quite different at low Reynolds number from that at the high Reynolds numbers of real aircraft. In particular at high Reynolds numbers the flow tends to stay attached to the airfoil for longer because the inertial forces are dominant with respect to the viscous forces which are responsible for the flow separation ultimately leading to the aerodynamic stall. For this reason wind tunnel results carried out at lower speeds and on smaller scales models of the real life counterparts often tend to overestimate the aerodynamic stall angle of attack. High-pressure wind tunnels are one solution to this problem.
In general, steady operation of an aircraft at an angle of attack above the critical angle is not possible because, after exceeding the critical angle, the loss of lift from the wing causes the nose of the aircraft to fall, reducing the angle of attack again. This nose drop, independent of control inputs, indicates the pilot has actually stalled the aircraft.This graph shows the stall angle, yet in practice most pilot operating handbooks (POH) or generic flight manuals describe stalling in terms of airspeed. This is because all aircraft are equipped with an airspeed indicator, but fewer aircraft have an angle of attack indicator. An aircraft's stalling speed is published by the manufacturer (and is required for certification by flight testing) for a range of weights and flap positions, but the stalling angle of attack is not published.
As speed reduces, angle of attack has to increase to keep lift constant until the critical angle is reached. The airspeed at which this angle is reached is the (1g, unaccelerated) stalling speed of the aircraft in that particular configuration. Deploying flaps/slats decreases the stall speed to allow the aircraft to take off and land at a lower speed.


== Aerodynamic description ==


=== Fixed-wing aircraft ===
A fixed-wing aircraft can be made to stall in any pitch attitude or bank angle or at any airspeed but deliberate stalling is commonly practiced by reducing the speed to the unaccelerated stall speed, at a safe altitude. Unaccelerated (1g) stall speed varies on different fixed-wing aircraft and is represented by colour codes on the airspeed indicator. As the plane flies at this speed, the angle of attack must be increased to prevent any loss of altitude or gain in airspeed (which corresponds to the stall angle described above). The pilot will notice the flight controls have become less responsive and may also notice some buffeting, a result of the turbulent air separated from the wing hitting the tail of the aircraft.
In most light aircraft, as the stall is reached, the aircraft will start to descend (because the wing is no longer producing enough lift to support the aircraft's weight) and the nose will pitch down. Recovery from the stall involves lowering the aircraft nose, to decrease the angle of attack and increase the air speed, until smooth air-flow over the wing is restored. Normal flight can be resumed once recovery is complete. The maneuver is normally quite safe, and, if correctly handled, leads to only a small loss in altitude (20–30 m/50–100 ft). It is taught and practised in order for pilots to recognize, avoid, and recover from stalling the aircraft. A pilot is required to demonstrate competency in controlling an aircraft during and after a stall for certification in the United States, and it is a routine maneuver for pilots when getting to know the handling of an unfamiliar aircraft type. The only dangerous aspect of a stall is a lack of altitude for recovery.
A special form of asymmetric stall in which the aircraft also rotates about its yaw axis is called a spin. A spin can occur if an aircraft is stalled and there is an asymmetric yawing moment applied to it. This yawing moment can be aerodynamic (sideslip angle, rudder, adverse yaw from the ailerons), thrust related (p-factor, one engine inoperative on a multi-engine non-centreline thrust aircraft), or from less likely sources such as severe turbulence. The net effect is that one wing is stalled before the other and the aircraft descends rapidly while rotating, and some aircraft cannot recover from this condition without correct pilot control inputs (which must stop yaw) and loading. A new solution to the problem of difficult (or impossible) stall-spin recovery is provided by the ballistic parachute recovery system.
The most common stall-spin scenarios occur on takeoff (departure stall) and during landing (base to final turn) because of insufficient airspeed during these maneuvers. Stalls also occur during a go-around manoeuvre if the pilot does not properly respond to the out-of-trim situation resulting from the transition from low power setting to high power setting at low speed. Stall speed is increased when the wing surfaces are contaminated with ice or frost creating a rougher surface, and heavier airframe due to ice accumulation.
Stalls occur not only at slow airspeed, but at any speed when the wings exceed their critical angle of attack. Attempting to increase the angle of attack at 1g by moving the control column back normally causes the aircraft to climb. However, aircraft often experience higher g-forces, such as when turning steeply or pulling out of a dive. In these cases, the wings are already operating at a higher angle of attack to create the necessary force (derived from lift) to accelerate in the desired direction. Increasing the g-loading still further, by pulling back on the controls, can cause the stalling angle to be exceeded, even though the aircraft is flying at a high speed. These ""high-speed stalls"" produce the same buffeting characteristics as 1g stalls and can also initiate a spin if there is also any yawing.


=== Characteristics ===
Different aircraft types have different stalling characteristics but they only have to be good enough to satisfy their particular Airworthiness authority. For example, the Short Belfast heavy freighter  had a marginal nose drop which was acceptable to the Royal Air Force. When the aircraft were sold to a civil operator they had to be fitted with a stick pusher to meet the civil requirements. Some aircraft may naturally have very good behaviour well beyond what is required. For example, first generation jet transports have been described as having an immaculate nose drop at the stall. Loss of lift on one wing is acceptable as long as the roll, including during stall recovery, doesn't exceed about 20 degrees, or in turning flight the roll shall not exceed 90 degrees bank. If pre-stall warning followed by nose drop and limited wing drop are naturally not present or are deemed to be unacceptably marginal by an Airworthiness authority the stalling behaviour has to be made good enough with airframe modifications or devices such as a stick shaker and pusher. These are described in ""Warning and safety devices"".


== Stall speeds ==

Stalls depend only on angle of attack, not airspeed. However, the slower an aircraft flies, the greater the angle of attack it needs to produce lift equal to the aircraft's weight. As the speed decreases further, at some point this angle will be equal to the critical (stall) angle of attack. This speed is called the ""stall speed"". An aircraft flying at its stall speed cannot climb, and an aircraft flying below its stall speed cannot stop descending. Any attempt to do so by increasing angle of attack, without first increasing airspeed, will result in a stall.
The actual stall speed will vary depending on the airplane's weight, altitude, configuration, and vertical and lateral acceleration. Speed definitions vary and include:

VS: Stall speed: the speed at which the airplane exhibits those qualities accepted as defining the stall.
VS0: The stall speed or minimum steady flight speed in landing configuration. The zero-thrust stall speed at the most extended landing flap setting.
VS1: The stall speed or minimum steady flight speed obtained in a specified configuration. The zero thrust stall speed at a specified flap setting.An airspeed indicator, for the purpose of flight-testing, may have the following markings: the bottom of the white arc indicates VS0 at maximum weight, while the bottom of the green arc indicates VS1 at maximum weight. While an aircraft's VS speed is computed by design, its VS0 and VS1 speeds must be demonstrated empirically by flight testing.


== In accelerated and turning flight ==

The normal stall speed, specified by the VS values above, always refers to straight and level flight, where the load factor is equal to 1g. However, if the aircraft is turning or pulling up from a dive, additional lift is required to provide the vertical or lateral acceleration, and so the stall speed is higher. An accelerated stall is a stall that occurs under such conditions.In a banked turn, the lift required is equal to the weight of the aircraft plus extra lift to provide the centripetal force necessary to perform the turn:

  
    
      
        L
        =
        n
        W
      
    
    {\displaystyle L=nW}
  where:

  
    
      
        L
      
    
    {\displaystyle L}
   = lift

  
    
      
        n
      
    
    {\displaystyle n}
   = load factor (greater than 1 in a turn)

  
    
      
        W
      
    
    {\displaystyle W}
   = weight of the aircraftTo achieve the extra lift, the lift coefficient, and so the angle of attack, will have to be higher than it would be in straight and level flight at the same speed. Therefore, given that the stall always occurs at the same critical angle of attack, by increasing the load factor (e.g. by tightening the turn) the critical angle will be reached at a higher airspeed:

  
    
      
        
          V
          
            st
          
        
        =
        
          V
          
            s
          
        
        
          
            n
          
        
      
    
    {\displaystyle V_{\text{st}}=V_{\text{s}}{\sqrt {n}}}
  where:

  
    
      
        
          V
          
            st
          
        
      
    
    {\displaystyle V_{\text{st}}}
   = stall speed

  
    
      
        
          V
          
            s
          
        
      
    
    {\displaystyle V_{\text{s}}}
   = stall speed of the aircraft in straight, level flight

  
    
      
        n
      
    
    {\displaystyle n}
   = load factorThe table that follows gives some examples of the relation between the angle of bank and the square root of the load factor. It derives from the trigonometric relation (secant) between 
  
    
      
        L
      
    
    {\displaystyle L}
   and 
  
    
      
        W
      
    
    {\displaystyle W}
  .

For example, in a turn with bank angle of 45°, Vst is 19% higher than Vs.
According to Federal Aviation Administration (FAA) terminology, the above example illustrates a so-called turning flight stall, while the term accelerated is used to indicate an accelerated turning stall only, that is, a turning flight stall where the airspeed decreases at a given rate.Accelerated stalls also pose a risk in powerful propeller aircraft with a tendency to roll in reaction to engine torque. When such an aircraft is flying close to its stall speed in straight and level flight, the sudden application of full power may roll the aircraft and create the same aerodynamic conditions that induce an accelerated stall in turning flight. An aircraft that displays this rolling tendency is the Mitsubishi MU-2; pilots of this aircraft are trained to avoid sudden and drastic increases in power at low altitude and low airspeed, as an accelerated stall under these conditions is very difficult to safely recover from.A notable example of an air accident involving a low-altitude turning flight stall is the 1994 Fairchild Air Force Base B-52 crash.


== Types ==


=== Dynamic stall ===
Dynamic stall is a non-linear unsteady aerodynamic effect that occurs when airfoils rapidly change the angle of attack. The rapid change can cause a strong vortex to be shed from the leading edge of the aerofoil, and travel backwards above the wing. The vortex, containing high-velocity airflows, briefly increases the lift produced by the wing. As soon as it passes behind the trailing edge, however, the lift reduces dramatically, and the wing is in normal stall.Dynamic stall is an effect most associated with helicopters and flapping wings, though also occurs in wind turbines, and due to gusting airflow. During forward flight, some regions of a helicopter blade may incur flow that reverses (compared to the direction of blade movement), and thus includes rapidly changing angles of attack. Oscillating (flapping) wings, such as those of insects like the bumblebee—may rely almost entirely on dynamic stall for lift production, provided the oscillations are fast compared to the speed of flight, and the angle of the wing changes rapidly compared to airflow direction.Stall delay can occur on airfoils subject to a high angle of attack and a three-dimensional flow. When the angle of attack on an airfoil is increasing rapidly, the flow will remain substantially attached to the airfoil to a significantly higher angle of attack than can be achieved in steady-state conditions. As a result, the stall is delayed momentarily and a lift coefficient significantly higher than the steady-state maximum is achieved. The effect was first noticed on propellers.


=== Deep stall ===

A deep stall (or super-stall) is a dangerous type of stall that affects certain aircraft designs, notably jet aircraft with a T-tail configuration and rear-mounted engines. In these designs, the turbulent wake of a stalled main wing, nacelle-pylon wakes and the wake from the fuselage ""blanket"" the horizontal stabilizer, rendering the elevators ineffective and preventing the aircraft from recovering from the stall. Taylor states T-tail propeller aircraft, unlike jet aircraft, do not usually require a stall recovery system during stall flight testing due to increased airflow over the wing root from the prop wash. Nor do they have rear mounted nacelles which can contribute substantially to the problem. The A400M was fitted with a vertical tail booster for some flight tests in case of deep stall.Trubshaw gives a broad definition of deep stall as penetrating to such angles of attack 
  
    
      
        α
      
    
    {\textstyle \alpha }
   that pitch control effectiveness is reduced by the wing and nacelle wakes. He also gives a definition that relates deep stall to a locked-in condition where recovery is impossible. This is a single value of 
  
    
      
        α
      
    
    {\textstyle \alpha }
  , for a given aircraft configuration, where there is no pitching moment, i.e. a trim point.
Typical values both for the range of deep stall, as defined above, and the locked-in trim point are given for the Douglas DC-9 Series 10 by Schaufele. These values are from wind tunnel tests for an early design. The final design had no locked in trim point so recovery from the deep stall region was possible, as required to meet certification rules. Normal stall beginning at the 'g' break (sudden decrease of the vertical load factor) was at 18 degrees 
  
    
      
        α
      
    
    {\textstyle \alpha }
  , deep stall started at about 30 degrees and the locked-in unrecoverable trim point was at 47 degrees.
The very high 
  
    
      
        α
      
    
    {\textstyle \alpha }
   for a deep stall locked-in condition occurs well beyond the normal stall but can be attained very rapidly as the aircraft is unstable beyond the normal stall and requires immediate action to arrest it. The loss of lift causes high sink rates which, together with the low forward speed at the normal stall, give a high 
  
    
      
        α
      
    
    {\textstyle \alpha }
   with little or no rotation of the aircraft. BAC 1-11 G-ASHG, during stall flight tests before the type was modified to prevent a locked-in deep stall condition, descended at over 10,000 feet per minute (50 m/s) and struck the ground in a flat attitude moving only 70 feet (20 m) forward after initial impact. Sketches which show how the wing wake blankets the tail may be misleading if they imply that deep stall requires a high body angle. Taylor and Ray show how the aircraft attitude in the deep stall is relatively flat, even less than during the normal stall, with very high negative flight path angles.
Effects similar to deep stall had been known to occur on some aircraft designs before the term was coined. A prototype Gloster Javelin (serial WD808) was lost in a crash on 11 June 1953, to a ""locked in"" stall. However, Waterton states that the trimming tailplane was found to be the wrong way for recovery. Low speed handling tests were being done to assess a new wing. Handley Page Victor XL159 was lost to a ""stable stall"" on 23 March 1962. It had been clearing the fixed droop leading edge with the test being stall approach, landing configuration, C of G aft. The brake parachute had not been streamed as it may have hindered rear crew escape.The name ""deep stall"" first came into widespread use after the crash of the prototype BAC 1-11 G-ASHG on 22 October 1963, which killed its crew. This led to changes to the aircraft, including the installation of a stick shaker (see below) to clearly warn the pilot of an impending stall. Stick shakers are now a standard part of commercial airliners. Nevertheless, the problem continues to cause accidents; on 3 June 1966, a Hawker Siddeley Trident (G-ARPY), was lost to deep stall; deep stall is suspected to be cause of another Trident (the British European Airways Flight 548 G-ARPI) crash – known as the ""Staines Disaster"" – on 18 June 1972 when the crew failed to notice the conditions and had disabled the stall recovery system. On 3 April 1980, a prototype of the Canadair Challenger business jet crashed after initially entering a deep stall from 17,000 ft and having both engines flame-out. It recovered from the deep stall after deploying the anti-spin parachute but crashed after being unable to jettison the chute or relight the engines. One of the test pilots was unable to escape from the aircraft in time and was killed. On the 26 July 1993, a Canadair CRJ-100 was lost in flight testing due to a deep stall. It has been reported that a Boeing 727 entered a deep stall in a flight test, but the pilot was able to rock the airplane to increasingly higher bank angles until the nose finally fell through and normal control response was recovered. A 727 accident on 1 December 1974, has also been attributed to a deep stall. The crash of West Caribbean Airways Flight 708 in 2005 was also attributed to a deep stall.
Deep stalls can occur at apparently normal pitch attitudes, if the aircraft is descending quickly enough. The airflow is coming from below, so the angle of attack is increased. Early speculation on reasons for the crash of Air France Flight 447 blamed an unrecoverable deep stall since it descended in an almost flat attitude (15 degrees) at an angle of attack of 35 degrees or more. However it was held in a stalled glide by the pilots who held the nose up amid all the confusion of what was actually happening to the aircraft.Canard-configured aircraft are also at risk of getting into a deep stall. Two Velocity aircraft crashed due to locked-in deep stalls. Testing revealed that the addition of leading-edge cuffs to the outboard wing prevented the aircraft from getting into a deep stall. The Piper Advanced Technologies PAT-1, N15PT, another canard-configured aircraft, also crashed in an accident attributed to a deep stall. Wind tunnel testing of the design at the NASA Langley Research Center showed that it was vulnerable to a deep stall.In the early 1980s, a Schweizer SGS 1-36 sailplane was modified for NASA's controlled deep-stall flight program.


=== Tip stall ===
Wing sweep and taper cause stalling at the tip of a wing before the root. The position of a swept wing along the fuselage has to be such that the lift from the wing root, well forward of the aircraft center of gravity (c.g.), must be balanced by the wing tip, well aft of the c.g. If the tip stalls first the balance of the aircraft is upset causing dangerous nose pitch up. Swept wings have to incorporate features which prevent pitch-up caused by premature tip stall.
A swept wing has a higher lift coefficient on its outer panels than on the inner wing, causing them to reach their maximum lift capability first and to stall first. This is caused by the downwash pattern associated with swept/tapered wings. To delay tip stall the outboard wing is given washout to reduce its angle of attack. The root can also be modified with a suitable leading-edge and airfoil section to make sure it stalls before the tip. However, when taken beyond stalling incidence the tips may still become fully stalled before the inner wing despite initial separation occurring inboard. This causes pitch-up after the stall and entry to a super-stall on those aircraft with super-stall characteristics. Span-wise flow of the boundary layer is also present on swept wings and causes tip stall. The amount of boundary layer air flowing outboard can be reduced by generating vortices with a leading-edge device such as a fence, notch, saw tooth or a set of vortex generators behind the leading edge.


== Warning and safety devices ==
Fixed-wing aircraft can be equipped with devices to prevent or postpone a stall or to make it less (or in some cases more) severe, or to make recovery easier.

An aerodynamic twist can be introduced to the wing with the leading edge near the wing tip twisted downward. This is called washout and causes the wing root to stall before the wing tip. This makes the stall gentle and progressive. Since the stall is delayed at the wing tips, where the ailerons are, roll control is maintained when the stall begins.
A stall strip is a small sharp-edged device that, when attached to the leading edge of a wing, encourages the stall to start there in preference to any other location on the wing. If attached close to the wing root, it makes the stall gentle and progressive; if attached near the wing tip, it encourages the aircraft to drop a wing when stalling.
A stall fence is a flat plate in the direction of the chord to stop separated flow progressing out along the wing
Vortex generators, tiny strips of metal or plastic placed on top of the wing near the leading edge that protrude past the boundary layer into the free stream. As the name implies, they energize the boundary layer by mixing free stream airflow with boundary layer flow thereby creating vortices, this increases the momentum in the boundary layer. By increasing the momentum of the boundary layer, airflow separation and the resulting stall may be delayed.
An anti-stall strake is a leading edge extension that generates a vortex on the wing upper surface to postpone the stall.
A stick pusher is a mechanical device that prevents the pilot from stalling an aircraft. It pushes the elevator control forward as the stall is approached, causing a reduction in the angle of attack. In generic terms, a stick pusher is known as a stall identification device or stall identification system.
A stick shaker is a mechanical device that shakes the pilot's controls to warn of the onset of stall.
A stall warning is an electronic or mechanical device that sounds an audible warning as the stall speed is approached. The majority of aircraft contain some form of this device that warns the pilot of an impending stall. The simplest such device is a stall warning horn, which consists of either a pressure sensor or a movable metal tab that actuates a switch, and produces an audible warning in response.
An angle-of-attack indicator for light aircraft, the ""AlphaSystemsAOA"" and a nearly identical ""Lift Reserve Indicator"", are both pressure differential instruments that display margin above stall and/or angle of attack on an instantaneous, continuous readout. The General Technics CYA-100 displays true angle of attack via a magnetically coupled vane. An AOA indicator provides a visual display of the amount of available lift throughout its slow speed envelope regardless of the many variables that act upon an aircraft. This indicator is immediately responsive to changes in speed, angle of attack, and wind conditions, and automatically compensates for aircraft weight, altitude, and temperature.
An angle of attack limiter or an ""alpha"" limiter is a flight computer that automatically prevents pilot input from causing the plane to rise over the stall angle. Some alpha limiters can be disabled by the pilot.Stall warning systems often involve inputs from a broad range of sensors and systems to include a dedicated angle of attack sensor.
Blockage, damage, or inoperation of stall and angle of attack (AOA) probes can lead to unreliability of the stall warning, and cause the stick pusher, overspeed warning, autopilot, and yaw damper to malfunction.If a forward canard is used for pitch control, rather than an aft tail, the canard is designed to meet the airflow at a slightly greater angle of attack than the wing. Therefore, when the aircraft pitch increases abnormally, the canard will usually stall first, causing the nose to drop and so preventing the wing from reaching its critical AOA. Thus, the risk of main wing stalling is greatly reduced. However, if the main wing stalls, recovery becomes difficult, as the canard is more deeply stalled and angle of attack increases rapidly.If an aft tail is used, the wing is designed to stall before the tail. In this case, the wing can be flown at higher lift coefficient (closer to stall) to produce more overall lift.
Most military combat aircraft have an angle of attack indicator among the pilot's instruments, which lets the pilot know precisely how close to the stall point the aircraft is. Modern airliner instrumentation may also measure angle of attack, although this information may not be directly displayed on the pilot's display, instead driving a stall warning indicator or giving performance information to the flight computer (for fly by wire systems).


== Flight beyond the stall ==
As a wing stalls, aileron effectiveness is reduced, rendering the plane difficult to control and increasing the risk of a spin. Post stall, steady flight beyond the stalling angle (where the coefficient of lift is largest) requires engine thrust to replace lift as well as alternative controls to replace the loss of effectiveness of the ailerons. For high-powered aircraft, the loss of lift (and increase in drag) beyond the stall angle is less of a problem than maintaining control. Some aircraft may be subject to post-stall gyration (e.g. the F-4) or susceptible to entering a flat-spin (e.g. F-14). Control beyond-stall can be provided by reaction control systems (e.g. NF-104A), vectored thrust, as well as a rolling stabilator (or taileron). The enhanced manoeuvering capability by flights at very high angles of attack can provide a tactical advantage for military fighters such as the F-22 Raptor. Short-term stalls at 90–120° (e.g. Pugachev's Cobra) are sometimes performed at airshows. The highest angle of attack in sustained flight so far demonstrated was 70 degrees in the X-31 at the Dryden Flight Research Center.  Sustained post-stall flight is a type of supermaneuverability.


== Spoilers ==

Except for flight training, airplane testing, and aerobatics, a stall is usually an undesirable event. Spoilers (sometimes called lift dumpers), however, are devices that are intentionally deployed to create a carefully controlled flow separation over part of an aircraft's wing to reduce the lift it generates, increase the drag, and allow the aircraft to descend more rapidly without gaining speed. Spoilers are also deployed asymmetrically (one wing only) to enhance roll control. Spoilers can also be used on aborted take-offs and after main wheel contact on landing to increase the aircraft's weight on its wheels for better braking action.
Unlike powered airplanes, which can control descent by increasing or decreasing thrust, gliders have to increase drag to increase the rate of descent. In high-performance gliders, spoiler deployment is extensively used to control the approach to landing.
Spoilers can also be thought of as  ""lift reducers"" because they reduce the lift of the wing in which the spoiler resides. For example, an uncommanded roll to the left could be reversed by raising the right wing spoiler (or only a few of the spoilers present in large airliner wings). This has the advantage of avoiding the need to increase lift in the wing that is dropping (which may bring that wing closer to stalling).


== History ==
Otto Lilienthal died while flying in 1896 as the result of a stall. Wilbur Wright encountered stalls for the first time in 1901, while flying his second glider. Awareness of Lilienthal's accident and Wilbur's experience, motivated the Wright Brothers to design their plane in ""canard"" configuration. This made recoveries from stalls easier and more gentle. The design saved the brothers' lives more than once.The aircraft engineer Juan de la Cierva worked on his ""Autogiro"" project to develop a rotary wing aircraft which, he hoped, would be unable to stall and which therefore would be safer than aeroplanes. In developing the resulting ""autogyro"" aircraft, he solved many engineering problems which made the helicopter possible.


== See also ==
ArticlesAviation safety
Coffin corner (aerodynamics)
Compressor stall
Lift coefficient
Spin (flight)
Spoiler (aeronautics)
Wing twistNotable accidents1963 BAC One-Eleven test crash
1966 Felthorpe Trident crash
British European Airways Flight 548
China Airlines Flight 140
China Airlines Flight 676
Air France Flight 447
Colgan Air Flight 3407
Turkish Airlines Flight 1951
Indonesia AirAsia Flight 8501


== Notes ==


== References ==
USAF & NATO Report RTO-TR-015 AC/323/(HFM-015)/TP-1 (2001
Anderson, J.D., A History of Aerodynamics (1997). Cambridge University Press.  ISBN 0-521-66955-3
Chapter 4, ""Slow Flight, Stalls, and Spins,"" in the Airplane Flying Handbook. (FAA H-8083-3A)
L. J. Clancy (1975), Aerodynamics, Pitman Publishing Limited, London.  ISBN 0-273-01120-0
Stengel, R. (2004), Flight Dynamics, Princeton University Press, ISBN 0-691-11407-2
Alpha Systems AOA Website for information on AOA and Lift Reserve Indicators [1]
4239-01 Angle of Attack (AoA) Sensor Specifications [2]
Airplane flying Handbook. Federal Aviation Administration ISBN 1-60239-003-7  Pub. Skyhorse Publishing Inc.
Federal Aviation Administration (25 September 2000), Stall and Spin Awareness Training, AC No: 61-67C
Prof. Dr Mustafa Cavcar, ""Stall Speed"" [3]","pandas(index=6, _1=6, text='in fluid dynamics, a stall is a reduction in the lift coefficient generated by a foil as angle of attack increases. this occurs when the critical angle of attack of the foil is exceeded.  the critical angle of attack is typically about 15 degrees, but it may vary significantly depending on the fluid, foil, and reynolds number. stalls in fixed-wing flight are often experienced as a sudden reduction in lift as the pilot increases the wing\'s angle of attack and exceeds its critical angle of attack (which may be due to slowing down below stall speed in level flight). a stall does not mean that the engine(s) have stopped working, or that the aircraft has stopped moving—the effect is the same even in an unpowered glider aircraft. vectored thrust in manned and unmanned aircraft is used to maintain altitude or controlled flight with wings stalled by replacing lost wing lift with engine or propeller thrust, thereby giving rise to post-stall technology.because stalls are most commonly discussed in connection with aviation, this article discusses stalls as they relate mainly to aircraft, in particular fixed-wing aircraft. the principles of stall discussed here translate to foils in other fluids as well.   == formal definition ==  a stall is a condition in aerodynamics and aviation such that if the angle of attack increases beyond a certain point, then lift begins to decrease. the angle at which this occurs is called the critical angle of attack. this angle is dependent upon the airfoil section or profile of the wing, its planform, its aspect ratio, and other factors, but is typically in the range of 8 to 20 degrees relative to the incoming wind (""relative wind"") for most subsonic airfoils. the critical angle of attack is the angle of attack on the lift coefficient versus angle-of-attack (cl~alpha) curve at which the maximum lift coefficient occurs.stalling is caused by flow separation which, in turn, is caused by the air flowing against a rising pressure. whitford describes three types of stall: trailing-edge, leading-edge and thin-aerofoil, each with distinctive cl~alpha features. for the trailing-edge stall, separation begins at small angles of attack near the trailing edge of the wing while the rest of the flow over the wing remains attached. as angle of attack increases, the separated regions on the top of the wing increase in size as the flow separation moves forward, and this hinders the ability of the wing to create lift. this is shown by the reduction in lift-slope on a cl~alpha curve as the lift nears its maximum value. the separated flow usually causes buffeting. beyond the critical angle of attack, separated flow is so dominant that additional increases in angle of attack cause the lift to fall from its peak value. piston-engined and early jet transports had very good stall behaviour with pre-stall buffet warning and, if ignored, a straight nose-drop for a natural recovery. wing developments that came with the introduction of turbo-prop engines introduced unacceptable stall behaviour. leading-edge developments on high-lift wings, and the introduction of rear-mounted engines and high-set tailplanes on the next generation of jet transports, also introduced unacceptable stall behaviour. the probability of achieving the stall speed inadvertently, a potentially hazardous event, had been calculated, in 1965, at about once in every 100,000 flights, often enough to justify the cost of development of warning devices, such as stick shakers, and devices to automatically provide an adequate nose-down pitch, such as stick pushers.when the mean angle of attack of the wings is beyond the stall a spin, which is an autorotation of a stalled wing, may develop. a spin follows departures in roll, yaw and pitch from balanced flight. for example, a roll is naturally damped with an unstalled wing, but with wings stalled the damping moment is replaced with a propelling moment.   == variation of lift with angle of attack ==  the graph shows that the greatest amount of lift is produced as the critical angle of attack is reached (which in early-20th century aviation was called the ""burble point""). this angle is 17.5 degrees in this case, but it varies from airfoil to airfoil. in particular, for aerodynamically thick airfoils (thickness to chord ratios of around 10%), the critical angle is higher than with a thin airfoil of the same camber. symmetric airfoils have lower critical angles (but also work efficiently in inverted flight). the graph shows that, as the angle of attack exceeds the critical angle, the lift produced by the airfoil decreases. the information in a graph of this kind is gathered using a model of the airfoil in a wind tunnel. because aircraft models are normally used, rather than full-size machines, special care is needed to make sure that data is taken in the same reynolds number regime (or scale speed) as in free flight. the separation of flow from the upper wing surface at high angles of attack is quite different at low reynolds number from that at the high reynolds numbers of real aircraft. in particular at high reynolds numbers the flow tends to stay attached to the airfoil for longer because the inertial forces are dominant with respect to the viscous forces which are responsible for the flow separation ultimately leading to the aerodynamic stall. for this reason wind tunnel results carried out at lower speeds and on smaller scales models of the real life counterparts often tend to overestimate the aerodynamic stall angle of attack. high-pressure wind tunnels are one solution to this problem. in general, steady operation of an aircraft at an angle of attack above the critical angle is not possible because, after exceeding the critical angle, the loss of lift from the wing causes the nose of the aircraft to fall, reducing the angle of attack again. this nose drop, independent of control inputs, indicates the pilot has actually stalled the aircraft.this graph shows the stall angle, yet in practice most pilot operating handbooks (poh) or generic flight manuals describe stalling in terms of airspeed. this is because all aircraft are equipped with an airspeed indicator, but fewer aircraft have an angle of attack indicator. an aircraft\'s stalling speed is published by the manufacturer (and is required for certification by flight testing) for a range of weights and flap positions, but the stalling angle of attack is not published. as speed reduces, angle of attack has to increase to keep lift constant until the critical angle is reached. the airspeed at which this angle is reached is the (1g, unaccelerated) stalling speed of the aircraft in that particular configuration. deploying flaps/slats decreases the stall speed to allow the aircraft to take off and land at a lower speed.   == aerodynamic description == wing sweep and taper cause stalling at the tip of a wing before the root. the position of a swept wing along the fuselage has to be such that the lift from the wing root, well forward of the aircraft center of gravity (c.g.), must be balanced by the wing tip, well aft of the c.g. if the tip stalls first the balance of the aircraft is upset causing dangerous nose pitch up. swept wings have to incorporate features which prevent pitch-up caused by premature tip stall. a swept wing has a higher lift coefficient on its outer panels than on the inner wing, causing them to reach their maximum lift capability first and to stall first. this is caused by the downwash pattern associated with swept/tapered wings. to delay tip stall the outboard wing is given washout to reduce its angle of attack. the root can also be modified with a suitable leading-edge and airfoil section to make sure it stalls before the tip. however, when taken beyond stalling incidence the tips may still become fully stalled before the inner wing despite initial separation occurring inboard. this causes pitch-up after the stall and entry to a super-stall on those aircraft with super-stall characteristics. span-wise flow of the boundary layer is also present on swept wings and causes tip stall. the amount of boundary layer air flowing outboard can be reduced by generating vortices with a leading-edge device such as a fence, notch, saw tooth or a set of vortex generators behind the leading edge.   == warning and safety devices == fixed-wing aircraft can be equipped with devices to prevent or postpone a stall or to make it less (or in some cases more) severe, or to make recovery easier.  an aerodynamic twist can be introduced to the wing with the leading edge near the wing tip twisted downward. this is called washout and causes the wing root to stall before the wing tip. this makes the stall gentle and progressive. since the stall is delayed at the wing tips, where the ailerons are, roll control is maintained when the stall begins. a stall strip is a small sharp-edged device that, when attached to the leading edge of a wing, encourages the stall to start there in preference to any other location on the wing. if attached close to the wing root, it makes the stall gentle and progressive; if attached near the wing tip, it encourages the aircraft to drop a wing when stalling. a stall fence is a flat plate in the direction of the chord to stop separated flow progressing out along the wing vortex generators, tiny strips of metal or plastic placed on top of the wing near the leading edge that protrude past the boundary layer into the free stream. as the name implies, they energize the boundary layer by mixing free stream airflow with boundary layer flow thereby creating vortices, this increases the momentum in the boundary layer. by increasing the momentum of the boundary layer, airflow separation and the resulting stall may be delayed. an anti-stall strake is a leading edge extension that generates a vortex on the wing upper surface to postpone the stall. a stick pusher is a mechanical device that prevents the pilot from stalling an aircraft. it pushes the elevator control forward as the stall is approached, causing a reduction in the angle of attack. in generic terms, a stick pusher is known as a stall identification device or stall identification system. a stick shaker is a mechanical device that shakes the pilot\'s controls to warn of the onset of stall. a stall warning is an electronic or mechanical device that sounds an audible warning as the stall speed is approached. the majority of aircraft contain some form of this device that warns the pilot of an impending stall. the simplest such device is a stall warning horn, which consists of either a pressure sensor or a movable metal tab that actuates a switch, and produces an audible warning in response. an angle-of-attack indicator for light aircraft, the ""alphasystemsaoa"" and a nearly identical ""lift reserve indicator"", are both pressure differential instruments that display margin above stall and/or angle of attack on an instantaneous, continuous readout. the general technics cya-100 displays true angle of attack via a magnetically coupled vane. an aoa indicator provides a visual display of the amount of available lift throughout its slow speed envelope regardless of the many variables that act upon an aircraft. this indicator is immediately responsive to changes in speed, angle of attack, and wind conditions, and automatically compensates for aircraft weight, altitude, and temperature. an angle of attack limiter or an ""alpha"" limiter is a flight computer that automatically prevents pilot input from causing the plane to rise over the stall angle. some alpha limiters can be disabled by the pilot.stall warning systems often involve inputs from a broad range of sensors and systems to include a dedicated angle of attack sensor. blockage, damage, or inoperation of stall and angle of attack (aoa) probes can lead to unreliability of the stall warning, and cause the stick pusher, overspeed warning, autopilot, and yaw damper to malfunction.if a forward canard is used for pitch control, rather than an aft tail, the canard is designed to meet the airflow at a slightly greater angle of attack than the wing. therefore, when the aircraft pitch increases abnormally, the canard will usually stall first, causing the nose to drop and so preventing the wing from reaching its critical aoa. thus, the risk of main wing stalling is greatly reduced. however, if the main wing stalls, recovery becomes difficult, as the canard is more deeply stalled and angle of attack increases rapidly.if an aft tail is used, the wing is designed to stall before the tail. in this case, the wing can be flown at higher lift coefficient (closer to stall) to produce more overall lift. most military combat aircraft have an angle of attack indicator among the pilot\'s instruments, which lets the pilot know precisely how close to the stall point the aircraft is. modern airliner instrumentation may also measure angle of attack, although this information may not be directly displayed on the pilot\'s display, instead driving a stall warning indicator or giving performance information to the flight computer (for fly by wire systems).   == flight beyond the stall == as a wing stalls, aileron effectiveness is reduced, rendering the plane difficult to control and increasing the risk of a spin. post stall, steady flight beyond the stalling angle (where the coefficient of lift is largest) requires engine thrust to replace lift as well as alternative controls to replace the loss of effectiveness of the ailerons. for high-powered aircraft, the loss of lift (and increase in drag) beyond the stall angle is less of a problem than maintaining control. some aircraft may be subject to post-stall gyration (e.g. the f-4) or susceptible to entering a flat-spin (e.g. f-14). control beyond-stall can be provided by reaction control systems (e.g. nf-104a), vectored thrust, as well as a rolling stabilator (or taileron). the enhanced manoeuvering capability by flights at very high angles of attack can provide a tactical advantage for military fighters such as the f-22 raptor. short-term stalls at 90–120° (e.g. pugachev\'s cobra) are sometimes performed at airshows. the highest angle of attack in sustained flight so far demonstrated was 70 degrees in the x-31 at the dryden flight research center.  sustained post-stall flight is a type of supermaneuverability.   == spoilers ==  except for flight training, airplane testing, and aerobatics, a stall is usually an undesirable event. spoilers (sometimes called lift dumpers), however, are devices that are intentionally deployed to create a carefully controlled flow separation over part of an aircraft\'s wing to reduce the lift it generates, increase the drag, and allow the aircraft to descend more rapidly without gaining speed. spoilers are also deployed asymmetrically (one wing only) to enhance roll control. spoilers can also be used on aborted take-offs and after main wheel contact on landing to increase the aircraft\'s weight on its wheels for better braking action. unlike powered airplanes, which can control descent by increasing or decreasing thrust, gliders have to increase drag to increase the rate of descent. in high-performance gliders, spoiler deployment is extensively used to control the approach to landing. spoilers can also be thought of as  ""lift reducers"" because they reduce the lift of the wing in which the spoiler resides. for example, an uncommanded roll to the left could be reversed by raising the right wing spoiler (or only a few of the spoilers present in large airliner wings). this has the advantage of avoiding the need to increase lift in the wing that is dropping (which may bring that wing closer to stalling).   == history == otto lilienthal died while flying in 1896 as the result of a stall. wilbur wright encountered stalls for the first time in 1901, while flying his second glider. awareness of lilienthal\'s accident and wilbur\'s experience, motivated the wright brothers to design their plane in ""canard"" configuration. this made recoveries from stalls easier and more gentle. the design saved the brothers\' lives more than once.the aircraft engineer juan de la cierva worked on his ""autogiro"" project to develop a rotary wing aircraft which, he hoped, would be unable to stall and which therefore would be safer than aeroplanes. in developing the resulting ""autogyro"" aircraft, he solved many engineering problems which made the helicopter possible.   == see also == articlesaviation safety coffin corner (aerodynamics) compressor stall lift coefficient spin (flight) spoiler (aeronautics) wing twistnotable accidents1963 bac one-eleven test crash 1966 felthorpe trident crash british european airways flight 548 china airlines flight 140 china airlines flight 676 air france flight 447 colgan air flight 3407 turkish airlines flight 1951 indonesia airasia flight 8501   == notes ==   == references == usaf & nato report rto-tr-015 ac/323/(hfm-015)/tp-1 (2001 anderson, j.d., a history of aerodynamics (1997). cambridge university press.  isbn 0-521-66955-3 chapter 4, ""slow flight, stalls, and spins,"" in the airplane flying handbook. (faa h-8083-3a) l. j. clancy (1975), aerodynamics, pitman publishing limited, london.  isbn 0-273-01120-0 stengel, r. (2004), flight dynamics, princeton university press, isbn 0-691-11407-2 alpha systems aoa website for information on aoa and lift reserve indicators [1] 4239-01 angle of attack (aoa) sensor specifications [2] airplane flying handbook. federal aviation administration isbn 1-60239-003-7  pub. skyhorse publishing inc. federal aviation administration (25 september 2000), stall and spin awareness training, ac no: 61-67c prof. dr mustafa cavcar, ""stall speed"" [3]')"
7,"The wingspan (or just span) of a bird or an airplane is the distance from one wingtip to the other wingtip. For example, the Boeing 777-200 has a wingspan of 60.93 metres (199 ft 11 in), and a wandering albatross (Diomedea exulans) caught in 1965 had a wingspan of 3.63 metres (11 ft 11 in), the official record for a living bird.
The term wingspan, more technically extent, is also used for other winged animals such as pterosaurs, bats, insects, etc., and other aircraft such as ornithopters.
In humans, the term wingspan also refers to the arm span, which is distance between the length from one end of an individual's arms (measured at the fingertips) to the other when raised parallel to the ground at shoulder height at a 90º angle. Former professional basketball player Manute Bol stands at 7 ft 7 in (2.31 m) and owns one of the largest wingspans at 8 ft 6 in (2.59 m).


== Wingspan of aircraft ==
The wingspan of an aircraft is always measured in a straight line, from wingtip to wingtip, independently of wing shape or sweep.


=== Implications for aircraft design and animal evolution ===
The lift from wings is proportional to their area, so the heavier the animal or aircraft the bigger that area must be.  The area is the product of the span times the width (mean chord) of the wing, so either a long, narrow wing or a shorter, broader wing will support the same mass.  For efficient steady flight, the ratio of span to chord, the aspect ratio, should be as high as possible (the constraints are usually structural) because this lowers the lift-induced drag associated with the inevitable wingtip vortices. Long-ranging birds, like albatrosses, and most commercial aircraft maximize aspect ratio.  Alternatively, animals and aircraft which depend on maneuverability (fighters, predators and the preyed upon, and those who live amongst trees and bushes, insect catchers, etc.) need to be able to roll fast to turn, and the high moment of inertia of long narrow wings, as well as the high angular drag and quick balancing of aileron lift with wing lift at a low rotation rate, produces lower roll rates.  For them, short-span, broad wings are preferred. Additionally, ground handling in aircraft is a significant problem for very high aspect ratios and flying animals may encounter similar issues.
The highest aspect ratio man-made wings are aircraft propellers, in their most extreme form as helicopter rotors.


== Wingspan of flying animals ==
To measure the wingspan of a bird, a live or freshly-dead specimen is placed flat on its back, the wings are grasped at the wrist joints, ankles and the distance is measured between the tips of the longest primary feathers on each wing.The wingspan of an insect refers to the wingspan of pinned specimens, and may refer to the distance between the centre of the thorax to the apex of the wing doubled or to the width between the apices with the wings set with the trailing wing edge perpendicular to the body.


== Wingspan in sports ==
In basketball and gridiron football, a fingertip-to-fingertip measurement is used to determine the player's wingspan, also called armspan. This is called reach in boxing terminology.  The wingspan of 16-year-old BeeJay Anya, a top basketball Junior Class of 2013 prospect who played for the NC State Wolfpack, was officially measured at 7 feet 9 inches (2.36 m) across, one of the longest of all National Basketball Association draft prospects, and the longest ever for a non-7-foot player, though Anya went undrafted in 2017. The wingspan of Manute Bol, at 8 feet 6 inches (2.59 m), is (as of 2013) the longest in NBA history, and his vertical reach was 10 feet 5 inches (3.18 m).


== Wingspan records ==


=== Largest wingspan ===
Aircraft (current):  Scaled Composites Stratolaunch — 117 m (385 ft) 
Bat: Large flying fox – 1.5 m (4 ft 11 in)
Bird: Wandering albatross – 3.63 m (11 ft 11 in)
Bird (extinct): Argentavis – Estimated 7 m (23 ft 0 in)
Reptile (extinct): Quetzalcoatlus pterosaur – 10–11 m (33–36 ft)
Insect: White witch moth – 28 cm (11.0 in)
Insect (extinct): Meganeuropsis (relative of dragonflies) – estimated up to 71 cm (28.0 in)


=== Smallest wingspan ===
Aircraft (biplane): Starr Bumble Bee II – 1.68 m (5 ft 6 in)
Aircraft (jet): Bede BD-5 – 4.27 m (14 ft 0 in)
Aircraft (twin engine): Colomban Cri-cri – 4.9 m (16 ft 1 in)
Bat: Bumblebee bat – 16 cm (6.3 in)  
Bird: Bee hummingbird – 6.5 cm (2.6 in)
Insect: Tanzanian parasitic wasp (Fairyfly) – 0.2 mm (0.0079 in)


== References ==","pandas(index=7, _1=7, text=""the wingspan (or just span) of a bird or an airplane is the distance from one wingtip to the other wingtip. for example, the boeing 777-200 has a wingspan of 60.93 metres (199 ft 11 in), and a wandering albatross (diomedea exulans) caught in 1965 had a wingspan of 3.63 metres (11 ft 11 in), the official record for a living bird. the term wingspan, more technically extent, is also used for other winged animals such as pterosaurs, bats, insects, etc., and other aircraft such as ornithopters. in humans, the term wingspan also refers to the arm span, which is distance between the length from one end of an individual's arms (measured at the fingertips) to the other when raised parallel to the ground at shoulder height at a 90º angle. former professional basketball player manute bol stands at 7 ft 7 in (2.31 m) and owns one of the largest wingspans at 8 ft 6 in (2.59 m).   == wingspan of aircraft == the wingspan of an aircraft is always measured in a straight line, from wingtip to wingtip, independently of wing shape or sweep. aircraft (biplane): starr bumble bee ii – 1.68 m (5 ft 6 in) aircraft (jet): bede bd-5 – 4.27 m (14 ft 0 in) aircraft (twin engine): colomban cri-cri – 4.9 m (16 ft 1 in) bat: bumblebee bat – 16 cm (6.3 in) bird: bee hummingbird – 6.5 cm (2.6 in) insect: tanzanian parasitic wasp (fairyfly) – 0.2 mm (0.0079 in)   == references =="")"
8,"Working mass, also referred to as reaction mass, is a mass against which a system operates in order to produce acceleration.
In the case of a rocket, for example, the reaction mass is the fuel shot backwards to provide propulsion. All acceleration requires an exchange of momentum, which can be thought of as the ""unit of movement"". Momentum is related to mass and velocity, as given by the formula P = mv, where P is the momentum, m the mass, and v the velocity. The velocity of a body is easily changeable, but in most cases the mass is not, which makes it important.


== Rockets and rocket-like reaction engines ==
In rockets, the total velocity change can be calculated (using the Tsiolkovsky rocket equation) as follows:

  
    
      
        Δ
        
        v
        =
        u
        
        ln
        ⁡
        
          (
          
            
              
                m
                +
                M
              
              M
            
          
          )
        
      
    
    {\displaystyle \Delta \,v=u\,\ln \left({\frac {m+M}{M}}\right)}
  
Where:

v = ship velocity.
u = exhaust velocity.
M = ship mass, not including the working mass.
m = total mass ejected from the ship (working mass).The term working mass is used primarily in the aerospace field. In more ""down to earth"" examples the working mass is typically provided by the Earth, which contains so much momentum in comparison to most vehicles that the amount it gains or loses can be ignored. However, in the case of an aircraft the working mass is the air, and in the case of a rocket, it is the rocket fuel itself. Most rocket engines use light-weight fuels (liquid hydrogen, oxygen, or kerosene) accelerated to super-sonic speeds. However, ion engines often use heavier elements like xenon as the reaction mass, accelerated to much higher speeds using electric fields.
In many cases the working mass is separate from the energy used to accelerate it. In a car the engine provides power to the wheels, which then accelerates the Earth backward to make the car move forward. This is not the case for most rockets however, where the rocket propellant is the working mass, as well as the energy source. This means that rockets stop accelerating as soon as they run out of fuel, regardless of other power sources they may have. This can be a problem for satellites that need to be repositioned often, as it limits their useful life. In general, the exhaust velocity should be close to the ship velocity for optimum energy efficiency.  This limitation of rocket propulsion is one of the main motivations for the ongoing interest in field propulsion technology.


== See also ==
Rocket equation","pandas(index=8, _1=8, text='working mass, also referred to as reaction mass, is a mass against which a system operates in order to produce acceleration. in the case of a rocket, for example, the reaction mass is the fuel shot backwards to provide propulsion. all acceleration requires an exchange of momentum, which can be thought of as the ""unit of movement"". momentum is related to mass and velocity, as given by the formula p = mv, where p is the momentum, m the mass, and v the velocity. the velocity of a body is easily changeable, but in most cases the mass is not, which makes it important.   == rockets and rocket-like reaction engines == in rockets, the total velocity change can be calculated (using the tsiolkovsky rocket equation) as follows:     δ  v = u  ln \u2061  (    mm  m   )      where:  v = ship velocity. u = exhaust velocity. m = ship mass, not including the working mass. m = total mass ejected from the ship (working mass).the term working mass is used primarily in the aerospace field. in more ""down to earth"" examples the working mass is typically provided by the earth, which contains so much momentum in comparison to most vehicles that the amount it gains or loses can be ignored. however, in the case of an aircraft the working mass is the air, and in the case of a rocket, it is the rocket fuel itself. most rocket engines use light-weight fuels (liquid hydrogen, oxygen, or kerosene) accelerated to super-sonic speeds. however, ion engines often use heavier elements like xenon as the reaction mass, accelerated to much higher speeds using electric fields. in many cases the working mass is separate from the energy used to accelerate it. in a car the engine provides power to the wheels, which then accelerates the earth backward to make the car move forward. this is not the case for most rockets however, where the rocket propellant is the working mass, as well as the energy source. this means that rockets stop accelerating as soon as they run out of fuel, regardless of other power sources they may have. this can be a problem for satellites that need to be repositioned often, as it limits their useful life. in general, the exhaust velocity should be close to the ship velocity for optimum energy efficiency.  this limitation of rocket propulsion is one of the main motivations for the ongoing interest in field propulsion technology.   == see also == rocket equation')"
9,"In fluid dynamics, the drag coefficient (commonly denoted as: 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
  , 
  
    
      
        
          c
          
            x
          
        
      
    
    {\displaystyle c_{x}}
   or 
  
    
      
        
          c
          
            w
          
        
      
    
    {\displaystyle c_{w}}
  ) is a dimensionless quantity that is used to quantify the drag or resistance of an object in a fluid environment, such as air or water. It is used in the drag equation in which a lower drag coefficient indicates the object will have less aerodynamic or hydrodynamic drag. The drag coefficient is always associated with a particular surface area.The drag coefficient of any object comprises the effects of the two basic contributors to fluid dynamic drag: skin friction and form drag. The drag coefficient of a lifting airfoil or hydrofoil also includes the effects of lift-induced drag. The drag coefficient of a complete structure such as an aircraft also includes the effects of interference drag.


== Definition ==

The drag coefficient 
  
    
      
        
          c
          
            
              d
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }}
   is defined as

  
    
      
        
          c
          
            
              d
            
          
        
        =
        
          
            
              
                2
                
                  F
                  
                    
                      d
                    
                  
                
              
              
                ρ
                
                  u
                  
                    2
                  
                
                A
              
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }={\dfrac {2F_{\mathrm {d} }}{\rho u^{2}A}}}
  where:

  
    
      
        
          F
          
            
              d
            
          
        
      
    
    {\displaystyle F_{\mathrm {d} }}
   is the drag force, which is by definition the force component in the direction of the flow velocity,

  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the mass density of the fluid,

  
    
      
        u
      
    
    {\displaystyle u}
   is the flow speed of the object relative to the fluid,

  
    
      
        A
      
    
    {\displaystyle A}
   is the reference area.The reference area depends on what type of drag coefficient is being measured. For automobiles and many other objects, the reference area is the projected frontal area of the vehicle. This may not necessarily be the cross-sectional area of the vehicle, depending on where the cross-section is taken. For example, for a sphere 
  
    
      
        A
        =
        π
        
          r
          
            2
          
        
      
    
    {\displaystyle A=\pi r^{2}}
   (note this is not the surface area = 
  
    
      
        4
        π
        
          r
          
            2
          
        
      
    
    {\displaystyle 4\pi r^{2}}
  ).
For airfoils, the reference area is the nominal wing area. Since this tends to be large compared to the frontal area, the resulting drag coefficients tend to be low, much lower than for a car with the same drag, frontal area, and speed.
Airships and some bodies of revolution use the volumetric drag coefficient, in which the reference area is the square of the cube root of the airship volume (volume to the two-thirds power). Submerged streamlined bodies use the wetted surface area.
Two objects having the same reference area moving at the same speed through a fluid will experience a drag force proportional to their respective drag coefficients. Coefficients for unstreamlined objects can be 1 or more, for streamlined objects much less.
It has been demonstrated that drag coefficient 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   is a function of Bejan number (
  
    
      
        B
        e
      
    
    {\displaystyle Be}
  ), Reynolds number (
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  ) and the ratio between wet area 
  
    
      
        
          A
          
            w
          
        
      
    
    {\displaystyle A_{w}}
   and front area 
  
    
      
        
          A
          
            f
          
        
      
    
    {\displaystyle A_{f}}
  :
  
    
      
        
          c
          
            d
          
        
        =
        2
        
          
            
              A
              
                w
              
            
            
              A
              
                f
              
            
          
        
        
          
            
              B
              e
            
            
              R
              
                e
                
                  L
                
                
                  2
                
              
            
          
        
      
    
    {\displaystyle c_{d}=2{\frac {A_{w}}{A_{f}}}{\frac {Be}{Re_{L}^{2}}}}
  
where 
  
    
      
        R
        
          e
          
            L
          
        
      
    
    {\displaystyle Re_{L}}
   is the Reynolds Number related to fluid path length 
  
    
      
        L
      
    
    {\displaystyle L}
  .


== Background ==

The drag equation

  
    
      
        
          F
          
            d
          
        
        =
        
          
            
              1
              2
            
          
        
        ρ
        
          u
          
            2
          
        
        
          c
          
            d
          
        
        A
      
    
    {\displaystyle F_{d}={\tfrac {1}{2}}\rho u^{2}c_{d}A}
  is essentially a statement that the drag force on any object is proportional to the density of the fluid and proportional to the square of the relative flow speed between the object and the fluid.
Cd is not a constant but varies as a function of flow speed, flow direction, object position, object size, fluid density and fluid viscosity. Speed, kinematic viscosity and a characteristic length scale of the object are incorporated into a dimensionless quantity called the Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  . 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is thus a function of 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  . In a compressible flow, the speed of sound is relevant, and 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is also a function of Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
  .
For certain body shapes, the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   only depends on the Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
  , Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
   and the direction of the flow. For low Mach number 
  
    
      
        
          M
          a
        
      
    
    {\displaystyle \scriptstyle Ma}
  , the drag coefficient is independent of Mach number. Also, the variation with Reynolds number 
  
    
      
        
          R
          e
        
      
    
    {\displaystyle \scriptstyle Re}
   within a practical range of interest is usually small, while for cars at highway speed and aircraft at cruising speed, the incoming flow direction is also more-or-less the same. Therefore, the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   can often be treated as a constant.For a streamlined body to achieve a low drag coefficient, the boundary layer around the body must remain attached to the surface of the body for as long as possible, causing the wake to be narrow. A high form drag results in a broad wake. The boundary layer will transition from laminar to turbulent if Reynolds number of the flow around the body is sufficiently great. Larger velocities, larger objects, and lower viscosities contribute to larger Reynolds numbers.

For other objects, such as small particles, one can no longer consider that the drag coefficient 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   is constant, but certainly is a function of Reynolds number.
At a low Reynolds number, the flow around the object does not transition to turbulent but remains laminar, even up to the point at which it separates from the surface of the object. At very low Reynolds numbers, without flow separation, the drag force 
  
    
      
        
          
            F
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle F_{\mathrm {d} }}
   is proportional to 
  
    
      
        
          v
        
      
    
    {\displaystyle \scriptstyle v}
   instead of 
  
    
      
        
          
            v
            
              2
            
          
        
      
    
    {\displaystyle \scriptstyle v^{2}}
  ; for a sphere this is known as Stokes' law. The Reynolds number will be low for small objects, low velocities, and high viscosity fluids.A 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   equal to 1 would be obtained in a case where all of the fluid approaching the object is brought to rest, building up stagnation pressure over the whole front surface. The top figure shows a flat plate with the fluid coming from the right and stopping at the plate. The graph to the left of it shows equal pressure across the surface. In a real flat plate, the fluid must turn around the sides, and full stagnation pressure is found only at the center, dropping off toward the edges as in the lower figure and graph. Only considering the front side, the 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   of a real flat plate would be less than 1; except that there will be suction on the backside: a negative pressure (relative to ambient). The overall 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   of a real square flat plate perpendicular to the flow is often given as 1.17. Flow patterns and therefore 
  
    
      
        
          
            C
            
              
                d
              
            
          
        
      
    
    {\displaystyle \scriptstyle C_{\mathrm {d} }}
   for some shapes can change with the Reynolds number and the roughness of the surfaces.


== Drag coefficient examples ==


=== General ===
In general, 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   is not an absolute constant for a given body shape. It varies with the speed of airflow (or more generally with Reynolds number 
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  ). A smooth sphere, for example, has a 
  
    
      
        
          c
          
            d
          
        
      
    
    {\displaystyle c_{d}}
   that varies from high values for laminar flow to 0.47 for turbulent flow.  Although the drag coefficient decreases with increasing 
  
    
      
        R
        e
      
    
    {\displaystyle Re}
  , the drag force increases.


=== Aircraft ===
As noted above, aircraft use their wing area as the reference area when computing 
  
    
      
        
          c
          
            
              d
            
          
        
      
    
    {\displaystyle c_{\mathrm {d} }}
  , while automobiles (and many other objects) use frontal cross-sectional area; thus, coefficients are not directly comparable between these classes of vehicles. In the aerospace industry, the drag coefficient is sometimes expressed in drag counts where 1 drag count = 0.0001 of a 
  
    
      
        
          C
          
            d
          
        
      
    
    {\displaystyle C_{d}}
  .


=== Automobile ===


== Blunt and streamlined body flows ==


=== Concept ===
Drag, in the context of fluid dynamics, refers to forces that act on a solid object in the direction of the relative flow velocity (note that the diagram below shows the drag in the opposite direction to the flow). The aerodynamic forces on a body come primarily from differences in pressure and viscous shearing stresses. Thereby, the drag force on a body could be divided into two components, namely frictional drag (viscous drag) and pressure drag (form drag). The net drag force could be decomposed as follows:

  
    
      
        
          
            
              
                
                  c
                  
                    
                      d
                    
                  
                
              
              
                
                =
                
                  
                    
                      
                        2
                        
                          F
                          
                            
                              d
                            
                          
                        
                      
                      
                        ρ
                        
                          v
                          
                            2
                          
                        
                        A
                      
                    
                  
                
              
            
            
              
              
                
                =
                
                  c
                  
                    
                      p
                    
                  
                
                +
                
                  c
                  
                    
                      f
                    
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      
                        
                          
                            
                              1
                              
                                ρ
                                
                                  v
                                  
                                    2
                                  
                                
                                A
                              
                            
                          
                        
                        
                          
                            ∫
                            
                              S
                            
                          
                          
                            d
                          
                          A
                          (
                          p
                          −
                          
                            p
                            
                              o
                            
                          
                          )
                          
                            (
                            
                              
                                
                                  
                                    
                                      n
                                    
                                    ^
                                  
                                
                              
                              ⋅
                              
                                
                                  
                                    
                                      i
                                    
                                    ^
                                  
                                
                              
                            
                            )
                          
                        
                      
                      ⏟
                    
                  
                  
                    
                      c
                      
                        
                          p
                        
                      
                    
                  
                
                +
                
                  
                    
                      
                        
                          
                            
                              1
                              
                                ρ
                                
                                  v
                                  
                                    2
                                  
                                
                                A
                              
                            
                          
                        
                        
                          
                            ∫
                            
                              S
                            
                          
                          
                            d
                          
                          A
                          
                            (
                            
                              
                                
                                  
                                    
                                      t
                                    
                                    ^
                                  
                                
                              
                              ⋅
                              
                                
                                  
                                    
                                      i
                                    
                                    ^
                                  
                                
                              
                            
                            )
                          
                          
                            T
                            
                              w
                            
                          
                        
                      
                      ⏟
                    
                  
                  
                    
                      c
                      
                        
                          f
                        
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}c_{\mathrm {d} }&={\dfrac {2F_{\mathrm {d} }}{\rho v^{2}A}}\\&=c_{\mathrm {p} }+c_{\mathrm {f} }\\&=\underbrace {{\dfrac {1}{\rho v^{2}A}}\displaystyle \int \limits _{S}\mathrm {d} A(p-p_{o})\left({\hat {\mathbf {n} }}\cdot {\hat {\mathbf {i} }}\right)} _{c_{\mathrm {p} }}+\underbrace {{\dfrac {1}{\rho v^{2}A}}\displaystyle \int \limits _{S}\mathrm {d} A\left({\hat {\mathbf {t} }}\cdot {\hat {\mathbf {i} }}\right)T_{w}} _{c_{\mathrm {f} }}\end{aligned}}}
  where:

  
    
      
        
          c
          
            
              p
            
          
        
      
    
    {\displaystyle c_{\mathrm {p} }}
   is the pressure drag coefficient,

  
    
      
        
          c
          
            
              f
            
          
        
      
    
    {\displaystyle c_{\mathrm {f} }}
   is the friction drag coefficient,

  
    
      
        
          
            
              
                t
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {t} }}}
   = Tangential direction to the surface with area dA,

  
    
      
        
          
            
              
                n
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {n} }}}
   = Normal direction to the surface with area dA,

  
    
      
        
          T
          
            
              w
            
          
        
      
    
    {\displaystyle T_{\mathrm {w} }}
   is the shear Stress acting on the surface dA,

  
    
      
        
          p
          
            
              o
            
          
        
      
    
    {\displaystyle p_{\mathrm {o} }}
   is the pressure far away from the surface dA,

  
    
      
        p
      
    
    {\displaystyle p}
   is pressure at surface dA,

  
    
      
        
          
            
              
                i
              
              ^
            
          
        
      
    
    {\displaystyle {\hat {\mathbf {i} }}}
   is the unit vector in direction of free stream flowTherefore, when the drag is dominated by a frictional component, the body is called a streamlined body; whereas in the case of dominant pressure drag, the body is called a blunt or bluff body. Thus, the shape of the body and the angle of attack determine the type of drag. For example, an airfoil is considered as a body with a small angle of attack by the fluid flowing across it. This means that it has attached boundary layers, which produce much less pressure drag.

The wake produced is very small and drag is dominated by the friction component. Therefore, such a body (here an airfoil) is described as streamlined, whereas for bodies with fluid flow at high angles of attack, boundary layer separation takes place. This mainly occurs due to adverse pressure gradients at the top and rear parts of an airfoil.
Due to this, wake formation takes place, which consequently leads to eddy formation and pressure loss due to pressure drag. In such situations, the airfoil is stalled and has higher pressure drag than friction drag. In this case, the body is described as a blunt body.
A streamlined body looks like a fish (Tuna), Oropesa, etc. or an airfoil with small angle of attack, whereas a blunt body looks like a brick, a cylinder or an airfoil with high angle of attack. For a given frontal area and velocity, a streamlined body will have lower resistance than a blunt body. Cylinders and spheres are taken as blunt bodies because the drag is dominated by the pressure component in the wake region at high Reynolds number.
To reduce this drag, either the flow separation could be reduced or the surface area in contact with the fluid could be reduced (to reduce friction drag). This reduction is necessary in devices like cars, bicycle, etc. to avoid vibration and noise production.


==== Practical example ====
The aerodynamic design of cars has evolved from the 1920s to the end of the 20th century. This change in design from a blunt body to a more streamlined body reduced the drag coefficient from about 0.95 to 0.30.

Time history of cars' aerodynamic drag in comparison to change in geometry of streamlined bodies (blunt to streamline).


== See also ==
Automotive aerodynamics
Automobile drag coefficient
Ballistic coefficient
Drag crisis
Zero-lift drag coefficient


== Notes ==


== References ==
L. J. Clancy (1975): Aerodynamics. Pitman Publishing Limited, London, ISBN 0-273-01120-0
Abbott, Ira H., and Von Doenhoff, Albert E. (1959): Theory of Wing Sections. Dover Publications Inc., New York, Standard Book Number 486-60586-8
Hoerner, Dr. Sighard F., Fluid-Dynamic Drag, Hoerner Fluid Dynamics, Bricktown New Jersey, 1965.
Bluff Body: http://user.engineering.uiowa.edu/~me_160/lecture_notes/Bluff%20Body2.pdf
Drag of Blunt Bodies and Streamlined Bodies: http://www.princeton.edu/~asmits/Bicycle_web/blunt.html
Hucho, W.H., Janssen, L.J., Emmelmann, H.J. 6(1975): The optimization of body details-A method for reducing the aerodynamics drag. SAE 760185.","pandas(index=9, _1=9, text=""in fluid dynamics, the drag coefficient (commonly denoted as:     c  d      is the unit vector in direction of free stream flowtherefore, when the drag is dominated by a frictional component, the body is called a streamlined body; whereas in the case of dominant pressure drag, the body is called a blunt or bluff body. thus, the shape of the body and the angle of attack determine the type of drag. for example, an airfoil is considered as a body with a small angle of attack by the fluid flowing across it. this means that it has attached boundary layers, which produce much less pressure drag.  the wake produced is very small and drag is dominated by the friction component. therefore, such a body (here an airfoil) is described as streamlined, whereas for bodies with fluid flow at high angles of attack, boundary layer separation takes place. this mainly occurs due to adverse pressure gradients at the top and rear parts of an airfoil. due to this, wake formation takes place, which consequently leads to eddy formation and pressure loss due to pressure drag. in such situations, the airfoil is stalled and has higher pressure drag than friction drag. in this case, the body is described as a blunt body. a streamlined body looks like a fish (tuna), oropesa, etc. or an airfoil with small angle of attack, whereas a blunt body looks like a brick, a cylinder or an airfoil with high angle of attack. for a given frontal area and velocity, a streamlined body will have lower resistance than a blunt body. cylinders and spheres are taken as blunt bodies because the drag is dominated by the pressure component in the wake region at high reynolds number. to reduce this drag, either the flow separation could be reduced or the surface area in contact with the fluid could be reduced (to reduce friction drag). this reduction is necessary in devices like cars, bicycle, etc. to avoid vibration and noise production. the aerodynamic design of cars has evolved from the 1920s to the end of the 20th century. this change in design from a blunt body to a more streamlined body reduced the drag coefficient from about 0.95 to 0.30.  time history of cars' aerodynamic drag in comparison to change in geometry of streamlined bodies (blunt to streamline).   == see also == automotive aerodynamics automobile drag coefficient ballistic coefficient drag crisis zero-lift drag coefficient   == notes ==   == references == l. j. clancy (1975): aerodynamics. pitman publishing limited, london, isbn 0-273-01120-0 abbott, ira h., and von doenhoff, albert e. (1959): theory of wing sections. dover publications inc., new york, standard book number 486-60586-8 hoerner, dr. sighard f., fluid-dynamic drag, hoerner fluid dynamics, bricktown new jersey, 1965. bluff body: http://user.engineering.uiowa.edu/~me_160/lecture_notes/bluff%20body2.pdf drag of blunt bodies and streamlined bodies: http://www.princeton.edu/~asmits/bicycle_web/blunt.html hucho, w.h., janssen, l.j., emmelmann, h.j. 6(1975): the optimization of body details-a method for reducing the aerodynamics drag. sae 760185."")"
10,"The Boeing X-51 Waverider is an unmanned research scramjet experimental aircraft for hypersonic flight at Mach 5 (3,300 mph; 5,300 km/h) and an altitude of 70,000 feet (21,000 m). The aircraft was designated X-51 in 2005. It completed its first powered hypersonic flight on 26 May 2010.  After two unsuccessful test flights, the X-51 completed a flight of over six minutes and reached speeds of over Mach 5 for 210 seconds on 1 May 2013 for the longest duration powered hypersonic flight.
Waverider refers in general to aircraft that take advantage of compression lift produced by their own shock waves. The X-51 program was a cooperative effort by the United States Air Force, DARPA, NASA, Boeing, and Pratt & Whitney Rocketdyne. The program was managed by the Aerospace Systems Directorate within the U.S. Air Force Research Laboratory (AFRL).  X-51 technology is proposed for use in the High Speed Strike Weapon (HSSW), a Mach 5+ missile which could enter service in the mid-2020s.


== Design and development ==
In the 1990s, the Air Force Research Laboratory (AFRL) began the HyTECH program for hypersonic propulsion.  Pratt & Whitney received a contract from the AFRL to develop a hydrocarbon-fueled scramjet engine which led to the development of the SJX61 engine. The SJX61 engine was originally meant for the NASA X-43C, which was eventually canceled.  The engine was applied to the AFRL's Scramjet Engine Demonstrator program in late 2003.  The scramjet flight test vehicle was designated X-51 on 27 September 2005.

In flight demonstrations, the X-51 is carried by a B-52 to an altitude of about 50,000 feet (15 km; 9.5 mi) and then released over the Pacific Ocean.  The X-51 is initially propelled by an MGM-140 ATACMS solid rocket booster to approximately Mach 4.5 (3,000 mph; 4,800 km/h). The booster is then jettisoned and the vehicle's Pratt & Whitney Rocketdyne SJY61 scramjet accelerates it to a top flight speed near Mach 6 (4,000 mph; 6,400 km/h).  The X-51 uses JP-7 fuel for the SJY61 scramjet, carrying 270 lb (120 kg) on board.


=== Applications for hypersonic technology ===
DARPA once viewed X-51 as a stepping stone to Blackswift, a planned hypersonic demonstrator which was canceled in October 2008.In May 2013, the U.S. Air Force planned to apply X-51 technology to the High Speed Strike Weapon (HSSW), a missile similar in size to the X-51.  The HSSW could fly in 2020 and enter service in the mid-2020s.  It is envisioned to have a range of 500–600 nmi, fly at Mach 5–6, and fit on an F-35 or in the internal bay of a B-2 bomber.


== Testing ==


=== Ground and unpowered testing ===
Ground tests of the X-51A began in late 2005.  A preliminary version of the X-51, the ""Ground Demonstrator Engine No. 2"", completed wind tunnel tests at the NASA Langley Research Center on 27 July 2006.  Testing continued there until a simulated X-51 flight at Mach 5 was successfully completed on 30 April 2007. The testing is intended to observe acceleration between Mach 4 and Mach 6 and to demonstrate that hypersonic thrust ""isn't just luck"".  Four captive test flights were initially planned for 2009. However, the first captive flight of the X-51A on a B-52 was conducted on 9 December 2009, with further flights in early 2010.


=== Powered flight testing ===
The first powered flight of the X-51 was planned for 25 May 2010, but the presence of a cargo ship traveling through a portion of the Naval Air Station Point Mugu Sea Range caused a 24-hour delay. The X-51 completed its first powered flight successfully on 26 May 2010.  It reached a speed of Mach 5 (3,300 mph; 5,300 km/h), an altitude of 70,000 feet (21,000 m) and flew for over 200 seconds; it did not meet the planned 300 second flight duration, however.  The test had the longest hypersonic flight time of 140 seconds while under its scramjet power. The X-43 had the previous longest flight burn time of 12 seconds, while setting a new speed record of Mach 9.68.
Three more test flights were planned and used the same flight trajectory.  Boeing proposed to the Air Force Research Laboratory (AFRL) that two test flights be added to increase the total to six, with flights taking place at four to six week intervals, provided there are no failures.The second test flight was initially scheduled for 24 March 2011, but was not conducted due to unfavorable test conditions.  The flight took place on 13 June 2011.  However, the flight over the Pacific Ocean ended early due to an inlet unstart event after being boosted to Mach 5 speed.  The flight data from the test was being investigated.  A B-52 released the X-51 at an approximate altitude of 50,000 feet (15,000 m). The X-51's scramjet engine lit on ethylene, but did not properly transition to JP-7 fuel operation.The third test flight took place on 14 August 2012.  The X-51 was to make a 300-second (5 minutes) experimental flight at speeds of Mach 5 (3,300 mph; 5,312 km/h). After separating from its rocket booster, the craft lost control and crashed into the Pacific.  The Air Force Research Laboratory (AFRL) determined the problem was the X-51's upper right aerodynamic fin unlocked during flight and became uncontrollable; all four fins are needed for aerodynamic control.  The aircraft lost control before the scramjet engine could ignite.On 1 May 2013, the X-51 performed its first fully successful flight test on its fourth test flight.  The X-51 and booster detached from a B-52H and was powered to Mach 4.8 (3,200 mph; 5,100 km/h) by the booster rocket.  It then separated cleanly from the booster and ignited its own engine.  The test aircraft then accelerated to Mach 5.1 (3,400 mph; 5,400 km/h) and flew for 210 seconds until running out of fuel and plunging into the Pacific Ocean off Point Mugu for over six minutes of total flight time; this test was the longest air-breathing hypersonic flight.  Researchers collected telemetry data for 370 seconds of flight.  The test signified the completion of the program.  The Air Force Research Laboratory believes the successful flight will serve as research for practical applications of hypersonic flight, such as a missile, reconnaissance, transport, and air-breathing first stage for a space system.


== Specifications ==
Data from Boeing, Air Force
Crew: None
Length: 25 ft 0 in (7.62 m)
Empty weight: 4,000 lb (1,814 kg)
Powerplant: 1 × MGM-140 ATACMS rocket booster
Powerplant: 1 × Pratt & Whitney Rocketdyne SJY61 scramjetPerformance

Maximum speed: 3,900 mph (6,200 km/h, 3,400 kn)
Maximum speed: Mach 5.1
Range: 460 mi (740 km, 400 nmi)
Service ceiling: 70,000 ft (21,300 m)


== See also ==
Boeing Small Launch Vehicle concept, includes a hypersonic waverider as the second stage
Flight airspeed record
Scramjet programs


== References ==


== External links ==
X-51 fact sheet on USAF site
""WaveRider page on Boeing.com"". Archived from the original on 14 November 2012.
""AFRL mulls adding scope to X-51A Waverider hypersonic tests"". Flight International, March 2009.
""Pratt & Whitney Rocketdyne Scramjet Excels in USAF Tests"". Aviation Week - subscription
YouTube video of FoxNews report, preceding test flight
YouTube video of test flight, shot from NASA chase plane","pandas(index=10, _1=10, text='the boeing x-51 waverider is an unmanned research scramjet experimental aircraft for hypersonic flight at mach 5 (3,300 mph; 5,300 km/h) and an altitude of 70,000 feet (21,000 m). the aircraft was designated x-51 in 2005. it completed its first powered hypersonic flight on 26 may 2010.  after two unsuccessful test flights, the x-51 completed a flight of over six minutes and reached speeds of over mach 5 for 210 seconds on 1 may 2013 for the longest duration powered hypersonic flight. waverider refers in general to aircraft that take advantage of compression lift produced by their own shock waves. the x-51 program was a cooperative effort by the united states air force, darpa, nasa, boeing, and pratt & whitney rocketdyne. the program was managed by the aerospace systems directorate within the u.s. air force research laboratory (afrl).  x-51 technology is proposed for use in the high speed strike weapon (hssw), a mach 5missile which could enter service in the mid-2020s.   == design and development == in the 1990s, the air force research laboratory (afrl) began the hytech program for hypersonic propulsion.  pratt & whitney received a contract from the afrl to develop a hydrocarbon-fueled scramjet engine which led to the development of the sjx61 engine. the sjx61 engine was originally meant for the nasa x-43c, which was eventually canceled.  the engine was applied to the afrl\'s scramjet engine demonstrator program in late 2003.  the scramjet flight test vehicle was designated x-51 on 27 september 2005.  in flight demonstrations, the x-51 is carried by a b-52 to an altitude of about 50,000 feet (15 km; 9.5 mi) and then released over the pacific ocean.  the x-51 is initially propelled by an mgm-140 atacms solid rocket booster to approximately mach 4.5 (3,000 mph; 4,800 km/h). the booster is then jettisoned and the vehicle\'s pratt & whitney rocketdyne sjy61 scramjet accelerates it to a top flight speed near mach 6 (4,000 mph; 6,400 km/h).  the x-51 uses jp-7 fuel for the sjy61 scramjet, carrying 270 lb (120 kg) on board. the first powered flight of the x-51 was planned for 25 may 2010, but the presence of a cargo ship traveling through a portion of the naval air station point mugu sea range caused a 24-hour delay. the x-51 completed its first powered flight successfully on 26 may 2010.  it reached a speed of mach 5 (3,300 mph; 5,300 km/h), an altitude of 70,000 feet (21,000 m) and flew for over 200 seconds; it did not meet the planned 300 second flight duration, however.  the test had the longest hypersonic flight time of 140 seconds while under its scramjet power. the x-43 had the previous longest flight burn time of 12 seconds, while setting a new speed record of mach 9.68. three more test flights were planned and used the same flight trajectory.  boeing proposed to the air force research laboratory (afrl) that two test flights be added to increase the total to six, with flights taking place at four to six week intervals, provided there are no failures.the second test flight was initially scheduled for 24 march 2011, but was not conducted due to unfavorable test conditions.  the flight took place on 13 june 2011.  however, the flight over the pacific ocean ended early due to an inlet unstart event after being boosted to mach 5 speed.  the flight data from the test was being investigated.  a b-52 released the x-51 at an approximate altitude of 50,000 feet (15,000 m). the x-51\'s scramjet engine lit on ethylene, but did not properly transition to jp-7 fuel operation.the third test flight took place on 14 august 2012.  the x-51 was to make a 300-second (5 minutes) experimental flight at speeds of mach 5 (3,300 mph; 5,312 km/h). after separating from its rocket booster, the craft lost control and crashed into the pacific.  the air force research laboratory (afrl) determined the problem was the x-51\'s upper right aerodynamic fin unlocked during flight and became uncontrollable; all four fins are needed for aerodynamic control.  the aircraft lost control before the scramjet engine could ignite.on 1 may 2013, the x-51 performed its first fully successful flight test on its fourth test flight.  the x-51 and booster detached from a b-52h and was powered to mach 4.8 (3,200 mph; 5,100 km/h) by the booster rocket.  it then separated cleanly from the booster and ignited its own engine.  the test aircraft then accelerated to mach 5.1 (3,400 mph; 5,400 km/h) and flew for 210 seconds until running out of fuel and plunging into the pacific ocean off point mugu for over six minutes of total flight time; this test was the longest air-breathing hypersonic flight.  researchers collected telemetry data for 370 seconds of flight.  the test signified the completion of the program.  the air force research laboratory believes the successful flight will serve as research for practical applications of hypersonic flight, such as a missile, reconnaissance, transport, and air-breathing first stage for a space system.   == specifications == data from boeing, air force crew: none length: 25 ft 0 in (7.62 m) empty weight: 4,000 lb (1,814 kg) powerplant: 1 × mgm-140 atacms rocket booster powerplant: 1 × pratt & whitney rocketdyne sjy61 scramjetperformance  maximum speed: 3,900 mph (6,200 km/h, 3,400 kn) maximum speed: mach 5.1 range: 460 mi (740 km, 400 nmi) service ceiling: 70,000 ft (21,300 m)   == see also == boeing small launch vehicle concept, includes a hypersonic waverider as the second stage flight airspeed record scramjet programs   == references ==   == external links == x-51 fact sheet on usaf site ""waverider page on boeing.com"". archived from the original on 14 november 2012. ""afrl mulls adding scope to x-51a waverider hypersonic tests"". flight international, march 2009. ""pratt & whitney rocketdyne scramjet excels in usaf tests"". aviation week - subscription youtube video of foxnews report, preceding test flight youtube video of test flight, shot from nasa chase plane')"
11,"A leading-edge extension (LEX) is a small extension to an aircraft wing surface, forward of the leading edge. The primary reason for adding an extension is to improve the airflow at high angles of attack and low airspeeds, to improve handling and delay the stall. A dog tooth can also improve airflow and reduce drag at higher speeds.


== Leading–edge slat ==

A leading-edge slat is an aerodynamic surface running spanwise just ahead of the wing leading edge. It creates a leading edge slot between the slat and wing which directs air over the wing surface, helping to maintain smooth airflow at low speeds and high angles of attack. This delays the stall, allowing the aircraft to fly at a higher angle of attack. Slats may be made fixed, or retractable in normal flight to minimize drag.


== Dogtooth extension ==

A dogtooth is a small, sharp zig-zag break in the leading edge of a wing. It is usually used on a swept wing, to generate a vortex flow field to prevent separated flow from progressing outboard at high angle of attack. The effect is the same as a wing fence. It can also be used on straight wings in a drooped leading edge arrangement.Many high-performance aircraft use the dogtooth design, which induces a vortex over the wing to control  boundary layer spanwise extension, increasing lift and improving resistance to stall. Some of the best-known uses of the dogtooth are in the stabilizer of the F-15 Eagle and the wings of the F-4 Phantom II, F/A-18 Super Hornet, CF-105 Arrow, F-8U Crusader, and the Ilyushin Il-62. Where the dogtooth is added as an afterthought, as for example on the Hawker Hunter and some variants of the Quest Kodiak, the dogtooth is created by adding an extension to the outer section of the leading edge.


== Leading-edge cuff ==

A leading edge cuff (or wing cuff) is a fixed aerodynamic device employed on fixed-wing aircraft to introduce a sharp discontinuity in the leading edge of the wing in the same way as a dogtooth. It also typically has a slightly drooped leading edge to improve low-speed characteristics.


== Leading-edge root extension ==

A leading-edge root extension (LERX) is a small fillet, typically roughly triangular in shape, running forward from the leading edge of the wing root to a point along the fuselage.  These are often called simply leading-edge extensions (LEX), although they are not the only kind. To avoid ambiguity, this article uses the term LERX.
On a modern fighter aircraft LERXes induce controlled airflow over the wing at high angles of attack, so delaying the stall and consequent loss of lift. In cruising flight the effect of the LERX is minimal. However at high angles of attack, as often encountered in a dog fight or during takeoff and landing, the LERX generates a high-speed vortex that attaches to the top of the wing. The vortex action maintains the attachment of the airflow to the upper-wing surface well past the normal stall point at which the airflow separates from the wing surface, thus sustaining lift at very high angles.
LERX were first used on the Northrop F-5 ""Freedom fighter"" which flew in 1959, and have since become commonplace on many combat aircraft. The F/A-18 Hornet has especially large examples, as does the Sukhoi Su-27 and the CAC/PAC JF-17 Thunder. The Su-27 LERX help make some advanced maneuvers possible, such as the Pugachev's Cobra, the Cobra Turn and the Kulbit.
A long, narrow sideways extension to the fuselage, attached in this position, is an example of a chine.


== Leading-edge vortex controller ==
Leading-edge vortex controller (LEVCON) systems are a continuation of leading-edge root extension (LERX) technology, but with actuation that allows the leading edge vortices to be modified without adjusting the aircraft's attitude. Otherwise they operate on the same principles as the LERX system to create lift augmenting leading edge vortices during high angle of attack flight.
This system has been incorporated in the Russian Sukhoi Su-57 and Indian HAL LCA Navy.The LEVCONs actuation ability also improves its performance over the LERX system in other areas.
When combined with the thrust vectoring controller (TVC), the aircraft controllability at extreme angles of attack is further increased, which assists in stunts which require supermaneuverability such as Pugachev's Cobra.  
Additionally, on the Sukhoi Su-57 the LEVCON system is used for increased departure-resistance in the event of TVC failure at a post-stall attitude. It can also be used for trimming the aircraft, and optimizing the lift to drag ratio during cruise.


== See also ==
Strake (aviation)
Vortex generator


== References ==","pandas(index=11, _1=11, text='a leading-edge extension (lex) is a small extension to an aircraft wing surface, forward of the leading edge. the primary reason for adding an extension is to improve the airflow at high angles of attack and low airspeeds, to improve handling and delay the stall. a dog tooth can also improve airflow and reduce drag at higher speeds.   == leading–edge slat ==  a leading-edge slat is an aerodynamic surface running spanwise just ahead of the wing leading edge. it creates a leading edge slot between the slat and wing which directs air over the wing surface, helping to maintain smooth airflow at low speeds and high angles of attack. this delays the stall, allowing the aircraft to fly at a higher angle of attack. slats may be made fixed, or retractable in normal flight to minimize drag.   == dogtooth extension ==  a dogtooth is a small, sharp zig-zag break in the leading edge of a wing. it is usually used on a swept wing, to generate a vortex flow field to prevent separated flow from progressing outboard at high angle of attack. the effect is the same as a wing fence. it can also be used on straight wings in a drooped leading edge arrangement.many high-performance aircraft use the dogtooth design, which induces a vortex over the wing to control  boundary layer spanwise extension, increasing lift and improving resistance to stall. some of the best-known uses of the dogtooth are in the stabilizer of the f-15 eagle and the wings of the f-4 phantom ii, f/a-18 super hornet, cf-105 arrow, f-8u crusader, and the ilyushin il-62. where the dogtooth is added as an afterthought, as for example on the hawker hunter and some variants of the quest kodiak, the dogtooth is created by adding an extension to the outer section of the leading edge.   == leading-edge cuff ==  a leading edge cuff (or wing cuff) is a fixed aerodynamic device employed on fixed-wing aircraft to introduce a sharp discontinuity in the leading edge of the wing in the same way as a dogtooth. it also typically has a slightly drooped leading edge to improve low-speed characteristics.   == leading-edge root extension ==  a leading-edge root extension (lerx) is a small fillet, typically roughly triangular in shape, running forward from the leading edge of the wing root to a point along the fuselage.  these are often called simply leading-edge extensions (lex), although they are not the only kind. to avoid ambiguity, this article uses the term lerx. on a modern fighter aircraft lerxes induce controlled airflow over the wing at high angles of attack, so delaying the stall and consequent loss of lift. in cruising flight the effect of the lerx is minimal. however at high angles of attack, as often encountered in a dog fight or during takeoff and landing, the lerx generates a high-speed vortex that attaches to the top of the wing. the vortex action maintains the attachment of the airflow to the upper-wing surface well past the normal stall point at which the airflow separates from the wing surface, thus sustaining lift at very high angles. lerx were first used on the northrop f-5 ""freedom fighter"" which flew in 1959, and have since become commonplace on many combat aircraft. the f/a-18 hornet has especially large examples, as does the sukhoi su-27 and the cac/pac jf-17 thunder. the su-27 lerx help make some advanced maneuvers possible, such as the pugachev\'s cobra, the cobra turn and the kulbit. a long, narrow sideways extension to the fuselage, attached in this position, is an example of a chine.   == leading-edge vortex controller == leading-edge vortex controller (levcon) systems are a continuation of leading-edge root extension (lerx) technology, but with actuation that allows the leading edge vortices to be modified without adjusting the aircraft\'s attitude. otherwise they operate on the same principles as the lerx system to create lift augmenting leading edge vortices during high angle of attack flight. this system has been incorporated in the russian sukhoi su-57 and indian hal lca navy.the levcons actuation ability also improves its performance over the lerx system in other areas. when combined with the thrust vectoring controller (tvc), the aircraft controllability at extreme angles of attack is further increased, which assists in stunts which require supermaneuverability such as pugachev\'s cobra. additionally, on the sukhoi su-57 the levcon system is used for increased departure-resistance in the event of tvc failure at a post-stall attitude. it can also be used for trimming the aircraft, and optimizing the lift to drag ratio during cruise.   == see also == strake (aviation) vortex generator   == references ==')"
12,"A vortex generator (VG) is an aerodynamic device, consisting of a small vane usually attached to a lifting surface (or airfoil, such as an aircraft wing) or a rotor blade of a wind turbine. VGs may also be attached to some part of an aerodynamic vehicle such as an aircraft fuselage or a car.  When the airfoil or the body is in motion relative to the air, the VG creates a vortex,  which, by removing some part of the slow-moving boundary layer in contact with the airfoil surface, delays local flow separation and aerodynamic stalling, thereby improving the effectiveness of wings and control surfaces, such as flaps, elevators, ailerons, and rudders.


== Method of operation ==
Vortex generators are most often used to delay flow separation.  To accomplish this they are often placed on the external surfaces of vehicles and wind turbine blades. On both aircraft and wind turbine blades they are usually installed quite close to the leading edge of the aerofoil in order to maintain steady airflow over the control surfaces at the trailing edge. VGs are typically rectangular or triangular, about as tall as the local boundary layer, and run in spanwise lines usually near the thickest part of the wing. They can be seen on the wings and vertical tails of many airliners.
Vortex generators are positioned obliquely so that they have an angle of attack with respect to the local airflow in order to create a tip vortex which draws energetic, rapidly moving outside air into the slow-moving boundary layer in contact with the surface. A turbulent boundary layer is less likely to separate than a laminar one, and is therefore desirable to ensure effectiveness of trailing-edge control surfaces. Vortex generators are used to trigger this transition. Other devices such as vortilons, leading-edge extensions, and leading-edge cuffs, also delay flow separation at high angles of attack by re-energizing the boundary layer.Examples of aircraft which use VGs include the ST Aerospace A-4SU Super Skyhawk and Symphony SA-160. For swept-wing transonic designs, VGs alleviate potential shock-stall problems (e.g., Harrier, Blackburn Buccaneer, Gloster Javelin).


== Aftermarket installation ==
Many aircraft carry vane vortex generators from time of manufacture, but there are also aftermarket suppliers who sell VG kits to improve the STOL performance of some light aircraft.  Aftermarket suppliers claim (i) that VGs lower stall speed and reduce take-off and landing speeds, and (ii) that VGs increase the effectiveness of ailerons, elevators and rudders, thereby improving controllability and safety at low speeds. For home-built and experimental kitplanes, VGs are cheap, cost-effective and can be installed quickly; but for certified aircraft installations, certification costs can be high, making the modification a relatively expensive process.Owners fit aftermarket VGs primarily to gain benefits at low speeds, but a downside is that such VGs may reduce cruise speed slightly. In tests performed on a Cessna 182 and a Piper PA-28-235 Cherokee, independent reviewers have documented a loss of cruise speed of 1.5 to 2.0 kn (2.8 to 3.7 km/h) .  However, these losses are relatively minor, since an aircraft wing at high speed has a small angle of attack, thereby reducing VG drag to a minimum.Owners have reported that on the ground, it can be harder to clear snow and ice from wing surfaces with VGs than from a smooth wing, but VGs are not generally prone to inflight icing as they reside within the boundary layer of airflow. VGs may also have sharp edges which can tear the fabric of airframe covers and may thus require special covers to be made.For twin-engined aircraft, manufacturers claim that VGs reduce single-engine control speed (Vmca), increase zero fuel and gross weight, improve the effectiveness of ailerons and rudder, provide a smoother ride in turbulence and make the aircraft a more stable instrument platform.


== Increase in maximum takeoff weight ==
Some VG kits available for light twin-engine airplanes may allow an increase in maximum takeoff weight.  The maximum takeoff weight of a twin-engine airplane is determined by structural requirements and single-engine climb performance requirements (which are lower for a lower stall speed).  For many light twin-engine airplanes, the single-engine climb performance requirements determine a lower maximum weight rather than the structural requirements.  Consequently, anything that can be done to improve the single-engine-inoperative climb performance will bring about an increase in maximum takeoff weight.In the US from 1945 until 1991,
the one-engine-inoperative climb requirement for multi-engine airplanes with a maximum takeoff weight of 6,000 lb (2,700 kg) or less was as follows:

All multi-engine airplanes having a stalling speed 
  
    
      
        
          V
          
            s
            0
          
        
      
    
    {\displaystyle V_{s0}}
   greater than 70 miles per hour shall have a steady rate of climb of at least 
  
    
      
        0.02
        (
        
          V
          
            s
            0
          
        
        
          )
          
            2
          
        
      
    
    {\displaystyle 0.02(V_{s0})^{2}}
   in feet per minute at an altitude of 5,000 feet with the critical engine inoperative and the remaining engines operating at not more than maximum continuous power, the inoperative propeller in the minimum drag position, landing gear retracted, wing flaps in the most favorable position …
where 
  
    
      
        
          V
          
            s
            0
          
        
      
    
    {\displaystyle V_{s0}}
   is the stalling speed in the landing configuration in miles per hour.
Installation of vortex generators can usually bring about a slight reduction in stalling speed of an airplane and therefore reduce the required one-engine-inoperative climb performance.  The reduced requirement for climb performance allows an increase in maximum takeoff weight, at least up to the maximum weight allowed by structural requirements.  An increase in maximum weight allowed by structural requirements can usually be achieved by specifying a maximum zero fuel weight or, if a maximum zero fuel weight is already specified as one of the airplane's limitations, by specifying a new higher maximum zero fuel weight.  For these reasons, vortex generator kits for many light twin-engine airplanes are accompanied by a reduction in maximum zero fuel weight and an increase in maximum takeoff weight.The one-engine-inoperative rate-of-climb requirement does not apply to single-engine airplanes, so gains in the maximum takeoff weight (based on stall speed or structural considerations) are less significant compared to those for 1945–1991 twins.
After 1991, the airworthiness certification requirements in the USA specify the one-engine-inoperative climb requirement as a gradient independent of stalling speed, so there is less opportunity for vortex generators to increase the maximum takeoff weight of multi-engine airplanes whose certification basis is FAR 23 at amendment 23-42 or later.


== Maximum landing weight ==
Because the landing weights of most light aircraft are determined by structural considerations and not by stall speed, most VG kits increase only the takeoff weight and not the landing weight.  Any increase in landing weight would require either structural modifications or re-testing the aircraft at the higher landing weight to demonstrate that the certification requirements are still met.  However, after a lengthy flight, sufficient fuel may have been used, thereby bringing the aircraft back below the permitted maximum landing weight.


== Aircraft noise reduction ==
Vortex generators have been used on the wing underside of Airbus A320 family aircraft to reduce noise generated by airflow over circular pressure equalisation vents for the fuel tanks. Lufthansa claims a noise reduction of up to 2 dB can thus be achieved.


== See also ==
Turbulator
Boundary layer suction
Boundary layer control
Circulation control wing


== References ==


== External links ==
Vortex Generators: 50 Years of Performance Benefits, a history of VGs","pandas(index=12, _1=12, text=""a vortex generator (vg) is an aerodynamic device, consisting of a small vane usually attached to a lifting surface (or airfoil, such as an aircraft wing) or a rotor blade of a wind turbine. vgs may also be attached to some part of an aerodynamic vehicle such as an aircraft fuselage or a car.  when the airfoil or the body is in motion relative to the air, the vg creates a vortex,  which, by removing some part of the slow-moving boundary layer in contact with the airfoil surface, delays local flow separation and aerodynamic stalling, thereby improving the effectiveness of wings and control surfaces, such as flaps, elevators, ailerons, and rudders.   == method of operation == vortex generators are most often used to delay flow separation.  to accomplish this they are often placed on the external surfaces of vehicles and wind turbine blades. on both aircraft and wind turbine blades they are usually installed quite close to the leading edge of the aerofoil in order to maintain steady airflow over the control surfaces at the trailing edge. vgs are typically rectangular or triangular, about as tall as the local boundary layer, and run in spanwise lines usually near the thickest part of the wing. they can be seen on the wings and vertical tails of many airliners. vortex generators are positioned obliquely so that they have an angle of attack with respect to the local airflow in order to create a tip vortex which draws energetic, rapidly moving outside air into the slow-moving boundary layer in contact with the surface. a turbulent boundary layer is less likely to separate than a laminar one, and is therefore desirable to ensure effectiveness of trailing-edge control surfaces. vortex generators are used to trigger this transition. other devices such as vortilons, leading-edge extensions, and leading-edge cuffs, also delay flow separation at high angles of attack by re-energizing the boundary layer.examples of aircraft which use vgs include the st aerospace a-4su super skyhawk and symphony sa-160. for swept-wing transonic designs, vgs alleviate potential shock-stall problems (e.g., harrier, blackburn buccaneer, gloster javelin).   == aftermarket installation == many aircraft carry vane vortex generators from time of manufacture, but there are also aftermarket suppliers who sell vg kits to improve the stol performance of some light aircraft.  aftermarket suppliers claim (i) that vgs lower stall speed and reduce take-off and landing speeds, and (ii) that vgs increase the effectiveness of ailerons, elevators and rudders, thereby improving controllability and safety at low speeds. for home-built and experimental kitplanes, vgs are cheap, cost-effective and can be installed quickly; but for certified aircraft installations, certification costs can be high, making the modification a relatively expensive process.owners fit aftermarket vgs primarily to gain benefits at low speeds, but a downside is that such vgs may reduce cruise speed slightly. in tests performed on a cessna 182 and a piper pa-28-235 cherokee, independent reviewers have documented a loss of cruise speed of 1.5 to 2.0 kn (2.8 to 3.7 km/h) .  however, these losses are relatively minor, since an aircraft wing at high speed has a small angle of attack, thereby reducing vg drag to a minimum.owners have reported that on the ground, it can be harder to clear snow and ice from wing surfaces with vgs than from a smooth wing, but vgs are not generally prone to inflight icing as they reside within the boundary layer of airflow. vgs may also have sharp edges which can tear the fabric of airframe covers and may thus require special covers to be made.for twin-engined aircraft, manufacturers claim that vgs reduce single-engine control speed (vmca), increase zero fuel and gross weight, improve the effectiveness of ailerons and rudder, provide a smoother ride in turbulence and make the aircraft a more stable instrument platform.   == increase in maximum takeoff weight == some vg kits available for light twin-engine airplanes may allow an increase in maximum takeoff weight.  the maximum takeoff weight of a twin-engine airplane is determined by structural requirements and single-engine climb performance requirements (which are lower for a lower stall speed).  for many light twin-engine airplanes, the single-engine climb performance requirements determine a lower maximum weight rather than the structural requirements.  consequently, anything that can be done to improve the single-engine-inoperative climb performance will bring about an increase in maximum takeoff weight.in the us from 1945 until 1991, the one-engine-inoperative climb requirement for multi-engine airplanes with a maximum takeoff weight of 6,000 lb (2,700 kg) or less was as follows:  all multi-engine airplanes having a stalling speed     v  s 0      is the stalling speed in the landing configuration in miles per hour. installation of vortex generators can usually bring about a slight reduction in stalling speed of an airplane and therefore reduce the required one-engine-inoperative climb performance.  the reduced requirement for climb performance allows an increase in maximum takeoff weight, at least up to the maximum weight allowed by structural requirements.  an increase in maximum weight allowed by structural requirements can usually be achieved by specifying a maximum zero fuel weight or, if a maximum zero fuel weight is already specified as one of the airplane's limitations, by specifying a new higher maximum zero fuel weight.  for these reasons, vortex generator kits for many light twin-engine airplanes are accompanied by a reduction in maximum zero fuel weight and an increase in maximum takeoff weight.the one-engine-inoperative rate-of-climb requirement does not apply to single-engine airplanes, so gains in the maximum takeoff weight (based on stall speed or structural considerations) are less significant compared to those for 1945–1991 twins. after 1991, the airworthiness certification requirements in the usa specify the one-engine-inoperative climb requirement as a gradient independent of stalling speed, so there is less opportunity for vortex generators to increase the maximum takeoff weight of multi-engine airplanes whose certification basis is far 23 at amendment 23-42 or later.   == maximum landing weight == because the landing weights of most light aircraft are determined by structural considerations and not by stall speed, most vg kits increase only the takeoff weight and not the landing weight.  any increase in landing weight would require either structural modifications or re-testing the aircraft at the higher landing weight to demonstrate that the certification requirements are still met.  however, after a lengthy flight, sufficient fuel may have been used, thereby bringing the aircraft back below the permitted maximum landing weight.   == aircraft noise reduction == vortex generators have been used on the wing underside of airbus a320 family aircraft to reduce noise generated by airflow over circular pressure equalisation vents for the fuel tanks. lufthansa claims a noise reduction of up to 2 db can thus be achieved.   == see also == turbulator boundary layer suction boundary layer control circulation control wing   == references ==   == external links == vortex generators: 50 years of performance benefits, a history of vgs"")"
13,"In aerospace engineering, payload fraction is a common term used to characterize the efficiency of a particular design. Payload fraction is calculated by dividing the weight of the payload by the takeoff weight of aircraft. 
Fuel represents a considerable amount of the overall takeoff weight, and for shorter trips it is quite common to load less fuel in order to carry a lighter load. For this reason the useful load fraction calculates a similar number, but based on the combined weight of the payload and fuel together.
Propeller-driven airliners had useful load fractions on the order of 25-35%. Modern jet airliners have considerably higher useful load fractions, on the order of 45-55%.
For spacecraft the payload fraction is often less than 1%, while the useful load fraction is perhaps 90%. In this case the useful load fraction is not a useful term, because spacecraft typically cannot reach orbit without a full fuel load. For this reason the related term  propellant mass fraction, is used instead. However, if the latter is large, the payload can only be small.


== Examples ==
Note: the above table may incorrectly include the mass of the empty upper stage or stages.


== See also ==
Tsiolkovsky rocket equation


== References ==","pandas(index=13, _1=13, text='in aerospace engineering, payload fraction is a common term used to characterize the efficiency of a particular design. payload fraction is calculated by dividing the weight of the payload by the takeoff weight of aircraft. fuel represents a considerable amount of the overall takeoff weight, and for shorter trips it is quite common to load less fuel in order to carry a lighter load. for this reason the useful load fraction calculates a similar number, but based on the combined weight of the payload and fuel together. propeller-driven airliners had useful load fractions on the order of 25-35%. modern jet airliners have considerably higher useful load fractions, on the order of 45-55%. for spacecraft the payload fraction is often less than 1%, while the useful load fraction is perhaps 90%. in this case the useful load fraction is not a useful term, because spacecraft typically cannot reach orbit without a full fuel load. for this reason the related term  propellant mass fraction, is used instead. however, if the latter is large, the payload can only be small.   == examples == note: the above table may incorrectly include the mass of the empty upper stage or stages.   == see also == tsiolkovsky rocket equation   == references ==')"
14,"A tiger team is composed of specialists assembled to work on a specific goal or to solve a particular problem.


== Term ==
A 1964 paper entitled Program Management in Design and Development used the term tiger teams and defined it as ""a team of undomesticated and uninhibited technical specialists, selected for their experience, energy, and imagination, and assigned to track down relentlessly every possible source of failure in a spacecraft subsystem or simulation"". The paper consists of anecdotes and answers to questions from a panel on improving issues in program management concerning testing and quality assurance in aerospace vehicle development and production. One of the authors was Walter C. Williams, an engineer at the Manned Spacecraft Center and part of the Edwards Air Force Base National Advisory Committee for Aeronautics. Williams suggests that tiger teams are an effective and useful method for advancing the reliability of systems and subsystems in the context of actual flight environments. Jane Goodall, Liam Hunt and Kate Herron, among others, have noted that tigers are not naturally cooperative animals and have suggested referring to “chimpanzee teams” because of the intense cooperation that occurs in chimpanzee social groups.


== Examples ==
A tiger team was crucial to the Apollo 13 lunar landing mission in 1970. During the mission, part of the Apollo 13 Service Module malfunctioned and exploded. A team of specialists was formed to fix the issue and bring the astronauts back to Earth safely, led by NASA Flight and Mission Operations Director Gene Kranz. Kranz and the members of his ""White Team"", later designated the ""Tiger Team"", received the Presidential Medal of Freedom for their efforts in the Apollo 13 mission.
In security work, a tiger team is a group that tests an organization's ability to protect its assets by attempting to defeat its physical or information security. In this context, the tiger team is often a permanent team as security is typically an ongoing priority. For example, one implementation of an information security tiger team approach divides the team into two co-operating groups: one for vulnerability research, which finds and researches the technical aspects of a vulnerability, and one for vulnerability management, which manages communication and feedback between the team and the organization, as well as ensuring each discovered vulnerability is tracked throughout its life-cycle and ultimately resolved.
An initiative involving tiger teams was implemented by the United States Department of Energy (DOE) under then-Secretary James D. Watkins. From 1989 through 1992 the DOE formed tiger teams to assess 35 DOE facilities for compliance with environment, safety, and health requirements. Beginning in October 1991 smaller tiger teams were formed to perform more detailed follow up assessments to focus on the most pressing issues.
The NASA Engineering and Safety Center (NESC) puts together ""tiger teams"" of engineers and scientists from multiple NASA centers to assist solving complex problems when requested by a project or program.


== See also ==
Penetration test
Red team


== References ==


== External links ==
""All About Tiger Team""This article is based on material taken from  the Free On-line Dictionary of Computing  prior to 1 November 2008 and incorporated under the ""relicensing"" terms of the GFDL, version 1.3 or later.","pandas(index=14, _1=14, text='a tiger team is composed of specialists assembled to work on a specific goal or to solve a particular problem.   == term == a 1964 paper entitled program management in design and development used the term tiger teams and defined it as ""a team of undomesticated and uninhibited technical specialists, selected for their experience, energy, and imagination, and assigned to track down relentlessly every possible source of failure in a spacecraft subsystem or simulation"". the paper consists of anecdotes and answers to questions from a panel on improving issues in program management concerning testing and quality assurance in aerospace vehicle development and production. one of the authors was walter c. williams, an engineer at the manned spacecraft center and part of the edwards air force base national advisory committee for aeronautics. williams suggests that tiger teams are an effective and useful method for advancing the reliability of systems and subsystems in the context of actual flight environments. jane goodall, liam hunt and kate herron, among others, have noted that tigers are not naturally cooperative animals and have suggested referring to “chimpanzee teams” because of the intense cooperation that occurs in chimpanzee social groups.   == examples == a tiger team was crucial to the apollo 13 lunar landing mission in 1970. during the mission, part of the apollo 13 service module malfunctioned and exploded. a team of specialists was formed to fix the issue and bring the astronauts back to earth safely, led by nasa flight and mission operations director gene kranz. kranz and the members of his ""white team"", later designated the ""tiger team"", received the presidential medal of freedom for their efforts in the apollo 13 mission. in security work, a tiger team is a group that tests an organization\'s ability to protect its assets by attempting to defeat its physical or information security. in this context, the tiger team is often a permanent team as security is typically an ongoing priority. for example, one implementation of an information security tiger team approach divides the team into two co-operating groups: one for vulnerability research, which finds and researches the technical aspects of a vulnerability, and one for vulnerability management, which manages communication and feedback between the team and the organization, as well as ensuring each discovered vulnerability is tracked throughout its life-cycle and ultimately resolved. an initiative involving tiger teams was implemented by the united states department of energy (doe) under then-secretary james d. watkins. from 1989 through 1992 the doe formed tiger teams to assess 35 doe facilities for compliance with environment, safety, and health requirements. beginning in october 1991 smaller tiger teams were formed to perform more detailed follow up assessments to focus on the most pressing issues. the nasa engineering and safety center (nesc) puts together ""tiger teams"" of engineers and scientists from multiple nasa centers to assist solving complex problems when requested by a project or program.   == see also == penetration test red team   == references ==   == external links == ""all about tiger team""this article is based on material taken from  the free on-line dictionary of computing  prior to 1 november 2008 and incorporated under the ""relicensing"" terms of the gfdl, version 1.3 or later.')"
15,"A multistage rocket, or step rocket, is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. A tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. The result is effectively two or more rockets stacked on top of or attached next to each other. Two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched.
By jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. Each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. This staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height. 
In serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. In parallel staging schemes solid or liquid rocket boosters are used to assist with launch. These are sometimes referred to as ""stage 0"". In the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. When the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. The first stage then burns to completion and falls off. This leaves a smaller rocket, with the second stage on the bottom, which then fires. Known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. In some cases with serial staging, the upper stage ignites before the separation—the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles.
A multistage rocket is required to reach orbital speed. Single-stage-to-orbit designs are sought, but have not yet been demonstrated.


== Performance ==

The reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. This relation is given by the classical rocket equation:

  
    
      
        Δ
        v
        =
        
          v
          
            e
          
        
        ln
        ⁡
        
          (
          
            
              
                m
                
                  0
                
              
              
                m
                
                  f
                
              
            
          
          )
        
      
    
    {\displaystyle \Delta v=v_{\text{e}}\ln \left({\frac {m_{0}}{m_{f}}}\right)}
  where:

  
    
      
        Δ
        v
         
      
    
    {\displaystyle \Delta v\ }
   is delta-v  of the vehicle (change of velocity plus losses due to gravity and atmospheric drag);

  
    
      
        
          m
          
            0
          
        
      
    
    {\displaystyle m_{0}}
   is the initial total (wet) mass, equal to final (dry) mass plus propellant;

  
    
      
        
          m
          
            f
          
        
      
    
    {\displaystyle m_{f}}
   is the final (dry) mass, after the propellant is expended;

  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   is the effective exhaust velocity (determined by propellant, engine design and throttle condition);

  
    
      
        ln
      
    
    {\displaystyle \ln }
   is the natural logarithm function.The delta v required to reach low Earth orbit (or the required velocity of a sufficiently heavy suborbital payload) requires a wet to dry mass ratio larger than can realistically be achieved in a single rocket stage. The multistage rocket overcomes this limit by splitting the delta-v into fractions. As each lower stage drops off and the succeeding stage fires, the rest of the rocket is still traveling near the burnout speed. Each lower stage's dry mass includes the propellant in the upper stages, and each succeeding upper stage has reduced its dry mass by discarding the useless dry mass of the spent lower stages.
A further advantage is that each stage can use a different type of rocket engine, each tuned for its particular operating conditions. Thus the lower-stage engines are designed for use at atmospheric pressure, while the upper stages can use engines suited to near vacuum conditions. Lower stages tend to require more structure than upper as they need to bear their own weight plus that of the stages above them. Optimizing the structure of each stage decreases the weight of the total vehicle and provides further advantage.
The advantage of staging comes at the cost of the lower stages lifting engines which are not yet being used, as well as making the entire rocket more complex and harder to build than a single stage. In addition, each staging event is a possible point of launch failure, due to separation failure, ignition failure, or stage collision. Nevertheless, the savings are so great that every rocket ever used to deliver a payload into orbit has had staging of some sort.
One of the most common measures of rocket efficiency is its specific impulse, which is defined as the thrust per flow rate (per second) of propellant consumption:

  
    
      
        
          I
          
            
              s
              p
            
          
        
      
    
    {\displaystyle I_{\mathrm {sp} }}
   = 
  
    
      
         
        
          
            T
            
              
                
                  
                    d
                    m
                  
                  
                    d
                    t
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle \ {\frac {T}{{\frac {dm}{dt}}g_{\mathrm {0} }}}}
  When rearranging the equation such that thrust is calculated as a result of the other factors, we have:

  
    
      
        T
        =
        
          I
          
            
              s
              p
            
          
        
        
          g
          
            
              0
            
          
        
        
          
            
              d
              m
            
            
              d
              t
            
          
        
      
    
    {\displaystyle T=I_{\mathrm {sp} }g_{\mathrm {0} }{\frac {dm}{dt}}}
  These equations show that a higher specific impulse means a more efficient rocket engine, capable of burning for longer periods of time.  In terms of staging, the initial rocket stages usually have a lower specific impulse rating, trading efficiency for superior thrust in order to quickly push the rocket into higher altitudes.  Later stages of the rocket usually have a higher specific impulse rating because the vehicle is further outside the atmosphere and the exhaust gas does not need to expand against as much atmospheric pressure.
When selecting the ideal rocket engine to use as an initial stage for a launch vehicle, a useful performance metric to examine is the thrust-to-weight ratio, and is calculated by the equation:

  
    
      
        T
        W
        R
        =
        
          
            T
            
              m
              
                g
                
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle TWR={\frac {T}{mg_{\mathrm {0} }}}}
  The common thrust-to-weight ratio of a launch vehicle is within the range of 1.3 to 2.0.
Another performance metric to keep in mind when designing each rocket stage in a mission is the burn time, which is the amount of time the rocket engine will last before it has exhausted all of its propellant.  For most non-final stages, thrust and specific impulse can be assumed constant, which allows the equation for burn time to be written as:

  
    
      
        Δ
        
          t
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
            T
          
        
        ×
        (
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle \Delta {t}={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }}{T}}\times (m_{\mathrm {0} }-m_{\mathrm {f} })}
  Where 
  
    
      
        
          m
          
            
              0
            
          
        
      
    
    {\displaystyle m_{\mathrm {0} }}
   and 
  
    
      
        
          m
          
            
              f
            
          
        
      
    
    {\displaystyle m_{\mathrm {f} }}
   are the initial and final masses of the rocket stage respectively.  In conjunction with the burnout time, the burnout height and velocity are obtained using the same values, and are found by these two equations:

  
    
      
        
          h
          
            
              b
              o
            
          
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
            
            
              m
              
                
                  e
                
              
            
          
        
        ×
        (
        
          m
          
            
              f
            
          
        
         
        
          l
          n
        
        (
        
          m
          
            
              f
            
          
        
        
          /
        
        
          m
          
            
              0
            
          
        
        )
        +
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle h_{\mathrm {bo} }={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }}{m_{\mathrm {e} }}}\times (m_{\mathrm {f} }~\mathrm {ln} (m_{\mathrm {f} }/m_{\mathrm {0} })+m_{\mathrm {0} }-m_{\mathrm {f} })}
  

  
    
      
        
          v
          
            
              b
              o
            
          
        
        =
        
          
            
              
                I
                
                  
                    s
                    p
                  
                
              
              
                g
                
                  
                    0
                  
                
              
              
                m
                
                  
                    0
                  
                
              
            
            
              m
              
                
                  f
                
              
            
          
        
        −
        
          
            
              g
              
                
                  0
                
              
            
            
              m
              
                
                  e
                
              
            
          
        
        (
        
          m
          
            
              0
            
          
        
        −
        
          m
          
            
              f
            
          
        
        )
      
    
    {\displaystyle v_{\mathrm {bo} }={\frac {I_{\mathrm {sp} }g_{\mathrm {0} }m_{\mathrm {0} }}{m_{\mathrm {f} }}}-{\frac {g_{\mathrm {0} }}{m_{\mathrm {e} }}}(m_{\mathrm {0} }-m_{\mathrm {f} })}
  When dealing with the problem of calculating the total burnout velocity or time for the entire rocket system, the general procedure for doing so is as follows:
Partition the problem calculations into however many stages the rocket system comprises.
Calculate the initial and final mass for each individual stage.
Calculate the burnout velocity, and sum it with the initial velocity for each individual stage.  Assuming each stage occurs immediately after the previous, the burnout velocity becomes the initial velocity for the following stage.
Repeat the previous two steps until the burnout time and/or velocity has been calculated for the final stage.It is important to note that the burnout time does not define the end of the rocket stage's motion, as the vehicle will still have a velocity that will allow it to coast upward for a brief amount of time until the acceleration of the planet's gravity gradually changes it to a downward direction.  The velocity and altitude of the rocket after burnout can be easily modeled using the basic physics equations of motion.
When comparing one rocket with another, it is impractical to directly compare the rocket's certain trait with the same trait of another because their individual attributes are often not independent of one another.  For this reason, dimensionless ratios have been designed to enable a more meaningful comparison between rockets.  The first is the initial to final mass ratio, which is the ratio between the rocket stage's full initial mass and the rocket stage's final mass once all of its fuel has been consumed.  The equation for this ratio is:

  
    
      
        η
        =
        
          
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    p
                  
                
              
              +
              
                m
                
                  
                    P
                    L
                  
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                    L
                  
                
              
            
          
        
      
    
    {\displaystyle \eta ={\frac {m_{\mathrm {E} }+m_{\mathrm {p} }+m_{\mathrm {PL} }}{m_{\mathrm {E} }+m_{\mathrm {PL} }}}}
  Where 
  
    
      
        
          m
          
            
              E
            
          
        
      
    
    {\displaystyle m_{\mathrm {E} }}
   is the empty mass of the stage, 
  
    
      
        
          m
          
            
              p
            
          
        
      
    
    {\displaystyle m_{\mathrm {p} }}
   is the mass of the propellant, and 
  
    
      
        
          m
          
            
              P
              L
            
          
        
      
    
    {\displaystyle m_{\mathrm {PL} }}
   is the mass of the payload.  
The second dimensionless performance quantity is the structural ratio, which is the ratio between the empty mass of the stage, and the combined empty mass and propellant mass as shown in this equation:

  
    
      
        ϵ
        =
        
          
            
              m
              
                
                  E
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                  
                
              
            
          
        
      
    
    {\displaystyle \epsilon ={\frac {m_{\mathrm {E} }}{m_{\mathrm {E} }+m_{\mathrm {P} }}}}
  The last major dimensionless performance quantity is the payload ratio, which is the ratio between the payload mass and the combined mass of the empty rocket stage and the propellant:

  
    
      
        λ
        =
        
          
            
              m
              
                
                  P
                  L
                
              
            
            
              
                m
                
                  
                    E
                  
                
              
              +
              
                m
                
                  
                    P
                  
                
              
            
          
        
      
    
    {\displaystyle \lambda ={\frac {m_{\mathrm {PL} }}{m_{\mathrm {E} }+m_{\mathrm {P} }}}}
  After comparing the three equations for the dimensionless quantities,  it is easy to see that they are not independent of each other, and in fact, the initial to final mass ratio can be rewritten in terms of structural ratio and payload ratio:

  
    
      
        η
        =
        
          
            
              1
              +
              λ
            
            
              ϵ
              +
              λ
            
          
        
      
    
    {\displaystyle \eta ={\frac {1+\lambda }{\epsilon +\lambda }}}
  These performance ratios can also be used as references for how efficient a rocket system will be when performing optimizations and comparing varying configurations for a mission.


== Component selection and sizing ==

For initial sizing, the rocket equations can be used to derive the amount of propellant needed for the rocket based on the specific impulse of the engine and the total impulse required in N*s.  The equation is:

  
    
      
        
          m
          
            
              p
            
          
        
        =
        
          I
          
            
              t
              o
              t
            
          
        
        
          /
        
        (
        g
        ∗
        
          I
          
            
              s
              p
            
          
        
        )
      
    
    {\displaystyle m_{\mathrm {p} }=I_{\mathrm {tot} }/(g*I_{\mathrm {sp} })}
  where g is the gravity constant of Earth.  This also enables the volume of storage required for the fuel to be calculated if the density of the fuel is known, which is almost always the case when designing the rocket stage.  The volume is yielded when dividing the mass of the propellant by its density.  Asides from the fuel required, the mass of the rocket structure itself must also be determined, which requires taking into account the mass of the required thrusters, electronics, instruments, power equipment, etc. These are known quantities for typical off the shelf hardware that should be considered in the mid to late stages of the design, but for preliminary and conceptual design, a simpler approach can be taken.  Assuming one engine for a rocket stage provides all of the total impulse for that particular segment, a mass fraction can be used to determine the mass of the system.  The mass of the stage transfer hardware such as initiators and safe-and-arm devices are very small by comparison and can be considered negligible.  
For modern day solid rocket motors, it is a safe and reasonable assumption to say that 91 to 94 percent of the total mass is fuel.  It is also important to note there is a small percentage of ""residual"" propellant that will be left stuck and unusable inside the tank, and should also be taken into consideration when determining amount of fuel for the rocket.  A common initial estimate for this residual propellant is five percent.  With this ratio and the mass of the propellant calculated, the mass of the empty rocket weight can be determined.  
Sizing rockets using a liquid bipropellant requires a slightly more involved approach because of the fact that there are two separate tanks that are required:  One for the fuel, and one for the oxidizer.  The ratio of these two quantities is known as the mixture ratio, and is defined by the equation:

  
    
      
        O
        
          /
        
        F
        =
        
          m
          
            
              o
              x
            
          
        
        
          /
        
        
          m
          
            
              f
              u
              e
              l
            
          
        
      
    
    {\displaystyle O/F=m_{\mathrm {ox} }/m_{\mathrm {fuel} }}
  Where 
  
    
      
        
          m
          
            
              o
              x
            
          
        
      
    
    {\displaystyle m_{\mathrm {ox} }}
   is the mass of the oxidizer and 
  
    
      
        
          m
          
            
              f
              u
              e
              l
            
          
        
      
    
    {\displaystyle m_{\mathrm {fuel} }}
   is the mass of the fuel.  This mixture ratio not only governs the size of each tank, but also the specific impulse of the rocket.  Determining the ideal mixture ratio is a balance of compromises between various aspects of the rocket being designed, and can vary depending on the type of fuel and oxidizer combination being used.  For example, a mixture ratio of a bipropellant could be adjusted such that it may not have the optimal specific impulse, but will result in fuel tanks of equal size.  This would yield simpler and cheaper manufacturing, packing, configuring, and integrating of the fuel systems with the rest of the rocket, and can become a benefit that could outweigh the drawbacks of a less efficient specific impulse rating.  But suppose the defining constraint for the launch system is volume, and a low density fuel is required such as hydrogen.  This example would be solved by using an oxidizer-rich mixture ratio, reducing efficiency and specific impulse rating, but will meet a smaller tank volume requirement.


== Optimal staging and restricted staging ==


=== Optimal ===
The ultimate goal of optimal staging is to maximize the payload ratio (see ratios under performance), meaning the largest amount of payload is carried up to the required burnout velocity using the least amount of non-payload mass, which comprises everything else. Here are a few quick rules and guidelines to follow in order to reach optimal staging:
Initial stages should have lower 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  , and later/final stages should have higher 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  .
The stages with the lower 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
   should contribute more ΔV.
The next stage is always a smaller size than the previous stage.
Similar stages should provide similar ΔV.The payload ratio can be calculated for each individual stage, and when multiplied together in sequence, will yield the overall payload ratio of the entire system.  It is important to note that when computing payload ratio for individual stages, the payload includes the mass of all the stages after the current one.  The overall payload ratio is:

  
    
      
        λ
        =
        
          ∏
          
            i
            =
            1
          
          
            n
          
        
        
          λ
          
            i
          
        
      
    
    {\displaystyle \lambda =\prod _{i=1}^{n}\lambda _{i}}
  Where n is the number of stages the rocket system comprises.  Similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and ΔV requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above.  Two common methods of determining this perfect ΔV partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error.  For the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage.  From there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system.


=== Restricted ===
Restricted rocket staging is based on the simplified assumption that each of the stages of the rocket system have the same specific impulse, structural ratio, and payload ratio, the only difference being the total mass of each increasing stage is less than that of the previous stage.  Although this assumption may not be the ideal approach to yielding an efficient or optimal system, it greatly simplifies the equations for determining the burnout velocities, burnout times, burnout altitudes, and mass of each stage.  This would make for a better approach to a conceptual design in a situation where a basic understanding of the system behavior is preferential to a detailed, accurate design.  
One important concept to understand when undergoing restricted rocket staging, is how the burnout velocity is affected by the number of stages that split up the rocket system.  Increasing the number of stages for a rocket while keeping the specific impulse, payload ratios and structural ratios constant will always yield a higher burnout velocity than the same systems that use fewer stages.  However, the law of diminishing returns is evident in that each increment in number of stages gives less of an improvement in burnout velocity than the previous increment.  The burnout velocity gradually converges towards an asymptotic value as the number of stages increases towards a very high number.  In addition to diminishing returns in burnout velocity improvement, the main reason why real world rockets seldom use more than three stages is because of increase of weight and complexity in the system for each added stage, ultimately yielding a higher cost for deployment.


== Tandem vs parallel staging design ==
A rocket system that implements tandem staging means that each individual stage runs in order one after the other.  The rocket breaks free from the previous stage, then begins burning through the next stage in straight succession.  On the other hand, a rocket that implements parallel staging has two or more different stages that are active at the same time.  For example, the Space Shuttle has two Solid Rocket Boosters that burn simultaneously.  Upon launch, the boosters ignite, and at the end of the stage, the two boosters are discarded while the external fuel tank is kept for another stage.  
Most quantitative approaches to the design of the rocket system's performance are focused on tandem staging, but the approach can be easily modified to include parallel staging.  To begin with, the different stages of the rocket should be clearly defined.  Continuing with the previous example, the end of the first stage which is sometimes referred to as 'stage 0', can be defined as when the side boosters separate from the main rocket.  From there, the final mass of stage one can be considered the sum of the empty mass of stage one, the mass of stage two (the main rocket and the remaining unburned fuel) and the mass of the payload.


== Upper stages ==
High-altitude and space-bound upper stages are designed to operate with little or no atmospheric pressure. This allows the use of lower pressure combustion chambers and engine nozzles with optimal vacuum expansion ratios. Some upper stages, especially those using hypergolic propellants like Delta-K or Ariane 5 ES second stage, are pressure fed, which eliminates the need for complex turbopumps. Other upper stages, such as the Centaur or DCSS, use liquid hydrogen expander cycle engines, or gas generator cycle engines like the Ariane 5 ECA's HM7B or the S-IVB's J-2. These stages are usually tasked with completing orbital injection and accelerating payloads into higher energy orbits such as GTO or to escape velocity. Upper stages, such as Fregat, used primarily to bring payloads from low Earth orbit to GTO or beyond are sometimes referred to as space tugs.


== Assembly ==
Each individual stage is generally assembled at its manufacturing site and shipped to the launch site; the term vehicle assembly refers to the mating of all rocket stage(s) and the spacecraft payload into a single assembly known as a space vehicle. Single-stage vehicles (suborbital), and multistage vehicles on the smaller end of the size range, can usually be assembled directly on the launch pad by lifting the stage(s) and spacecraft vertically in place by means of a crane.
This is generally not practical for larger space vehicles, which are assembled off the pad and moved into place on the launch site by various methods. NASA's Apollo/Saturn V manned Moon landing vehicle, and  Space Shuttle, were assembled vertically onto mobile launcher platforms with attached launch umbilical towers, in a Vehicle Assembly Building, and then a special crawler-transporter moved the entire vehicle stack to the launch pad in an upright position. 
In contrast, vehicles such as the Russian Soyuz rocket and the SpaceX Falcon 9 are assembled horizontally in a processing hangar, transported horizontally, and then brought upright at the pad.


== Passivation and space debris ==
Spent upper stages of launch vehicles are a significant source of space debris remaining in orbit in a non-operational state for many years after use, and occasionally, large debris fields created from the breakup of a single upper stage while in orbit.After the 1990s, spent upper stages are generally passivated after their use as a launch vehicle is complete in order to minimize risks while the stage remains derelict in orbit.  Passivation means removing any sources of stored energy remaining on the vehicle, as by dumping fuel or discharging batteries.
Many early upper stages, in both the Soviet and U.S. space programs, were not passivated after mission completion.  During the initial attempts to characterize the space debris problem, it became evident that a good proportion of all debris was due to the breaking up of rocket upper stages, particularly unpassivated upper-stage propulsion units.


== History and development ==
An illustration and description in the 14th century Chinese Huolongjing by Jiao Yu and Liu Bowen shows the oldest known multistage rocket; this was the ""fire-dragon issuing from the water"" (火龙出水, huǒ lóng chū shuǐ), used mostly by the Chinese navy. It was a two-stage rocket that had booster rockets that would eventually burn out, yet before they did they automatically ignited a number of smaller rocket arrows that were shot out of the front end of the missile, which was shaped like a dragon's head with an open mouth. This multi-stage rocket may be considered the ancestor to the modern YingJi-62 ASCM. The British scientist and historian Joseph Needham points out that the written material and depicted illustration of this rocket come from the oldest stratum of the Huolongjing, which can be dated roughly 1300–1350 AD (from the book's part 1, chapter 3, page 23).Another example of an early multistaged rocket is the Juhwa (走火) of Korean development. It was proposed by medieval Korean engineer, scientist and inventor Choe Museon and developed by the Firearms Bureau (火㷁道監) during the 14th century. The rocket had the length of 15 cm and 13 cm; the diameter was 2.2 cm. It was attached to an arrow 110 cm long; experimental records show that the first results were around 200m in range. There are records that show Korea kept developing this technology until it came to produce the Singijeon, or 'magical machine arrows' in the 16th century. 
The earliest experiments with multistage rockets in Europe were made in 1551 by Austrian Conrad Haas (1509–1576), the arsenal master of the town of Hermannstadt, Transylvania (now Sibiu/Hermannstadt, Romania). This concept was developed independently by at least four individuals:

Kazimieras Simonavičius of the Polish–Lithuanian Commonwealth (1600–1651)
the Russian Konstantin Tsiolkovsky (1857–1935)
the American Robert Goddard (1882–1945)
the German Hermann Oberth (1894–1989), born in Hermannstadt, TransylvaniaThe first high-speed multistage rockets were the RTV-G-4 Bumper rockets tested at the White Sands Proving Ground and later at Cape Canaveral from 1948 to 1950. These consisted of a V-2 rocket and a WAC Corporal sounding rocket. The greatest altitude ever reached was 393 km, attained on February 24, 1949, at White Sands.
In 1947, the Soviet rocket engineer and scientist Mikhail Tikhonravov developed a theory of parallel stages, which he called ""packet rockets"". In his scheme, three parallel stages were fired from liftoff, but all three engines were fueled from the outer two stages, until they are empty and could be ejected. This is more efficient than sequential staging, because the second-stage engine is never just dead weight. In 1951, Soviet engineer and scientist Dmitry Okhotsimsky carried out a pioneering engineering study of general sequential and parallel staging, with and without the pumping of fuel between stages. The design of the R-7 Semyorka emerged from that study.  The trio of rocket engines used in the first stage of the American Atlas I and Atlas II launch vehicles, arranged in a row, used parallel staging in a similar way: the outer pair of booster engines existed as a jettisonable pair which would, after they shut down, drop away with the lowermost outer skirt structure, leaving the central sustainer engine to complete the first stage's engine burn towards apogee or orbit.


== Separation events ==
Separation of each portion of a multistage rocket introduces additional risk into the success of the launch mission.  Reducing the number of separation events results in a reduction in complexity. 
Separation events occur when stages or strap-on boosters separate after use, when the payload fairing separates prior to orbital insertion, or when used, a launch escape system which separates after the early phase of a launch. Pyrotechnic fasteners or pneumatic systems like on the Falcon 9 Full Thrust are typically used to separate rocket stages.


== Three stage to orbit ==
The three-stage-to-orbit launch system is a commonly used rocket system to attain Earth orbit.
The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity.
It is intermediate between a four-stage-to-orbit launcher and a two-stage-to-orbit launcher.


=== Examples of three stage to orbit systems ===
Saturn V
Vanguard
Ariane 4 (optional boosters)
Ariane 2
Ariane 1 (four stages)
GSLV (three stages and boosters)
PSLV (four stages)
Proton (optional fourth stage)
Long March 5 (optional boosters and optional third stage)
Long March 1, Long March 1D
Zenit-3SL
Minotaur IV (four stages)
Minotaur V (five stages)
Unha-3
KSLV-2 ""Nuri""


=== Examples of two stages with boosters ===
Other designs (in fact, most modern medium- to heavy-lift designs) do not have all three stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with two core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.

US Space Shuttle — SRB first stage ; External Tank + SSME second stage ; OMS on internal tanks third stage ;
Angara A5
Ariane 5
Atlas V 551
Delta II  third stage)
Delta III
Delta IV-Medium+ and -Heavy
Falcon Heavy
Geosynchronous Satellite Launch Vehicle Mk III (However, like the Titan IIIC, the GSLV MkIII is launched solely by the side boosters. The main core only ignites a few minutes into flight, shortly before the boosters are jettisoned.)
H-IIA, H-IIB
Soyuz
Space Launch System
Titan IV
Long March 2E, Long March 2F, Long March 3B


== Four stage to orbit ==
The four-stage-to-orbit launch system is a rocket system used to attain Earth orbit.
The spacecraft uses three distinct stages to provide propulsion consecutively in order to achieve orbital velocity.
It is intermediate between a five-stage-to-orbit launcher and a three-stage-to-orbit launcher.


=== Examples of four stage to orbit systems ===
Ariane 1
PSLV
Minotaur IV
Proton (optional fourth stage)
Minotaur V (five stages)
ASLV (five stages)


=== Examples of three stages with boosters ===
Other designs do not have all four stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with three core stages. In these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. The boosters are jettisoned a few minutes into flight to reduce weight.

Long March 5 (optional boosters and optional third stage)


== See also ==
Multistage rocket
Three-stage-to-orbit
Two-stage-to-orbit
Single-stage-to-orbit
Adapter
Reusable launch system
Space tug
Apogee kick motor
Conrad Haas
Modular rocket


== References ==","pandas(index=15, _1=15, text='a multistage rocket, or step rocket, is a launch vehicle that uses two or more rocket stages, each of which contains its own engines and propellant. a tandem or serial stage is mounted on top of another stage; a parallel stage is attached alongside another stage. the result is effectively two or more rockets stacked on top of or attached next to each other. two-stage rockets are quite common, but rockets with as many as five separate stages have been successfully launched. by jettisoning stages when they run out of propellant, the mass of the remaining rocket is decreased. each successive stage can also be optimized for its specific operating conditions, such as decreased atmospheric pressure at higher altitudes. this staging allows the thrust of the remaining stages to more easily accelerate the rocket to its final speed and height. in serial or tandem staging schemes, the first stage is at the bottom and is usually the largest, the second stage and subsequent upper stages are above it, usually decreasing in size. in parallel staging schemes solid or liquid rocket boosters are used to assist with launch. these are sometimes referred to as ""stage 0"". in the typical case, the first-stage and booster engines fire to propel the entire rocket upwards. when the boosters run out of fuel, they are detached from the rest of the rocket (usually with some kind of small explosive charge or explosive bolts) and fall away. the first stage then burns to completion and falls off. this leaves a smaller rocket, with the second stage on the bottom, which then fires. known in rocketry circles as staging, this process is repeated until the desired final velocity is achieved. in some cases with serial staging, the upper stage ignites before the separation—the interstage ring is designed with this in mind, and the thrust is used to help positively separate the two vehicles. a multistage rocket is required to reach orbital speed. single-stage-to-orbit designs are sought, but have not yet been demonstrated.   == performance ==  the reason multi-stage rockets are required is the limitation the laws of physics place on the maximum velocity achievable by a rocket of given fueled-to-dry mass ratio. this relation is given by the classical rocket equation:     δ v =  v  e   ln \u2061  (    m  0    m  f     )     where n is the number of stages the rocket system comprises.  similar stages yielding the same payload ratio simplify this equation, however that is seldom the ideal solution for maximizing payload ratio, and δv requirements may have to be partitioned unevenly as suggested in guideline tips 1 and 2 from above.  two common methods of determining this perfect δv partition between stages are either a technical algorithm that generates an analytical solution that can be implemented by a program, or simple trial and error.  for the trial and error approach, it is best to begin with the final stage, calculating the initial mass which becomes the payload for the previous stage.  from there it is easy to progress all the way down to the initial stage in the same manner, sizing all the stages of the rocket system. other designs do not have all four stages inline on the main stack, instead having strap-on boosters for the ""stage-0"" with three core stages. in these designs, the boosters and first stage fire simultaneously instead of consecutively, providing extra initial thrust to lift the full launcher weight and overcome gravity losses and atmospheric drag. the boosters are jettisoned a few minutes into flight to reduce weight.  long march 5 (optional boosters and optional third stage)   == see also == multistage rocket three-stage-to-orbit two-stage-to-orbit single-stage-to-orbit adapter reusable launch system space tug apogee kick motor conrad haas modular rocket   == references ==')"
16,"The advanced multi-mission operations system (AMMOS) is a common set of services and tools created by the Interplanetary Network Directorate, a division of the Jet Propulsion Laboratory, for use in JPL's operation of spacecraft. These tools include a means by which mission planning and analysis can be undertaken, as well as developing pre-planned command sequences for the spacecraft. AMMOS also provides a means by which downlinked data can be displayed and manipulated, including key mission telemetry such as readings of temperature, pressure, power, and other critical indicators. This common toolset allows space missions to minimize the cost of developing operations infrastructure, which is very important in light of recent restricted spending by space agencies.


== References ==
JPL AMMOS Landing Page","pandas(index=16, _1=16, text=""the advanced multi-mission operations system (ammos) is a common set of services and tools created by the interplanetary network directorate, a division of the jet propulsion laboratory, for use in jpl's operation of spacecraft. these tools include a means by which mission planning and analysis can be undertaken, as well as developing pre-planned command sequences for the spacecraft. ammos also provides a means by which downlinked data can be displayed and manipulated, including key mission telemetry such as readings of temperature, pressure, power, and other critical indicators. this common toolset allows space missions to minimize the cost of developing operations infrastructure, which is very important in light of recent restricted spending by space agencies.   == references == jpl ammos landing page"")"
17,"The maiden flight, also known as first flight, of an aircraft is the first occasion on which an aircraft leaves the ground under its own power. The same term is also used for the first launch of rockets.
The maiden flight of a new aircraft type is always a historic occasion for the type and can be quite emotional for those involved. In the early days of aviation it could be dangerous, because the exact handling characteristics of the aircraft were generally unknown. The maiden flight of a new type is almost invariably flown by a highly experienced test pilot. Maiden flights are usually accompanied by a chase plane, to verify items like altitude, airspeed, and general airworthiness.
A maiden flight is only one stage in the development of an aircraft type. Unless the type is a pure research aircraft (such as the X-15), the aircraft must be tested extensively to ensure that it delivers the desired performance with an acceptable margin of safety. In the case of civilian aircraft, a new type must be certified by a governing agency (such as the Federal Aviation Administration in the United States) before it can enter operation.


== Notable maiden flights (aircraft) ==

An incomplete list of maiden flights of notable aircraft types, organized by date, follows.

June, 1875 – Thomas Moy's Aerial Steamer, London, England (pilotless, tethered)
October 9, 1890 – Clément Ader – took off from Gretz-Armainvilliers, Ouest of Paris, France.
August 14, 1901 – Gustave Whitehead from Leutershausen, Bavaria.
May 15, 1902 – Lyman Gilmore – took off from Grass Valley, California.
March 31, 1903 – Richard Pearse – took off from Waitohi Flat, Temuka, South Island, New Zealand.
December 17, 1903 – Wright brothers Wright Flyer – first heavier-than-air powered aircraft. Took off four miles south of Kitty Hawk, North Carolina.
March 18, 1906 – Traian Vuia, a Romanian inventor and engineer, who flew 11 meters in his self-named monoplane at Montesson near Paris, France.
October 23, 1906 – Alberto Santos-Dumont 14-bis made a manned powered flight in Bagatelle Park, Paris, France, that was the first to be publicly witnessed by a crowd.
July 4, 1908 - Glenn Curtiss flew the first pre-announced public flight in the United States of America of a heavier-than-air flying machine. He flew 5,080 feet, to win the Scientific American Trophy and its $2,500 purse (equivalent to $71,000 in 2019).
December 22, 1916 - Sopwith Camel - this iconic biplane first took off from Brooklands, Weybridge, Surrey.
July 28, 1935 – Boeing B-17 Flying Fortress – World War II American heavy bomber.
December 17, 1935 – Douglas DC-3 – propeller-driven passenger and cargo aircraft of which more than 10,000 were produced.
December 29, 1939 – Consolidated B-24 – World War II American heavy bomber.
November 2, 1947 – Hughes H-4 Hercules – only flight of this oversized flying boat whose common name is Spruce Goose.
July 27, 1949 – de Havilland Comet – first jet airliner.
August 23, 1954 – Lockheed C-130 Hercules – military transport plane.
May 27, 1955 – Sud Aviation Caravelle – first jet airliner with engines mounted in the tail.
March 25, 1958 - Avro Canada CF-105 Arrow - Canadian supersonic fighter interceptor. First non-experimental aircraft designed and equipped with a fly-by-wire flight control system.
April 25, 1962 – Lockheed A-12 – supersonic reconnaissance aircraft.
June 29, 1962 – Vickers VC10 – first airliner with 4 engines mounted in the tail.
April 9, 1967 – Boeing 737 – short-to-medium-range airliner.
October 4, 1968 – Tupolev Tu-154 – Soviet/Russian airliner, still in operation.
December 31, 1968 – Tupolev Tu-144 – Soviet supersonic airliner.
February 9, 1969 – Boeing 747 – first widebody airliner.
March 2, 1969 – Anglo-French Concorde – supersonic airliner.
September 19, 1969 – Mil Mi-24 – Russian/Soviet-made helicopter used by many countries to this day.
October 28, 1972 – Airbus A300 – first Airbus aircraft, short- to medium-range wide-body jet airliner.
February 22, 1987 – Airbus A320 airliner – first civilian aircraft to have an all-digital fly-by-wire system.
December 21, 1988 – Antonov An-225 Mriya – jet with the longest fuselage and wingspan and overall heaviest aircraft.
June 12, 1994 – Boeing 777 – long-range airliner with the most powerful jet engines ever made.
April 27, 2005 – Airbus A380 – double-decker jet airliner, currently largest capacity in the world, took off from Toulouse–Blagnac Airport.
December 11, 2009 – Airbus A400M – military cargo plane, Airbus' first propeller plane.
December 15, 2009 – Boeing 787 Dreamliner – first major widebody airliner to use non-metal composite materials for most of its construction.
November 11, 2015 - Mitsubishi Regional Jet - Japanese twin-engine regional jet, the first designed and built in Japan, took off from Mitsubishi Heavy Industries, Tokyo.
May 5, 2017 - Comac C919 - Chinese commercial aircraft.
January 25, 2020 - Boeing 777X - The world's longest and largest twin-engine airliner


== Notable maiden flights (rockets) ==
October 3, 1942 - V-2 Rocket made its first successful test flight. The nose cone crossed the Karman line, widely considered the end of Earth's atmosphere, making it the first human-made object to reach space.
August 3, 1953 - PGM-11 Redstone, designed by Wernher von Braun, was the US's first large ballistic missile. Launched from Cape Canaveral Air Force Station Launch Complex 4, it flew for 80 seconds until an engine failure caused it to crash into the sea.
October 4, 1957 - Sputnik, first orbital rocket.
December 22, 1960 - Vostok-K, first human-rated rocket (first manned flight April 12, 1961).
November 9, 1967 - Saturn V, most powerful rocket launched so far, was used to launch humans to the Moon.
April 12, 1981 - Space Shuttle, first partially reusable launch system, largest payload at the time of its maiden flight.
December 21, 2004 - Delta IV Heavy, largest payload at the time of its maiden flight.
February 6, 2018 - Falcon Heavy, largest payload at the time of its maiden flight, partially reusable.


== See also ==
Flight test
Maiden voyage


== References ==","pandas(index=17, _1=17, text=""the maiden flight, also known as first flight, of an aircraft is the first occasion on which an aircraft leaves the ground under its own power. the same term is also used for the first launch of rockets. the maiden flight of a new aircraft type is always a historic occasion for the type and can be quite emotional for those involved. in the early days of aviation it could be dangerous, because the exact handling characteristics of the aircraft were generally unknown. the maiden flight of a new type is almost invariably flown by a highly experienced test pilot. maiden flights are usually accompanied by a chase plane, to verify items like altitude, airspeed, and general airworthiness. a maiden flight is only one stage in the development of an aircraft type. unless the type is a pure research aircraft (such as the x-15), the aircraft must be tested extensively to ensure that it delivers the desired performance with an acceptable margin of safety. in the case of civilian aircraft, a new type must be certified by a governing agency (such as the federal aviation administration in the united states) before it can enter operation.   == notable maiden flights (aircraft) ==  an incomplete list of maiden flights of notable aircraft types, organized by date, follows.  june, 1875 – thomas moy's aerial steamer, london, england (pilotless, tethered) october 9, 1890 – clément ader – took off from gretz-armainvilliers, ouest of paris, france. august 14, 1901 – gustave whitehead from leutershausen, bavaria. may 15, 1902 – lyman gilmore – took off from grass valley, california. march 31, 1903 – richard pearse – took off from waitohi flat, temuka, south island, new zealand. december 17, 1903 – wright brothers wright flyer – first heavier-than-air powered aircraft. took off four miles south of kitty hawk, north carolina. march 18, 1906 – traian vuia, a romanian inventor and engineer, who flew 11 meters in his self-named monoplane at montesson near paris, france. october 23, 1906 – alberto santos-dumont 14-bis made a manned powered flight in bagatelle park, paris, france, that was the first to be publicly witnessed by a crowd. july 4, 1908 - glenn curtiss flew the first pre-announced public flight in the united states of america of a heavier-than-air flying machine. he flew 5,080 feet, to win the scientific american trophy and its $2,500 purse (equivalent to $71,000 in 2019). december 22, 1916 - sopwith camel - this iconic biplane first took off from brooklands, weybridge, surrey. july 28, 1935 – boeing b-17 flying fortress – world war ii american heavy bomber. december 17, 1935 – douglas dc-3 – propeller-driven passenger and cargo aircraft of which more than 10,000 were produced. december 29, 1939 – consolidated b-24 – world war ii american heavy bomber. november 2, 1947 – hughes h-4 hercules – only flight of this oversized flying boat whose common name is spruce goose. july 27, 1949 – de havilland comet – first jet airliner. august 23, 1954 – lockheed c-130 hercules – military transport plane. may 27, 1955 – sud aviation caravelle – first jet airliner with engines mounted in the tail. march 25, 1958 - avro canada cf-105 arrow - canadian supersonic fighter interceptor. first non-experimental aircraft designed and equipped with a fly-by-wire flight control system. april 25, 1962 – lockheed a-12 – supersonic reconnaissance aircraft. june 29, 1962 – vickers vc10 – first airliner with 4 engines mounted in the tail. april 9, 1967 – boeing 737 – short-to-medium-range airliner. october 4, 1968 – tupolev tu-154 – soviet/russian airliner, still in operation. december 31, 1968 – tupolev tu-144 – soviet supersonic airliner. february 9, 1969 – boeing 747 – first widebody airliner. march 2, 1969 – anglo-french concorde – supersonic airliner. september 19, 1969 – mil mi-24 – russian/soviet-made helicopter used by many countries to this day. october 28, 1972 – airbus a300 – first airbus aircraft, short- to medium-range wide-body jet airliner. february 22, 1987 – airbus a320 airliner – first civilian aircraft to have an all-digital fly-by-wire system. december 21, 1988 – antonov an-225 mriya – jet with the longest fuselage and wingspan and overall heaviest aircraft. june 12, 1994 – boeing 777 – long-range airliner with the most powerful jet engines ever made. april 27, 2005 – airbus a380 – double-decker jet airliner, currently largest capacity in the world, took off from toulouse–blagnac airport. december 11, 2009 – airbus a400m – military cargo plane, airbus' first propeller plane. december 15, 2009 – boeing 787 dreamliner – first major widebody airliner to use non-metal composite materials for most of its construction. november 11, 2015 - mitsubishi regional jet - japanese twin-engine regional jet, the first designed and built in japan, took off from mitsubishi heavy industries, tokyo. may 5, 2017 - comac c919 - chinese commercial aircraft. january 25, 2020 - boeing 777x - the world's longest and largest twin-engine airliner   == notable maiden flights (rockets) == october 3, 1942 - v-2 rocket made its first successful test flight. the nose cone crossed the karman line, widely considered the end of earth's atmosphere, making it the first human-made object to reach space. august 3, 1953 - pgm-11 redstone, designed by wernher von braun, was the us's first large ballistic missile. launched from cape canaveral air force station launch complex 4, it flew for 80 seconds until an engine failure caused it to crash into the sea. october 4, 1957 - sputnik, first orbital rocket. december 22, 1960 - vostok-k, first human-rated rocket (first manned flight april 12, 1961). november 9, 1967 - saturn v, most powerful rocket launched so far, was used to launch humans to the moon. april 12, 1981 - space shuttle, first partially reusable launch system, largest payload at the time of its maiden flight. december 21, 2004 - delta iv heavy, largest payload at the time of its maiden flight. february 6, 2018 - falcon heavy, largest payload at the time of its maiden flight, partially reusable.   == see also == flight test maiden voyage   == references =="")"
18,"A navigation light, also known as a running or position light, is a source of illumination on a vessel, aircraft or spacecraft.  Navigation lights give information on a craft's position, heading, and status.  Their placement is mandated by international conventions or civil authorities.  Navigation lights are not intended to provide illumination for the craft making the passage, only for other craft to be aware of it.


== Marine navigation lights ==
In 1838 the United States passed an act requiring steamboats running between sunset and sunrise to carry one or more signal lights; colour, visibility and location were not specified.
In 1846 the United Kingdom passed legislation enabling the Lord High Admiral  to publish regulations requiring all sea-going steam vessels to carry lights.  The admiralty exercised these powers in 1848 and required steam vessels to display red and green sidelights as well as a white masthead light whilst under way and a single white light when at anchor.
In 1849 the U.S. Congress extended the light requirements to sailing vessels.
In 1889 the United States convened the first International Maritime Conference to consider regulations for preventing collisions. The resulting Washington Conference Rules were adopted by the U.S. in 1890 and became effective internationally in 1897. Within these rules was the requirement for steamships to carry a second mast head light.
The international 1948 Safety of Life at Sea Conference recommended a mandatory second masthead light solely for power driven vessels over 150 feet in length and a fixed sternlight for almost all vessels. The regulations have changed little since then.The International Regulations for Preventing Collisions at Sea established in 1972 stipulates the requirements for the navigation lights required on a vessel.


=== Basic lighting ===

To avoid collisions, vessels mount navigation lights that permit other vessels to determine the type and relative angle of a vessel, and thus decide if there is a danger of collision. In general sailing vessels are required to carry a green light that shines from dead ahead to 2 points (​22 1⁄2°) abaft the beam on the starboard side (the right side from the perspective of someone on board facing forward), a red light from dead ahead to two points abaft the beam on the port side (left side) and a white light that shines from astern to two points abaft the beam on both sides. Power driven vessels in addition  to these lights, must carry either one or two (depending on length) white masthead lights that shine from ahead to two points abaft the beam on both sides. If two masthead lights are carried then the aft one must be higher than the forward one.
Small power driven vessels (under 12 metres (39 ft)) may carry a single all-round white light in place of the two or three white lights carried by larger vessels, they must also carry red and green navigation lights.  Vessels under 7 metres (23 ft) with a maximum speed of less than 7 knots are not required to carry navigation lights, but must be capable of showing a white light. Hovercraft at all times and some boats operating in crowded areas may also carry a yellow flashing beacon for added visibility during day or night.


=== Lights of special significance ===
In addition to red, white and green running lights, a combination of red, white and green Mast Lights placed on a mast higher than all the running lights, and viewable from all directions, may be used to indicate the type of craft or the service it is performing. See ""User Guide"" in external links.

Ships at anchor display one or two white anchor lights (depending on the vessel's length) that can be seen from all directions. If two lights are shown then the forward light is higher than the aft one.
Boats classed as ""small"" are not compelled to carry navigation lights and may make use of a handheld torch.


== Aviation navigation lights ==

Aircraft external lights are any light fitted to the exterior of an aircraft. They are usually used to increase visibility to others, and to signal actions such as entering an active runway or starting up an engine. Historically, incandescent bulbs have been used to provide light, however recently Light-emitting diodes have been used.

Aircraft navigation lights follow the convention of marine vessels established a half-century earlier, with a red navigation light located on the left wingtip leading edge and a green light on the right wingtip leading edge. A white navigation light is as far aft as possible on the tail or each wing tip. High-intensity strobe lights are located on the aircraft to aid in collision avoidance. Anti-collision lights are flashing lights on the top and bottom of the fuselage, wingtips and tail tip. Their purpose is to alert others when something is happening that ground crew and other aircraft need to be aware of, such as running engines or entering active runways.
In civil aviation, pilots must keep navigation lights on from sunset to sunrise. High-intensity white strobe lights are part of the anti-collision light system, as well as the red rotating beacon.
All aircraft built after 11 March 1996 must have an anti-collision light system (strobe lights or rotating beacon) turned on for all flight activities in poor visibility. The anti-collision system is recommended in good visibility, where only strobes and beacon are required. For example, just before pushback, the pilot must keep the beacon lights on to notify ground crews that the engines are about to start. These beacon lights stay on for the duration of the flight. While taxiing, the taxi lights are on. When coming onto the runway, the taxi lights go off and the landing lights and strobes go on. When passing 10,000 feet, the landing lights are no longer required, and the pilot can elect to turn them off. The same cycle in reverse order applies when landing. Landing lights are bright white, forward and downward facing lights on the front of an aircraft. Their purpose is to allow the pilot to see the landing area, and to allow ground crew to see the approaching aircraft.
Civilian commercial airliners also have other non-navigational lights. These include logo lights, which illuminate the company logo on the tail fin. These lights are optional to turn on, though most pilots switch them on at night to increase visibility from other aircraft. Modern airliners also have a wing light. These are positioned on the outer side just in front of the engine cowlings on the fuselage. These are not required to be on, but in some cases pilots turn these lights on for engine checks and also while passengers board the aircraft for better visibility of the ground near the aircraft.  While seldom seen, the International Code of Signals allows for the exclusive use of a flashing blue lights (60 to 100 flashes/minute) and visible from as many directions as possible, by medical aircraft to signal their identity.


== Spacecraft navigation lights ==

In 2011, ORBITEC developed the first Light-emitting diode (LED) lighting system for use around spacecraft.  Currently, Cygnus spacecraft, which are unmanned transport vessels designed for cargo transport to the International Space Station, utilize a navigational lighting system consisting of five flashing high power LED lights.  The Cygnus displays a flashing red light on the port side of the vessel, a flashing green on the starboard side of the vessel, two flashing white lights on the top and one flashing yellow on the bottom side of the fuselage.The SpaceX Dragon and Dragon 2 spacecraft also feature a flashing strobe along with red and green lights.


== See also ==
Formation light
Landing lights


== Notes ==


== References ==


== External links ==
Navigation Lights User Guide","pandas(index=18, _1=18, text='a navigation light, also known as a running or position light, is a source of illumination on a vessel, aircraft or spacecraft.  navigation lights give information on a craft\'s position, heading, and status.  their placement is mandated by international conventions or civil authorities.  navigation lights are not intended to provide illumination for the craft making the passage, only for other craft to be aware of it.   == marine navigation lights == in 1838 the united states passed an act requiring steamboats running between sunset and sunrise to carry one or more signal lights; colour, visibility and location were not specified. in 1846 the united kingdom passed legislation enabling the lord high admiral  to publish regulations requiring all sea-going steam vessels to carry lights.  the admiralty exercised these powers in 1848 and required steam vessels to display red and green sidelights as well as a white masthead light whilst under way and a single white light when at anchor. in 1849 the u.s. congress extended the light requirements to sailing vessels. in 1889 the united states convened the first international maritime conference to consider regulations for preventing collisions. the resulting washington conference rules were adopted by the u.s. in 1890 and became effective internationally in 1897. within these rules was the requirement for steamships to carry a second mast head light. the international 1948 safety of life at sea conference recommended a mandatory second masthead light solely for power driven vessels over 150 feet in length and a fixed sternlight for almost all vessels. the regulations have changed little since then.the international regulations for preventing collisions at sea established in 1972 stipulates the requirements for the navigation lights required on a vessel. in addition to red, white and green running lights, a combination of red, white and green mast lights placed on a mast higher than all the running lights, and viewable from all directions, may be used to indicate the type of craft or the service it is performing. see ""user guide"" in external links.  ships at anchor display one or two white anchor lights (depending on the vessel\'s length) that can be seen from all directions. if two lights are shown then the forward light is higher than the aft one. boats classed as ""small"" are not compelled to carry navigation lights and may make use of a handheld torch.   == aviation navigation lights ==  aircraft external lights are any light fitted to the exterior of an aircraft. they are usually used to increase visibility to others, and to signal actions such as entering an active runway or starting up an engine. historically, incandescent bulbs have been used to provide light, however recently light-emitting diodes have been used.  aircraft navigation lights follow the convention of marine vessels established a half-century earlier, with a red navigation light located on the left wingtip leading edge and a green light on the right wingtip leading edge. a white navigation light is as far aft as possible on the tail or each wing tip. high-intensity strobe lights are located on the aircraft to aid in collision avoidance. anti-collision lights are flashing lights on the top and bottom of the fuselage, wingtips and tail tip. their purpose is to alert others when something is happening that ground crew and other aircraft need to be aware of, such as running engines or entering active runways. in civil aviation, pilots must keep navigation lights on from sunset to sunrise. high-intensity white strobe lights are part of the anti-collision light system, as well as the red rotating beacon. all aircraft built after 11 march 1996 must have an anti-collision light system (strobe lights or rotating beacon) turned on for all flight activities in poor visibility. the anti-collision system is recommended in good visibility, where only strobes and beacon are required. for example, just before pushback, the pilot must keep the beacon lights on to notify ground crews that the engines are about to start. these beacon lights stay on for the duration of the flight. while taxiing, the taxi lights are on. when coming onto the runway, the taxi lights go off and the landing lights and strobes go on. when passing 10,000 feet, the landing lights are no longer required, and the pilot can elect to turn them off. the same cycle in reverse order applies when landing. landing lights are bright white, forward and downward facing lights on the front of an aircraft. their purpose is to allow the pilot to see the landing area, and to allow ground crew to see the approaching aircraft. civilian commercial airliners also have other non-navigational lights. these include logo lights, which illuminate the company logo on the tail fin. these lights are optional to turn on, though most pilots switch them on at night to increase visibility from other aircraft. modern airliners also have a wing light. these are positioned on the outer side just in front of the engine cowlings on the fuselage. these are not required to be on, but in some cases pilots turn these lights on for engine checks and also while passengers board the aircraft for better visibility of the ground near the aircraft.  while seldom seen, the international code of signals allows for the exclusive use of a flashing blue lights (60 to 100 flashes/minute) and visible from as many directions as possible, by medical aircraft to signal their identity.   == spacecraft navigation lights ==  in 2011, orbitec developed the first light-emitting diode (led) lighting system for use around spacecraft.  currently, cygnus spacecraft, which are unmanned transport vessels designed for cargo transport to the international space station, utilize a navigational lighting system consisting of five flashing high power led lights.  the cygnus displays a flashing red light on the port side of the vessel, a flashing green on the starboard side of the vessel, two flashing white lights on the top and one flashing yellow on the bottom side of the fuselage.the spacex dragon and dragon 2 spacecraft also feature a flashing strobe along with red and green lights.   == see also == formation light landing lights   == notes ==   == references ==   == external links == navigation lights user guide')"
19,"Bleed air is compressed air taken from the compressor stage of a gas turbine upstream of its fuel-burning sections.  Automatic air supply and cabin pressure controller (ASCPCs) valves bleed air from high or low stage engine compressor sections. Low stage air is used during high power setting operation, and high during descent and other low power setting operations. Bleed air from that system can be utilized for internal cooling of the engine, cross-starting another engine, engine and airframe anti-icing, cabin pressurization, pneumatic actuators, air-driven motors, pressurizing the hydraulic reservoir, and waste and water storage tanks. Some engine maintenance manuals refer to such systems as ""customer bleed air"". Bleed air is valuable in an aircraft for two properties: high temperature and high pressure (typical values are 200–250 °C and 275 kPa (40 PSI), for regulated bleed air exiting the engine pylon for use throughout the aircraft).


== Uses ==

In civil aircraft, bleed air's primary use is to provide pressure for the aircraft cabin by supplying air to the environmental control system. Additionally, bleed air is used to keep critical parts of the plane (such as the wing leading edges) ice-free.Bleed air is used on many aircraft systems because it is easily available, reliable, and a potent source of power. For example, bleed air from an airplane engine is used to start the remaining engines. Lavatory water storage tanks are pressurized by bleed air that is fed through a pressure regulator.When used for cabin pressurization, the bleed air from the engine must first be cooled (as it exits the compressor stage at temperatures as high as 250 °C) by passing it through an air-to-air heat exchanger cooled by cold outside air. It is then fed to an air cycle machine unit that regulates the temperature and flow of air into the cabin, keeping the environment comfortable.Bleed air is also used to heat the engine intakes. This prevents ice from forming, accumulating, breaking loose, and being ingested by the engine, possibly damaging it.On aircraft powered by jet engines, a similar system is used for wing anti-icing by the 'hot-wing' method. In icing conditions, water droplets condensing on a wing's leading edge can freeze. If that happens, the ice build-up adds weight and changes the shape of the wing, causing a degradation in performance, and possibly a critical loss of control or lift. To prevent this, hot bleed air is pumped through the inside of the wing's leading edge, heating it to a temperature above freezing, which prevents the formation of ice. The air then exits through small holes in the wing edge.
On propeller-driven aircraft, it is common to use bleed air to inflate a rubber boot on the leading edge, breaking the ice loose after it has already formed.Bleed air from the high-pressure compressor of the engine is used to supply reaction control valves as used for part of the flight control system in the Harrier jump jet family of military aircraft.


== Contamination ==

On about 1 in 5,000 flights, bleed air used for air conditioning and pressurization can be contaminated by chemicals such as oil or hydraulic fluid. This is known as a fume event. While those chemicals can be irritating, such events have not been established to cause long-term harm.Certain neurological and respiratory ill health effects have been linked anecdotally to exposure to bleed air that has been alleged to have been contaminated with toxic levels on commercial and military aircraft. This alleged long-term illness is referred to as aerotoxic syndrome, but it is not a medically recognized syndrome. One potential contaminant is tricresyl phosphate.Many lobbying groups have been set up to advocate for research into this hazard, including the Aviation Organophosphate Information Site (AOPIS) (2001), the Global Cabin Air Quality Executive (2006) and the UK-based Aerotoxic Association (2007). Cabin Environment Research is one of many functions of the ACER Group, but their researchers have not yet established any causal relationship.Although a study made for the EU in 2014 confirmed that contamination of cabin air could be a problem, that study also stated:

""A lot of reported fume events caused comfort limitations for the occupants but posed no danger. A verification of cabin air contamination with toxic substances (e.g. TCP/TOCP) was not possible with the fume events the BFU investigated.""While no scientific evidence to date has found that airliner cabin air has been contaminated to toxic levels (exceeding known safe levels, in ppm, of any dangerous chemical), a court in Australia in March 2010 found in favor of a former airline flight attendant who claimed she suffered chronic respiratory problems after being exposed to oil fumes on a trip in March 1992. Such testing is infrequent due to Boeing's refusal to install air quality sensors in its planes, fearing lawsuits from crew or passengers over fume events, and airlines refused to allow flight attendants to carry air samplers after Congress mandated chemical measurements.The FAA has revoked the medical certificates of several pilots who developed neurological issues after fume events. A judge who awarded workers' compensation to a pilot who had suffered toxic encephalopathy (brain damage) from a fume event condemned the airline industry's obstructionism around fume events.In July 2015, pilots on a Spirit Airlines flight were partially incapacitated by fumes in bleed air.


== Bleedless aircraft ==
Bleed air systems have been in use for several decades in passenger jets. Recent improvements in solid-state electronics have enabled pneumatic power systems to be replaced by electric power systems. In a bleedless aircraft such as the Boeing 787, each engine has two variable-frequency electrical generators to compensate for not providing compressed air to external systems. Eliminating bleed air and replacing it with extra electric generation is believed to provide a net improvement in engine efficiency, lower weight, and ease of maintenance.


=== Benefits ===
A bleedless aircraft achieves fuel efficiency by eliminating the process of compressing and decompressing air, and by reducing the aircraft's mass due to the removal of ducts, valves, heat exchangers, and other heavy equipment.The APU (auxiliary power unit) does not need to supply bleed air when the main engines are not operating. Aerodynamics are improved due to the lack of bleed air vent holes on the wings. By driving cabin air supply compressors at the minimum required speed, no energy wasting modulating valves are required. High-temperature, high-pressure air cycle machine (ACM) packs can be replaced with low temperature, low-pressure packs to increase efficiency. At cruise altitude, where most aircraft spend the majority of their time and burn the majority of their fuel, the ACM packs can be bypassed entirely, saving even more energy. Since no bleed air is taken from the engines for the cabin, the potential of engine oil contamination of the cabin air supply is eliminated.Lastly, advocates of the design say it improves safety as heated air is confined to the engine pod, as opposed to being pumped through pipes and heat exchangers in the wing and near the cabin, where a leak could damage surrounding systems.


== See also ==
Aerotoxic syndrome
Cabin pressurization
Environmental control system (aircraft)
Ice protection system


== References ==","pandas(index=19, _1=19, text='bleed air is compressed air taken from the compressor stage of a gas turbine upstream of its fuel-burning sections.  automatic air supply and cabin pressure controller (ascpcs) valves bleed air from high or low stage engine compressor sections. low stage air is used during high power setting operation, and high during descent and other low power setting operations. bleed air from that system can be utilized for internal cooling of the engine, cross-starting another engine, engine and airframe anti-icing, cabin pressurization, pneumatic actuators, air-driven motors, pressurizing the hydraulic reservoir, and waste and water storage tanks. some engine maintenance manuals refer to such systems as ""customer bleed air"". bleed air is valuable in an aircraft for two properties: high temperature and high pressure (typical values are 200–250 °c and 275 kpa (40 psi), for regulated bleed air exiting the engine pylon for use throughout the aircraft).   == uses ==  in civil aircraft, bleed air\'s primary use is to provide pressure for the aircraft cabin by supplying air to the environmental control system. additionally, bleed air is used to keep critical parts of the plane (such as the wing leading edges) ice-free.bleed air is used on many aircraft systems because it is easily available, reliable, and a potent source of power. for example, bleed air from an airplane engine is used to start the remaining engines. lavatory water storage tanks are pressurized by bleed air that is fed through a pressure regulator.when used for cabin pressurization, the bleed air from the engine must first be cooled (as it exits the compressor stage at temperatures as high as 250 °c) by passing it through an air-to-air heat exchanger cooled by cold outside air. it is then fed to an air cycle machine unit that regulates the temperature and flow of air into the cabin, keeping the environment comfortable.bleed air is also used to heat the engine intakes. this prevents ice from forming, accumulating, breaking loose, and being ingested by the engine, possibly damaging it.on aircraft powered by jet engines, a similar system is used for wing anti-icing by the \'hot-wing\' method. in icing conditions, water droplets condensing on a wing\'s leading edge can freeze. if that happens, the ice build-up adds weight and changes the shape of the wing, causing a degradation in performance, and possibly a critical loss of control or lift. to prevent this, hot bleed air is pumped through the inside of the wing\'s leading edge, heating it to a temperature above freezing, which prevents the formation of ice. the air then exits through small holes in the wing edge. on propeller-driven aircraft, it is common to use bleed air to inflate a rubber boot on the leading edge, breaking the ice loose after it has already formed.bleed air from the high-pressure compressor of the engine is used to supply reaction control valves as used for part of the flight control system in the harrier jump jet family of military aircraft.   == contamination ==  on about 1 in 5,000 flights, bleed air used for air conditioning and pressurization can be contaminated by chemicals such as oil or hydraulic fluid. this is known as a fume event. while those chemicals can be irritating, such events have not been established to cause long-term harm.certain neurological and respiratory ill health effects have been linked anecdotally to exposure to bleed air that has been alleged to have been contaminated with toxic levels on commercial and military aircraft. this alleged long-term illness is referred to as aerotoxic syndrome, but it is not a medically recognized syndrome. one potential contaminant is tricresyl phosphate.many lobbying groups have been set up to advocate for research into this hazard, including the aviation organophosphate information site (aopis) (2001), the global cabin air quality executive (2006) and the uk-based aerotoxic association (2007). cabin environment research is one of many functions of the acer group, but their researchers have not yet established any causal relationship.although a study made for the eu in 2014 confirmed that contamination of cabin air could be a problem, that study also stated:  ""a lot of reported fume events caused comfort limitations for the occupants but posed no danger. a verification of cabin air contamination with toxic substances (e.g. tcp/tocp) was not possible with the fume events the bfu investigated.""while no scientific evidence to date has found that airliner cabin air has been contaminated to toxic levels (exceeding known safe levels, in ppm, of any dangerous chemical), a court in australia in march 2010 found in favor of a former airline flight attendant who claimed she suffered chronic respiratory problems after being exposed to oil fumes on a trip in march 1992. such testing is infrequent due to boeing\'s refusal to install air quality sensors in its planes, fearing lawsuits from crew or passengers over fume events, and airlines refused to allow flight attendants to carry air samplers after congress mandated chemical measurements.the faa has revoked the medical certificates of several pilots who developed neurological issues after fume events. a judge who awarded workers\' compensation to a pilot who had suffered toxic encephalopathy (brain damage) from a fume event condemned the airline industry\'s obstructionism around fume events.in july 2015, pilots on a spirit airlines flight were partially incapacitated by fumes in bleed air.   == bleedless aircraft == bleed air systems have been in use for several decades in passenger jets. recent improvements in solid-state electronics have enabled pneumatic power systems to be replaced by electric power systems. in a bleedless aircraft such as the boeing 787, each engine has two variable-frequency electrical generators to compensate for not providing compressed air to external systems. eliminating bleed air and replacing it with extra electric generation is believed to provide a net improvement in engine efficiency, lower weight, and ease of maintenance. a bleedless aircraft achieves fuel efficiency by eliminating the process of compressing and decompressing air, and by reducing the aircraft\'s mass due to the removal of ducts, valves, heat exchangers, and other heavy equipment.the apu (auxiliary power unit) does not need to supply bleed air when the main engines are not operating. aerodynamics are improved due to the lack of bleed air vent holes on the wings. by driving cabin air supply compressors at the minimum required speed, no energy wasting modulating valves are required. high-temperature, high-pressure air cycle machine (acm) packs can be replaced with low temperature, low-pressure packs to increase efficiency. at cruise altitude, where most aircraft spend the majority of their time and burn the majority of their fuel, the acm packs can be bypassed entirely, saving even more energy. since no bleed air is taken from the engines for the cabin, the potential of engine oil contamination of the cabin air supply is eliminated.lastly, advocates of the design say it improves safety as heated air is confined to the engine pod, as opposed to being pumped through pipes and heat exchangers in the wing and near the cabin, where a leak could damage surrounding systems.   == see also == aerotoxic syndrome cabin pressurization environmental control system (aircraft) ice protection system   == references ==')"
20,"The pressure coefficient is a dimensionless number which describes the relative pressures throughout a flow field in fluid dynamics.  The pressure coefficient is used in aerodynamics and hydrodynamics.  Every point in a fluid flow field has its own unique pressure coefficient, 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
  .
In many situations in aerodynamics and hydrodynamics, the pressure coefficient at a point near a body is independent of body size.  Consequently, an engineering model can be tested in a wind tunnel or water tunnel, pressure coefficients can be determined at critical locations around the model, and these pressure coefficients can be used with confidence to predict the fluid pressure at those critical locations around a full-size aircraft or boat.


== Definition ==
The pressure coefficient is a parameter for studying both incompressible/compressible fluids such as water and air.  The relationship between the dimensionless coefficient and the dimensional numbers is

  
    
      
        
          C
          
            p
          
        
        =
        
          
            
              p
              −
              
                p
                
                  ∞
                
              
            
            
              
                
                  1
                  2
                
              
              
                ρ
                
                  ∞
                
              
              
                V
                
                  ∞
                
                
                  2
                
              
            
          
        
        =
        
          
            
              p
              −
              
                p
                
                  ∞
                
              
            
            
              
                p
                
                  0
                
              
              −
              
                p
                
                  ∞
                
              
            
          
        
      
    
    {\displaystyle C_{p}={p-p_{\infty } \over {\frac {1}{2}}\rho _{\infty }V_{\infty }^{2}}={p-p_{\infty } \over p_{0}-p_{\infty }}}
  where:

  
    
      
        p
      
    
    {\displaystyle p}
   is the static pressure at the point at which pressure coefficient is being evaluated

  
    
      
        
          p
          
            ∞
          
        
      
    
    {\displaystyle p_{\infty }}
   is the static pressure in the freestream (i.e. remote from any disturbance)

  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
   is the stagnation pressure in the freestream (i.e. remote from any disturbance)

  
    
      
        
          ρ
          
            ∞
          
        
      
    
    {\displaystyle \rho _{\infty }}
   is the freestream fluid density (Air at sea level and 15 °C is 1.225 
  
    
      
        
          
            k
            g
            
              /
            
            
              m
              
                3
              
            
          
        
      
    
    {\displaystyle {\rm {kg/m^{3}}}}
  )

  
    
      
        
          V
          
            ∞
          
        
      
    
    {\displaystyle V_{\infty }}
   is the freestream velocity of the fluid, or the velocity of the body through the fluid


== Incompressible flow ==

Using Bernoulli's Equation, the pressure coefficient can be further simplified for potential flows (inviscid, and steady):

  
    
      
        
          C
          
            p
          
        
        0
        =
        
          C
          
            p
          
        
        
          
            |
          
          
            M
            a
            
            ≈
            
            0
          
        
        =
        
          1
          −
          
            
              (
            
          
          
            
              u
              
                u
                
                  ∞
                
              
            
          
          
            
              
                )
              
            
            
              2
            
          
        
      
    
    {\displaystyle C_{p}0=C_{p}|_{Ma\,\approx \,0}={1-{\bigg (}{\frac {u}{u_{\infty }}}{\bigg )}^{2}}}
  where u is the flow speed at the point at which pressure coefficient is being evaluated, and Ma is the Mach number: the flow speed is negligible in comparison with the speed of sound. For a case of an incompressible but viscous fluid, this represents the profile pressure coefficient, since it is associated with the pressure hydrodynamic forces rather than the viscous ones.
This relationship is valid for the flow of incompressible fluids where variations in speed and pressure are sufficiently small that variations in fluid density can be neglected. This is a reasonable assumption when the Mach Number is less than about 0.3.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of zero indicates the pressure is the same as the free stream pressure.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of one corresponds to the stagnation pressure and indicates a stagnation point.
the most negative values of 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   in a liquid flow can be summed to the cavitation number to give the cavitation margin. If this margin is positive, the flow is locally fully liquid, while if it is zero or negative the flow is cavitating or gas.
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   of minus one is significant in the design of gliders because this indicates a perfect location for a ""Total energy"" port for supply of signal pressure to the Variometer, a special Vertical Speed Indicator which reacts to vertical movements of the atmosphere but does not react to vertical maneuvering of the glider.
In the fluid flow field around a body there will be points having positive pressure coefficients up to one, and negative pressure coefficients including coefficients less than minus one, but nowhere will the coefficient exceed plus one because the highest pressure that can be achieved is the stagnation pressure.


== Compressible flow ==

In the flow of compressible fluids such as air, and particularly the high-speed flow of compressible fluids, 
  
    
      
        
          ρ
          
            v
            
              2
            
          
        
        
          /
        
        2
      
    
    {\displaystyle {\rho v^{2}}/2}
   (the dynamic pressure) is no longer an accurate measure of the difference between stagnation pressure and static pressure.  Also, the familiar relationship that stagnation pressure is equal to total pressure does not always hold true.  (It is always true in isentropic flow but the presence of shock waves can cause the flow to depart from isentropic.)  As a result, pressure coefficients can be greater than one in compressible flow.

  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   greater than one indicates the freestream flow is compressible.


=== Perturbation theory ===
The pressure coefficient 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
    can be estimated for irrotational and isentropic flow by introducing the potential  
  
    
      
        Φ
      
    
    {\displaystyle \Phi }
   and the perturbation potential 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
  , normalized by the free-stream velocity 
  
    
      
        
          u
          
            ∞
          
        
      
    
    {\displaystyle u_{\infty }}
  

  
    
      
        Φ
        =
        
          u
          
            ∞
          
        
        x
        +
        ϕ
        (
        x
        ,
        y
        ,
        z
        )
      
    
    {\displaystyle \Phi =u_{\infty }x+\phi (x,y,z)}
  
Using Bernoulli's Equation,

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      Φ
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  
                    
                      ∇
                      Φ
                      ⋅
                      ∇
                      Φ
                    
                    2
                  
                
                +
                
                  
                    γ
                    
                      γ
                      −
                      1
                    
                  
                
                
                  
                    p
                    ρ
                  
                
                =
                
                  constant
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial \Phi }{\partial t}}+{\frac {\nabla \Phi \cdot \nabla \Phi }{2}}+{\frac {\gamma }{\gamma -1}}{\frac {p}{\rho }}={\text{constant}}\end{aligned}}}
  
which can be rewritten as 

  
    
      
        
          
            
              
                
                  
                    
                      ∂
                      Φ
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  
                    
                      ∇
                      Φ
                      ⋅
                      ∇
                      Φ
                    
                    2
                  
                
                +
                
                  
                    
                      a
                      
                        2
                      
                    
                    
                      γ
                      −
                      1
                    
                  
                
                =
                
                  constant
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\partial \Phi }{\partial t}}+{\frac {\nabla \Phi \cdot \nabla \Phi }{2}}+{\frac {a^{2}}{\gamma -1}}={\text{constant}}\end{aligned}}}
  
here 
  
    
      
        a
      
    
    {\displaystyle a}
   is the sound speed.
The pressure coefficient becomes

  
    
      
        
          
            
              
                
                  C
                  
                    p
                  
                
              
              
                
                =
                
                  
                    
                      p
                      −
                      
                        p
                        
                          ∞
                        
                      
                    
                    
                      
                        
                          γ
                          2
                        
                      
                      
                        p
                        
                          ∞
                        
                      
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          
                            a
                            
                              a
                              
                                ∞
                              
                            
                          
                        
                        )
                      
                      
                        
                          
                            2
                            γ
                          
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          
                            
                              
                                γ
                                −
                                1
                              
                              
                                a
                                
                                  ∞
                                
                                
                                  2
                                
                              
                            
                          
                          (
                          
                            
                              
                                u
                                
                                  ∞
                                
                                
                                  2
                                
                              
                              2
                            
                          
                          −
                          
                            Φ
                            
                              t
                            
                          
                          −
                          
                            
                              
                                ∇
                                Φ
                                ⋅
                                ∇
                                Φ
                              
                              2
                            
                          
                          )
                          +
                          1
                        
                        )
                      
                      
                        
                          γ
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                ≈
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          1
                          −
                          
                            
                              
                                γ
                                −
                                1
                              
                              
                                a
                                
                                  ∞
                                
                                
                                  2
                                
                              
                            
                          
                          (
                          
                            ϕ
                            
                              t
                            
                          
                          +
                          
                            u
                            
                              ∞
                            
                          
                          
                            ϕ
                            
                              x
                            
                          
                          )
                        
                        )
                      
                      
                        
                          γ
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
            
              
              
                
                ≈
                −
                
                  
                    
                      2
                      
                        ϕ
                        
                          t
                        
                      
                    
                    
                      u
                      
                        ∞
                      
                      
                        2
                      
                    
                  
                
                −
                
                  
                    
                      2
                      
                        ϕ
                        
                          x
                        
                      
                    
                    
                      u
                      
                        ∞
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}C_{p}&={\frac {p-p_{\infty }}{{\frac {\gamma }{2}}p_{\infty }M^{2}}}={\frac {2}{\gamma M^{2}}}\left[\left({\frac {a}{a_{\infty }}}\right)^{\frac {2\gamma }{\gamma -1}}-1\right]\\&={\frac {2}{\gamma M^{2}}}\left[\left({\frac {\gamma -1}{a_{\infty }^{2}}}({\frac {u_{\infty }^{2}}{2}}-\Phi _{t}-{\frac {\nabla \Phi \cdot \nabla \Phi }{2}})+1\right)^{\frac {\gamma }{\gamma -1}}-1\right]\\&\approx {\frac {2}{\gamma M^{2}}}\left[\left(1-{\frac {\gamma -1}{a_{\infty }^{2}}}(\phi _{t}+u_{\infty }\phi _{x})\right)^{\frac {\gamma }{\gamma -1}}-1\right]\\&\approx -{\frac {2\phi _{t}}{u_{\infty }^{2}}}-{\frac {2\phi _{x}}{u_{\infty }}}\end{aligned}}}
  
here 
  
    
      
        
          a
          
            ∞
          
        
      
    
    {\displaystyle a_{\infty }}
   is the far-field sound speed.


=== Local piston theory ===
The classical piston theory is a powerful aerodynamic tool. From the use of the momentum equation and the assumption of isentropic perturbations, one obtains the following basic piston theory formula for the surface pressure:

  
    
      
        p
        =
        
          p
          
            ∞
          
        
        
          
            (
            
              1
              +
              
                
                  
                    γ
                    −
                    1
                  
                  2
                
              
              
                
                  w
                  a
                
              
            
            )
          
          
            
              
                2
                γ
              
              
                γ
                −
                1
              
            
          
        
      
    
    {\displaystyle p=p_{\infty }\left(1+{\frac {\gamma -1}{2}}{\frac {w}{a}}\right)^{\frac {2\gamma }{\gamma -1}}}
  
here 
  
    
      
        w
      
    
    {\displaystyle w}
   is the downwash speed and 
  
    
      
        a
      
    
    {\displaystyle a}
   is the sound speed.

  
    
      
        
          
            
              
                
                  C
                  
                    p
                  
                
                =
                
                  
                    
                      p
                      −
                      
                        p
                        
                          ∞
                        
                      
                    
                    
                      
                        
                          γ
                          2
                        
                      
                      
                        p
                        
                          ∞
                        
                      
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                =
                
                  
                    2
                    
                      γ
                      
                        M
                        
                          2
                        
                      
                    
                  
                
                
                  [
                  
                    
                      
                        (
                        
                          1
                          +
                          
                            
                              
                                γ
                                −
                                1
                              
                              2
                            
                          
                          
                            
                              w
                              a
                            
                          
                        
                        )
                      
                      
                        
                          
                            2
                            γ
                          
                          
                            γ
                            −
                            1
                          
                        
                      
                    
                    −
                    1
                  
                  ]
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}C_{p}={\frac {p-p_{\infty }}{{\frac {\gamma }{2}}p_{\infty }M^{2}}}={\frac {2}{\gamma M^{2}}}\left[\left(1+{\frac {\gamma -1}{2}}{\frac {w}{a}}\right)^{\frac {2\gamma }{\gamma -1}}-1\right]\end{aligned}}}
  
The surface is defined as 

  
    
      
        
          
            
              
                F
                (
                x
                ,
                y
                ,
                z
                ,
                t
                )
                =
                z
                −
                f
                (
                x
                ,
                y
                ,
                t
                )
                =
                0
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}F(x,y,z,t)=z-f(x,y,t)=0\end{aligned}}}
  
The slip velocity boundary condition leads to 

  
    
      
        
          
            
              
                
                  
                    
                      ∇
                      F
                    
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
                (
                
                  u
                  
                    ∞
                  
                
                +
                
                  ϕ
                  
                    x
                  
                
                ,
                
                  ϕ
                  
                    y
                  
                
                ,
                
                  ϕ
                  
                    z
                  
                
                )
                =
                
                  V
                  
                    wall
                  
                
                ⋅
                
                  
                    
                      ∇
                      F
                    
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
                =
                −
                
                  
                    
                      ∂
                      F
                    
                    
                      ∂
                      t
                    
                  
                
                
                  
                    1
                    
                      
                        |
                      
                      ∇
                      F
                      
                        |
                      
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {\nabla F}{|\nabla F|}}(u_{\infty }+\phi _{x},\phi _{y},\phi _{z})=V_{\text{wall}}\cdot {\frac {\nabla F}{|\nabla F|}}=-{\frac {\partial F}{\partial t}}{\frac {1}{|\nabla F|}}\end{aligned}}}
  
The downwash speed 
  
    
      
        w
      
    
    {\displaystyle w}
   is approximated as 

  
    
      
        
          
            
              
                w
                =
                
                  
                    
                      ∂
                      f
                    
                    
                      ∂
                      t
                    
                  
                
                +
                
                  u
                  
                    ∞
                  
                
                
                  
                    
                      ∂
                      f
                    
                    
                      ∂
                      x
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}w={\frac {\partial f}{\partial t}}+u_{\infty }{\frac {\partial f}{\partial x}}\end{aligned}}}
  


== Pressure distribution ==
An airfoil at a given angle of attack will have what is called a pressure distribution.  This pressure distribution is simply the pressure at all points around an airfoil.  Typically, graphs of these distributions are drawn so that negative numbers are higher on the graph, as the 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   for the upper surface of the airfoil will usually be farther below zero and will hence be the top line on the graph.


== Relationship with aerodynamic coefficients ==
All the three aerodynamic coefficients are integrals of the pressure coefficient curve along the chord.
The coefficient of lift for a two-dimensional airfoil section with strictly horizontal surfaces can be calculated from the coefficient of pressure distribution by integration, or calculating the area between the lines on the distribution. This expression is not suitable for direct numeric integration using the panel method of lift approximation, as it does not take into account the direction of pressure-induced lift. This equation is true only for zero angle of attack.

  
    
      
        
          C
          
            l
          
        
        =
        
          
            1
            
              
                x
                
                  T
                  E
                
              
              −
              
                x
                
                  L
                  E
                
              
            
          
        
        
          ∫
          
            
              x
              
                L
                E
              
            
          
          
            
              x
              
                T
                E
              
            
          
        
        
          (
          
            
              C
              
                
                  p
                  
                    l
                  
                
              
            
            (
            x
            )
            −
            
              C
              
                
                  p
                  
                    u
                  
                
              
            
            (
            x
            )
          
          )
        
        d
        x
      
    
    {\displaystyle C_{l}={\frac {1}{x_{TE}-x_{LE}}}\int \limits _{x_{LE}}^{x_{TE}}\left(C_{p_{l}}(x)-C_{p_{u}}(x)\right)dx}
  where:

  
    
      
        
          C
          
            
              p
              
                l
              
            
          
        
      
    
    {\displaystyle C_{p_{l}}}
   is pressure coefficient on the lower surface

  
    
      
        
          C
          
            
              p
              
                u
              
            
          
        
      
    
    {\displaystyle C_{p_{u}}}
   is pressure coefficient on the upper surface

  
    
      
        
          x
          
            L
            E
          
        
      
    
    {\displaystyle x_{LE}}
   is the leading edge location

  
    
      
        
          x
          
            T
            E
          
        
      
    
    {\displaystyle x_{TE}}
   is the trailing edge locationWhen the lower surface 
  
    
      
        
          C
          
            p
          
        
      
    
    {\displaystyle C_{p}}
   is higher (more negative) on the distribution it counts as a negative area as this will be producing down force rather than lift.


== See also ==
Lift coefficient
Drag coefficient
Pitching moment coefficient


== References ==

Abbott, I.H. and Von Doenhoff, A.E. (1959) Theory of Wing Sections, Dover Publications, Inc. New York, Standard Book No. 486-60586-8
Anderson, John D (2001) Fundamentals of Aerodynamic 3rd Edition, McGraw-Hill. ISBN 0-07-237335-0","pandas(index=20, _1=20, text='the pressure coefficient is a dimensionless number which describes the relative pressures throughout a flow field in fluid dynamics.  the pressure coefficient is used in aerodynamics and hydrodynamics.  every point in a fluid flow field has its own unique pressure coefficient,     c  p      is higher (more negative) on the distribution it counts as a negative area as this will be producing down force rather than lift.   == see also == lift coefficient drag coefficient pitching moment coefficient   == references ==  abbott, i.h. and von doenhoff, a.e. (1959) theory of wing sections, dover publications, inc. new york, standard book no. 486-60586-8 anderson, john d (2001) fundamentals of aerodynamic 3rd edition, mcgraw-hill. isbn 0-07-237335-0')"
21,"Energy–maneuverability theory is a model of aircraft performance. It was developed by Col. John Boyd, a fighter pilot, and Thomas P. Christie, a mathematician with the Air Force, and is useful in describing an aircraft's performance as the total of kinetic and potential energies or aircraft specific energy.  It relates the thrust, weight, aerodynamic drag, wing area, and other flight characteristics of an aircraft into a quantitative model. This allows combat capabilities of various aircraft or prospective design trade-offs to be predicted and compared.
All of these aspects of airplane performance are compressed into a single value by the following formula:

  
    
      
        
          
            
              
                
                  P
                  
                    S
                  
                
              
              
                =
              
              
                V
                
                  (
                  
                    
                      
                        T
                        −
                        D
                      
                      W
                    
                  
                  )
                
              
            
            
              
            
            
              
                V
              
              
                =
              
              
                
                  Speed
                
              
            
            
              
                T
              
              
                =
              
              
                
                  Thrust
                
              
            
            
              
                D
              
              
                =
              
              
                
                  Drag
                
              
            
            
              
                W
              
              
                =
              
              
                
                  Weight
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{rcl}P_{S}&=&V\left({\frac {T-D}{W}}\right)\\\\V&=&{\text{Speed}}\\T&=&{\text{Thrust}}\\D&=&{\text{Drag}}\\W&=&{\text{Weight}}\end{array}}}
  In words, the specific excess energy is proportional to the ratio of net motive forces compared to the weight of the plane and proportional to speed. (Note that dimensionally, 
  
    
      
        
          P
          
            S
          
        
      
    
    {\displaystyle P_{S}}
   has units of ""speed,"" not ""specific energy"" (energy per unit mass).)
The net motive force is found by calculating the engine's ability to move the plane after accounting for friction and other aerodynamic issues that slow down the plane.  The ratio (T-D)/W is similar to T/W, the Thrust-to-weight ratio, which is also used as a figure of merit for airplanes and rockets.  By normalizing the motive forces to the weight of the plane, it is clear how efficient the plane is.  A very large engine may be able to generate a huge thrust but could be so heavy that it would not even lift itself.  The ratio is unity (T-D)/W = 1 when the engine is powerful enough to keep the plane at constant speed in a 90 degree ascending trajectory.  Fighter jets, such as the F-16 have a T/W ratio close to 1, depending on fuel weight and armament.
The difference between T/W and (T-D)/W is that T/W does not include the effects of friction and other aerodynamic losses.  When a plane is moving very slowly, these losses are small and can be ignored.  However, T/W does not accurately describe the performance of the plane at its normal operating conditions.  By including drag in the formula, the aerodynamics of the plane are also summarized in the 
  
    
      
        
          P
          
            S
          
        
      
    
    {\displaystyle P_{S}}
   value.
The specific excess energy model is proportional to the speed of the plane.  This means that the faster the plane is capable of flying, the better its score.  The other parts of this model (thrust, drag, and weight) may say that a plane is excellent but a good fighter must also go fast.
Boyd, a U.S. jet fighter pilot in the Korean War, began developing the theory in the early 1960s.  He teamed with mathematician Thomas Christie at Eglin Air Force Base to use the base's high-speed computer to compare the performance envelopes of U.S. and Soviet aircraft from the Korean and Vietnam Wars.  They completed a two-volume report on their studies in 1964.  Energy Maneuverability came to be accepted within the U.S. Air Force and brought about improvements in the requirements for the F-15 Eagle and later the F-16 Fighting Falcon fighters.


== Notes ==


== See also ==
Lagrangian mechanics


== References ==","pandas(index=21, _1=21, text='energy–maneuverability theory is a model of aircraft performance. it was developed by col. john boyd, a fighter pilot, and thomas p. christie, a mathematician with the air force, and is useful in describing an aircraft\'s performance as the total of kinetic and potential energies or aircraft specific energy.  it relates the thrust, weight, aerodynamic drag, wing area, and other flight characteristics of an aircraft into a quantitative model. this allows combat capabilities of various aircraft or prospective design trade-offs to be predicted and compared. all of these aspects of airplane performance are compressed into a single value by the following formula:          p  s     =   v  (    t − d  w   )         v   =    speed      t   =    thrust      d   =    drag      w   =    weight         value. the specific excess energy model is proportional to the speed of the plane.  this means that the faster the plane is capable of flying, the better its score.  the other parts of this model (thrust, drag, and weight) may say that a plane is excellent but a good fighter must also go fast. boyd, a u.s. jet fighter pilot in the korean war, began developing the theory in the early 1960s.  he teamed with mathematician thomas christie at eglin air force base to use the base\'s high-speed computer to compare the performance envelopes of u.s. and soviet aircraft from the korean and vietnam wars.  they completed a two-volume report on their studies in 1964.  energy maneuverability came to be accepted within the u.s. air force and brought about improvements in the requirements for the f-15 eagle and later the f-16 fighting falcon fighters.   == notes ==   == see also == lagrangian mechanics   == references ==')"
22,"Damage tolerance is a property of a structure relating to its ability to sustain defects safely until repair can be effected. The approach to engineering design to account for damage tolerance is based on the assumption that flaws can exist in any structure and such flaws propagate with usage. This approach is commonly used in aerospace engineering, mechanical engineering, and civil engineering to manage the extension of cracks in structure through the application of the principles of fracture mechanics.  In engineering, a structure is considered to be damage tolerant if a maintenance program has been implemented that will result in the detection and repair of accidental damage, corrosion and fatigue cracking before such damage reduces the residual strength of the structure below an acceptable limit. 


== History ==
Structures upon which human life depends have long been recognized as needing an element of fail-safety.  When describing his flying machine, Leonardo da Vinci noted that ""In constructing wings one should make one chord to bear the strain and a looser one in the same position so that if one breaks under the strain, the other is in the position to serve the same function.""Prior to the 1970s, the prevailing engineering philosophy of aircraft structures was to ensure that airworthiness was maintained with a single part broken, a redundancy requirement known as fail-safety.  However, advances in fracture mechanics, along with infamous catastrophic fatigue failures such as those in the de Havilland Comet prompted a change in requirements for aircraft.  It was discovered that a phenomenon known as  multiple-site damage could cause many small cracks in the structure, which grow slowly by themselves, to join one another over time, creating a much larger crack, and significantly reducing the expected time until failure 


== Safe-life structure ==
Not all structure must demonstrate detectable crack propagation to ensure safety of operation.  Some structures operate under the safe-life design principle, where an extremely low level of risk is accepted through a combination of testing and analysis that the part will never form a detectable crack due to fatigue during the service life of the part.  This is achieved through a significant reduction of stresses below the typical fatigue capability of the part.  Safe-life structures are employed when the cost or infeasibility of inspections outweighs the weight penalty and development costs associated with safe-life structures.  An example of a safe-life component is the helicopter rotor blade.  Due to the extremely large numbers of cycles endured by the rotating component, an undetectable crack may grow to a critical length in a single flight and before the aircraft lands, result in a catastrophic failure that regular maintenance could not have prevented.


== Damage tolerance analysis ==
In ensuring the continued safe operation of the damage tolerant structure, inspection schedules are devised.  This schedule is based on many criteria, including:

assumed initial damaged condition of the structure
stresses in the structure (both fatigue and operational maximum stresses) that cause crack growth from the damaged condition
geometry of the material which intensifies or reduces the stresses on the crack tip
ability of the material to withstand cracking due to stresses in the expected environment
largest crack size that the structure can endure before catastrophic failure
likelihood that a particular inspection method will reveal a crack
acceptable level of risk that a certain structure will be completely failed
expected duration after manufacture until a detectable crack will form
assumption of failure in adjacent components which may have the effect of changing stresses in the structure of interestThese factors affect how long the structure may operate normally in the damaged condition before one or more inspection intervals has the opportunity to discover the damaged state and effect a repair.  The interval between inspections must be selected with a certain minimum safety, and also must balance the expense of the inspections, the weight penalty of lowering fatigue stresses, and the opportunity costs associated with a structure being out of service for maintenance.


== Non-destructive inspections ==
Manufacturers and operators of aircraft, trains, and civil engineering structures like bridges have a financial interest in ensuring that the inspection schedule is as cost-efficient as possible.  In the example of aircraft, because these structures are often revenue producing, there is an opportunity cost associated with the maintenance of the aircraft (lost ticket revenue), in addition to the cost of maintenance itself.  Thus, this maintenance is desired to be performed infrequently, even when such increased intervals cause increased complexity and cost to the overhaul.  Crack growth, as shown by fracture mechanics, is exponential in nature; meaning that the crack growth rate is a function of an exponent of the current crack size (see Paris' law). This means that only the largest cracks influence the overall strength of a structure; small internal damages do not necessarily decrease the strength. A desire for infrequent inspection intervals, combined with the exponential growth of cracks in structure has led to the development of non-destructive testing methods which allow inspectors to look for very tiny cracks which are often invisible to the naked eye. Examples of this technology include eddy current, ultrasonic, dye penetrant, and X-ray inspections.  By catching structural cracks when they are very small, and growing slowly, these non-destructive inspections can reduce the amount of maintenance checks, and allow damage to be caught when it is small, and still inexpensive to repair. As an example, such repair can be achieved by drilling a small hole at the crack tip, thus effectively turning the crack into a keyhole-notch.


== References ==


== Further reading ==
Peggy C. Miedlar; Alan P. Berens; Allan Gunderson & J. P. Gallagher, Damage Tolerant Design Handbook: Guidelines for the Analysis and Design of Damage Tolerant Aircraft Structures, University of Dayton Research Institute, archived from the original on July 1, 2016, retrieved June 1, 2016","pandas(index=22, _1=22, text='damage tolerance is a property of a structure relating to its ability to sustain defects safely until repair can be effected. the approach to engineering design to account for damage tolerance is based on the assumption that flaws can exist in any structure and such flaws propagate with usage. this approach is commonly used in aerospace engineering, mechanical engineering, and civil engineering to manage the extension of cracks in structure through the application of the principles of fracture mechanics.  in engineering, a structure is considered to be damage tolerant if a maintenance program has been implemented that will result in the detection and repair of accidental damage, corrosion and fatigue cracking before such damage reduces the residual strength of the structure below an acceptable limit.   == history == structures upon which human life depends have long been recognized as needing an element of fail-safety.  when describing his flying machine, leonardo da vinci noted that ""in constructing wings one should make one chord to bear the strain and a looser one in the same position so that if one breaks under the strain, the other is in the position to serve the same function.""prior to the 1970s, the prevailing engineering philosophy of aircraft structures was to ensure that airworthiness was maintained with a single part broken, a redundancy requirement known as fail-safety.  however, advances in fracture mechanics, along with infamous catastrophic fatigue failures such as those in the de havilland comet prompted a change in requirements for aircraft.  it was discovered that a phenomenon known as  multiple-site damage could cause many small cracks in the structure, which grow slowly by themselves, to join one another over time, creating a much larger crack, and significantly reducing the expected time until failure   == safe-life structure == not all structure must demonstrate detectable crack propagation to ensure safety of operation.  some structures operate under the safe-life design principle, where an extremely low level of risk is accepted through a combination of testing and analysis that the part will never form a detectable crack due to fatigue during the service life of the part.  this is achieved through a significant reduction of stresses below the typical fatigue capability of the part.  safe-life structures are employed when the cost or infeasibility of inspections outweighs the weight penalty and development costs associated with safe-life structures.  an example of a safe-life component is the helicopter rotor blade.  due to the extremely large numbers of cycles endured by the rotating component, an undetectable crack may grow to a critical length in a single flight and before the aircraft lands, result in a catastrophic failure that regular maintenance could not have prevented.   == damage tolerance analysis == in ensuring the continued safe operation of the damage tolerant structure, inspection schedules are devised.  this schedule is based on many criteria, including:  assumed initial damaged condition of the structure stresses in the structure (both fatigue and operational maximum stresses) that cause crack growth from the damaged condition geometry of the material which intensifies or reduces the stresses on the crack tip ability of the material to withstand cracking due to stresses in the expected environment largest crack size that the structure can endure before catastrophic failure likelihood that a particular inspection method will reveal a crack acceptable level of risk that a certain structure will be completely failed expected duration after manufacture until a detectable crack will form assumption of failure in adjacent components which may have the effect of changing stresses in the structure of interestthese factors affect how long the structure may operate normally in the damaged condition before one or more inspection intervals has the opportunity to discover the damaged state and effect a repair.  the interval between inspections must be selected with a certain minimum safety, and also must balance the expense of the inspections, the weight penalty of lowering fatigue stresses, and the opportunity costs associated with a structure being out of service for maintenance.   == non-destructive inspections == manufacturers and operators of aircraft, trains, and civil engineering structures like bridges have a financial interest in ensuring that the inspection schedule is as cost-efficient as possible.  in the example of aircraft, because these structures are often revenue producing, there is an opportunity cost associated with the maintenance of the aircraft (lost ticket revenue), in addition to the cost of maintenance itself.  thus, this maintenance is desired to be performed infrequently, even when such increased intervals cause increased complexity and cost to the overhaul.  crack growth, as shown by fracture mechanics, is exponential in nature; meaning that the crack growth rate is a function of an exponent of the current crack size (see paris\' law). this means that only the largest cracks influence the overall strength of a structure; small internal damages do not necessarily decrease the strength. a desire for infrequent inspection intervals, combined with the exponential growth of cracks in structure has led to the development of non-destructive testing methods which allow inspectors to look for very tiny cracks which are often invisible to the naked eye. examples of this technology include eddy current, ultrasonic, dye penetrant, and x-ray inspections.  by catching structural cracks when they are very small, and growing slowly, these non-destructive inspections can reduce the amount of maintenance checks, and allow damage to be caught when it is small, and still inexpensive to repair. as an example, such repair can be achieved by drilling a small hole at the crack tip, thus effectively turning the crack into a keyhole-notch.   == references ==   == further reading == peggy c. miedlar; alan p. berens; allan gunderson & j. p. gallagher, damage tolerant design handbook: guidelines for the analysis and design of damage tolerant aircraft structures, university of dayton research institute, archived from the original on july 1, 2016, retrieved june 1, 2016')"
23,"Araldite is a registered trademark of Huntsman Advanced Materials (previously part of Ciba-Geigy) referring to their range of engineering and structural epoxy, acrylic, and polyurethane adhesives. The name was first used in 1946 for a two-part epoxy adhesive.
Araldite adhesive sets by the interaction of a resin with a hardener. Heat is not necessary although warming will reduce the curing time and improve the strength of the bond. After curing, the joint is claimed to be impervious to boiling water and all common organic solvents. It is available in many different types of pack, the most common containing two different tubes, one each for the resin and the hardener. Other variations include double syringe-type packages which automatically measure equal parts. This type of dispensing is not exact, however, and also poses the problem of unintentional mixing of resin and hardener.


== History ==
Aero Research Limited (ARL), founded in the UK in 1934, developed a new  synthetic-resin adhesive for bonding metals, glass, porcelain, china and other materials. The name ""Araldite"" recalls the ARL brand: ARaLdite. 
De Trey Frères SA of Switzerland carried out the first production of epoxy resins. They licensed the process to  Ciba AG in the early 1940s and Ciba first demonstrated a product under the tradename ""Araldite"" at the Swiss Industries Fair in 1945. Ciba went on to become one of the three major epoxy-resin producers worldwide. Ciba's epoxy business was spun off and later sold in the late 1990s and became the advanced materials business unit of Huntsman Corporation of the US.


== Notable applications ==
Beginning in 1940, Araldite was used in the production of the De Havilland Mosquito aircraft.
Araldite adhesive is used to join together the two sections of carbon composite which make up the monocoque of the Lamborghini Aventador.
The use of Araldite adhesive in architecture to bond thin joints of pre-cast concrete units was pioneered by Ove Arup in Coventry cathedral and the Sydney Opera House. At Coventry cathedral, Araldite adhesive was used to bond its columns and fins, while at Sydney Opera House, Araldite adhesive was used to bond the rib sections of the shells, since a traditional concrete joint would have slowed construction, as it would need 24 hours to cure before stressing.
Highmark Manufacturing uses Araldite epoxy resin in the manufacture of advanced ballistic protection body armour.
Schlösser Metallbau, a manufacturer of metal parts for railway carriages, uses Araldite epoxy resin to bond aluminium profiles of cab doorframes on the DBAG Class 423 Siemens Bombardier train.
Fischer Composite Technology GmbH uses the Araldite RTM System to produce carbon composite side blades for the Audi R8.
Araldite epoxy resin is commonly used as an embedding medium for electron microscopy.
Some Flamenco guitarists (e.g. Paco Peña) use it to reinforce their fingernails.
Brian May used it to seal the pickups in his homemade Red Special guitar to prevent microphonic feedback.


== Advertising ==
In 1983, British advertising agency FCO Univas set up a visual stunt presentation of the strength of Araldite adhesive by gluing a yellow Ford Cortina to a billboard on Cromwell Road, London, with the tagline ""It also sticks handles to teapots"".  Later, to demonstrate more of its strength, a red Cortina was placed on top of the yellow Cortina, with the tagline ""The tension mounts"". Finally, the car was removed from the billboard, leaving a hole on the billboard and a tagline ""How did we pull it off?"".


== See also ==
Aerolite
J-B Weld
Redux


== References ==


== External links ==
Specifications for 'Araldite Super Strength'","pandas(index=23, _1=23, text='araldite is a registered trademark of huntsman advanced materials (previously part of ciba-geigy) referring to their range of engineering and structural epoxy, acrylic, and polyurethane adhesives. the name was first used in 1946 for a two-part epoxy adhesive. araldite adhesive sets by the interaction of a resin with a hardener. heat is not necessary although warming will reduce the curing time and improve the strength of the bond. after curing, the joint is claimed to be impervious to boiling water and all common organic solvents. it is available in many different types of pack, the most common containing two different tubes, one each for the resin and the hardener. other variations include double syringe-type packages which automatically measure equal parts. this type of dispensing is not exact, however, and also poses the problem of unintentional mixing of resin and hardener.   == history == aero research limited (arl), founded in the uk in 1934, developed a new  synthetic-resin adhesive for bonding metals, glass, porcelain, china and other materials. the name ""araldite"" recalls the arl brand: araldite. de trey frères sa of switzerland carried out the first production of epoxy resins. they licensed the process to  ciba ag in the early 1940s and ciba first demonstrated a product under the tradename ""araldite"" at the swiss industries fair in 1945. ciba went on to become one of the three major epoxy-resin producers worldwide. ciba\'s epoxy business was spun off and later sold in the late 1990s and became the advanced materials business unit of huntsman corporation of the us.   == notable applications == beginning in 1940, araldite was used in the production of the de havilland mosquito aircraft. araldite adhesive is used to join together the two sections of carbon composite which make up the monocoque of the lamborghini aventador. the use of araldite adhesive in architecture to bond thin joints of pre-cast concrete units was pioneered by ove arup in coventry cathedral and the sydney opera house. at coventry cathedral, araldite adhesive was used to bond its columns and fins, while at sydney opera house, araldite adhesive was used to bond the rib sections of the shells, since a traditional concrete joint would have slowed construction, as it would need 24 hours to cure before stressing. highmark manufacturing uses araldite epoxy resin in the manufacture of advanced ballistic protection body armour. schlösser metallbau, a manufacturer of metal parts for railway carriages, uses araldite epoxy resin to bond aluminium profiles of cab doorframes on the dbag class 423 siemens bombardier train. fischer composite technology gmbh uses the araldite rtm system to produce carbon composite side blades for the audi r8. araldite epoxy resin is commonly used as an embedding medium for electron microscopy. some flamenco guitarists (e.g. paco peña) use it to reinforce their fingernails. brian may used it to seal the pickups in his homemade red special guitar to prevent microphonic feedback.   == advertising == in 1983, british advertising agency fco univas set up a visual stunt presentation of the strength of araldite adhesive by gluing a yellow ford cortina to a billboard on cromwell road, london, with the tagline ""it also sticks handles to teapots"".  later, to demonstrate more of its strength, a red cortina was placed on top of the yellow cortina, with the tagline ""the tension mounts"". finally, the car was removed from the billboard, leaving a hole on the billboard and a tagline ""how did we pull it off?"".   == see also == aerolite j-b weld redux   == references ==   == external links == specifications for \'araldite super strength\'')"
24,"In aviation, hot and high is a condition of low air density due to high ambient temperature and high airport elevation. Air density decreases with increasing temperature and altitude. The reduced density reduces the performance of the aircraft's engine and also provides less lift, requiring a higher speed to lift the plane off the ground. Aviators gauge air density by calculating the density altitude.An airport may be especially hot or high, without the other condition being present. Temperature and pressure altitude can change from one hour to the next. The fact that temperature decreases as altitude increases mitigates the ""hot and high"" effect to a small extent.


== Negative effects of reduced engine power due to hot and high conditions ==
Airplanes require a longer takeoff run, potentially exceeding the amount of available runway.
Reduced take-off power hampers an aircraft's ability to climb. In some cases, an aircraft may be unable to climb rapidly enough to clear terrain surrounding a mountain airport.
Helicopters may be forced to operate in the shaded portion of the height-velocity diagram in order to become airborne at all. This creates the potential for an uncontrollable descent in the event of an engine failure.
In some cases, aircraft have landed at high-altitude airports by taking advantage of cold temperatures only to become stranded as temperatures warmed and air density decreased.
While unsafe at any altitude, an overloaded aircraft is much more dangerous under hot and high conditions.


== Improving hot and high performance ==
Some ways to increase aircraft performance in hot and high conditions include:

Reduce aircraft weight. Weight can be reduced by carrying only enough fuel to reach the (lower-altitude) destination rather than filling the tanks completely. In some cases, unnecessary equipment can be removed from the aircraft. In many cases, however, the only practical way to adequately reduce aircraft weight is to depart with a smaller passenger, cargo, or weapons load. Consequently, hot and high conditions at the originating airport may prevent a commercial aircraft from operating with a load large enough to be profitable, or may constrain the firepower that a combat aircraft can bring to bear when conducting a long-range airstrike.
Increase engine power. More powerful engines can improve an airplane's acceleration and reduce its takeoff run. More powerful engines are generally larger and heavier and use more fuel during cruise, however, increasing the fuel load needed to reach the same destination. The added weight of the fuel and engines may negate the potential performance gain, and the added cost of the extra fuel may constrain the profitability of a commercial aircraft. On the other hand, replacing an older, less efficient engine with a newer engine of more advanced design can increase both power output and efficiency while sometimes even decreasing weight. In this situation, the only real disadvantage is the cost of the upgrade.
Utilize assisted take off devices, such as rockets, to increase acceleration and rate of climb.
Inject distilled water into the engine compressor or combustor. The primary purpose of water injection into jet engines is to increase the mass being accelerated, thereby increasing the force created by the engine. A secondary purpose is to lower the combustion temperature so that higher power settings may be used without causing engine temperatures to exceed limits.


== Jet or rocket assisted take off ==

Auxiliary rockets and/or jet engines can help a fully loaded aircraft to take off within the length of the runway. The rockets are usually one-time units that are jettisoned after takeoff. This practice was common in the 1950s and 60s, when the lower levels of thrust from military turbojets was inadequate for takeoff from shorter runways or with very heavy payloads. It is now seldom used.
Auxiliary jets and rockets have rarely been used on civil aircraft due to the risk of aircraft damage and loss of control if something were to go wrong during their use. Boeing did, however, produce a version of its popular Boeing 727 with JATO primarily for ""hot and high"" operations out of Mexico City Airport (MMMX) and La Paz, Bolivia. The boosters were located adjacent to the main landing gear at the wing root on each side of the aircraft.


== Specialized aircraft ==
Several manufacturers of early jet airliners offered variants optimized for hot and high operations. Such aircraft generally offered the largest wings and/or the most powerful engines in the model lineup coupled with a small fuselage to reduce weight. Some such aircraft include:

The BAC One-Eleven 475 combined the short body of the series 400 with the more powerful engines and improved wings of the series 500.  This aircraft also featured stronger landing gear for rough field operations.
The Boeing 707-220, which was a 707-120 airframe fitted with more powerful Pratt & Whitney JT4A engines, civilian versions of the military J75. The 707-220 had extremely high fuel consumption, and only 5 were built, all for Braniff International Airways. The 707-220 was rendered redundant by the release of the turbofan-powered 707-120B, which had even greater power along with much lower fuel consumption.
The Convair 880. Although Convair only offered one configuration of this aircraft, it had more power and a smaller fuselage than its competitors from Boeing and Douglas. Convair essentially wagered the success of the entire 880 model line on the appeal of an aircraft optimized for hot and high operations. The wager failed; only sixty-five 880s were sold and Convair's nascent airliner business soon collapsed.
The De Havilland Canada Dash 8-200, which is a -100 airframe fitted with larger engines of the -300 for hot and high operations. They proved successful and eventually replaced the -100 production line.
The Lockheed L-1011-200, which was otherwise an L-1011-100 with more powerful RB.211-524B engines.
The McDonnell Douglas DC-9-20, which combined the smaller fuselage of the DC-9-10 with the larger wings and more powerful engines of the DC-9-30, and was significantly outsold by both.
The McDonnell Douglas DC-10-15, which combined the fuselage of the DC-10-10 with the larger engines of the DC-10-30. These were specifically designed for and sold to Aeromexico and Mexicana. Only seven were built.
The Vickers VC10, which was designed to meet BOAC requirements for a large airliner that could operate medium range flights from short runways in southern Asia and Africa. The rear-mounted engines gave a more efficient wing and made them less vulnerable to runway debris. The resulting high fuel consumption compared to the contemporary Boeing 707 prompted all other major airlines to dismiss the VC10.
The McDonnell Douglas MD-82 was a hot and high version of the MD-80, and sold well, which generally is extremely rare for a type of performance-specialised aircraftThe marketing failure of most of these airplanes demonstrated that airlines were generally unwilling to accept reduced efficiency at cruise and smaller ultimate load-carrying capacity in return for a slight performance gain at particular airports. Rather than accepting these drawbacks, it was easier for airlines to demand the construction of longer runways, operate with smaller loads as conditions dictated, or simply drop the unprofitable destinations.
Furthermore, as the second generation of jet airliners began to appear in the 1970s, some aircraft were designed to eliminate the need for a special ""hot and high"" variant – for instance, the Airbus A300 can perform a 15/0 takeoff, where the leading edge slats are adjusted to 15 degrees and the flaps kept retracted. This takeoff technique is only used at hot and high airports, for it enables a higher climb limit weight and improves second segment climb performance.
Most jetliner manufacturers have dropped the ""hot and high"" variants from their model lineups.


== Hot and high airports ==
Notable examples of hot and high airports include:
Addis Ababa, Ethiopia – Bole Airport
Albuquerque, New Mexico, United States - Albuquerque International Sunport, especially from late spring to early autumn
Brasília, Brazil – Brasília Airport
Bogotá, Colombia – El Dorado Airport
Calgary, Alberta, Canada – Calgary International Airport, especially from late spring to early autumn
Daulat Beg Oldi, Ladakh, India - Daulat Beg Oldi Advanced Landing Ground (The world's highest airstrip at 16,700 feet (5,100 m). Climate ranges from a maximum of 35 °C (95 °F) in summer to −35 °C (−31 °F) in winter )
Denver, Colorado – Denver International Airport, especially from late spring to early autumn
Edwards Air Force Base, California, United States
Guatemala City, Guatemala - La Aurora International Airport, the highest international airport in Central America (4,951 feet (1,509 m)). It is hot from late February to late October
Harare, Zimbabwe – Robert Gabriel Mugabe International Airport
Johannesburg, South Africa – O. R. Tambo International Airport
Kabul, Afghanistan – Kabul Airport
Kampala, Uganda - Entebbe International Airport
Kunming, Yunnan, China - Kunming Changshui International Airport
Kuwait City, Kuwait - Kuwait International Airport (while only at an elevation of 206 feet (63 m), it is widely considered one of the world's hottest airports, as temperatures can reach up to 114 °F (46 °C) on an average summer day) 
La Paz, Bolivia – El Alto International Airport (not generally a ""hot"" airport, as average high temperatures are never more than 15 °C (59 °F) throughout the year, but the world's highest commercial airport with regularly scheduled international flights at 13,325 feet (4,061 m))
Las Vegas, Nevada, United States – McCarran International Airport
Leh, Ladakh, India - Kushok Bakula Rimpochee Airport - (One of the highest commercial airports in the world at 10,700 feet (3,300 m). Surrounded by high mountain peaks and with temperatures ranging from −42 °C (-43.6 °F) in winter to 33 °C (91.4 °F) in summer, it is an extremely challenging airport to fly from)
Lhasa, Tibet, China - Lhasa Gonggar Airport
Medellín, Colombia - José María Córdoba International Airport ( hot and high tests for the Airbus A380 were done there).
Mexico City, Mexico – Mexico City International Airport
Nairobi, Kenya - Jomo Kenyatta International Airport
Phoenix, Arizona, United States – Phoenix Sky Harbor International Airport (altitude of 1,135 feet (346 m) is not extreme, but the area's hot desert climate gives it hot and high characteristics for most of the year)
Quito, Ecuador – Mariscal Sucre Airport
Salt Lake City, Utah, United States – Salt Lake City International Airport, especially from late spring to early autumn
Sanaa, Yemen – Sanaa International Airport
Siachen Glacier, India - Sonam Post, world's highest helipad (altitude of 21,000 feet (6,400 m) in the world's highest manned post.
Tehran, Iran - Tehran Imam Khomeini International Airport
Xining, Qinghai, China - Xining Caojiabao Airport
Yerevan, Armenia – Zvartnots International Airport
Windhoek, Namibia - Hosea Kutako International Airport


== References ==","pandas(index=24, _1=24, text='in aviation, hot and high is a condition of low air density due to high ambient temperature and high airport elevation. air density decreases with increasing temperature and altitude. the reduced density reduces the performance of the aircraft\'s engine and also provides less lift, requiring a higher speed to lift the plane off the ground. aviators gauge air density by calculating the density altitude.an airport may be especially hot or high, without the other condition being present. temperature and pressure altitude can change from one hour to the next. the fact that temperature decreases as altitude increases mitigates the ""hot and high"" effect to a small extent.   == negative effects of reduced engine power due to hot and high conditions == airplanes require a longer takeoff run, potentially exceeding the amount of available runway. reduced take-off power hampers an aircraft\'s ability to climb. in some cases, an aircraft may be unable to climb rapidly enough to clear terrain surrounding a mountain airport. helicopters may be forced to operate in the shaded portion of the height-velocity diagram in order to become airborne at all. this creates the potential for an uncontrollable descent in the event of an engine failure. in some cases, aircraft have landed at high-altitude airports by taking advantage of cold temperatures only to become stranded as temperatures warmed and air density decreased. while unsafe at any altitude, an overloaded aircraft is much more dangerous under hot and high conditions.   == improving hot and high performance == some ways to increase aircraft performance in hot and high conditions include:  reduce aircraft weight. weight can be reduced by carrying only enough fuel to reach the (lower-altitude) destination rather than filling the tanks completely. in some cases, unnecessary equipment can be removed from the aircraft. in many cases, however, the only practical way to adequately reduce aircraft weight is to depart with a smaller passenger, cargo, or weapons load. consequently, hot and high conditions at the originating airport may prevent a commercial aircraft from operating with a load large enough to be profitable, or may constrain the firepower that a combat aircraft can bring to bear when conducting a long-range airstrike. increase engine power. more powerful engines can improve an airplane\'s acceleration and reduce its takeoff run. more powerful engines are generally larger and heavier and use more fuel during cruise, however, increasing the fuel load needed to reach the same destination. the added weight of the fuel and engines may negate the potential performance gain, and the added cost of the extra fuel may constrain the profitability of a commercial aircraft. on the other hand, replacing an older, less efficient engine with a newer engine of more advanced design can increase both power output and efficiency while sometimes even decreasing weight. in this situation, the only real disadvantage is the cost of the upgrade. utilize assisted take off devices, such as rockets, to increase acceleration and rate of climb. inject distilled water into the engine compressor or combustor. the primary purpose of water injection into jet engines is to increase the mass being accelerated, thereby increasing the force created by the engine. a secondary purpose is to lower the combustion temperature so that higher power settings may be used without causing engine temperatures to exceed limits.   == jet or rocket assisted take off ==  auxiliary rockets and/or jet engines can help a fully loaded aircraft to take off within the length of the runway. the rockets are usually one-time units that are jettisoned after takeoff. this practice was common in the 1950s and 60s, when the lower levels of thrust from military turbojets was inadequate for takeoff from shorter runways or with very heavy payloads. it is now seldom used. auxiliary jets and rockets have rarely been used on civil aircraft due to the risk of aircraft damage and loss of control if something were to go wrong during their use. boeing did, however, produce a version of its popular boeing 727 with jato primarily for ""hot and high"" operations out of mexico city airport (mmmx) and la paz, bolivia. the boosters were located adjacent to the main landing gear at the wing root on each side of the aircraft.   == specialized aircraft == several manufacturers of early jet airliners offered variants optimized for hot and high operations. such aircraft generally offered the largest wings and/or the most powerful engines in the model lineup coupled with a small fuselage to reduce weight. some such aircraft include:  the bac one-eleven 475 combined the short body of the series 400 with the more powerful engines and improved wings of the series 500.  this aircraft also featured stronger landing gear for rough field operations. the boeing 707-220, which was a 707-120 airframe fitted with more powerful pratt & whitney jt4a engines, civilian versions of the military j75. the 707-220 had extremely high fuel consumption, and only 5 were built, all for braniff international airways. the 707-220 was rendered redundant by the release of the turbofan-powered 707-120b, which had even greater power along with much lower fuel consumption. the convair 880. although convair only offered one configuration of this aircraft, it had more power and a smaller fuselage than its competitors from boeing and douglas. convair essentially wagered the success of the entire 880 model line on the appeal of an aircraft optimized for hot and high operations. the wager failed; only sixty-five 880s were sold and convair\'s nascent airliner business soon collapsed. the de havilland canada dash 8-200, which is a -100 airframe fitted with larger engines of the -300 for hot and high operations. they proved successful and eventually replaced the -100 production line. the lockheed l-1011-200, which was otherwise an l-1011-100 with more powerful rb.211-524b engines. the mcdonnell douglas dc-9-20, which combined the smaller fuselage of the dc-9-10 with the larger wings and more powerful engines of the dc-9-30, and was significantly outsold by both. the mcdonnell douglas dc-10-15, which combined the fuselage of the dc-10-10 with the larger engines of the dc-10-30. these were specifically designed for and sold to aeromexico and mexicana. only seven were built. the vickers vc10, which was designed to meet boac requirements for a large airliner that could operate medium range flights from short runways in southern asia and africa. the rear-mounted engines gave a more efficient wing and made them less vulnerable to runway debris. the resulting high fuel consumption compared to the contemporary boeing 707 prompted all other major airlines to dismiss the vc10. the mcdonnell douglas md-82 was a hot and high version of the md-80, and sold well, which generally is extremely rare for a type of performance-specialised aircraftthe marketing failure of most of these airplanes demonstrated that airlines were generally unwilling to accept reduced efficiency at cruise and smaller ultimate load-carrying capacity in return for a slight performance gain at particular airports. rather than accepting these drawbacks, it was easier for airlines to demand the construction of longer runways, operate with smaller loads as conditions dictated, or simply drop the unprofitable destinations. furthermore, as the second generation of jet airliners began to appear in the 1970s, some aircraft were designed to eliminate the need for a special ""hot and high"" variant – for instance, the airbus a300 can perform a 15/0 takeoff, where the leading edge slats are adjusted to 15 degrees and the flaps kept retracted. this takeoff technique is only used at hot and high airports, for it enables a higher climb limit weight and improves second segment climb performance. most jetliner manufacturers have dropped the ""hot and high"" variants from their model lineups.   == hot and high airports == notable examples of hot and high airports include: addis ababa, ethiopia – bole airport albuquerque, new mexico, united states - albuquerque international sunport, especially from late spring to early autumn brasília, brazil – brasília airport bogotá, colombia – el dorado airport calgary, alberta, canada – calgary international airport, especially from late spring to early autumn daulat beg oldi, ladakh, india - daulat beg oldi advanced landing ground (the world\'s highest airstrip at 16,700 feet (5,100 m). climate ranges from a maximum of 35 °c (95 °f) in summer to −35 °c (−31 °f) in winter ) denver, colorado – denver international airport, especially from late spring to early autumn edwards air force base, california, united states guatemala city, guatemala - la aurora international airport, the highest international airport in central america (4,951 feet (1,509 m)). it is hot from late february to late october harare, zimbabwe – robert gabriel mugabe international airport johannesburg, south africa – o. r. tambo international airport kabul, afghanistan – kabul airport kampala, uganda - entebbe international airport kunming, yunnan, china - kunming changshui international airport kuwait city, kuwait - kuwait international airport (while only at an elevation of 206 feet (63 m), it is widely considered one of the world\'s hottest airports, as temperatures can reach up to 114 °f (46 °c) on an average summer day) la paz, bolivia – el alto international airport (not generally a ""hot"" airport, as average high temperatures are never more than 15 °c (59 °f) throughout the year, but the world\'s highest commercial airport with regularly scheduled international flights at 13,325 feet (4,061 m)) las vegas, nevada, united states – mccarran international airport leh, ladakh, india - kushok bakula rimpochee airport - (one of the highest commercial airports in the world at 10,700 feet (3,300 m). surrounded by high mountain peaks and with temperatures ranging from −42 °c (-43.6 °f) in winter to 33 °c (91.4 °f) in summer, it is an extremely challenging airport to fly from) lhasa, tibet, china - lhasa gonggar airport medellín, colombia - josé maría córdoba international airport ( hot and high tests for the airbus a380 were done there). mexico city, mexico – mexico city international airport nairobi, kenya - jomo kenyatta international airport phoenix, arizona, united states – phoenix sky harbor international airport (altitude of 1,135 feet (346 m) is not extreme, but the area\'s hot desert climate gives it hot and high characteristics for most of the year) quito, ecuador – mariscal sucre airport salt lake city, utah, united states – salt lake city international airport, especially from late spring to early autumn sanaa, yemen – sanaa international airport siachen glacier, india - sonam post, world\'s highest helipad (altitude of 21,000 feet (6,400 m) in the world\'s highest manned post. tehran, iran - tehran imam khomeini international airport xining, qinghai, china - xining caojiabao airport yerevan, armenia – zvartnots international airport windhoek, namibia - hosea kutako international airport   == references ==')"
25,"A geodetic airframe is a type of construction for the airframes of aircraft developed by British aeronautical engineer Barnes Wallis in the 1930s (who sometimes spelled it ""geodesic""). Earlier, it was used by Prof. Schütte for the Schütte Lanz Airship LS 1 in 1909. It makes use of a space frame formed from a spirally crossing basket-weave of load-bearing members. The principle is that two geodesic arcs can be drawn to intersect on a curving surface (the fuselage) in a manner that the torsional load on each cancels out that on the other.


== Early examples ==

The ""diagonal rider"" structural element was used by Joshua Humphreys in the first US Navy sail frigates in 1794. Diagonal riders are viewable in the interior hull structure of the preserved USS Constitution on display in Boston Harbor. The structure was a pioneering example of placing ""non-orthogonal"" structural components within an otherwise conventional structure for its time. The ""diagonal riders"" were included in these American naval vessels' construction as one of five elements to reduce the problem of hogging in the ship's hull, and did not make up the bulk of the vessel's structure, they do not constitute a completely ""geodetic"" space frame.Calling any diagonal wood brace (as used on gates, buildings, ships or other structures with cantilevered or diagonal loads) an example of geodesic design is a misnomer. In a geodetic structure, the strength and structural integrity, and indeed the shape, come from the diagonal ""braces"" - the structure does not need the ""bits in between"" for part of its strength (implicit in the name space frame) as does a more conventional wooden structure.


== Aeroplanes ==

The earliest-known use of a geodesic airframe design for any aircraft was for the pre-World War I Schütte-Lanz SL1 rigid airship's envelope structure of 1911, with the airship capable of up to a 38.3 km/h (23.8 mph) top airspeed.
The Latécoère 6 was a French four-engined biplane bomber of the early 1920s. It was of advanced all-metal construction and probably the first aircraft to use geodetic construction. Only one was built.
Barnes Wallis, inspired by his earlier experience with light alloy structures and the use of geodesically-arranged wiring to distribute the lifting loads of the gasbags in the design of the R100 airship, evolved the geodetic construction method (although it is commonly stated, there was no geodetic structure in R100). Wallis used the term ""geodetic"" to apply to the airframe and distinguish it from ""geodesic"" which is the proper term for a line on a curved surface, arising from geodesy.The system was later used by Wallis's employer, Vickers-Armstrongs in a series of bomber aircraft, the Wellesley, Wellington, Warwick and Windsor. In these aircraft, the fuselage was built up from a number of duralumin alloy channel-beams that were formed into a large framework. Wooden battens were screwed onto the metal, to which the doped linen skin of the aircraft was fixed.
The metal lattice-work gave a light structure with tremendous strength; any one of the stringers could support some of the load from the opposite side of the aircraft. Blowing out the structure from one side would still leave the load-bearing structure as a whole intact. As a result, Wellingtons with huge areas of framework missing continued to return home when other types would not have survived; the dramatic effect enhanced by the doped fabric skin burning off, leaving the naked frames exposed (see photo). The benefits of the geodesic construction were partly offset by the difficulty of modifying the physical structure of the aircraft to allow for a change in length, profile, wingspan etc.
Geodetic wing and fin structures—taken from the Wellington—were used on the post-war Vickers VC.1 Viking, though with a new fuselage and metal-skinned.


== See also ==
Design principle
Figure of the Earth
Geodesic dome
Geodesic (disambiguation)
Geodetic system


== References ==


=== Inline citations ===


=== Sources ===
Buttler, Tony (2004). British Secret Projects: Fighters & Bombers 1935-1950. Hinckley: Midland Publishing. p. 240 pages. ISBN 978-1-85780-179-8.
Murray, Iain (2009). Bouncing-Bomb Man: the Science of Sir Barnes Wallis. Haynes. ISBN 978-1-84425-588-7.","pandas(index=25, _1=25, text='a geodetic airframe is a type of construction for the airframes of aircraft developed by british aeronautical engineer barnes wallis in the 1930s (who sometimes spelled it ""geodesic""). earlier, it was used by prof. schütte for the schütte lanz airship ls 1 in 1909. it makes use of a space frame formed from a spirally crossing basket-weave of load-bearing members. the principle is that two geodesic arcs can be drawn to intersect on a curving surface (the fuselage) in a manner that the torsional load on each cancels out that on the other.   == early examples ==  the ""diagonal rider"" structural element was used by joshua humphreys in the first us navy sail frigates in 1794. diagonal riders are viewable in the interior hull structure of the preserved uss constitution on display in boston harbor. the structure was a pioneering example of placing ""non-orthogonal"" structural components within an otherwise conventional structure for its time. the ""diagonal riders"" were included in these american naval vessels\' construction as one of five elements to reduce the problem of hogging in the ship\'s hull, and did not make up the bulk of the vessel\'s structure, they do not constitute a completely ""geodetic"" space frame.calling any diagonal wood brace (as used on gates, buildings, ships or other structures with cantilevered or diagonal loads) an example of geodesic design is a misnomer. in a geodetic structure, the strength and structural integrity, and indeed the shape, come from the diagonal ""braces"" - the structure does not need the ""bits in between"" for part of its strength (implicit in the name space frame) as does a more conventional wooden structure.   == aeroplanes ==  the earliest-known use of a geodesic airframe design for any aircraft was for the pre-world war i schütte-lanz sl1 rigid airship\'s envelope structure of 1911, with the airship capable of up to a 38.3 km/h (23.8 mph) top airspeed. the latécoère 6 was a french four-engined biplane bomber of the early 1920s. it was of advanced all-metal construction and probably the first aircraft to use geodetic construction. only one was built. barnes wallis, inspired by his earlier experience with light alloy structures and the use of geodesically-arranged wiring to distribute the lifting loads of the gasbags in the design of the r100 airship, evolved the geodetic construction method (although it is commonly stated, there was no geodetic structure in r100). wallis used the term ""geodetic"" to apply to the airframe and distinguish it from ""geodesic"" which is the proper term for a line on a curved surface, arising from geodesy.the system was later used by wallis\'s employer, vickers-armstrongs in a series of bomber aircraft, the wellesley, wellington, warwick and windsor. in these aircraft, the fuselage was built up from a number of duralumin alloy channel-beams that were formed into a large framework. wooden battens were screwed onto the metal, to which the doped linen skin of the aircraft was fixed. the metal lattice-work gave a light structure with tremendous strength; any one of the stringers could support some of the load from the opposite side of the aircraft. blowing out the structure from one side would still leave the load-bearing structure as a whole intact. as a result, wellingtons with huge areas of framework missing continued to return home when other types would not have survived; the dramatic effect enhanced by the doped fabric skin burning off, leaving the naked frames exposed (see photo). the benefits of the geodesic construction were partly offset by the difficulty of modifying the physical structure of the aircraft to allow for a change in length, profile, wingspan etc. geodetic wing and fin structures—taken from the wellington—were used on the post-war vickers vc.1 viking, though with a new fuselage and metal-skinned.   == see also == design principle figure of the earth geodesic dome geodesic (disambiguation) geodetic system   == references == buttler, tony (2004). british secret projects: fighters & bombers 1935-1950. hinckley: midland publishing. p. 240 pages. isbn 978-1-85780-179-8. murray, iain (2009). bouncing-bomb man: the science of sir barnes wallis. haynes. isbn 978-1-84425-588-7.')"
26,"REFSMMAT is a term used by guidance, navigation, and control system flight controllers during the Apollo program, which carried over into the Space Shuttle program. REFSMMAT stands for ""Reference to Stable Member Matrix"". It is a numerical definition of a fixed orientation in space and is usually (but not always) defined with respect to the stars. It was used by the Apollo Primary Guidance, Navigation and Control System (PGNCS) as a reference to which the gimbal-mounted platform at its core should be oriented. Every operation within the spacecraft that required knowledge of direction was carried out with respect to the orientation of the guidance platform, itself aligned according to a particular REFSMMAT.
During an Apollo flight, the REFSMMAT being used, and therefore the orientation of the guidance platform, would change as operational needs required it, but never during a guidance process—that is, one REFSMMAT might be in use from launch through Trans-Lunar Injection, another from TLI to Midpoint, but would not change during the middle of a burn or set of maneuvers.For example, it was considered good practice to have the spacecraft displays show some meaningful attitude value that would be easy to monitor during an important engine burn. Flight controllers at mission control in Houston would calculate what attitude the spacecraft had to be at for that burn and would devise a REFSMMAT that matched it in some way. Then, when it came time for the burn, if the spacecraft was in its correct attitude, the crew would see their 8-ball display a simple attitude that would be easy to interpret, allowing errors to be easily tracked and corrected.In the hallowed halls of mission control, Captain Refsmmat was a Kilroy-type character, conceived as a joke spoken to a 'Flight Dynamics Branch' rookie by Flight Controller RETRO John Llewellyn, and first drawn by flight controller FIDO Ed Pavelka as the ""ideal mission controller"".  'Capt. Refsmmat' served during the Apollo and Skylab years as an aid to the esprit de corps within the mission control team.


== See also ==

Apollo program


== References ==


== External links ==","pandas(index=26, _1=26, text='refsmmat is a term used by guidance, navigation, and control system flight controllers during the apollo program, which carried over into the space shuttle program. refsmmat stands for ""reference to stable member matrix"". it is a numerical definition of a fixed orientation in space and is usually (but not always) defined with respect to the stars. it was used by the apollo primary guidance, navigation and control system (pgncs) as a reference to which the gimbal-mounted platform at its core should be oriented. every operation within the spacecraft that required knowledge of direction was carried out with respect to the orientation of the guidance platform, itself aligned according to a particular refsmmat. during an apollo flight, the refsmmat being used, and therefore the orientation of the guidance platform, would change as operational needs required it, but never during a guidance process—that is, one refsmmat might be in use from launch through trans-lunar injection, another from tli to midpoint, but would not change during the middle of a burn or set of maneuvers.for example, it was considered good practice to have the spacecraft displays show some meaningful attitude value that would be easy to monitor during an important engine burn. flight controllers at mission control in houston would calculate what attitude the spacecraft had to be at for that burn and would devise a refsmmat that matched it in some way. then, when it came time for the burn, if the spacecraft was in its correct attitude, the crew would see their 8-ball display a simple attitude that would be easy to interpret, allowing errors to be easily tracked and corrected.in the hallowed halls of mission control, captain refsmmat was a kilroy-type character, conceived as a joke spoken to a \'flight dynamics branch\' rookie by flight controller retro john llewellyn, and first drawn by flight controller fido ed pavelka as the ""ideal mission controller"".  \'capt. refsmmat\' served during the apollo and skylab years as an aid to the esprit de corps within the mission control team.   == see also ==  apollo program   == references ==   == external links ==')"
27,"Cabin pressurization is a process in which conditioned air is pumped into the cabin of an aircraft or spacecraft in order to create a safe and comfortable environment for passengers and crew flying at high altitudes. For aircraft, this air is usually bled off from the gas turbine engines at the compressor stage, and for spacecraft, it is carried in high-pressure, often cryogenic tanks. The air is cooled, humidified, and mixed with recirculated air if necessary before it is distributed to the cabin by one or more environmental control systems. The cabin pressure is regulated by the outflow valve.
While the first experimental pressurization systems saw use during the 1920s and 1930s, it was not until 1938 that the Boeing 307 Stratoliner, the first commercial aircraft to be equipped with a pressurized cabin, was introduced. The practice would become widespread a decade later, particularly with the introduction of the British de Havilland Comet in 1949, the world's first jetliner. While initially a success, two catastrophic failures in 1954 temporarily grounded the worldwide fleet; the cause was found to be a combination of progressive metal fatigue and aircraft skin stresses, both of which aeronautical engineers only had a limited understanding of at the time. The key engineering principles learned from the Comet were applied directly to the design of all subsequent jet airliners, such as the Boeing 707.
Certain aircraft have presented unusual pressurization scenarios. The supersonic airliner Concorde had a particularly high pressure differential due to flying at unusually high altitude (up to 60,000 feet (18,000 m) while maintaining a cabin altitude of 6,000 feet (1,800 m). This not only increased airframe weight, but also saw the use of smaller cabin windows than most other commercial passenger aircraft, intended to slow the decompression rate if a depressurization event occurred. The Aloha Airlines Flight 243 incident, involving a Boeing 737-200 that suffered catastrophic cabin failure mid-flight, was primarily caused by its continued operation despite having accumulated more than twice the number of flight cycles that the airframe was designed to endure. For increased passenger comfort, several modern airliners, such as the Boeing 787 Dreamliner and the Airbus A350 XWB, feature reduced operating cabin altitudes as well as greater humidity levels; the use of composite airframes has aided the adoption of such comfort-maximising practices.


== Need for cabin pressurization ==

Pressurization becomes increasingly necessary at altitudes above 10,000 feet (3,000 m) above sea level to protect crew and passengers from the risk of a number of physiological problems caused by the low outside air pressure above that altitude. For private aircraft operating in the US, crew members are required to use oxygen masks if the cabin altitude (a representation of the air pressure, see below) stays above 12,500 ft for more than 30 minutes, or if the cabin altitude reaches 14,000 ft at any time. At altitudes above 15,000 ft, passengers are required to be provided oxygen masks as well. On commercial aircraft, the cabin altitude must be maintained at 8,000 feet (2,400 m) or less. Pressurization of the cargo hold is also required to prevent damage to pressure-sensitive goods that might leak, expand, burst or be crushed on re-pressurization. The principal physiological problems are listed below.

Hypoxia
The lower partial pressure of oxygen at high altitude reduces the alveolar oxygen tension in the lungs and subsequently in the brain, leading to sluggish thinking, dimmed vision, loss of consciousness, and ultimately death. In some individuals, particularly those with heart or lung disease, symptoms may begin as low as 5,000 feet (1,500 m), although most passengers can tolerate altitudes of 8,000 feet (2,400 m) without ill effect. At this altitude, there is about 25% less oxygen than there is at sea level.
Hypoxia may be addressed by the administration of supplemental oxygen, either through an oxygen mask or through a nasal cannula. Without pressurization, sufficient oxygen can be delivered up to an altitude of about 40,000 feet (12,000 m). This is because a person who is used to living at sea level needs about 0.20 bar partial oxygen pressure to function normally and that pressure can be maintained up to about 40,000 feet (12,000 m) by increasing the mole fraction of oxygen in the air that is being breathed. At 40,000 feet (12,000 m), the ambient air pressure falls to about 0.2 bar, at which maintaining a minimum partial pressure of oxygen of 0.2 bar requires breathing 100% oxygen using an oxygen mask.
Emergency oxygen supply masks in the passenger compartment of airliners do not need to be pressure-demand masks because most flights stay below 40,000 feet (12,000 m). Above that altitude the partial pressure of oxygen will fall below 0.2 bar even at 100% oxygen and some degree of cabin pressurization or rapid descent will be essential to avoid the risk of hypoxia.
Altitude sickness
Hyperventilation, the body's most common response to hypoxia, does help to partially restore the partial pressure of oxygen in the blood, but it also causes carbon dioxide (CO2) to out-gas, raising the blood pH and inducing alkalosis. Passengers may experience fatigue, nausea, headaches, sleeplessness, and (on extended flights) even pulmonary oedema. These are the same symptoms that mountain climbers experience, but the limited duration of powered flight makes the development of pulmonary oedema unlikely. Altitude sickness may be controlled by a full pressure suit with helmet and faceplate, which completely envelops the body in a pressurized environment; however, this is impractical for commercial passengers.
Decompression sickness
The low partial pressure of gases, principally nitrogen (N2) but including all other gases, may cause dissolved gases in the bloodstream to precipitate out, resulting in gas embolism, or bubbles in the bloodstream. The mechanism is the same as that of compressed-air divers on ascent from depth. Symptoms may include the early symptoms of ""the bends""—tiredness, forgetfulness, headache, stroke, thrombosis, and subcutaneous itching—but rarely the full symptoms thereof. Decompression sickness may also be controlled by a full-pressure suit as for altitude sickness.
Barotrauma
As the aircraft climbs or descends, passengers may experience discomfort or acute pain as gases trapped within their bodies expand or contract. The most common problems occur with air trapped in the middle ear (aerotitis) or paranasal sinuses by a blocked Eustachian tube or sinuses. Pain may also be experienced in the gastrointestinal tract or even the teeth (barodontalgia). Usually these are not severe enough to cause actual trauma but can result in soreness in the ear that persists after the flight and can exacerbate or precipitate pre-existing medical conditions, such as pneumothorax.


== Cabin altitude ==

The pressure inside the cabin is technically referred to as the equivalent effective cabin altitude or more commonly as the cabin altitude. This is defined as the equivalent altitude above mean sea level having the same atmospheric pressure according to a standard atmospheric model such as the International Standard Atmosphere. Thus a cabin altitude of zero would have the pressure found at mean sea level, which is taken to be 101.325 kilopascals (14.696 psi).


=== Aircraft ===
In airliners, cabin altitude during flight is kept above sea level in order to reduce stress on the pressurized part of the fuselage; this stress is proportional to the difference in pressure inside and outside the cabin. In a typical commercial passenger flight, the cabin altitude is programmed to rise gradually from the altitude of the airport of origin to a regulatory maximum of 8,000 ft (2,400 m). This cabin altitude is maintained while the aircraft is cruising at its maximum altitude and then reduced gradually during descent until the cabin pressure matches the ambient air pressure at the destination.Keeping the cabin altitude below 8,000 ft (2,400 m) generally prevents significant hypoxia, altitude sickness, decompression sickness, and barotrauma. Federal Aviation Administration (FAA) regulations in the U.S. mandate that under normal operating conditions, the cabin altitude may not exceed this limit at the maximum operating altitude of the aircraft. This mandatory maximum cabin altitude does not eliminate all physiological problems; passengers with conditions such as pneumothorax are advised not to fly until fully healed, and people suffering from a cold or other infection may still experience pain in the ears and sinuses. The rate of change of cabin altitude strongly affects comfort as humans are sensitive to pressure changes in the inner ear and sinuses and this has to be managed carefully. Scuba divers flying within the ""no fly"" period after a dive are at risk of decompression sickness because the accumulated nitrogen in their bodies can form bubbles when exposed to reduced cabin pressure.
The cabin altitude of the Boeing 767 is typically about 7,000 feet (2,100 m) when cruising at 37,000 feet (11,000 m). This is typical for older jet airliners. A design goal for many, but not all, newer aircraft is to provide a lower cabin altitude than older designs. This can be beneficial for passenger comfort. For example, the Bombardier Global Express business jet can provide a cabin altitude of 4,500 ft (1,400 m) when cruising at 41,000 feet (12,000 m). The Emivest SJ30 business jet can provide a sea-level cabin altitude when cruising at 41,000 feet (12,000 m). One study of eight flights in Airbus A380 aircraft found a median cabin pressure altitude of 6,128 feet (1,868 m), and 65 flights in Boeing 747-400 aircraft found a median cabin pressure altitude of 5,159 feet (1,572 m).Before 1996, approximately 6,000 large commercial transport airplanes were assigned a type certificate to fly up to 45,000 ft (14,000 m) without having to meet high-altitude special conditions. In 1996, the FAA adopted Amendment 25-87, which imposed additional high-altitude cabin pressure specifications for new-type aircraft designs. Aircraft certified to operate above 25,000 ft (7,600 m) ""must be designed so that occupants will not be exposed to cabin pressure altitudes in excess of 15,000 ft (4,600 m) after any probable failure condition in the pressurization system"". In the event of a decompression that results from ""any failure condition not shown to be extremely improbable"", the plane must be designed such that occupants will not be exposed to a cabin altitude exceeding 25,000 ft (7,600 m) for more than 2 minutes, nor to an altitude exceeding 40,000 ft (12,000 m) at any time. In practice, that new Federal Aviation Regulations amendment imposes an operational ceiling of 40,000 ft (12,000 m) on the majority of newly designed commercial aircraft. Aircraft manufacturers can apply for a relaxation of this rule if the circumstances warrant it. In 2004, Airbus acquired an FAA exemption to allow the cabin altitude of the A380 to reach 43,000 ft (13,000 m) in the event of a decompression incident and to exceed 40,000 ft (12,000 m) for one minute. This allows the A380 to operate at a higher altitude than other newly designed civilian aircraft.


=== Spacecraft ===
Russian engineers used an air-like nitrogen/oxygen mixture, kept at a cabin altitude near zero at all times, in their 1961 Vostok, 1964 Voskhod, and 1967 to present Soyuz spacecraft. This requires a heavier space vehicle design, because the spacecraft cabin structure must withstand the stress of 14.7 pounds per square inch (1 bar) against the vacuum of space, and also because an inert nitrogen mass must be carried. Care must also be taken to avoid decompression sickness when cosmonauts perform extravehicular activity, as current soft space suits are pressurized with pure oxygen at relatively low pressure in order to provide reasonable flexibility.By contrast, the United States used a pure oxygen atmosphere for its 1961 Mercury, 1965 Gemini, and 1967 Apollo spacecraft, mainly in order to avoid decompression sickness. Mercury used a cabin altitude of 24,800 feet (7,600 m) (5.5 pounds per square inch (0.38 bar)); Gemini used an altitude of 25,700 feet (7,800 m) (5.3 psi (0.37 bar)); and Apollo used 27,000 feet (8,200 m) (5.0 psi (0.34 bar)) in space. This allowed for a lighter space vehicle design. This is possible because at 100% oxygen, enough oxygen gets to the bloodstream to allow astronauts to operate normally. Before launch, the pressure was kept at slightly higher than sea level at a constant 5.3 psi (0.37 bar) above ambient for Gemini, and 2 psi (0.14 bar) above sea level at launch for Apollo), and transitioned to the space cabin altitude during ascent. However, the high pressure pure oxygen atmosphere proved to be a fatal fire hazard in Apollo, contributing to the deaths of the entire crew of Apollo 1 during a 1967 ground test. After this, NASA revised its procedure to use a nitrogen/oxygen mix at zero cabin altitude at launch, but kept the low-pressure pure oxygen atmosphere at 5 psi (0.34 bar) in space.After the Apollo program, the United States used standard air-like cabin atmospheres for Skylab, the Space Shuttle orbiter, and the International Space Station.


== Mechanics ==
Pressurization is achieved by the design of an airtight fuselage engineered to be pressurized with a source of compressed air and controlled by an environmental control system (ECS). The most common source of compressed air for pressurization is bleed air extracted from the compressor stage of a gas turbine engine, from a low or intermediate stage and also from an additional high stage; the exact stage can vary depending on engine type. By the time the cold outside air has reached the bleed air valves, it is at a very high pressure and has been heated to around 200 °C (392 °F). The control and selection of high or low bleed sources is fully automatic and is governed by the needs of various pneumatic systems at various stages of flight.The part of the bleed air that is directed to the ECS is then expanded to bring it to cabin pressure, which cools it. A final, suitable temperature is then achieved by adding back heat from the hot compressed air via a heat exchanger and air cycle machine known as a PAC (Pressurization and Air Conditioning) system. In some larger airliners, hot trim air can be added downstream of air conditioned air coming from the packs if it is needed to warm a section of the cabin that is colder than others.

At least two engines provide compressed bleed air for all the plane's pneumatic systems, to provide full redundancy. Compressed air is also obtained from the auxiliary power unit (APU), if fitted, in the event of an emergency and for cabin air supply on the ground before the main engines are started. Most modern commercial aircraft today have fully redundant, duplicated electronic controllers for maintaining pressurization along with a manual back-up control system.
All exhaust air is dumped to atmosphere via an outflow valve, usually at the rear of the fuselage. This valve controls the cabin pressure and also acts as a safety relief valve, in addition to other safety relief valves. If the automatic pressure controllers fail, the pilot can manually control the cabin pressure valve, according to the backup emergency procedure checklist. The automatic controller normally maintains the proper cabin pressure altitude by constantly adjusting the outflow valve position so that the cabin altitude is as low as practical without exceeding the maximum pressure differential limit on the fuselage. The pressure differential varies between aircraft types, typical values are between 540 hPa (7.8 psi) and 650 hPa (9.4 psi). At 39,000 feet (12,000 m), the cabin pressure would be automatically maintained at about 6,900 feet (2,100 m) (450 feet (140 m) lower than Mexico City), which is about 790 hPa (11.5 psi) of atmosphere pressure.Some aircraft, such as the Boeing 787 Dreamliner, have re-introduced electric compressors previously used on piston-engined airliners to provide pressurization. The use of electric compressors increases the electrical generation load on the engines and introduces a number of stages of energy transfer; therefore, it is unclear whether this increases the overall efficiency of the aircraft air handling system. It does, however, remove the danger of chemical contamination of the cabin, simplify engine design, avert the need to run high pressure pipework around the aircraft, and provide greater design flexibility.


== Unplanned decompression ==

Unplanned loss of cabin pressure at altitude/in space is rare but has resulted in a number of fatal accidents. Failures range from sudden, catastrophic loss of airframe integrity (explosive decompression) to slow leaks or equipment malfunctions that allow cabin pressure to drop.
Any failure of cabin pressurization above 10,000 feet (3,000 m) requires an emergency descent to 8,000 feet (2,400 m) or the closest to that while maintaining the Minimum Sector  Altitude (MSA), and the deployment of an oxygen mask for each seat. The oxygen systems have sufficient oxygen for all on board and give the pilots adequate time to descend to below 8,000 ft (2,400 m). Without emergency oxygen, hypoxia may lead to loss of consciousness and a subsequent loss of control of the aircraft. Modern airliners include a pressurized pure oxygen tank in the cockpit, giving the pilots more time to bring the aircraft to a safe altitude. The time of useful consciousness varies according to altitude. As the pressure falls the cabin air temperature may also plummet to the ambient outside temperature with a danger of hypothermia or frostbite.
For airliners that need to fly over terrain that does not allow reaching the safe altitude within a minimum of 30 minutes, pressurized oxygen bottles are mandatory since the chemical oxygen generators fitted to most planes cannot supply sufficient oxygen.
In jet fighter aircraft, the small size of the cockpit means that any decompression will be very rapid and would not allow the pilot time to put on an oxygen mask. Therefore, fighter jet pilots and aircrew are required to wear oxygen masks at all times.On June 30, 1971, the crew of Soyuz 11, Soviet cosmonauts Georgy Dobrovolsky, Vladislav Volkov, and Viktor Patsayev were killed after the cabin vent valve accidentally opened before atmospheric re-entry.


== History ==
The aircraft that pioneered pressurized cabin systems include:

Packard-Le Père LUSAC-11, (1920, a modified French design, not actually pressurized but with an enclosed, oxygen enriched cockpit)
Engineering Division USD-9A, a modified Airco DH.9A (1921 – the first aircraft to fly with the addition of a pressurized cockpit module)
Junkers Ju 49 (1931 – a German experimental aircraft purpose-built to test the concept of cabin pressurization)
Farman F.1000 (1932 – a French record breaking pressurized cockpit, experimental aircraft)
Chizhevski BOK-1 (1936 – a Russian experimental aircraft)
Lockheed XC-35 (1937 – an American pressurized aircraft. Rather than a pressure capsule enclosing the cockpit, the monocoque fuselage skin was the pressure vessel.)
Renard R.35 (1938 – the first pressurized piston airliner, which crashed on first flight)
Boeing 307 (1938 – the first pressurized airliner to enter commercial service)
Lockheed Constellation (1943 – the first pressurized airliner in wide service)
Avro Tudor (1946 – first British pressurized airliner)
de Havilland Comet (British, Comet 1 1949 – the first jetliner, Comet 4 1958 – resolving the Comet 1 problems)
Tupolev Tu-144 and Concorde (1968 USSR and 1969 Anglo-French respectively – first to operate at very high altitude)
SyberJet SJ30 (2005) First civilian business jet to certify 12.0 psi pressurization system allowing for a sea level cabin at 41,000 ft (12,000 m).In the late 1910s, attempts were being made to achieve higher and higher altitudes. In 1920, flights well over 37,000 ft (11,000 m) were first achieved by test pilot Lt. John A. Macready in a Packard-Le Père LUSAC-11 biplane at McCook Field in Dayton, Ohio. The flight was possible by releasing stored oxygen into the cockpit, which was released directly into an enclosed cabin and not to an oxygen mask, which was developed later. With this system flights nearing 40,000 ft (12,000 m) were possible, but the lack of atmospheric pressure at that altitude caused the pilot's heart to enlarge visibly, and many pilots reported health problems from such high altitude flights. Some early airliners had oxygen masks for the passengers for routine flights.
In 1921, a Wright-Dayton USD-9A reconnaissance biplane was modified with the addition of a completely enclosed air-tight chamber that could be pressurized with air forced into it by small external turbines. The chamber had a hatch only 22 in (0.56 m) in diameter that would be sealed by the pilot at 3,000 ft (910 m). The chamber contained only one instrument, an altimeter, while the conventional cockpit instruments were all mounted outside the chamber, visible through five small portholes. The first attempt to operate the aircraft was again made by Lt. John A. McCready, who discovered that the turbine was forcing air into the chamber faster than the small release valve provided could release it. As a result, the chamber quickly over pressurized, and the flight was abandoned. A second attempt had to be abandoned when the pilot discovered at 3,000 ft (910 m) that he was too short to close the chamber hatch. The first successful flight was finally made by test pilot Lt. Harrold Harris, making it the world's first flight by a pressurized aircraft.The first airliner with a pressurized cabin was the Boeing 307 Stratoliner, built in 1938, prior to World War II, though only ten were produced. The 307's ""pressure compartment was from the nose of the aircraft to a pressure bulkhead in the aft just forward of the horizontal stabilizer.""

World War II was a catalyst for aircraft development. Initially, the piston aircraft of World War II, though they often flew at very high altitudes, were not pressurized and relied on oxygen masks. This became impractical with the development of larger bombers where crew were required to move about the cabin and this led to the first bomber with cabin pressurization (though restricted to crew areas), the Boeing B-29 Superfortress. The control system for this was designed by Garrett AiResearch Manufacturing Company, drawing in part on licensing of patents held by Boeing for the Stratoliner.Post-war piston airliners such as the Lockheed Constellation (1943) extended the technology to civilian service. The piston engined airliners generally relied on electrical compressors to provide pressurized cabin air. Engine supercharging and cabin pressurization enabled planes like the Douglas DC-6, the Douglas DC-7, and the Constellation to have certified service ceilings from 24,000 ft (7,300 m) to 28,400 ft (8,700 m). Designing a pressurized fuselage to cope with that altitude range was within the engineering and metallurgical knowledge of that time. The introduction of jet airliners required a significant increase in cruise altitudes to the 30,000–41,000 ft (9,100–12,500 m) range, where jet engines are more fuel efficient. That increase in cruise altitudes required far more rigorous engineering of the fuselage, and in the beginning not all the engineering problems were fully understood.
The world's first commercial jet airliner was the British de Havilland Comet (1949) designed with a service ceiling of 36,000 ft (11,000 m). It was the first time that a large diameter, pressurized fuselage with windows had been built and flown at this altitude. Initially, the design was very successful but two catastrophic airframe failures in 1954 resulting in the total loss of the aircraft, passengers and crew grounded what was then the entire world jet airliner fleet. Extensive investigation and groundbreaking engineering analysis of the wreckage led to a number of very significant engineering advances that solved the basic problems of pressurized fuselage design at altitude. The critical problem proved to be a combination of an inadequate understanding of the effect of progressive metal fatigue as the fuselage undergoes repeated stress cycles coupled with a misunderstanding of how aircraft skin stresses are redistributed around openings in the fuselage such as windows and rivet holes.
The critical engineering principles concerning metal fatigue learned from the Comet 1 program were applied directly to the design of the Boeing 707 (1957) and all subsequent jet airliners. For example, detailed routine inspection processes were introduced, in addition to thorough visual inspections of the outer skin, mandatory structural sampling was routinely conducted by operators; the need to inspect areas not easily viewable by the naked eye led to the introduction of widespread radiography examination in aviation; this also had the advantage of detecting cracks and flaws too small to be seen otherwise. Another visibly noticeable legacy of the Comet disasters is the oval windows on every jet airliner; the metal fatigue cracks that destroyed the Comets were initiated by the small radius corners on the Comet 1's almost square windows. The Comet fuselage was redesigned and the Comet 4 (1958) went on to become a successful airliner, pioneering the first transatlantic jet service, but the program never really recovered from these disasters and was overtaken by the Boeing 707.Even following the Comet disasters, there were several subsequent catastrophic fatigue failures attributed to cabin pressurisation. Perhaps the most prominent example was Aloha Airlines Flight 243, involving a Boeing 737-200. In this case, the principal cause was the continued operation of the specific aircraft despite having accumulated 35,496 flight hours prior to the accident, those hours included over 89,680 flight cycles (takeoffs and landings), owing to its use on short flights; this amounted to more than twice the number of flight cycles that the airframe was designed to endure. Aloha 243 was able to land despite the substantial damage inflicted by the decompression, which had resulted in the loss of one member of the cabin crew; the incident had far-reaching effects on aviation safety policies and led to changes in operating procedures.The supersonic airliner Concorde had to deal with particularly high pressure differentials because it flew at unusually high altitude (up to 60,000 feet (18,000 m)) and maintained a cabin altitude of 6,000 ft (1,800 m). Despite this, its cabin altitude was intentionally maintained at 6,000 feet (1,800 m). This combination, while providing for increasing comfort, necessitated making Concorde a significantly heavier aircraft, which in turn contributed to the relatively high cost of a flight. Unusually, Concorde was provisioned with smaller cabin windows than most other commercial passenger aircraft in order to slow the rate of decompression in the event of a window seal failing. The high cruising altitude also required the use of high pressure oxygen and demand valves at the emergency masks unlike the continuous-flow masks used in conventional airliners. The FAA, which enforces minimum emergency descent rates for aircraft, determined that, in relation to Concorde's higher operating altitude, the best response to a pressure loss incident would be to perform a rapid descent.The designed operating cabin altitude for new aircraft is falling and this is expected to reduce any remaining physiological problems. Both the Boeing 787 Dreamliner and the Airbus A350 XWB airliners have made such modifications for increased passenger comfort. The 787's internal cabin pressure is the equivalent of 6,000 feet (1,800 m) altitude resulting in a higher pressure than for the 8,000 feet (2,400 m) altitude of older conventional aircraft; according to a joint study performed by Boeing and Oklahoma State University, such a level significantly improves comfort levels. Airbus has stated that the A350 XWB provides for a typical cabin altitude at or below 6,000 ft (1,800 m), along with a cabin atmosphere of 20% humidity and an airflow management system that adapts cabin airflow to passenger load with draught-free air circulation. The adoption of composite fuselages eliminates the threat posed by metal fatigue that would have been exacerbated by the higher cabin pressures being adopted by modern airliners, it also eliminates the risk of corrosion from the use of greater humidity levels.


== See also ==
Aerotoxic syndrome
Air cycle machine
Atmosphere (unit)
Compressed air
Fume event
Rarefaction
Space suit
Time of useful consciousness


== Footnotes ==


== General references ==
Seymour L. Chapin (August 1966). ""Garrett and Pressurized Flight: A Business Built on Thin Air"". Pacific Historical Review. 35 (3): 329–43. doi:10.2307/3636792. JSTOR 3636792.
Seymour L. Chapin (July 1971). ""Patent Interferences and the History of Technology: A High-flying Example"". Technology and Culture. 12 (3): 414–46. doi:10.2307/3102997. JSTOR 3102997.
Cornelisse, Diana G. Splendid Vision, Unswerving Purpose; Developing Air Power for the United States Air Force During the First Century of Powered Flight. Wright-Patterson Air Force Base, Ohio: U.S. Air Force Publications, 2002. ISBN 0-16-067599-5. pp. 128–29.
Portions from the United States Naval Flight Surgeon's Manual
""121 Dead in Greek Air Crash"", CNN


== External links ==
Video with Cabin Pressurization Demo in Civil Aircraft on YouTube","pandas(index=27, _1=27, text='cabin pressurization is a process in which conditioned air is pumped into the cabin of an aircraft or spacecraft in order to create a safe and comfortable environment for passengers and crew flying at high altitudes. for aircraft, this air is usually bled off from the gas turbine engines at the compressor stage, and for spacecraft, it is carried in high-pressure, often cryogenic tanks. the air is cooled, humidified, and mixed with recirculated air if necessary before it is distributed to the cabin by one or more environmental control systems. the cabin pressure is regulated by the outflow valve. while the first experimental pressurization systems saw use during the 1920s and 1930s, it was not until 1938 that the boeing 307 stratoliner, the first commercial aircraft to be equipped with a pressurized cabin, was introduced. the practice would become widespread a decade later, particularly with the introduction of the british de havilland comet in 1949, the world\'s first jetliner. while initially a success, two catastrophic failures in 1954 temporarily grounded the worldwide fleet; the cause was found to be a combination of progressive metal fatigue and aircraft skin stresses, both of which aeronautical engineers only had a limited understanding of at the time. the key engineering principles learned from the comet were applied directly to the design of all subsequent jet airliners, such as the boeing 707. certain aircraft have presented unusual pressurization scenarios. the supersonic airliner concorde had a particularly high pressure differential due to flying at unusually high altitude (up to 60,000 feet (18,000 m) while maintaining a cabin altitude of 6,000 feet (1,800 m). this not only increased airframe weight, but also saw the use of smaller cabin windows than most other commercial passenger aircraft, intended to slow the decompression rate if a depressurization event occurred. the aloha airlines flight 243 incident, involving a boeing 737-200 that suffered catastrophic cabin failure mid-flight, was primarily caused by its continued operation despite having accumulated more than twice the number of flight cycles that the airframe was designed to endure. for increased passenger comfort, several modern airliners, such as the boeing 787 dreamliner and the airbus a350 xwb, feature reduced operating cabin altitudes as well as greater humidity levels; the use of composite airframes has aided the adoption of such comfort-maximising practices.   == need for cabin pressurization ==  pressurization becomes increasingly necessary at altitudes above 10,000 feet (3,000 m) above sea level to protect crew and passengers from the risk of a number of physiological problems caused by the low outside air pressure above that altitude. for private aircraft operating in the us, crew members are required to use oxygen masks if the cabin altitude (a representation of the air pressure, see below) stays above 12,500 ft for more than 30 minutes, or if the cabin altitude reaches 14,000 ft at any time. at altitudes above 15,000 ft, passengers are required to be provided oxygen masks as well. on commercial aircraft, the cabin altitude must be maintained at 8,000 feet (2,400 m) or less. pressurization of the cargo hold is also required to prevent damage to pressure-sensitive goods that might leak, expand, burst or be crushed on re-pressurization. the principal physiological problems are listed below.  hypoxia the lower partial pressure of oxygen at high altitude reduces the alveolar oxygen tension in the lungs and subsequently in the brain, leading to sluggish thinking, dimmed vision, loss of consciousness, and ultimately death. in some individuals, particularly those with heart or lung disease, symptoms may begin as low as 5,000 feet (1,500 m), although most passengers can tolerate altitudes of 8,000 feet (2,400 m) without ill effect. at this altitude, there is about 25% less oxygen than there is at sea level. hypoxia may be addressed by the administration of supplemental oxygen, either through an oxygen mask or through a nasal cannula. without pressurization, sufficient oxygen can be delivered up to an altitude of about 40,000 feet (12,000 m). this is because a person who is used to living at sea level needs about 0.20 bar partial oxygen pressure to function normally and that pressure can be maintained up to about 40,000 feet (12,000 m) by increasing the mole fraction of oxygen in the air that is being breathed. at 40,000 feet (12,000 m), the ambient air pressure falls to about 0.2 bar, at which maintaining a minimum partial pressure of oxygen of 0.2 bar requires breathing 100% oxygen using an oxygen mask. emergency oxygen supply masks in the passenger compartment of airliners do not need to be pressure-demand masks because most flights stay below 40,000 feet (12,000 m). above that altitude the partial pressure of oxygen will fall below 0.2 bar even at 100% oxygen and some degree of cabin pressurization or rapid descent will be essential to avoid the risk of hypoxia. altitude sickness hyperventilation, the body\'s most common response to hypoxia, does help to partially restore the partial pressure of oxygen in the blood, but it also causes carbon dioxide (co2) to out-gas, raising the blood ph and inducing alkalosis. passengers may experience fatigue, nausea, headaches, sleeplessness, and (on extended flights) even pulmonary oedema. these are the same symptoms that mountain climbers experience, but the limited duration of powered flight makes the development of pulmonary oedema unlikely. altitude sickness may be controlled by a full pressure suit with helmet and faceplate, which completely envelops the body in a pressurized environment; however, this is impractical for commercial passengers. decompression sickness the low partial pressure of gases, principally nitrogen (n2) but including all other gases, may cause dissolved gases in the bloodstream to precipitate out, resulting in gas embolism, or bubbles in the bloodstream. the mechanism is the same as that of compressed-air divers on ascent from depth. symptoms may include the early symptoms of ""the bends""—tiredness, forgetfulness, headache, stroke, thrombosis, and subcutaneous itching—but rarely the full symptoms thereof. decompression sickness may also be controlled by a full-pressure suit as for altitude sickness. barotrauma as the aircraft climbs or descends, passengers may experience discomfort or acute pain as gases trapped within their bodies expand or contract. the most common problems occur with air trapped in the middle ear (aerotitis) or paranasal sinuses by a blocked eustachian tube or sinuses. pain may also be experienced in the gastrointestinal tract or even the teeth (barodontalgia). usually these are not severe enough to cause actual trauma but can result in soreness in the ear that persists after the flight and can exacerbate or precipitate pre-existing medical conditions, such as pneumothorax.   == cabin altitude ==  the pressure inside the cabin is technically referred to as the equivalent effective cabin altitude or more commonly as the cabin altitude. this is defined as the equivalent altitude above mean sea level having the same atmospheric pressure according to a standard atmospheric model such as the international standard atmosphere. thus a cabin altitude of zero would have the pressure found at mean sea level, which is taken to be 101.325 kilopascals (14.696 psi). russian engineers used an air-like nitrogen/oxygen mixture, kept at a cabin altitude near zero at all times, in their 1961 vostok, 1964 voskhod, and 1967 to present soyuz spacecraft. this requires a heavier space vehicle design, because the spacecraft cabin structure must withstand the stress of 14.7 pounds per square inch (1 bar) against the vacuum of space, and also because an inert nitrogen mass must be carried. care must also be taken to avoid decompression sickness when cosmonauts perform extravehicular activity, as current soft space suits are pressurized with pure oxygen at relatively low pressure in order to provide reasonable flexibility.by contrast, the united states used a pure oxygen atmosphere for its 1961 mercury, 1965 gemini, and 1967 apollo spacecraft, mainly in order to avoid decompression sickness. mercury used a cabin altitude of 24,800 feet (7,600 m) (5.5 pounds per square inch (0.38 bar)); gemini used an altitude of 25,700 feet (7,800 m) (5.3 psi (0.37 bar)); and apollo used 27,000 feet (8,200 m) (5.0 psi (0.34 bar)) in space. this allowed for a lighter space vehicle design. this is possible because at 100% oxygen, enough oxygen gets to the bloodstream to allow astronauts to operate normally. before launch, the pressure was kept at slightly higher than sea level at a constant 5.3 psi (0.37 bar) above ambient for gemini, and 2 psi (0.14 bar) above sea level at launch for apollo), and transitioned to the space cabin altitude during ascent. however, the high pressure pure oxygen atmosphere proved to be a fatal fire hazard in apollo, contributing to the deaths of the entire crew of apollo 1 during a 1967 ground test. after this, nasa revised its procedure to use a nitrogen/oxygen mix at zero cabin altitude at launch, but kept the low-pressure pure oxygen atmosphere at 5 psi (0.34 bar) in space.after the apollo program, the united states used standard air-like cabin atmospheres for skylab, the space shuttle orbiter, and the international space station.   == mechanics == pressurization is achieved by the design of an airtight fuselage engineered to be pressurized with a source of compressed air and controlled by an environmental control system (ecs). the most common source of compressed air for pressurization is bleed air extracted from the compressor stage of a gas turbine engine, from a low or intermediate stage and also from an additional high stage; the exact stage can vary depending on engine type. by the time the cold outside air has reached the bleed air valves, it is at a very high pressure and has been heated to around 200 °c (392 °f). the control and selection of high or low bleed sources is fully automatic and is governed by the needs of various pneumatic systems at various stages of flight.the part of the bleed air that is directed to the ecs is then expanded to bring it to cabin pressure, which cools it. a final, suitable temperature is then achieved by adding back heat from the hot compressed air via a heat exchanger and air cycle machine known as a pac (pressurization and air conditioning) system. in some larger airliners, hot trim air can be added downstream of air conditioned air coming from the packs if it is needed to warm a section of the cabin that is colder than others.  at least two engines provide compressed bleed air for all the plane\'s pneumatic systems, to provide full redundancy. compressed air is also obtained from the auxiliary power unit (apu), if fitted, in the event of an emergency and for cabin air supply on the ground before the main engines are started. most modern commercial aircraft today have fully redundant, duplicated electronic controllers for maintaining pressurization along with a manual back-up control system. all exhaust air is dumped to atmosphere via an outflow valve, usually at the rear of the fuselage. this valve controls the cabin pressure and also acts as a safety relief valve, in addition to other safety relief valves. if the automatic pressure controllers fail, the pilot can manually control the cabin pressure valve, according to the backup emergency procedure checklist. the automatic controller normally maintains the proper cabin pressure altitude by constantly adjusting the outflow valve position so that the cabin altitude is as low as practical without exceeding the maximum pressure differential limit on the fuselage. the pressure differential varies between aircraft types, typical values are between 540 hpa (7.8 psi) and 650 hpa (9.4 psi). at 39,000 feet (12,000 m), the cabin pressure would be automatically maintained at about 6,900 feet (2,100 m) (450 feet (140 m) lower than mexico city), which is about 790 hpa (11.5 psi) of atmosphere pressure.some aircraft, such as the boeing 787 dreamliner, have re-introduced electric compressors previously used on piston-engined airliners to provide pressurization. the use of electric compressors increases the electrical generation load on the engines and introduces a number of stages of energy transfer; therefore, it is unclear whether this increases the overall efficiency of the aircraft air handling system. it does, however, remove the danger of chemical contamination of the cabin, simplify engine design, avert the need to run high pressure pipework around the aircraft, and provide greater design flexibility.   == unplanned decompression ==  unplanned loss of cabin pressure at altitude/in space is rare but has resulted in a number of fatal accidents. failures range from sudden, catastrophic loss of airframe integrity (explosive decompression) to slow leaks or equipment malfunctions that allow cabin pressure to drop. any failure of cabin pressurization above 10,000 feet (3,000 m) requires an emergency descent to 8,000 feet (2,400 m) or the closest to that while maintaining the minimum sector  altitude (msa), and the deployment of an oxygen mask for each seat. the oxygen systems have sufficient oxygen for all on board and give the pilots adequate time to descend to below 8,000 ft (2,400 m). without emergency oxygen, hypoxia may lead to loss of consciousness and a subsequent loss of control of the aircraft. modern airliners include a pressurized pure oxygen tank in the cockpit, giving the pilots more time to bring the aircraft to a safe altitude. the time of useful consciousness varies according to altitude. as the pressure falls the cabin air temperature may also plummet to the ambient outside temperature with a danger of hypothermia or frostbite. for airliners that need to fly over terrain that does not allow reaching the safe altitude within a minimum of 30 minutes, pressurized oxygen bottles are mandatory since the chemical oxygen generators fitted to most planes cannot supply sufficient oxygen. in jet fighter aircraft, the small size of the cockpit means that any decompression will be very rapid and would not allow the pilot time to put on an oxygen mask. therefore, fighter jet pilots and aircrew are required to wear oxygen masks at all times.on june 30, 1971, the crew of soyuz 11, soviet cosmonauts georgy dobrovolsky, vladislav volkov, and viktor patsayev were killed after the cabin vent valve accidentally opened before atmospheric re-entry.   == history == the aircraft that pioneered pressurized cabin systems include:  packard-le père lusac-11, (1920, a modified french design, not actually pressurized but with an enclosed, oxygen enriched cockpit) engineering division usd-9a, a modified airco dh.9a (1921 – the first aircraft to fly with the addition of a pressurized cockpit module) junkers ju 49 (1931 – a german experimental aircraft purpose-built to test the concept of cabin pressurization) farman f.1000 (1932 – a french record breaking pressurized cockpit, experimental aircraft) chizhevski bok-1 (1936 – a russian experimental aircraft) lockheed xc-35 (1937 – an american pressurized aircraft. rather than a pressure capsule enclosing the cockpit, the monocoque fuselage skin was the pressure vessel.) renard r.35 (1938 – the first pressurized piston airliner, which crashed on first flight) boeing 307 (1938 – the first pressurized airliner to enter commercial service) lockheed constellation (1943 – the first pressurized airliner in wide service) avro tudor (1946 – first british pressurized airliner) de havilland comet (british, comet 1 1949 – the first jetliner, comet 4 1958 – resolving the comet 1 problems) tupolev tu-144 and concorde (1968 ussr and 1969 anglo-french respectively – first to operate at very high altitude) syberjet sj30 (2005) first civilian business jet to certify 12.0 psi pressurization system allowing for a sea level cabin at 41,000 ft (12,000 m).in the late 1910s, attempts were being made to achieve higher and higher altitudes. in 1920, flights well over 37,000 ft (11,000 m) were first achieved by test pilot lt. john a. macready in a packard-le père lusac-11 biplane at mccook field in dayton, ohio. the flight was possible by releasing stored oxygen into the cockpit, which was released directly into an enclosed cabin and not to an oxygen mask, which was developed later. with this system flights nearing 40,000 ft (12,000 m) were possible, but the lack of atmospheric pressure at that altitude caused the pilot\'s heart to enlarge visibly, and many pilots reported health problems from such high altitude flights. some early airliners had oxygen masks for the passengers for routine flights. in 1921, a wright-dayton usd-9a reconnaissance biplane was modified with the addition of a completely enclosed air-tight chamber that could be pressurized with air forced into it by small external turbines. the chamber had a hatch only 22 in (0.56 m) in diameter that would be sealed by the pilot at 3,000 ft (910 m). the chamber contained only one instrument, an altimeter, while the conventional cockpit instruments were all mounted outside the chamber, visible through five small portholes. the first attempt to operate the aircraft was again made by lt. john a. mccready, who discovered that the turbine was forcing air into the chamber faster than the small release valve provided could release it. as a result, the chamber quickly over pressurized, and the flight was abandoned. a second attempt had to be abandoned when the pilot discovered at 3,000 ft (910 m) that he was too short to close the chamber hatch. the first successful flight was finally made by test pilot lt. harrold harris, making it the world\'s first flight by a pressurized aircraft.the first airliner with a pressurized cabin was the boeing 307 stratoliner, built in 1938, prior to world war ii, though only ten were produced. the 307\'s ""pressure compartment was from the nose of the aircraft to a pressure bulkhead in the aft just forward of the horizontal stabilizer.""  world war ii was a catalyst for aircraft development. initially, the piston aircraft of world war ii, though they often flew at very high altitudes, were not pressurized and relied on oxygen masks. this became impractical with the development of larger bombers where crew were required to move about the cabin and this led to the first bomber with cabin pressurization (though restricted to crew areas), the boeing b-29 superfortress. the control system for this was designed by garrett airesearch manufacturing company, drawing in part on licensing of patents held by boeing for the stratoliner.post-war piston airliners such as the lockheed constellation (1943) extended the technology to civilian service. the piston engined airliners generally relied on electrical compressors to provide pressurized cabin air. engine supercharging and cabin pressurization enabled planes like the douglas dc-6, the douglas dc-7, and the constellation to have certified service ceilings from 24,000 ft (7,300 m) to 28,400 ft (8,700 m). designing a pressurized fuselage to cope with that altitude range was within the engineering and metallurgical knowledge of that time. the introduction of jet airliners required a significant increase in cruise altitudes to the 30,000–41,000 ft (9,100–12,500 m) range, where jet engines are more fuel efficient. that increase in cruise altitudes required far more rigorous engineering of the fuselage, and in the beginning not all the engineering problems were fully understood. the world\'s first commercial jet airliner was the british de havilland comet (1949) designed with a service ceiling of 36,000 ft (11,000 m). it was the first time that a large diameter, pressurized fuselage with windows had been built and flown at this altitude. initially, the design was very successful but two catastrophic airframe failures in 1954 resulting in the total loss of the aircraft, passengers and crew grounded what was then the entire world jet airliner fleet. extensive investigation and groundbreaking engineering analysis of the wreckage led to a number of very significant engineering advances that solved the basic problems of pressurized fuselage design at altitude. the critical problem proved to be a combination of an inadequate understanding of the effect of progressive metal fatigue as the fuselage undergoes repeated stress cycles coupled with a misunderstanding of how aircraft skin stresses are redistributed around openings in the fuselage such as windows and rivet holes. the critical engineering principles concerning metal fatigue learned from the comet 1 program were applied directly to the design of the boeing 707 (1957) and all subsequent jet airliners. for example, detailed routine inspection processes were introduced, in addition to thorough visual inspections of the outer skin, mandatory structural sampling was routinely conducted by operators; the need to inspect areas not easily viewable by the naked eye led to the introduction of widespread radiography examination in aviation; this also had the advantage of detecting cracks and flaws too small to be seen otherwise. another visibly noticeable legacy of the comet disasters is the oval windows on every jet airliner; the metal fatigue cracks that destroyed the comets were initiated by the small radius corners on the comet 1\'s almost square windows. the comet fuselage was redesigned and the comet 4 (1958) went on to become a successful airliner, pioneering the first transatlantic jet service, but the program never really recovered from these disasters and was overtaken by the boeing 707.even following the comet disasters, there were several subsequent catastrophic fatigue failures attributed to cabin pressurisation. perhaps the most prominent example was aloha airlines flight 243, involving a boeing 737-200. in this case, the principal cause was the continued operation of the specific aircraft despite having accumulated 35,496 flight hours prior to the accident, those hours included over 89,680 flight cycles (takeoffs and landings), owing to its use on short flights; this amounted to more than twice the number of flight cycles that the airframe was designed to endure. aloha 243 was able to land despite the substantial damage inflicted by the decompression, which had resulted in the loss of one member of the cabin crew; the incident had far-reaching effects on aviation safety policies and led to changes in operating procedures.the supersonic airliner concorde had to deal with particularly high pressure differentials because it flew at unusually high altitude (up to 60,000 feet (18,000 m)) and maintained a cabin altitude of 6,000 ft (1,800 m). despite this, its cabin altitude was intentionally maintained at 6,000 feet (1,800 m). this combination, while providing for increasing comfort, necessitated making concorde a significantly heavier aircraft, which in turn contributed to the relatively high cost of a flight. unusually, concorde was provisioned with smaller cabin windows than most other commercial passenger aircraft in order to slow the rate of decompression in the event of a window seal failing. the high cruising altitude also required the use of high pressure oxygen and demand valves at the emergency masks unlike the continuous-flow masks used in conventional airliners. the faa, which enforces minimum emergency descent rates for aircraft, determined that, in relation to concorde\'s higher operating altitude, the best response to a pressure loss incident would be to perform a rapid descent.the designed operating cabin altitude for new aircraft is falling and this is expected to reduce any remaining physiological problems. both the boeing 787 dreamliner and the airbus a350 xwb airliners have made such modifications for increased passenger comfort. the 787\'s internal cabin pressure is the equivalent of 6,000 feet (1,800 m) altitude resulting in a higher pressure than for the 8,000 feet (2,400 m) altitude of older conventional aircraft; according to a joint study performed by boeing and oklahoma state university, such a level significantly improves comfort levels. airbus has stated that the a350 xwb provides for a typical cabin altitude at or below 6,000 ft (1,800 m), along with a cabin atmosphere of 20% humidity and an airflow management system that adapts cabin airflow to passenger load with draught-free air circulation. the adoption of composite fuselages eliminates the threat posed by metal fatigue that would have been exacerbated by the higher cabin pressures being adopted by modern airliners, it also eliminates the risk of corrosion from the use of greater humidity levels.   == see also == aerotoxic syndrome air cycle machine atmosphere (unit) compressed air fume event rarefaction space suit time of useful consciousness   == footnotes ==   == general references == seymour l. chapin (august 1966). ""garrett and pressurized flight: a business built on thin air"". pacific historical review. 35 (3): 329–43. doi:10.2307/3636792. jstor 3636792. seymour l. chapin (july 1971). ""patent interferences and the history of technology: a high-flying example"". technology and culture. 12 (3): 414–46. doi:10.2307/3102997. jstor 3102997. cornelisse, diana g. splendid vision, unswerving purpose; developing air power for the united states air force during the first century of powered flight. wright-patterson air force base, ohio: u.s. air force publications, 2002. isbn 0-16-067599-5. pp. 128–29. portions from the united states naval flight surgeon\'s manual ""121 dead in greek air crash"", cnn   == external links == video with cabin pressurization demo in civil aircraft on youtube')"
28,"""Shirt-sleeve environment"" is a term used in aircraft design to describe the interior of an aircraft in which no special clothing need be worn. Early aircraft had no internal pressurization, so the crews of those that reached the stratosphere had to be garbed to withstand the low temperature and pressure of the air outside. Respirator masks needed to cover the mouth and nose. Silk socks were worn to retain heat. Sometimes leather clothing, such as boots, were electrically heated. When jet fighter aircraft reached still higher altitudes, something similar to a space suit had to be worn, and pilots of the highest reconnaissance aircraft wore real space suits. 
Commercial jet airliners fly in the stratosphere, but because they are pressurized, they could be said to have a shirt-sleeve environment. Crews of the US Apollo spacecraft always began the flight phases of launch, docking, and re-entry in space suits, although they could remove them for many hours. The Soviets tried to perfect this to save weight. This worked well, until an accidental depressurization on entry resulted in the deaths of an entire Soyuz crew. Protocols were changed shortly thereafter to require at least partial spacesuits. Early Soyuz spacecraft had no provision for space suits in the re-entry module, although the orbital module was intended for use as an airlock. Thus these operated in a shirt-sleeve environment except for spacewalks.
This term is also used in science fiction to describe an alien planet with an atmosphere breathable by humans without special equipment.The Space Shuttle's Spacelab Habitable module was an area with expanded volume for astronauts to work in a shirt sleeve environment and had space for equipment racks and related support equipment for operations in Low Earth orbit.One of the goals for MOLAB rover was to achieve a shirt-sleeve environment (compared to a lunar rover which was open to space and required the use of space suits to operate). One of the considerations was the habitable volume that could be occupied.


== References ==","pandas(index=28, _1=28, text='""shirt-sleeve environment"" is a term used in aircraft design to describe the interior of an aircraft in which no special clothing need be worn. early aircraft had no internal pressurization, so the crews of those that reached the stratosphere had to be garbed to withstand the low temperature and pressure of the air outside. respirator masks needed to cover the mouth and nose. silk socks were worn to retain heat. sometimes leather clothing, such as boots, were electrically heated. when jet fighter aircraft reached still higher altitudes, something similar to a space suit had to be worn, and pilots of the highest reconnaissance aircraft wore real space suits. commercial jet airliners fly in the stratosphere, but because they are pressurized, they could be said to have a shirt-sleeve environment. crews of the us apollo spacecraft always began the flight phases of launch, docking, and re-entry in space suits, although they could remove them for many hours. the soviets tried to perfect this to save weight. this worked well, until an accidental depressurization on entry resulted in the deaths of an entire soyuz crew. protocols were changed shortly thereafter to require at least partial spacesuits. early soyuz spacecraft had no provision for space suits in the re-entry module, although the orbital module was intended for use as an airlock. thus these operated in a shirt-sleeve environment except for spacewalks. this term is also used in science fiction to describe an alien planet with an atmosphere breathable by humans without special equipment.the space shuttle\'s spacelab habitable module was an area with expanded volume for astronauts to work in a shirt sleeve environment and had space for equipment racks and related support equipment for operations in low earth orbit.one of the goals for molab rover was to achieve a shirt-sleeve environment (compared to a lunar rover which was open to space and required the use of space suits to operate). one of the considerations was the habitable volume that could be occupied.   == references ==')"
29,"NOTAR (no tail rotor) is a helicopter  system which avoids the use of a tail rotor. It was developed by McDonnell Douglas Helicopter Systems (through their acquisition of Hughes Helicopters). The system uses a fan inside the tail boom to build a high volume of low-pressure air, which exits through two slots and creates a boundary layer flow of air along the tailboom utilizing the Coandă effect. The boundary layer changes the direction of airflow around the tailboom, creating thrust opposite the motion imparted to the fuselage by the torque effect of the main rotor. Directional yaw control is gained through a vented, rotating drum at the end of the tailboom, called the direct jet thruster. Advocates of NOTAR believe the system offers quieter and safer operation over a traditional tail rotor.


== Development ==

The use of directed air to provide anti-torque control had been tested as early as 1945 in the British Cierva W.9. During 1957, a Spanish prototype designed and built by Aerotecnica flew using exhaust gases from the turbine instead of a tail rotor. This model was designated as Aerotecnica AC-14. 
Development of the NOTAR system dates back to 1975, when engineers at Hughes Helicopters began concept development work. On December 17 1981, Hughes flew an OH-6A fitted with NOTAR for the first time. The OH-6A helicopter (serial number 65-12917) was supplied by the U.S. Army for Hughes to develop the NOTAR technology and was the second OH-6 built by Hughes for the U.S. Army.  A more heavily modified version of the prototype demonstrator first flew in March 1986 (by which time McDonnell Douglas had acquired Hughes Helicopters).   The original prototype last flew in June 1986 and is now at the U.S. Army Aviation Museum in Fort Rucker, Alabama.
A production model NOTAR 520N (N520NT) was later produced and first flew on May 1, 1990.  It crashed on September 27, 1994 when it collided with an AH-64D while flying as a chase aircraft for the Apache.


== Concept ==
Although the concept took over three years to refine, the NOTAR system is simple in theory and works to provide some directional control using the Coandă effect. A variable pitch fan is enclosed in the aft fuselage section immediately forward of the tail boom and driven by the main rotor transmission. This fan forces low pressure air through two slots on the right side of the tailboom, causing the downwash from the main rotor to hug the tailboom, producing lift, and thus a measure of directional control. This is augmented by a direct jet thruster and vertical stabilisers.
Benefits of the NOTAR system include increased safety (the tail rotor being vulnerable), and greatly reduced external noise as tail rotors on helicopters produce a lot of noise. NOTAR-equipped helicopters are among the quietest certified helicopters.

		
		


== Applications ==

There are several production helicopters that utilize the NOTAR system, which are produced by MD Helicopters:

MD 520N: a NOTAR variant of the Hughes/MD500 series helicopter.
MD 600N: a larger version of the MD 520N.
MD Explorer: a twin-engine, eight-seat light helicopter.and other designs:

Youngcopter Neo


== See also ==
Cierva W.9
Fenestron
Tip jet rotor
Coaxial rotors
Tandem rotors
Synchropter


== Notes ==


== References ==","pandas(index=29, _1=29, text='notar (no tail rotor) is a helicopter  system which avoids the use of a tail rotor. it was developed by mcdonnell douglas helicopter systems (through their acquisition of hughes helicopters). the system uses a fan inside the tail boom to build a high volume of low-pressure air, which exits through two slots and creates a boundary layer flow of air along the tailboom utilizing the coandă effect. the boundary layer changes the direction of airflow around the tailboom, creating thrust opposite the motion imparted to the fuselage by the torque effect of the main rotor. directional yaw control is gained through a vented, rotating drum at the end of the tailboom, called the direct jet thruster. advocates of notar believe the system offers quieter and safer operation over a traditional tail rotor.   == development ==  the use of directed air to provide anti-torque control had been tested as early as 1945 in the british cierva w.9. during 1957, a spanish prototype designed and built by aerotecnica flew using exhaust gases from the turbine instead of a tail rotor. this model was designated as aerotecnica ac-14. development of the notar system dates back to 1975, when engineers at hughes helicopters began concept development work. on december 17 1981, hughes flew an oh-6a fitted with notar for the first time. the oh-6a helicopter (serial number 65-12917) was supplied by the u.s. army for hughes to develop the notar technology and was the second oh-6 built by hughes for the u.s. army.  a more heavily modified version of the prototype demonstrator first flew in march 1986 (by which time mcdonnell douglas had acquired hughes helicopters).   the original prototype last flew in june 1986 and is now at the u.s. army aviation museum in fort rucker, alabama. a production model notar 520n (n520nt) was later produced and first flew on may 1, 1990.  it crashed on september 27, 1994 when it collided with an ah-64d while flying as a chase aircraft for the apache.   == concept == although the concept took over three years to refine, the notar system is simple in theory and works to provide some directional control using the coandă effect. a variable pitch fan is enclosed in the aft fuselage section immediately forward of the tail boom and driven by the main rotor transmission. this fan forces low pressure air through two slots on the right side of the tailboom, causing the downwash from the main rotor to hug the tailboom, producing lift, and thus a measure of directional control. this is augmented by a direct jet thruster and vertical stabilisers. benefits of the notar system include increased safety (the tail rotor being vulnerable), and greatly reduced external noise as tail rotors on helicopters produce a lot of noise. notar-equipped helicopters are among the quietest certified helicopters.          == applications ==  there are several production helicopters that utilize the notar system, which are produced by md helicopters:  md 520n: a notar variant of the hughes/md500 series helicopter. md 600n: a larger version of the md 520n. md explorer: a twin-engine, eight-seat light helicopter.and other designs:  youngcopter neo   == see also == cierva w.9 fenestron tip jet rotor coaxial rotors tandem rotors synchropter   == notes ==   == references ==')"
30,"Space environment is a branch of astronautics, aerospace engineering and space physics that seeks to understand and address conditions existing in space that affect the design and operation of spacecraft. A related subject, space weather, deals with dynamic processes in the solar-terrestrial system that can give rise to effects on spacecraft, but that can also affect the atmosphere, ionosphere and geomagnetic field, giving rise to several other kinds of effects on human technologies.
Effects on spacecraft can arise from radiation, space debris and meteoroid impact, upper atmospheric drag and spacecraft electrostatic charging.
Radiation in space usually comes from three main sources:

The Van Allen radiation belts
Solar proton events and solar energetic particles; and
Galactic cosmic rays.For long-duration missions, the high doses of radiation can damage electronic components and solar cells. A major concern is also radiation-induced ""single-event effects"" such as single event upset. Crewed missions usually avoid the radiation belts and the International Space Station is at an altitude well below the most severe regions of the radiation belts. During solar energetic events (solar flares and coronal mass ejections) particles can be accelerated to very high energies and can reach the Earth in times as short as 30 minutes (but usually take some hours). These particles are mainly protons and heavier ions that can cause radiation damage, disruption to logic circuits, and even hazards to astronauts. Crewed missions to return to the Moon or to travel to Mars will have to deal with the major problems presented by solar particle events to radiation safety, in addition to the important contribution to doses from the low-level background cosmic rays. In near-Earth orbits, the Earth's geomagnetic field screens spacecraft from a large part of these hazards - a process called geomagnetic shielding.
Space debris and meteoroids can impact spacecraft at high speeds, causing mechanical or electrical damage.  The average speed of space debris is 10 km/s (22,000 mph; 36,000 km/h) while the average speed of meteoroids is much greater.  For example, the meteoroids associated with the Perseid meteor shower travel at an average speed of 58 km/s (130,000 mph; 210,000 km/h).  Mechanical damage from debris impacts have been studied through space missions including LDEF, which had over 20,000 documented impacts through its 5.7-year mission.  Electrical anomalies associated with impact events include ESA's Olympus spacecraft, which lost attitude control during the 1993 Perseid meteor shower.  A similar event occurred with the Landsat 5 spacecraft during the 2009 Perseid meteor shower.Spacecraft electrostatic charging is caused by the hot plasma environment around the Earth. The plasma encountered in the region of the geostationary orbit becomes heated during geomagnetic substorms caused by disturbances in the solar wind. ""Hot"" electrons (with energies in the kilo-electron volt range) collect on surfaces of spacecraft and can establish electrostatic potentials of the order of kilovolts. As a result, discharges can occur and are known to be the source of many spacecraft anomalies.
Solutions devised by scientists and engineers include, but are not limited to, spacecraft shielding, special ""hardening"" of electronic systems, various collision detection systems. Evaluation of effects during spacecraft design includes application of various models of the environment, including radiation belt models, spacecraft-plasma interaction models and atmospheric models to predict drag effects encountered in lower orbits and during reentry.
The field often overlaps with the disciplines of astrophysics, atmospheric science, space physics, and geophysics, albeit usually with an emphasis on application.
The United States government maintains a Space Weather Prediction Center at Boulder, Colorado.  The Space Weather Prediction Center (SWPC) is part of the National Oceanic and Atmospheric Administration (NOAA).  SWPC is one of the National Weather Service's (NWS) National Centers for Environmental Prediction (NCEP).
Space weather effects on Earth can include ionospheric storms, temporary decreases in ozone densities, disruption to radio communication, to GPS signals and submarine positioning. Some scientists also theorize links between sunspot activity and ice ages. [1]


== See also ==
Astronautics
ECSS standard E-ST-10-04C on Space environment
Karman line
Outer space
Space Environment Data System (SEDAT)
Space Environment Information System (SPENVIS)
Space climate
Space science
Space weather
Space weathering
Space Weather Prediction Center (SWPC)


== References ==


== External links ==
Space Environment Technologies (SET)
Space Weather Center (SWC)
ESA Space Environment and Effects Analysis Section 
International Space Environment Service (ISES)","pandas(index=30, _1=30, text='space environment is a branch of astronautics, aerospace engineering and space physics that seeks to understand and address conditions existing in space that affect the design and operation of spacecraft. a related subject, space weather, deals with dynamic processes in the solar-terrestrial system that can give rise to effects on spacecraft, but that can also affect the atmosphere, ionosphere and geomagnetic field, giving rise to several other kinds of effects on human technologies. effects on spacecraft can arise from radiation, space debris and meteoroid impact, upper atmospheric drag and spacecraft electrostatic charging. radiation in space usually comes from three main sources:  the van allen radiation belts solar proton events and solar energetic particles; and galactic cosmic rays.for long-duration missions, the high doses of radiation can damage electronic components and solar cells. a major concern is also radiation-induced ""single-event effects"" such as single event upset. crewed missions usually avoid the radiation belts and the international space station is at an altitude well below the most severe regions of the radiation belts. during solar energetic events (solar flares and coronal mass ejections) particles can be accelerated to very high energies and can reach the earth in times as short as 30 minutes (but usually take some hours). these particles are mainly protons and heavier ions that can cause radiation damage, disruption to logic circuits, and even hazards to astronauts. crewed missions to return to the moon or to travel to mars will have to deal with the major problems presented by solar particle events to radiation safety, in addition to the important contribution to doses from the low-level background cosmic rays. in near-earth orbits, the earth\'s geomagnetic field screens spacecraft from a large part of these hazards - a process called geomagnetic shielding. space debris and meteoroids can impact spacecraft at high speeds, causing mechanical or electrical damage.  the average speed of space debris is 10 km/s (22,000 mph; 36,000 km/h) while the average speed of meteoroids is much greater.  for example, the meteoroids associated with the perseid meteor shower travel at an average speed of 58 km/s (130,000 mph; 210,000 km/h).  mechanical damage from debris impacts have been studied through space missions including ldef, which had over 20,000 documented impacts through its 5.7-year mission.  electrical anomalies associated with impact events include esa\'s olympus spacecraft, which lost attitude control during the 1993 perseid meteor shower.  a similar event occurred with the landsat 5 spacecraft during the 2009 perseid meteor shower.spacecraft electrostatic charging is caused by the hot plasma environment around the earth. the plasma encountered in the region of the geostationary orbit becomes heated during geomagnetic substorms caused by disturbances in the solar wind. ""hot"" electrons (with energies in the kilo-electron volt range) collect on surfaces of spacecraft and can establish electrostatic potentials of the order of kilovolts. as a result, discharges can occur and are known to be the source of many spacecraft anomalies. solutions devised by scientists and engineers include, but are not limited to, spacecraft shielding, special ""hardening"" of electronic systems, various collision detection systems. evaluation of effects during spacecraft design includes application of various models of the environment, including radiation belt models, spacecraft-plasma interaction models and atmospheric models to predict drag effects encountered in lower orbits and during reentry. the field often overlaps with the disciplines of astrophysics, atmospheric science, space physics, and geophysics, albeit usually with an emphasis on application. the united states government maintains a space weather prediction center at boulder, colorado.  the space weather prediction center (swpc) is part of the national oceanic and atmospheric administration (noaa).  swpc is one of the national weather service\'s (nws) national centers for environmental prediction (ncep). space weather effects on earth can include ionospheric storms, temporary decreases in ozone densities, disruption to radio communication, to gps signals and submarine positioning. some scientists also theorize links between sunspot activity and ice ages. [1]   == see also == astronautics ecss standard e-st-10-04c on space environment karman line outer space space environment data system (sedat) space environment information system (spenvis) space climate space science space weather space weathering space weather prediction center (swpc)   == references ==   == external links == space environment technologies (set) space weather center (swc) esa space environment and effects analysis section international space environment service (ises)')"
31,"Aerodynamic heating is the heating of a solid body produced by its high-speed passage through air (or by the passage of air past a static body), whereby its kinetic energy is converted to heat by adiabatic heating, and by skin friction on the surface of the object at a rate that depends on the viscosity and speed of the air. In science and engineering, it is most frequently a concern regarding meteors, atmospheric reentry of spacecraft, and the design of high-speed aircraft.


== Physics ==
When moving through air at high speeds, an object's kinetic energy is converted to heat through compression of and friction with the air. At low speeds, the object also loses heat to the air if the air is cooler. The combined temperature effect of heat from the air and from passage through it is called the stagnation temperature; the actual temperature is called the recovery temperature. These viscous dissipative effects to neighboring sub-layers make the boundary layer slow down via a non-isentropic process.  Heat then conducts into the surface material from the higher temperature air. The result is an increase in the temperature of the material and a loss of energy from the flow. The forced convection ensures that other material replenishes the gases that have cooled to continue the process.The stagnation and the recovery temperature of a flow increases with the speed of the flow and are greater at high speeds. The total thermal loading of the object is a function of both the recovery temperature and the mass flow rate of the flow. Aerodynamic heating is greatest at high speeds and in the lower atmosphere where the density is greater.  In addition to the convective process described above, there is also thermal radiation from the flow to the body and vice versa, with the net direction governed by their temperatures relative to each other.Aerodynamic heating increases with the speed of the vehicle. Its effects are minimal at subsonic speeds, but are significant enough at supersonic speeds beyond about Mach 2.2 that they affect design and material considerations for the vehicle's structure and internal systems. The heating effects are greatest at leading edges, but the whole vehicle heats up to a stable temperature if its speed remains constant. Aerodynamic heating is dealt with by the use of alloys that can withstand high temperatures, insulation of the exterior of the vehicle, or the use of ablative material.


== Aircraft ==
Aerodynamic heating is a concern for supersonic and hypersonic aircraft.
One of the main concerns caused by aerodynamic heating arises in the design of the wing. For subsonic speeds, two main goals of wing design are minimizing weight and maximizing strength. Aerodynamic heating, which occurs at supersonic and hypersonic speeds, adds an additional consideration in wing structure analysis. An idealized wing structure is made up of spars, stringers, and skin segments. In a wing that normally experiences subsonic speeds, there must be a sufficient number of stringers to withstand the axial and bending stresses induced by the lift force acting on the wing. In addition, the distance between the stringers must be small enough that the skin panels do not buckle, and the panels must be thick enough to withstand the shear stress and shear flow present in the panels due to the lifting force on the wing. However, the weight of the wing must be made as small as possible, so the choice of material for the stringers and the skin is an important factor.At supersonic speeds, aerodynamic heating adds another element to this structural analysis. At normal speeds, spars and stringers experience a load called Delta P, which is a function of the lift force, first and second moments of inertia, and length of the spar. When there are more spars and stringers, the Delta P in each member is reduced, and the area of the stringer can be reduced to meet critical stress requirements. However, the increase in temperature caused by energy flowing from the air (heated by skin friction at these high speeds) adds another load factor, called a thermal load, to the spars. This thermal load increases the net force felt by the stringers, and thus the area of the stringers must be increased in order for the critical stress requirement to be met.Another issue that aerodynamic heating causes for aircraft design is the effect of high temperatures on common material properties. Common materials used in aircraft wing design, such as aluminum and steel, experience a decrease in strength as temperatures get extremely high. The Young's Modulus of the material, defined as the ratio between stress and strain experienced by the material, decreases as the temperature increases. Young's Modulus is critical in the selection of materials for wing, as a higher value lets the material resist the yield and shear stress caused by the lift and thermal loads. This is because Young's Modulus is an important factor in the equations for calculating the critical buckling load for axial members and the critical buckling shear stress for skin panels. If the Young's Modulus of the material decreases at high temperatures caused by aerodynamic heating, then the wing design will call for larger spars and thicker skin segments in order to account for this decrease in strength as the aircraft goes supersonic. There are some materials that retain their strength at the high temperatures that aerodynamic heating induces. For example, Inconel X-750 was used on parts of the airframe of the X-15, a North American aircraft that flew at hypersonic speeds in 1958. Titanium is another high-strength material, even at high temperatures, and is often used for wing frames of supersonic aircraft. The SR-71 used titanium skin panels painted black to reduce the temperature and corrugated to accommodate expansion. Another important design concept for early supersonic aircraft wings was using a small thickness-to-chord ratio, so that the speed of the flow over the airfoil does not increase too much from the free stream speed. As the flow is already supersonic, increasing the speed even more would not be beneficial for the wing structure. Reducing the thickness of the wing brings the top and bottom stringers closer together, reducing the total moment of inertia of the structure. This increases axial load in the stringers, and thus the area, and weight, of the stringers must be increased. Some designs for hypersonic missiles have used liquid cooling of the leading edges (usually the fuel en route to the engine). The Sprint missile's heat shield needed several design iterations for Mach 10 temperatures.


== Reentry vehicles ==
Heating caused by the very high reentry speeds (greater than Mach 20) is sufficient to destroy the vehicle unless special techniques are used. The early space capsules such as used on Mercury, Gemini, and Apollo were given blunt shapes to produce a stand-off bow shock, allowing most of the heat to dissipate into the surrounding air. Additionally, these vehicles had ablative material that sublimates into a gas at high temperature. The act of sublimation absorbs the thermal energy from the aerodynamic heating and erodes the material rather than heating the capsule. The surface of the heat shield for the Mercury spacecraft had a coating of aluminum with glassfiber in many layers. As the temperature rose to 1,100 °C (1,400 K) the layers would evaporate and take the heat with it. The spacecraft would become hot but not harmfully so. The Space Shuttle used insulating tiles on its lower surface to absorb and radiate heat while preventing conduction to the aluminum airframe.  The damage to the heat shield during liftoff of Space Shuttle Columbia contributed to its destruction upon reentry.


== References ==

Moore, F.G., Approximate Methods for Weapon Aerodynamics, AIAA Progress in Astronautics and Aeronautics, Volume 186
Chapman, A.J., Heat Transfer, Third Edition, Macmillan Publishing Company, 1974
Bell Laboratories R&D, ABM Research and Development At Bell Laboratories, 1974. Stanley R. Mickelsen Safeguard Complex","pandas(index=31, _1=31, text=""aerodynamic heating is the heating of a solid body produced by its high-speed passage through air (or by the passage of air past a static body), whereby its kinetic energy is converted to heat by adiabatic heating, and by skin friction on the surface of the object at a rate that depends on the viscosity and speed of the air. in science and engineering, it is most frequently a concern regarding meteors, atmospheric reentry of spacecraft, and the design of high-speed aircraft.   == physics == when moving through air at high speeds, an object's kinetic energy is converted to heat through compression of and friction with the air. at low speeds, the object also loses heat to the air if the air is cooler. the combined temperature effect of heat from the air and from passage through it is called the stagnation temperature; the actual temperature is called the recovery temperature. these viscous dissipative effects to neighboring sub-layers make the boundary layer slow down via a non-isentropic process.  heat then conducts into the surface material from the higher temperature air. the result is an increase in the temperature of the material and a loss of energy from the flow. the forced convection ensures that other material replenishes the gases that have cooled to continue the process.the stagnation and the recovery temperature of a flow increases with the speed of the flow and are greater at high speeds. the total thermal loading of the object is a function of both the recovery temperature and the mass flow rate of the flow. aerodynamic heating is greatest at high speeds and in the lower atmosphere where the density is greater.  in addition to the convective process described above, there is also thermal radiation from the flow to the body and vice versa, with the net direction governed by their temperatures relative to each other.aerodynamic heating increases with the speed of the vehicle. its effects are minimal at subsonic speeds, but are significant enough at supersonic speeds beyond about mach 2.2 that they affect design and material considerations for the vehicle's structure and internal systems. the heating effects are greatest at leading edges, but the whole vehicle heats up to a stable temperature if its speed remains constant. aerodynamic heating is dealt with by the use of alloys that can withstand high temperatures, insulation of the exterior of the vehicle, or the use of ablative material.   == aircraft == aerodynamic heating is a concern for supersonic and hypersonic aircraft. one of the main concerns caused by aerodynamic heating arises in the design of the wing. for subsonic speeds, two main goals of wing design are minimizing weight and maximizing strength. aerodynamic heating, which occurs at supersonic and hypersonic speeds, adds an additional consideration in wing structure analysis. an idealized wing structure is made up of spars, stringers, and skin segments. in a wing that normally experiences subsonic speeds, there must be a sufficient number of stringers to withstand the axial and bending stresses induced by the lift force acting on the wing. in addition, the distance between the stringers must be small enough that the skin panels do not buckle, and the panels must be thick enough to withstand the shear stress and shear flow present in the panels due to the lifting force on the wing. however, the weight of the wing must be made as small as possible, so the choice of material for the stringers and the skin is an important factor.at supersonic speeds, aerodynamic heating adds another element to this structural analysis. at normal speeds, spars and stringers experience a load called delta p, which is a function of the lift force, first and second moments of inertia, and length of the spar. when there are more spars and stringers, the delta p in each member is reduced, and the area of the stringer can be reduced to meet critical stress requirements. however, the increase in temperature caused by energy flowing from the air (heated by skin friction at these high speeds) adds another load factor, called a thermal load, to the spars. this thermal load increases the net force felt by the stringers, and thus the area of the stringers must be increased in order for the critical stress requirement to be met.another issue that aerodynamic heating causes for aircraft design is the effect of high temperatures on common material properties. common materials used in aircraft wing design, such as aluminum and steel, experience a decrease in strength as temperatures get extremely high. the young's modulus of the material, defined as the ratio between stress and strain experienced by the material, decreases as the temperature increases. young's modulus is critical in the selection of materials for wing, as a higher value lets the material resist the yield and shear stress caused by the lift and thermal loads. this is because young's modulus is an important factor in the equations for calculating the critical buckling load for axial members and the critical buckling shear stress for skin panels. if the young's modulus of the material decreases at high temperatures caused by aerodynamic heating, then the wing design will call for larger spars and thicker skin segments in order to account for this decrease in strength as the aircraft goes supersonic. there are some materials that retain their strength at the high temperatures that aerodynamic heating induces. for example, inconel x-750 was used on parts of the airframe of the x-15, a north american aircraft that flew at hypersonic speeds in 1958. titanium is another high-strength material, even at high temperatures, and is often used for wing frames of supersonic aircraft. the sr-71 used titanium skin panels painted black to reduce the temperature and corrugated to accommodate expansion. another important design concept for early supersonic aircraft wings was using a small thickness-to-chord ratio, so that the speed of the flow over the airfoil does not increase too much from the free stream speed. as the flow is already supersonic, increasing the speed even more would not be beneficial for the wing structure. reducing the thickness of the wing brings the top and bottom stringers closer together, reducing the total moment of inertia of the structure. this increases axial load in the stringers, and thus the area, and weight, of the stringers must be increased. some designs for hypersonic missiles have used liquid cooling of the leading edges (usually the fuel en route to the engine). the sprint missile's heat shield needed several design iterations for mach 10 temperatures.   == reentry vehicles == heating caused by the very high reentry speeds (greater than mach 20) is sufficient to destroy the vehicle unless special techniques are used. the early space capsules such as used on mercury, gemini, and apollo were given blunt shapes to produce a stand-off bow shock, allowing most of the heat to dissipate into the surrounding air. additionally, these vehicles had ablative material that sublimates into a gas at high temperature. the act of sublimation absorbs the thermal energy from the aerodynamic heating and erodes the material rather than heating the capsule. the surface of the heat shield for the mercury spacecraft had a coating of aluminum with glassfiber in many layers. as the temperature rose to 1,100 °c (1,400 k) the layers would evaporate and take the heat with it. the spacecraft would become hot but not harmfully so. the space shuttle used insulating tiles on its lower surface to absorb and radiate heat while preventing conduction to the aluminum airframe.  the damage to the heat shield during liftoff of space shuttle columbia contributed to its destruction upon reentry.   == references ==  moore, f.g., approximate methods for weapon aerodynamics, aiaa progress in astronautics and aeronautics, volume 186 chapman, a.j., heat transfer, third edition, macmillan publishing company, 1974 bell laboratories r&d, abm research and development at bell laboratories, 1974. stanley r. mickelsen safeguard complex"")"
32,"In aeronautics, the rate of climb (RoC) is an aircraft's vertical speed – the positive or negative rate of altitude change with respect to time. In most ICAO member countries, even in otherwise metric countries, this is usually expressed in feet per minute (ft/min); elsewhere, it is commonly expressed in metres per second (m/s). The RoC in an aircraft is indicated with a vertical speed indicator (VSI) or instantaneous vertical speed indicator (IVSI).
The temporal rate of decrease in altitude is referred to as the rate of descent (RoD) or sink rate. 
A negative rate of climb corresponds to a positive rate of descent: RoD = −RoC.


== Speed and rate of climb ==
There are a number of designated airspeeds relating to optimum rates of ascent, the two most important of these are VX and VY.
VX is the indicated forward airspeed for best angle of climb. This is the speed at which an aircraft gains the most altitude in a given horizontal distance, typically used to avoid a collision with an object a short distance away. By contrast, VY is the indicated airspeed for best rate of climb,
a rate which allows the aircraft to climb to a specified altitude in the minimum amount of time regardless of the horizontal distance required. Except at the aircraft's ceiling, where they are equal, VX is always lower than VY.
Climbing at VX allows pilots to maximize altitude gain per horizontal distance. This occurs at the speed for which the difference between thrust and drag is the greatest (maximum excess thrust). In a jet airplane, this is approximately minimum drag speed, occurring at the bottom of the drag vs. speed curve.
Climbing at VY allows pilots to maximize altitude gain per time. This occurs at the speed where the difference between engine power and the power required to overcome the aircraft's drag is greatest (maximum excess power).Vx increases with altitude and VY decreases with altitude until they converge at the airplane's absolute ceiling, the altitude above which the airplane cannot climb in steady flight.
The Cessna 172 is a four-seat aircraft. At maximum weight it has a VY of 75 kn (139 km/h) indicated airspeed providing a rate of climb of 721 ft/min (3.66 m/s).
Rate of climb at maximum power for a small aircraft is typically specified in its normal operating procedures but for large jet airliners it is usually mentioned in emergency operating procedures.


== See also ==
ICAO recommendations on use of the International System of Units
Climb (aeronautics)
V speeds
Variometer


== References ==","pandas(index=32, _1=32, text=""in aeronautics, the rate of climb (roc) is an aircraft's vertical speed – the positive or negative rate of altitude change with respect to time. in most icao member countries, even in otherwise metric countries, this is usually expressed in feet per minute (ft/min); elsewhere, it is commonly expressed in metres per second (m/s). the roc in an aircraft is indicated with a vertical speed indicator (vsi) or instantaneous vertical speed indicator (ivsi). the temporal rate of decrease in altitude is referred to as the rate of descent (rod) or sink rate. a negative rate of climb corresponds to a positive rate of descent: rod = −roc.   == speed and rate of climb == there are a number of designated airspeeds relating to optimum rates of ascent, the two most important of these are vx and vy. vx is the indicated forward airspeed for best angle of climb. this is the speed at which an aircraft gains the most altitude in a given horizontal distance, typically used to avoid a collision with an object a short distance away. by contrast, vy is the indicated airspeed for best rate of climb, a rate which allows the aircraft to climb to a specified altitude in the minimum amount of time regardless of the horizontal distance required. except at the aircraft's ceiling, where they are equal, vx is always lower than vy. climbing at vx allows pilots to maximize altitude gain per horizontal distance. this occurs at the speed for which the difference between thrust and drag is the greatest (maximum excess thrust). in a jet airplane, this is approximately minimum drag speed, occurring at the bottom of the drag vs. speed curve. climbing at vy allows pilots to maximize altitude gain per time. this occurs at the speed where the difference between engine power and the power required to overcome the aircraft's drag is greatest (maximum excess power).vx increases with altitude and vy decreases with altitude until they converge at the airplane's absolute ceiling, the altitude above which the airplane cannot climb in steady flight. the cessna 172 is a four-seat aircraft. at maximum weight it has a vy of 75 kn (139 km/h) indicated airspeed providing a rate of climb of 721 ft/min (3.66 m/s). rate of climb at maximum power for a small aircraft is typically specified in its normal operating procedures but for large jet airliners it is usually mentioned in emergency operating procedures.   == see also == icao recommendations on use of the international system of units climb (aeronautics) v speeds variometer   == references =="")"
33,"Trajectory optimization is the process of designing a trajectory that minimizes (or maximizes) some measure of performance while satisfying a set of constraints. Generally speaking, trajectory optimization is a technique for computing an open-loop solution to an optimal control problem. It is often used for systems where computing the full closed-loop solution is not required, impractical or impossible.  If a trajectory optimization problem can be solved at a rate given by the inverse of the Lipschitz constant, then it can be used iteratively to generate a closed-loop solution in the sense of Caratheodory.  If only the first step of the trajectory is executed for an infinite-horizon problem, then this is known as Model Predictive Control (MPC).
Although the idea of trajectory optimization has been around for hundreds of years (calculus of variations, brachystochrone problem), it only became practical for real-world problems with the advent of the computer. Many of the original applications of trajectory optimization were in the aerospace industry, computing rocket and missile launch trajectories. More recently, trajectory optimization has also been used in a wide variety of industrial process and robotics applications.


== History ==
Trajectory optimization first showed up in 1697, with the introduction of the Brachystochrone problem: find the shape of a wire such that a bead sliding along it will move between two points in the minimum time. The interesting thing about this problem is that it is optimizing over a curve (the shape of the wire), rather than a single number. The most famous of the solutions was computed using calculus of variations.
In the 1950s, the digital computer started to make trajectory optimization practical for solving real-world problems. The first optimal control approaches grew out of the calculus of variations, based on the research of Gilbert Ames Bliss and Bryson in America, and Pontryagin in Russia. Pontryagin's maximum principle is of particular note. These early researchers created the foundation of what we now call indirect methods for trajectory optimization.
Much of the early work in trajectory optimization was focused on computing rocket thrust profiles, both in a vacuum and in the atmosphere. This early research discovered many basic principles that are still used today. 
Another successful application was the climb to altitude trajectories for the early jet aircraft.  Because of the high drag associated with the transonic drag region and the low thrust of early jet aircraft, trajectory optimization was the key to maximizing climb to altitude performance.  Optimal control based trajectories were responsible for some of the world records.  In these situations, the pilot followed a Mach versus altitude schedule based on optimal control solutions.
One of the important early problems in trajectory optimization was that of the singular arc, where Pontryagin's maximum principle fails to yield a complete solution. An example of a problem with singular control is the optimization of the thrust of a missile flying at a constant altitude and which is launched at low speed.  Here the problem is one of a bang-bang control at maximum possible thrust until the singular arc is reached.  Then the solution to the singular control provides a lower variable thrust until burnout.  At that point bang-bang control provides that the control or thrust go to its minimum value of zero.  This solution is the foundation of the boost-sustain rocket motor profile widely used today to maximize missile performance.


== Applications ==
There are a wide variety of applications for trajectory optimization, primarily in robotics: industry, manipulation, walking, path-planning, and aerospace. It can also be used for modeling and estimation.


=== Robotic manipulators ===
Depending on the configuration, open-chain robotic manipulators require a degree of trajectory optimization. For instance, a robotic arm with 7 joints and 7 links (7-DOF) is a redundant system where one cartesian position of an end-effector can correspond to an infinite number of joint angle positions, thus this redundancy can be used to optimize a trajectory to, for example, avoid any obstacles in the workspace or minimize the torque in the joints.


=== Quadrotor helicopters ===
Trajectory optimization is often used to compute trajectories for quadrotor helicopters. These applications typically used highly specialized algorithms.

One interesting application shown by the U.Penn GRASP Lab is computing a trajectory that allows a quadrotor to fly through a hoop as it is thrown. Another, this time by the ETH Zurich Flying Machine Arena,  involves two quadrotors tossing a pole back and forth between them, with it balanced like an inverted pendulum. The problem of computing minimum-energy trajectories for a quadcopter, has also been recently studied.


=== Manufacturing ===
Trajectory optimization is used in manufacturing, particularly for controlling chemical processes (such as in 

) or computing the desired path for robotic manipulators (such as in

).


=== Walking robots ===
There are a variety of different applications for trajectory optimization within the field of walking robotics. For example, one paper used trajectory optimization of bipedal gaits on a simple model to show that walking is energetically favorable for moving at a low speed and running is energetically favorable for moving at a high speed.

Like in many other applications, trajectory optimization can be used to compute a nominal trajectory, around which a stabilizing controller is built.

Trajectory optimization can be applied in detailed motion planning complex humanoid robots, such as Atlas.

Finally, trajectory optimization can be used for path-planning of robots with complicated dynamics constraints, using reduced complexity models.


=== Aerospace ===
For tactical missiles, the flight profiles are determined by the thrust and lift histories.  These histories can be controlled by a number of means including such techniques as using an angle of attack command history or an altitude/downrange schedule that the missile must follow.  Each combination of missile design factors, desired missile performance, and system constraints results in a new set of optimal control parameters.


== Terminology ==
Decision variables
The set of unknowns to be found using optimization.Trajectory optimization problem
A special type of optimization problem where the decision variables are functions, rather than real numbers.Parameter optimization
Any optimization problem where the decision variables are real numbers.Nonlinear program
A class of constrained parameter optimization where either the objective function or constraints are nonlinear.Indirect method
An indirect method for solving a trajectory optimization problem proceeds in three steps: 1) Analytically construct the necessary and sufficient conditions for optimality, 2) Discretize these conditions, constructing a constrained parameter optimization problem, 3) Solve that optimization problem.Direct method
A direct method for solving a trajectory optimization problem consists of two steps: 1) Discretize the trajectory optimization problem directly, converting it into a constrained parameter optimization problem, 2) Solve that optimization problem.Transcription
The process by which a trajectory optimization problem is converted into a parameter optimization problem. This is sometimes referred to as discretization. Transcription methods generally fall into two categories: shooting methods and collocation methods.Shooting method
A transcription method that is based on simulation, typically using explicit Runge--Kutta schemes.Collocation method (Simultaneous Method)
A transcription method that is based on function approximation, typically using implicit Runge--Kutta schemes.Pseudospectral method (Global Collocation)
A transcription method that represents the entire trajectory as a single high-order orthogonal polynomial.Mesh (Grid)
After transcription, the formerly continuous trajectory is now represented by a discrete set of points, known as mesh points or grid points.Mesh refinement
The process by which the discretization mesh is improved by solving a sequence of trajectory optimization problems. Mesh refinement is either performed by sub-dividing a trajectory segment or by increasing the order of the polynomial representing that segment.Multi-phase trajectory optimization problem
Trajectory optimization over a system with hybrid dynamics can be achieved by posing it as a multi-phase trajectory optimization problem. This is done by composing a sequence of standard trajectory optimization problems that are connected using constraints.


== Trajectory optimization techniques ==
The techniques to any optimization problems can be divided into two categories: indirect and direct. An indirect method works by analytically constructing the necessary and sufficient conditions for optimality, which are then solved numerically. A direct method attempts a direct numerical solution by constructing a sequence of continually improving approximations to the optimal solution. Direct and indirect methods can be blended by an application of the covector mapping principle of Ross and Fahroo.The optimal control problem is an infinite-dimensional optimization problem, since the decision variables are functions, rather than real numbers. All solution techniques perform transcription, a process by which the trajectory optimization problem (optimizing over functions) is converted into a constrained parameter optimization problem (optimizing over real numbers). Generally, this constrained parameter optimization problem is a non-linear program, although in special cases it can be reduced to a quadratic program or linear program.


=== Single shooting ===
Single shooting is the simplest type of trajectory optimization technique. The basic idea is similar to how you would aim a cannon: pick a set of parameters for the trajectory, simulate the entire thing, and then check to see if you hit the target. The entire trajectory is represented as a single segment, with a single constraint, known as a defect constraint, requiring that the final state of the simulation matches the desired final state of the system. Single shooting is effective for problems that are either simple or have an extremely good initialization. Both the indirect and direct formulation tend to have difficulties otherwise.


=== Multiple shooting ===
Multiple shooting is a simple extension to single shooting that renders it far more effective. Rather than representing the entire trajectory as a single simulation (segment), the algorithm breaks the trajectory into many shorter segments, and a defect constraint is added between each. The result is large sparse non-linear program, which tends to be easier to solve than the small dense programs produced by single shooting.


=== Direct collocation ===
Direct collocation methods work by approximating the state and control trajectories using polynomial splines. These methods are sometimes referred to as direct transcription. Trapezoidal collocation is a commonly used low-order direct collocation method. The dynamics, path objective, and control are all represented using linear splines, and the dynamics are satisfied using trapezoidal quadrature. Hermite-Simpson Collocation is a common medium-order direct collocation method. The state is represented by a cubic-Hermite spline, and the dynamics are satisfied using Simpson quadrature.


=== Orthogonal collocation ===
Orthogonal collocation is technically a subset of direct collocation, but the implementation details are so different that it can reasonably be considered its own set of methods. Orthogonal collocation differs from direct collocation in that it typically uses high-order splines, and each segment of the trajectory might be represented by a spline of a different order. The name comes from the use of orthogonal polynomials in the state and control splines.


=== Pseudospectral collocation ===
Pseudospectral collocation, also known as global collocation, is a subset of orthogonal collocation in which the entire trajectory is represented by a single high-order orthogonal polynomial. As a side note: some authors use orthogonal collocation and pseudospectral collocation interchangeably. When used to solve a trajectory optimization problem whose solution is smooth, a pseudospectral method will achieve spectral (exponential) convergence.


=== Differential dynamic programming ===
Differential dynamic programming, is a bit different than the other techniques described here. In particular, it does not cleanly separate the transcription and the optimization. Instead, it does a sequence of iterative forward and backward passes along the trajectory. Each forward pass satisfies the system dynamics, and each backward pass satisfies the optimality conditions for control. Eventually, this iteration converges to a trajectory that is both feasible and optimal.


== Comparison of techniques ==
There are many techniques to choose from when solving a trajectory optimization problem. There is no best method, but some methods might do a better job on specific problems. This section provides a rough understanding of the trade-offs between methods.


=== Indirect vs. direct methods ===
When solving a trajectory optimization problem with an indirect method, you must explicitly construct the adjoint equations and their gradients. This is often difficult to do, but it gives an excellent accuracy metric for the solution. Direct methods are much easier to set up and solve, but do not have a built-in accuracy metric. As a result, direct methods are more widely used, especially in non-critical applications. Indirect methods still have a place in specialized applications, particularly aerospace, where accuracy is critical.
One place where indirect methods have particular difficulty is on problems with path inequality constraints. These problems tend to have solutions for which the constraint is partially active. When constructing the adjoint equations for an indirect method, the user must explicitly write down when the constraint is active in the solution, which is difficult to know a priori. One solution is to use a direct method to compute an initial guess, which is then used to construct a multi-phase problem where the constraint is prescribed. The resulting problem can then be solved accurately using an indirect method.


=== Shooting vs. collocation ===
Single shooting methods are best used for problems where the control is very simple (or there is an extremely good initial guess). For example, a satellite mission planning problem where the only control is the magnitude and direction of an initial impulse from the engines.Multiple shooting tends to be good for problems with relatively simple control, but complicated dynamics. Although path constraints can be used, they make the resulting nonlinear program relatively difficult to solve.
Direct collocation methods are good for problems where the accuracy of the control and the state are similar. These methods tend to be less accurate than others (due to their low-order), but are particularly robust for problems with difficult path constraints.
Orthogonal collocation methods are best for obtaining high-accuracy solutions to problems where the accuracy of the control trajectory is important. Some implementations have trouble with path constraints. These methods are particularly good when the solution is smooth.


=== Mesh refinement: h vs. p ===
It is common to solve a trajectory optimization problem iteratively, each time using a discretization with more points. A h-method for mesh refinement works by increasing the number of trajectory segments along the trajectory, while a p-method increases the order of the transcription method within each segment.
Direct collocation methods tend to exclusively use h-method type refinement, since each method is a fixed order. Shooting methods and orthogonal collocation methods can both use h-method and p-method mesh refinement, and some use a combination, known as hp-adaptive meshing. It is best to use h-method when the solution is non-smooth, while a p-method is best for smooth solutions.


== Software ==
Examples of trajectory optimization programs include:

APMonitor: Large-scale optimization software based on orthogonal collocation.
ASTOS: Analysis, Simulation and Trajectory Optimization Software for Space Applications. The ASTOS software is a multi-purpose tool for space applications. Originally designed for trajectory optimization, it provides now modules for a variety of analysis, simulation and design capabilities
Bocop - The optimal control solver: Open source toolbox for optimal control problems (user friendly and advanced GUI for efficient use).
PyKEP, PyGMO (Open Source, from the European Space Agency for interplanetary trajectory optimization)
Copernicus Trajectory Design and Optimization System [1]
DIDO: MATLAB optimal control toolbox used at NASA and academia and distributed by Elissar Global.
QuickShot: A general-purpose, multi-threaded 3-DOF/4-DOF trajectory simulation tool for robust global optimization from SpaceWorks Enterprises, Inc.
DIRCOL: A general-purpose trajectory optimization software based on direct collocation.
Drake: A planning, control, and analysis toolbox for nonlinear dynamical systems.
FALCON.m: The FSD Optimal Control Tool for Matlab, developed at the Institute of Flight System Dynamics of Technical University of Munich.
Gekko (optimization software): A Python optimization package with trajectory optimization applications of HALE Aircraft and aerial towed cable systems.
General Mission Analysis Tool
GPOPS-II (General Purpose OPtimal Control Software) Solves multi-phase trajectory optimization problems. (Matlab)
HamPath: On solving optimal control problems by indirect and path following methods (Matlab and Python interfaces).
JModelica.org (Modelica-based open source platform for dynamic optimization)
LOTOS (Low-Thrust Orbit Transfer Trajectory Optimization Software) from Astos Solutions
MIDACO Optimization software particularly developed for interplanetary space trajectories. (Avail. in Matlab, Octave, Python, C/C++, R and Fortran)
OpenOCL Open Optimal Control Library, optimal control modeling library, automatic differentiation, non-linear optimization, Matlab/Octave.
OTIS (Optimal Trajectories by Implicit Simulation) [2]
Opty Python package utilizing SymPy for symbolic description of ordinary differential equations to form constraints needed to solve optimal control and parameter identification problems using the direct collocation method and non-linear programming.
POST (Program to Optimize Simulated Trajectories) [3], [4]
OptimTraj: An open-source trajectory optimization library for Matlab
ZOOM, Conceptual Design and Analysis of Rocket Configurations and Trajectories) [5]
PSOPT, an open source optimal control software package written in C++ that uses direct collocation methods [6]
OpenGoddard An open source optimal control software package written in Python that uses pseudospectral methods.
Systems Tool Kit Astrogator (STK Astrogator): A specialized analysis module for orbit maneuver and space trajectory design. Astrogator offers orthogonal-collocation-based trajectory optimization using high-fidelity force models.
beluga: An open source Python package for trajectory optimization using indirect methods.A collection of low thrust trajectory optimization tools, including members of the Low Thrust Trajectory Tool (LTTT) set, can be found here: LTTT Suite Optimization Tools.


== References ==","pandas(index=33, _1=33, text=""trajectory optimization is the process of designing a trajectory that minimizes (or maximizes) some measure of performance while satisfying a set of constraints. generally speaking, trajectory optimization is a technique for computing an open-loop solution to an optimal control problem. it is often used for systems where computing the full closed-loop solution is not required, impractical or impossible.  if a trajectory optimization problem can be solved at a rate given by the inverse of the lipschitz constant, then it can be used iteratively to generate a closed-loop solution in the sense of caratheodory.  if only the first step of the trajectory is executed for an infinite-horizon problem, then this is known as model predictive control (mpc). although the idea of trajectory optimization has been around for hundreds of years (calculus of variations, brachystochrone problem), it only became practical for real-world problems with the advent of the computer. many of the original applications of trajectory optimization were in the aerospace industry, computing rocket and missile launch trajectories. more recently, trajectory optimization has also been used in a wide variety of industrial process and robotics applications.   == history == trajectory optimization first showed up in 1697, with the introduction of the brachystochrone problem: find the shape of a wire such that a bead sliding along it will move between two points in the minimum time. the interesting thing about this problem is that it is optimizing over a curve (the shape of the wire), rather than a single number. the most famous of the solutions was computed using calculus of variations. in the 1950s, the digital computer started to make trajectory optimization practical for solving real-world problems. the first optimal control approaches grew out of the calculus of variations, based on the research of gilbert ames bliss and bryson in america, and pontryagin in russia. pontryagin's maximum principle is of particular note. these early researchers created the foundation of what we now call indirect methods for trajectory optimization. much of the early work in trajectory optimization was focused on computing rocket thrust profiles, both in a vacuum and in the atmosphere. this early research discovered many basic principles that are still used today. another successful application was the climb to altitude trajectories for the early jet aircraft.  because of the high drag associated with the transonic drag region and the low thrust of early jet aircraft, trajectory optimization was the key to maximizing climb to altitude performance.  optimal control based trajectories were responsible for some of the world records.  in these situations, the pilot followed a mach versus altitude schedule based on optimal control solutions. one of the important early problems in trajectory optimization was that of the singular arc, where pontryagin's maximum principle fails to yield a complete solution. an example of a problem with singular control is the optimization of the thrust of a missile flying at a constant altitude and which is launched at low speed.  here the problem is one of a bang-bang control at maximum possible thrust until the singular arc is reached.  then the solution to the singular control provides a lower variable thrust until burnout.  at that point bang-bang control provides that the control or thrust go to its minimum value of zero.  this solution is the foundation of the boost-sustain rocket motor profile widely used today to maximize missile performance.   == applications == there are a wide variety of applications for trajectory optimization, primarily in robotics: industry, manipulation, walking, path-planning, and aerospace. it can also be used for modeling and estimation. it is common to solve a trajectory optimization problem iteratively, each time using a discretization with more points. a h-method for mesh refinement works by increasing the number of trajectory segments along the trajectory, while a p-method increases the order of the transcription method within each segment. direct collocation methods tend to exclusively use h-method type refinement, since each method is a fixed order. shooting methods and orthogonal collocation methods can both use h-method and p-method mesh refinement, and some use a combination, known as hp-adaptive meshing. it is best to use h-method when the solution is non-smooth, while a p-method is best for smooth solutions.   == software == examples of trajectory optimization programs include:  apmonitor: large-scale optimization software based on orthogonal collocation. astos: analysis, simulation and trajectory optimization software for space applications. the astos software is a multi-purpose tool for space applications. originally designed for trajectory optimization, it provides now modules for a variety of analysis, simulation and design capabilities bocop - the optimal control solver: open source toolbox for optimal control problems (user friendly and advanced gui for efficient use). pykep, pygmo (open source, from the european space agency for interplanetary trajectory optimization) copernicus trajectory design and optimization system [1] dido: matlab optimal control toolbox used at nasa and academia and distributed by elissar global. quickshot: a general-purpose, multi-threaded 3-dof/4-dof trajectory simulation tool for robust global optimization from spaceworks enterprises, inc. dircol: a general-purpose trajectory optimization software based on direct collocation. drake: a planning, control, and analysis toolbox for nonlinear dynamical systems. falcon.m: the fsd optimal control tool for matlab, developed at the institute of flight system dynamics of technical university of munich. gekko (optimization software): a python optimization package with trajectory optimization applications of hale aircraft and aerial towed cable systems. general mission analysis tool gpops-ii (general purpose optimal control software) solves multi-phase trajectory optimization problems. (matlab) hampath: on solving optimal control problems by indirect and path following methods (matlab and python interfaces). jmodelica.org (modelica-based open source platform for dynamic optimization) lotos (low-thrust orbit transfer trajectory optimization software) from astos solutions midaco optimization software particularly developed for interplanetary space trajectories. (avail. in matlab, octave, python, c/c, r and fortran) openocl open optimal control library, optimal control modeling library, automatic differentiation, non-linear optimization, matlab/octave. otis (optimal trajectories by implicit simulation) [2] opty python package utilizing sympy for symbolic description of ordinary differential equations to form constraints needed to solve optimal control and parameter identification problems using the direct collocation method and non-linear programming. post (program to optimize simulated trajectories) [3], [4] optimtraj: an open-source trajectory optimization library for matlab zoom, conceptual design and analysis of rocket configurations and trajectories) [5] psopt, an open source optimal control software package written in cthat uses direct collocation methods [6] opengoddard an open source optimal control software package written in python that uses pseudospectral methods. systems tool kit astrogator (stk astrogator): a specialized analysis module for orbit maneuver and space trajectory design. astrogator offers orthogonal-collocation-based trajectory optimization using high-fidelity force models. beluga: an open source python package for trajectory optimization using indirect methods.a collection of low thrust trajectory optimization tools, including members of the low thrust trajectory tool (lttt) set, can be found here: lttt suite optimization tools.   == references =="")"
34,"Mach tuck is an aerodynamic effect whereby the nose of an aircraft tends to pitch downward as the airflow around the wing reaches supersonic speeds. This diving tendency is also known as tuck under. The aircraft will first experience this effect at significantly below Mach 1.


== Causes ==
Mach tuck is usually caused by two things, a rearward movement of the centre of pressure of the wing and a decrease in wing downwash velocity at the tailplane both of which cause a nose down pitching moment. For a particular aircraft design only one of these may be significant in causing a tendency to dive, delta-winged aircraft with no foreplane or tailplane in the first case and, for example, the Lockheed P-38 in the second case. Alternatively, a particular design may have no significant tendency, for example the Fokker F28 Fellowship.As an aerofoil generating lift moves through the air, the air flowing over the top surface accelerates to a higher local speed than the air flowing over the bottom surface. When the aircraft speed reaches its critical Mach number the accelerated airflow locally reaches the speed of sound and creates a small shock wave, even though the aircraft is still travelling below the speed of sound. The region in front of the shock wave generates high lift. As the aircraft itself flies faster, the shock wave over the wing gets stronger and moves rearwards, creating high lift further back along the wing. This rearward movement of lift causes the aircraft to tuck or pitch nose-down.
The severity of Mach tuck on any given design is affected by the thickness of the aerofoil, the sweep angle of the wing, and the location of the tailplane relative to the main wing.A tailplane which is positioned further aft can provide a larger stabilizing pitch-up moment.
The camber and thickness of the aerofoil affect the critical Mach number, with a more highly curved upper surface causing a lower critical Mach number.
On a swept wing the shock wave typically forms first at the wing root, especially if it is more cambered than the wing tip. As speed increases, the shock wave and associated lift extend outwards and, because the wing is swept, backwards.
The changing airflow over the wing can reduce the downwash over a conventional tailplane, promoting a stronger nose-down pitching moment.
Another problem with a separate horizontal stabiliser is that it can itself achieve local supersonic flow with its own shock wave. This can affect the operation of a conventional elevator control surface.
Aircraft without enough elevator authority to maintain trim and fly level can enter a steep, sometimes unrecoverable dive.  Until the aircraft is supersonic, the faster top shock wave can reduce the authority of the elevator and horizontal stabilizers.All transonic and supersonic aircraft experience Mach tuck.


== Recovery ==
Recovery is sometimes impossible in subsonic aircraft; however, as an aircraft descends into lower, warmer, denser air, control authority (meaning the ability to control the aircraft) may return because drag tends to slow the aircraft while the speed of sound and control authority both increase.
To prevent Mach stall from progressing, the pilot should keep the airspeed below the type's critical Mach number by reducing throttle, extending speed brakes, and if possible, extending the landing gear.


== Design features ==
A number of design techniques are used to counter the effects of Mach tuck.
On both conventional tailplane and canard foreplane configurations, the horizontal stabiliser may be made large and powerful enough to correct the large trim changes associated with Mach tuck. In place of the conventional elevator control surface, the whole stabiliser may be made moveable or ""all-flying"", sometimes called a stabilator. This both increases the authority of the stabilizer over a wider range of aircraft pitch, but also avoids the controllability issues associated with a separate elevator.Aircraft that fly supersonic for long periods, such as Concorde, may compensate for Mach tuck by moving fuel between tanks in the fuselage to change the position of the centre of mass to match  the changing location of the centre of pressure, thereby minimizing the amount of aerodynamic trim required.
A Mach trimmer is a device which varies the pitch trim automatically as a function of Mach number to oppose Mach tuck and maintain level flight.


== History ==
 The fastest World War II fighters were the first aircraft to experience Mach tuck.   Their wings were not designed to counter Mach tuck because research on supersonic airfoils was just beginning; areas of supersonic flow, together with shock waves and flow separation, were present on the wing. This condition was known at the time as compressibility burble and was known to exist on propeller tips at high aircraft speeds.The P-38 was the first 400 mph fighter, and it suffered more than the usual teething troubles. It had a thick, high-lift wing,  distinctive twin booms and a single, central nacelle containing the cockpit and armament.  It quickly accelerated to terminal velocity in a dive. The short stubby fuselage had a detrimental effect in reducing the critical Mach number of the 15% thick wing center section with high velocities over the canopy adding to those on the upper surface of the wing. Mach tuck occurred at speeds above Mach 0.65; the air flow over the wing center section became transonic, causing a loss of lift. The resultant change in downwash at the tail caused a nose-down pitching moment and the dive to steepen (Mach tuck). The aircraft was very stable in this condition making recovery from the dive very difficult. 
Dive recovery (auxiliary) flaps were added to the underside of the wing (P-38J-LO) to increase the wing lift and downwash at the tail to allow recovery from transonic dives.


== References ==

 This article incorporates public domain material from the United States Government document: ""Airplane Flying Handbook"". This article incorporates public domain material from the United States Government document: ""Pilot's Handbook of Aeronautical Knowledge"".","pandas(index=34, _1=34, text='mach tuck is an aerodynamic effect whereby the nose of an aircraft tends to pitch downward as the airflow around the wing reaches supersonic speeds. this diving tendency is also known as tuck under. the aircraft will first experience this effect at significantly below mach 1.   == causes == mach tuck is usually caused by two things, a rearward movement of the centre of pressure of the wing and a decrease in wing downwash velocity at the tailplane both of which cause a nose down pitching moment. for a particular aircraft design only one of these may be significant in causing a tendency to dive, delta-winged aircraft with no foreplane or tailplane in the first case and, for example, the lockheed p-38 in the second case. alternatively, a particular design may have no significant tendency, for example the fokker f28 fellowship.as an aerofoil generating lift moves through the air, the air flowing over the top surface accelerates to a higher local speed than the air flowing over the bottom surface. when the aircraft speed reaches its critical mach number the accelerated airflow locally reaches the speed of sound and creates a small shock wave, even though the aircraft is still travelling below the speed of sound. the region in front of the shock wave generates high lift. as the aircraft itself flies faster, the shock wave over the wing gets stronger and moves rearwards, creating high lift further back along the wing. this rearward movement of lift causes the aircraft to tuck or pitch nose-down. the severity of mach tuck on any given design is affected by the thickness of the aerofoil, the sweep angle of the wing, and the location of the tailplane relative to the main wing.a tailplane which is positioned further aft can provide a larger stabilizing pitch-up moment. the camber and thickness of the aerofoil affect the critical mach number, with a more highly curved upper surface causing a lower critical mach number. on a swept wing the shock wave typically forms first at the wing root, especially if it is more cambered than the wing tip. as speed increases, the shock wave and associated lift extend outwards and, because the wing is swept, backwards. the changing airflow over the wing can reduce the downwash over a conventional tailplane, promoting a stronger nose-down pitching moment. another problem with a separate horizontal stabiliser is that it can itself achieve local supersonic flow with its own shock wave. this can affect the operation of a conventional elevator control surface. aircraft without enough elevator authority to maintain trim and fly level can enter a steep, sometimes unrecoverable dive.  until the aircraft is supersonic, the faster top shock wave can reduce the authority of the elevator and horizontal stabilizers.all transonic and supersonic aircraft experience mach tuck.   == recovery == recovery is sometimes impossible in subsonic aircraft; however, as an aircraft descends into lower, warmer, denser air, control authority (meaning the ability to control the aircraft) may return because drag tends to slow the aircraft while the speed of sound and control authority both increase. to prevent mach stall from progressing, the pilot should keep the airspeed below the type\'s critical mach number by reducing throttle, extending speed brakes, and if possible, extending the landing gear.   == design features == a number of design techniques are used to counter the effects of mach tuck. on both conventional tailplane and canard foreplane configurations, the horizontal stabiliser may be made large and powerful enough to correct the large trim changes associated with mach tuck. in place of the conventional elevator control surface, the whole stabiliser may be made moveable or ""all-flying"", sometimes called a stabilator. this both increases the authority of the stabilizer over a wider range of aircraft pitch, but also avoids the controllability issues associated with a separate elevator.aircraft that fly supersonic for long periods, such as concorde, may compensate for mach tuck by moving fuel between tanks in the fuselage to change the position of the centre of mass to match  the changing location of the centre of pressure, thereby minimizing the amount of aerodynamic trim required. a mach trimmer is a device which varies the pitch trim automatically as a function of mach number to oppose mach tuck and maintain level flight.   == history == the fastest world war ii fighters were the first aircraft to experience mach tuck.   their wings were not designed to counter mach tuck because research on supersonic airfoils was just beginning; areas of supersonic flow, together with shock waves and flow separation, were present on the wing. this condition was known at the time as compressibility burble and was known to exist on propeller tips at high aircraft speeds.the p-38 was the first 400 mph fighter, and it suffered more than the usual teething troubles. it had a thick, high-lift wing,  distinctive twin booms and a single, central nacelle containing the cockpit and armament.  it quickly accelerated to terminal velocity in a dive. the short stubby fuselage had a detrimental effect in reducing the critical mach number of the 15% thick wing center section with high velocities over the canopy adding to those on the upper surface of the wing. mach tuck occurred at speeds above mach 0.65; the air flow over the wing center section became transonic, causing a loss of lift. the resultant change in downwash at the tail caused a nose-down pitching moment and the dive to steepen (mach tuck). the aircraft was very stable in this condition making recovery from the dive very difficult. dive recovery (auxiliary) flaps were added to the underside of the wing (p-38j-lo) to increase the wing lift and downwash at the tail to allow recovery from transonic dives.   == references ==  this article incorporates public domain material from the united states government document: ""airplane flying handbook"". this article incorporates public domain material from the united states government document: ""pilot\'s handbook of aeronautical knowledge"".')"
35,"A parafoil is a nonrigid (textile) airfoil with an aerodynamic cell structure which is inflated by the wind. Ram-air inflation forces the parafoil into a classic wing cross-section. Parafoils are most commonly constructed out of ripstop nylon.
The device was developed in 1964 by Domina Jalbert (1904–1991). Jalbert had a history of designing kites and was involved in the development of hybrid balloon-kite aerial platforms for carrying scientific instruments. He envisaged the parafoil would be used to suspend an aerial platform or for the recovery of space equipment. A patent was granted in 1966. US patent 3285546 
Deployment shock prevented the parafoil's immediate acceptance as a parachute. It was not until the addition of a drag canopy on the riser lines (known as a ""slider"") which slowed their spread that the parafoil became a suitable parachute. Compared to a simple round canopy, a parafoil parachute has greater steerability, will glide further and allows greater control of the rate of descent; the parachute format is mechanically a glider of the free-flight kite type and such aspects spawned paraglider use.The air flow into the parafoil is coming more from below than the flight path might suggest, so the frontmost ropes tow against the airflow. When gliding, the angle of attack is lowered and the airflow meets the parafoil head on. This makes it difficult to achieve an optimum gliding angle without the parafoil deflating.
In 1984 Jalbert was awarded the Fédération Aéronautique Internationale (FAI) Gold Parachuting Medal for inventing the parafoil.Parafoils see wide use in a variety of windsports such as kite flying, powered parachutes, paragliding, kitesurfing, speed flying, wingsuit flying and skydiving. The world's largest kite is a parafoil-variant.Today, SpaceX uses steerable Parafoils to recover the Fairings of their Falcon 9 Rocket on two ships, GO Ms. Tree and GO Ms. Chief.


== Patents ==
U.S. Patent 3,285,546 Multi-cell wing type aerial device, filed October 1964, issued November 1966


== See also ==
Foil kite


== References ==","pandas(index=35, _1=35, text='a parafoil is a nonrigid (textile) airfoil with an aerodynamic cell structure which is inflated by the wind. ram-air inflation forces the parafoil into a classic wing cross-section. parafoils are most commonly constructed out of ripstop nylon. the device was developed in 1964 by domina jalbert (1904–1991). jalbert had a history of designing kites and was involved in the development of hybrid balloon-kite aerial platforms for carrying scientific instruments. he envisaged the parafoil would be used to suspend an aerial platform or for the recovery of space equipment. a patent was granted in 1966. us patent 3285546 deployment shock prevented the parafoil\'s immediate acceptance as a parachute. it was not until the addition of a drag canopy on the riser lines (known as a ""slider"") which slowed their spread that the parafoil became a suitable parachute. compared to a simple round canopy, a parafoil parachute has greater steerability, will glide further and allows greater control of the rate of descent; the parachute format is mechanically a glider of the free-flight kite type and such aspects spawned paraglider use.the air flow into the parafoil is coming more from below than the flight path might suggest, so the frontmost ropes tow against the airflow. when gliding, the angle of attack is lowered and the airflow meets the parafoil head on. this makes it difficult to achieve an optimum gliding angle without the parafoil deflating. in 1984 jalbert was awarded the fédération aéronautique internationale (fai) gold parachuting medal for inventing the parafoil.parafoils see wide use in a variety of windsports such as kite flying, powered parachutes, paragliding, kitesurfing, speed flying, wingsuit flying and skydiving. the world\'s largest kite is a parafoil-variant.today, spacex uses steerable parafoils to recover the fairings of their falcon 9 rocket on two ships, go ms. tree and go ms. chief.   == patents == u.s. patent 3,285,546 multi-cell wing type aerial device, filed october 1964, issued november 1966   == see also == foil kite   == references ==')"
36,"Rudder ratio refers to a value that is monitored by the computerized flight control systems in modern aircraft. The ratio relates the  aircraft airspeed to the rudder deflection setting that is in effect at the time. As an aircraft accelerates, the deflection of the rudder needs to be reduced proportionately within the range of the rudder pedal depression by the pilot. This automatic reduction process is needed because if the rudder is fully deflected when the aircraft is in high-speed flight, it will cause the plane to sharply and violently yaw, or swing from side to side, leading to loss of control and rudder, tail and other damages, even causing the aircraft to crash.


== See also ==
American Airlines Flight 587","pandas(index=36, _1=36, text='rudder ratio refers to a value that is monitored by the computerized flight control systems in modern aircraft. the ratio relates the  aircraft airspeed to the rudder deflection setting that is in effect at the time. as an aircraft accelerates, the deflection of the rudder needs to be reduced proportionately within the range of the rudder pedal depression by the pilot. this automatic reduction process is needed because if the rudder is fully deflected when the aircraft is in high-speed flight, it will cause the plane to sharply and violently yaw, or swing from side to side, leading to loss of control and rudder, tail and other damages, even causing the aircraft to crash.   == see also == american airlines flight 587')"
37,"In aerodynamics, the flight envelope, service envelope, or performance envelope of an aircraft or spacecraft refers to the capabilities of a design in terms of airspeed and load factor or atmospheric density, often simplified to altitude for Earth-borne aircraft. The term is somewhat loosely applied, and can also refer to other measurements such as manoeuvrability. When a plane is pushed, for instance by diving it at high speeds, it is said to be flown ""outside the envelope"", something considered rather dangerous.
Flight envelope is one of a number of related terms that are all used in a similar fashion. It is perhaps the most common term because it is the oldest, first being used in the early days of test flying. It is closely related to more modern terms known as extra power and a doghouse plot which are different ways of describing a flight envelope. In addition, the term has been widened in scope outside the field of engineering, to refer to the strict limits in which an event will take place or more generally to the predictable behaviour of a given phenomenon or situation, and hence, its ""flight envelope"".


== Extra power ==
Extra power, or specific excess power, is a very basic method of determining an aircraft's flight envelope. It is easily calculated, but as a downside does not tell very much about the actual performance of the aircraft at different altitudes. 
Choosing any particular set of parameters will generate the needed power for a particular aircraft for those conditions. For instance a Cessna 150 at 2,500-foot (760 m) altitude and 90-mile-per-hour (140 km/h) speed needs about 60 horsepower (45 kW) to fly straight and level. The C150 is normally equipped with a 100-horsepower (75 kW) engine, so in this particular case the plane has 40 horsepower (30 kW) of extra power. In overall terms this is very little extra power, 60% of the engine's output is already used up just keeping the plane in the air. The leftover 40 hp is all that the aircraft has to manoeuvre with, meaning it can climb, turn, or speed up only a small amount. To put this in perspective, the C150 could not maintain a 2g (20 m/s²) turn, which would require a minimum of 120 horsepower (89 kW) under the same conditions.
For the same conditions a fighter aircraft might require considerably more power due to their wings being designed for high speed, high agility, or both. It could require 10,000 horsepower (7.5 MW) to achieve similar performance. However modern jet engines can provide considerable power with the equivalent of 50,000 horsepower (37 MW) not being atypical. With this amount of extra power the aircraft can achieve very high maximum rate of climb, even climb straight up, make powerful continual manoeuvres, or fly at very high speeds.


== Doghouse plot ==

A doghouse plot generally shows the relation between speed at level flight and altitude, although other variables are also possible. It takes more effort to make than an extra power calculation, but in turn provides much more information such as ideal flight altitude. The plot typically looks something like an upside-down U and is commonly referred to as a doghouse plot due to its resemblance to a kennel (sometimes known as a 'doghouse' in American English). The diagram on the right shows a very simplified plot which shall be used to explain the general shape of the plot.
The outer edges of the diagram, the envelope, show the possible conditions that the aircraft can reach in straight and level flight. For instance, the aircraft described by the black altitude envelope on the right can fly at altitudes up to about 52,000 feet (16,000 m), at which point the thinner air means it can no longer climb. The aircraft can also fly at up to Mach 1.1 at sea level, but no faster. This outer surface of the curve represents the zero-extra-power condition. All of the area under the curve represents conditions that the plane can fly at with power to spare, for instance, this aircraft can fly at Mach 0.5 at 30,000 feet (9,100 m) while using less than full power.
In the case of high-performance aircraft, including fighters, this ""1-g"" line showing straight-and-level flight is augmented with additional lines showing the maximum performance at various g loadings. In the diagram at right, the green line represents, 2-g, the blue line 3-g, and so on. The F-16 Fighting Falcon has a very small area just below Mach 1 and close to sea level where it can maintain a 9-g turn.
Flying outside the envelope is possible, since it represents the straight-and-level condition only. For instance diving the aircraft allows higher speeds, using gravity as a source of additional power. Likewise higher altitude can be reached by first speeding up and then going ballistic, a manoeuvre known as a zoom climb.


=== Stalling speed ===
All fixed-wing aircraft have a minimum speed at which they can maintain level flight, the stall speed (left limit line in the diagram). As the aircraft gains altitude the stall speed increases; since the wing is not growing any larger the only way to support the aircraft's weight with less air is to increase speed. While the exact numbers will vary widely from aircraft to aircraft, the nature of this relationship is typically the same; plotted on a graph of speed (x-axis) vs. altitude (y-axis) it forms a diagonal line.


=== Service ceiling ===
Inefficiencies in the wings also make this line ""tilt over"" with increased altitude, until it becomes horizontal and no additional speed will result in increased altitude. This maximum altitude is known as the service ceiling (top limit line in the diagram), and is often quoted for aircraft performance. The area where the altitude for a given speed can no longer be increased at level flight is known as zero rate of climb and is caused by the lift of the aircraft getting smaller at higher altitudes, until it no longer exceeds gravity.


=== Top speed ===
The right side of the graph represents the maximum speed of the aircraft. This is typically sloped in the same manner as the stall line due to air resistance getting lower at higher altitudes, up to the point where an increase in altitude no longer increases the maximum speed due to lack of oxygen to feed the engines.
The power needed varies almost linearly with altitude, but the nature of drag means that it varies with the square of speed—in other words it is typically easier to go higher than faster, up to the altitude where lack of oxygen for the engines starts to play a significant role.


== Velocity vs. load factor chart ==

A chart of velocity versus load factor (or V-n diagram) is another way of showing limits of aircraft performance. It shows how much load factor can be safely achieved at different airspeeds.At higher temperatures, air is less dense and planes must fly faster to generate the same amount of lift. High heat may reduce the amount of cargo a plane can carry, increase the length of runway a plane needs to take off,
and make it more difficult to avoid obstacles such as mountains. In unusual weather conditions this may make it unsafe or uneconomical to fly, occasionally resulting in the cancellation of commercial flights.


== Sidenotes ==
Although it is easy to compare aircraft on simple numbers such as maximum speed or service ceiling, an examination of the flight envelope will reveal far more information. Generally a design with a larger area under the curve will have better all-around performance. This is because when the plane is not flying at the edges of the envelope, its extra power will be greater, and that means more power for things like climbing or manoeuvring. General aviation aircraft have very small flight envelopes, with speeds ranging from perhaps 50 to 200 mph, whereas the extra power available to modern fighter aircraft result in huge flight envelopes with many times the area. As a tradeoff however, military aircraft often have a higher stalling speed. As a result of this the landing speed is also higher.


== ""Pushing the envelope"" ==

This phrase is used to refer to an aircraft being taken to or beyond its designated altitude and speed limits. By extension, this phrase may be used to mean testing other limits, either within aerospace or in other fields e.g. Plus ultra (motto).


== See also ==
Coffin corner (aviation)
Manoeuvring speed
Helicopter height–velocity diagram
Küssner effect


== Notes ==","pandas(index=37, _1=37, text='in aerodynamics, the flight envelope, service envelope, or performance envelope of an aircraft or spacecraft refers to the capabilities of a design in terms of airspeed and load factor or atmospheric density, often simplified to altitude for earth-borne aircraft. the term is somewhat loosely applied, and can also refer to other measurements such as manoeuvrability. when a plane is pushed, for instance by diving it at high speeds, it is said to be flown ""outside the envelope"", something considered rather dangerous. flight envelope is one of a number of related terms that are all used in a similar fashion. it is perhaps the most common term because it is the oldest, first being used in the early days of test flying. it is closely related to more modern terms known as extra power and a doghouse plot which are different ways of describing a flight envelope. in addition, the term has been widened in scope outside the field of engineering, to refer to the strict limits in which an event will take place or more generally to the predictable behaviour of a given phenomenon or situation, and hence, its ""flight envelope"".   == extra power == extra power, or specific excess power, is a very basic method of determining an aircraft\'s flight envelope. it is easily calculated, but as a downside does not tell very much about the actual performance of the aircraft at different altitudes. choosing any particular set of parameters will generate the needed power for a particular aircraft for those conditions. for instance a cessna 150 at 2,500-foot (760 m) altitude and 90-mile-per-hour (140 km/h) speed needs about 60 horsepower (45 kw) to fly straight and level. the c150 is normally equipped with a 100-horsepower (75 kw) engine, so in this particular case the plane has 40 horsepower (30 kw) of extra power. in overall terms this is very little extra power, 60% of the engine\'s output is already used up just keeping the plane in the air. the leftover 40 hp is all that the aircraft has to manoeuvre with, meaning it can climb, turn, or speed up only a small amount. to put this in perspective, the c150 could not maintain a 2g (20 m/s²) turn, which would require a minimum of 120 horsepower (89 kw) under the same conditions. for the same conditions a fighter aircraft might require considerably more power due to their wings being designed for high speed, high agility, or both. it could require 10,000 horsepower (7.5 mw) to achieve similar performance. however modern jet engines can provide considerable power with the equivalent of 50,000 horsepower (37 mw) not being atypical. with this amount of extra power the aircraft can achieve very high maximum rate of climb, even climb straight up, make powerful continual manoeuvres, or fly at very high speeds.   == doghouse plot ==  a doghouse plot generally shows the relation between speed at level flight and altitude, although other variables are also possible. it takes more effort to make than an extra power calculation, but in turn provides much more information such as ideal flight altitude. the plot typically looks something like an upside-down u and is commonly referred to as a doghouse plot due to its resemblance to a kennel (sometimes known as a \'doghouse\' in american english). the diagram on the right shows a very simplified plot which shall be used to explain the general shape of the plot. the outer edges of the diagram, the envelope, show the possible conditions that the aircraft can reach in straight and level flight. for instance, the aircraft described by the black altitude envelope on the right can fly at altitudes up to about 52,000 feet (16,000 m), at which point the thinner air means it can no longer climb. the aircraft can also fly at up to mach 1.1 at sea level, but no faster. this outer surface of the curve represents the zero-extra-power condition. all of the area under the curve represents conditions that the plane can fly at with power to spare, for instance, this aircraft can fly at mach 0.5 at 30,000 feet (9,100 m) while using less than full power. in the case of high-performance aircraft, including fighters, this ""1-g"" line showing straight-and-level flight is augmented with additional lines showing the maximum performance at various g loadings. in the diagram at right, the green line represents, 2-g, the blue line 3-g, and so on. the f-16 fighting falcon has a very small area just below mach 1 and close to sea level where it can maintain a 9-g turn. flying outside the envelope is possible, since it represents the straight-and-level condition only. for instance diving the aircraft allows higher speeds, using gravity as a source of additional power. likewise higher altitude can be reached by first speeding up and then going ballistic, a manoeuvre known as a zoom climb. the right side of the graph represents the maximum speed of the aircraft. this is typically sloped in the same manner as the stall line due to air resistance getting lower at higher altitudes, up to the point where an increase in altitude no longer increases the maximum speed due to lack of oxygen to feed the engines. the power needed varies almost linearly with altitude, but the nature of drag means that it varies with the square of speed—in other words it is typically easier to go higher than faster, up to the altitude where lack of oxygen for the engines starts to play a significant role.   == velocity vs. load factor chart ==  a chart of velocity versus load factor (or v-n diagram) is another way of showing limits of aircraft performance. it shows how much load factor can be safely achieved at different airspeeds.at higher temperatures, air is less dense and planes must fly faster to generate the same amount of lift. high heat may reduce the amount of cargo a plane can carry, increase the length of runway a plane needs to take off, and make it more difficult to avoid obstacles such as mountains. in unusual weather conditions this may make it unsafe or uneconomical to fly, occasionally resulting in the cancellation of commercial flights.   == sidenotes == although it is easy to compare aircraft on simple numbers such as maximum speed or service ceiling, an examination of the flight envelope will reveal far more information. generally a design with a larger area under the curve will have better all-around performance. this is because when the plane is not flying at the edges of the envelope, its extra power will be greater, and that means more power for things like climbing or manoeuvring. general aviation aircraft have very small flight envelopes, with speeds ranging from perhaps 50 to 200 mph, whereas the extra power available to modern fighter aircraft result in huge flight envelopes with many times the area. as a tradeoff however, military aircraft often have a higher stalling speed. as a result of this the landing speed is also higher.   == ""pushing the envelope"" ==  this phrase is used to refer to an aircraft being taken to or beyond its designated altitude and speed limits. by extension, this phrase may be used to mean testing other limits, either within aerospace or in other fields e.g. plus ultra (motto).   == see also == coffin corner (aviation) manoeuvring speed helicopter height–velocity diagram küssner effect   == notes ==')"
38,"This is an alphabetical list of articles pertaining specifically to aerospace engineering. For a broad overview of engineering, see List of engineering topics. For biographies, see List of engineers.


== A ==
Ablative laser propulsion —
Absolute value —
Acceleration —
Action —
Advanced Space Vision System —
Aeroacoustics —
Aerobrake —
Aerobraking —
Aerocapture —
Aerodynamics —
Aeroelasticity —
Aeronautical abbreviations —
Aeronautics —
Aerospace engineering —
Aerospike engine —
Aerostat —
Aft-crossing trajectory —
Aileron —
Air-augmented rocket —
Aircraft —
Aircraft flight control systems —
Aircraft flight mechanics —
Airfoil —
Airlock —
Airship —
Alcubierre drive —
Angle of attack —
Angular momentum —
Angular velocity —
Antimatter rocket —
Apsis —
Arcjet rocket —
Areal velocity —
ARP4761 —
Aspect ratio (wing) —
Astrodynamics —
Atmospheric reentry —
Attitude control —
Avionics —
[1] —


== B ==
Balloon —
Ballute —
Beam-powered propulsion —
Bernoulli's equation —
Bi-elliptic transfer —
Big dumb booster —
Bipropellant rocket —
Bleed air —
Booster rocket —
Breakthrough Propulsion Physics Program —
Buoyancy —
Bussard ramjet —–


== C ==
Canard —
Centennial challenges —
Center of gravity —
Center of mass —
Center of pressure —
Chord —
Collimated light —
Compressibility —
Computational fluid dynamics —
Computing —
Control engineering —
Conservation of momentum —
Crew Exploration Vehicle —
Critical mach —
Centrifugal compressor —
Chevron nozzle —


== D ==
De Laval nozzle —
Deflection —
Delta-v —
Delta-v budget —
Density —
Derivative —
Digital Datcom —
Displacement (vector) —
DO-178B —
DO-254 —
Drag (physics) —
Drag coefficient —
Drag equation —
Dual mode propulsion rocket —
Delta wing—


== E ==
Earth's atmosphere —
Electrostatic ion thruster —
Elliptic partial differential equation —
Energy —
Engineering —
Engineering economics —
Enstrophy —
Equation of motion —
Euler angles —
European Space Agency —
Expander cycle (rocket) —


== F ==
Field Emission Electric Propulsion —
Fixed-wing aircraft —
Flight control surfaces —
Flight control system (aircraft) —
Flight control system (helicopter) —
Flight dynamics —
Floatstick —
Fluid —
Fluid dynamics —
Fluid mechanics —
Fluid statics —
Force —
Freefall —
Fuselage —
Future Air Navigation System —
Flying wing —


== G ==
Gas-generator cycle (rocket) —
Geostationary orbit —
Geosynchronous orbit—
Glide ratio —
GPS —
Gravitational constant —
Gravitational slingshot —
Gravity —
Gravity turn —
Guidance, navigation and control —
Guidance system —


== H ==
Hall effect thruster —
Heat shield —
Helicopter —
Hohmann transfer orbit —
Hybrid rocket —
Hydrodynamics —
Hydrostatics —
Hyperbolic partial differential equation —
Hypersonic —
HyShot —


== I ==
Impulse —
Inertial navigation system —
Instrument landing system —
Integral —
Internal combustion —
Interplanetary Transport Network —
Interplanetary travel —
Interstellar travel —
Ion thruster —
ISRO


== J ==
Jet engine —


== K ==
Kepler's laws of planetary motion —
Kessler syndrome —
Kestrel rocket engine —
Kinetic energy —
Kite —
Kutta condition —
Kutta–Joukowski theorem —


== L ==
Landing —
Landing gear —
Lagrangian —
Lagrangian point —
Laser broom —
Laser Camera System —
Latus rectum —
Launch window —
Law of universal gravitation —
Leading edge —
Lift —
Lift coefficient —
Lightcraft —
Lighter than air —
Liquid air cycle engine —
Liquid fuels —
Liquid rocket propellants —
Lithobraking —
Loiter —
Low Earth orbit —
Lunar space elevator —


== M ==
Mach number —
Magnetic sail —
Magnetoplasmadynamic thruster —
Mass —
Mass driver —
Mechanics of fluids —
Membrane mirror —
Metre per second —
Microwave landing system —
Mini-magnetospheric plasma propulsion —
Missile guidance —
Moment of inertia —
Momentum —
Momentum wheel —
Monopropellant rocket —
Motion —
Multistage rocket —


== N ==
Nanotechnology —
NASA —
Navier-Stokes equations —
Newton (unit) —
Newton's laws of motion —
Nose cone design —
Nozzle —


== O ==
Orbit —
Orbit phasing —
Orbiter Boom Sensor System —
Orbital elements —
Orbital inclination change —
Orbital maneuver —
Orbital node —
Orbital period —
Orbital stationkeeping —
Osculating orbit —


== P ==
Parallel axes rule —
Parasitic drag —
Parawing —
Perpendicular axes rule —
Physics —
Planetary orbit —
Plasma (physics) —
Plug nozzle —
Pogo oscillation —
Prandtl-Glauert singularity —
Precession —
Pressure —
Pressure altitude —
Pressure-fed engine —
Propeller —
Proper orbital elements —
Pulsed inductive thruster —
Pulsed plasma thruster —
Propulsion —


== Q ==


== R ==
Radar —
Railgun —
Ram accelerator —
Ramjet —
Reaction control system —
Reentry —
Reflection —
Relativistic rocket —
Remote Manipulator System —
Resistojet rocket —
Reusable launch system —
Reynolds number —
RL-10 (rocket engine) —
Rocket —
Rocket engine nozzle —
Rocket fuel —
Rocket launch —
Rudder —


== S ==
SABRE —
Satellite —
Saturn (rocket family) —
Scalar (physics) —
Schlieren —
Schlieren photography —
Scramjet —
Second moment of area —
Shock wave —
SI —
Single-stage to orbit —
Skyhook (structure) —
Stream function —
Streamline —
Solar panel —
Solar sail —
Solar thermal rocket —
Solid of revolution —
Solid rocket —
Sound barrier —
Space activity suit —
Space elevator —
Space fountain —
Space plane —
Space Shuttle —
Space Shuttle external tank —
Space Shuttle Main Engine —
Space station —
Space suit —
Space technology —
Space transport —
Spacecraft —
Spacecraft design —
Spacecraft propulsion —
Special relativity —
Specific impulse —
Speed of sound —
Staged combustion cycle (rocket) —
Subsonic —
Supersonic —
Surface of revolution —
Sweep theory —


== T ==
Tait–Bryan rotations —
Temperature —
Terminal velocity —
Test target —
Tether propulsion —
Thermal protection system —
Thermodynamics —
Thrust —
Thrust vector control —
Thruster —
Torricelli's equation —
Trajectory —
Trailing edge —
Trans Lunar Injection —
Transonic —
Transverse wave —
Tripropellant rocket —
Tsiolkovsky rocket equation —
Turbomachinery —
Two stage to orbit —


== U ==
UFO
UAV


== V ==
V-2 rocket —
Variable specific impulse magnetoplasma rocket —
Velocity —
Viscometer —
Viscosity —
Vortex generator —


== W ==
Wave drag —
Weight —
Weight function —
Wind tunnel —
Wing —
Woodward effect —
Wright Flyer —
Wright Glider of 1902 —


== X ==


== Y ==


== Z ==","pandas(index=38, _1=38, text=""this is an alphabetical list of articles pertaining specifically to aerospace engineering. for a broad overview of engineering, see list of engineering topics. for biographies, see list of engineers.   == a == ablative laser propulsion — absolute value — acceleration — action — advanced space vision system — aeroacoustics — aerobrake — aerobraking — aerocapture — aerodynamics — aeroelasticity — aeronautical abbreviations — aeronautics — aerospace engineering — aerospike engine — aerostat — aft-crossing trajectory — aileron — air-augmented rocket — aircraft — aircraft flight control systems — aircraft flight mechanics — airfoil — airlock — airship — alcubierre drive — angle of attack — angular momentum — angular velocity — antimatter rocket — apsis — arcjet rocket — areal velocity — arp4761 — aspect ratio (wing) — astrodynamics — atmospheric reentry — attitude control — avionics — [1] —   == b == balloon — ballute — beam-powered propulsion — bernoulli's equation — bi-elliptic transfer — big dumb booster — bipropellant rocket — bleed air — booster rocket — breakthrough propulsion physics program — buoyancy — bussard ramjet —–   == c == canard — centennial challenges — center of gravity — center of mass — center of pressure — chord — collimated light — compressibility — computational fluid dynamics — computing — control engineering — conservation of momentum — crew exploration vehicle — critical mach — centrifugal compressor — chevron nozzle —   == d == de laval nozzle — deflection — delta-v — delta-v budget — density — derivative — digital datcom — displacement (vector) — do-178b — do-254 — drag (physics) — drag coefficient — drag equation — dual mode propulsion rocket — delta wing—   == e == earth's atmosphere — electrostatic ion thruster — elliptic partial differential equation — energy — engineering — engineering economics — enstrophy — equation of motion — euler angles — european space agency — expander cycle (rocket) —   == f == field emission electric propulsion — fixed-wing aircraft — flight control surfaces — flight control system (aircraft) — flight control system (helicopter) — flight dynamics — floatstick — fluid — fluid dynamics — fluid mechanics — fluid statics — force — freefall — fuselage — future air navigation system — flying wing —   == g == gas-generator cycle (rocket) — geostationary orbit — geosynchronous orbit— glide ratio — gps — gravitational constant — gravitational slingshot — gravity — gravity turn — guidance, navigation and control — guidance system —   == h == hall effect thruster — heat shield — helicopter — hohmann transfer orbit — hybrid rocket — hydrodynamics — hydrostatics — hyperbolic partial differential equation — hypersonic — hyshot —   == i == impulse — inertial navigation system — instrument landing system — integral — internal combustion — interplanetary transport network — interplanetary travel — interstellar travel — ion thruster — isro   == j == jet engine —   == k == kepler's laws of planetary motion — kessler syndrome — kestrel rocket engine — kinetic energy — kite — kutta condition — kutta–joukowski theorem —   == l == landing — landing gear — lagrangian — lagrangian point — laser broom — laser camera system — latus rectum — launch window — law of universal gravitation — leading edge — lift — lift coefficient — lightcraft — lighter than air — liquid air cycle engine — liquid fuels — liquid rocket propellants — lithobraking — loiter — low earth orbit — lunar space elevator —   == m == mach number — magnetic sail — magnetoplasmadynamic thruster — mass — mass driver — mechanics of fluids — membrane mirror — metre per second — microwave landing system — mini-magnetospheric plasma propulsion — missile guidance — moment of inertia — momentum — momentum wheel — monopropellant rocket — motion — multistage rocket —   == n == nanotechnology — nasa — navier-stokes equations — newton (unit) — newton's laws of motion — nose cone design — nozzle —   == o == orbit — orbit phasing — orbiter boom sensor system — orbital elements — orbital inclination change — orbital maneuver — orbital node — orbital period — orbital stationkeeping — osculating orbit —   == p == parallel axes rule — parasitic drag — parawing — perpendicular axes rule — physics — planetary orbit — plasma (physics) — plug nozzle — pogo oscillation — prandtl-glauert singularity — precession — pressure — pressure altitude — pressure-fed engine — propeller — proper orbital elements — pulsed inductive thruster — pulsed plasma thruster — propulsion —   == q ==   == r == radar — railgun — ram accelerator — ramjet — reaction control system — reentry — reflection — relativistic rocket — remote manipulator system — resistojet rocket — reusable launch system — reynolds number — rl-10 (rocket engine) — rocket — rocket engine nozzle — rocket fuel — rocket launch — rudder —   == s == sabre — satellite — saturn (rocket family) — scalar (physics) — schlieren — schlieren photography — scramjet — second moment of area — shock wave — si — single-stage to orbit — skyhook (structure) — stream function — streamline — solar panel — solar sail — solar thermal rocket — solid of revolution — solid rocket — sound barrier — space activity suit — space elevator — space fountain — space plane — space shuttle — space shuttle external tank — space shuttle main engine — space station — space suit — space technology — space transport — spacecraft — spacecraft design — spacecraft propulsion — special relativity — specific impulse — speed of sound — staged combustion cycle (rocket) — subsonic — supersonic — surface of revolution — sweep theory —   == t == tait–bryan rotations — temperature — terminal velocity — test target — tether propulsion — thermal protection system — thermodynamics — thrust — thrust vector control — thruster — torricelli's equation — trajectory — trailing edge — trans lunar injection — transonic — transverse wave — tripropellant rocket — tsiolkovsky rocket equation — turbomachinery — two stage to orbit —   == u == ufo uav   == v == v-2 rocket — variable specific impulse magnetoplasma rocket — velocity — viscometer — viscosity — vortex generator —   == w == wave drag — weight — weight function — wind tunnel — wing — woodward effect — wright flyer — wright glider of 1902 —   == x ==   == y ==   == z =="")"
39,"Corrected Flow is the mass flow that would pass through a device (e.g. compressor, bypass duct, etc.) if the inlet pressure and temperature corresponded to ambient conditions at Sea Level, on a Standard Day (e.g. 101.325 kPa, 288.15 K).
Corrected Flow, 
  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }}
  , can be calculated as follows, assuming Imperial Units:

  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        =
        w
        
          
            T
            
              /
            
            518.67
          
        
        
          /
        
        (
        P
        
          /
        
        14.696
        )
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }=w{\sqrt {T/518.67}}/(P/14.696)}
  
Corrected Flow is often given the symbol 
  
    
      
        w
        c
      
    
    {\displaystyle wc}
   or 
  
    
      
        w
        r
      
    
    {\displaystyle wr}
   (for referred flow).
So-called Non-Dimensional Flow, 
  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}}
  , is proportional to Corrected Flow:

  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
        =
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        ∗
        
          
            518.67
          
        
        
          /
        
        
          14.696
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}=w{\sqrt {\theta }}/{\delta }*{\sqrt {518.67}}/{14.696}}
  
The equivalent equations for Preferred SI Units are: (101.325kPa, 288.15K)

  
    
      
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        =
        w
        
          
            T
            
              /
            
            288.15
          
        
        
          /
        
        (
        P
        
          /
        
        101.325
        )
      
    
    {\displaystyle w{\sqrt {\theta }}/{\delta }=w{\sqrt {T/288.15}}/(P/101.325)}
  

  
    
      
        w
        
          
            T
          
        
        
          /
        
        
          P
        
        =
        w
        
          
            θ
          
        
        
          /
        
        
          δ
        
        ∗
        
          
            288.15
          
        
        
          /
        
        
          101.325
        
      
    
    {\displaystyle w{\sqrt {T}}/{P}=w{\sqrt {\theta }}/{\delta }*{\sqrt {288.15}}/{101.325}}
  
Nomenclature:

  
    
      
        P
      
    
    {\displaystyle P}
   Stagnation (or Total) Pressure (in kPa)

  
    
      
        T
      
    
    {\displaystyle T}
   Stagnation (or Total) Temperature (in K)

  
    
      
        w
      
    
    {\displaystyle w}
   Real Mass Flow

  
    
      
        
          δ
        
      
    
    {\displaystyle {\delta }}
   Referred Pressure

  
    
      
        
          θ
        
      
    
    {\displaystyle {\theta }}
   Referred TemperatureIn relative form, Corrected Flow, Referred Flow and Non-Dimensional Flow are all measures of axial Mach number.
Side note:
If the mass flow can be considered an energy source such as fuel flow, the corrected flow is calculated as follows:

  
    
      
        w
        
          /
        
        (
        δ
        ∗
        
          
            θ
          
        
        )
      
    
    {\displaystyle w/(\delta *{\sqrt {\theta }})}
  
Note: 
The source of coefficients is coming out from the fact that rotating components will in fact change the fluid properties, because of vibrations (compressibility of the flow change). 
So if one wants to correct the fuel flow then a theta correction exponent should be found through iterations, but for corrections for flow at engine entry (W2) then this aspect on rotating components vibration is not accounted because much smaller than one inside an engine. As a consequence the inlet flow is corrected just by square root of Theta and divided by delta.  


== See also ==
Compressor map
Turbine map
Corrected speed","pandas(index=39, _1=39, text='corrected flow is the mass flow that would pass through a device (e.g. compressor, bypass duct, etc.) if the inlet pressure and temperature corresponded to ambient conditions at sea level, on a standard day (e.g. 101.325 kpa, 288.15 k). corrected flow,    w   θ    /   δ      note: the source of coefficients is coming out from the fact that rotating components will in fact change the fluid properties, because of vibrations (compressibility of the flow change). so if one wants to correct the fuel flow then a theta correction exponent should be found through iterations, but for corrections for flow at engine entry (w2) then this aspect on rotating components vibration is not accounted because much smaller than one inside an engine. as a consequence the inlet flow is corrected just by square root of theta and divided by delta.   == see also == compressor map turbine map corrected speed')"
40,"An arming plug is a small plug that is fitted into flight hardware to enable functions that, for instrument or personnel safety, should not be activated before flight.  In the case of a missile or bomb, the (lack of the) arming plug prevents explosion before flight; in the case of a spacecraft or scientific sounding rocket, it might prevent premature firing of a hydrazine thruster system (hydrazine is extremely toxic) or block cryogenic or photographic film systems from operating before launch.","pandas(index=40, _1=40, text='an arming plug is a small plug that is fitted into flight hardware to enable functions that, for instrument or personnel safety, should not be activated before flight.  in the case of a missile or bomb, the (lack of the) arming plug prevents explosion before flight; in the case of a spacecraft or scientific sounding rocket, it might prevent premature firing of a hydrazine thruster system (hydrazine is extremely toxic) or block cryogenic or photographic film systems from operating before launch.')"
41,"Corrected speed is the speed a component would rotate at if the inlet temperature corresponded to ambient conditions at sea level, on a standard day (i.e. 288.15 K).
Corrected speed 
  
    
      
        N
        
          /
        
        
          
            θ
          
        
      
    
    {\displaystyle N/{\sqrt {\theta }}}
   can be calculated as follows:

  
    
      
        N
        
          /
        
        
          
            θ
          
        
        =
        N
        
          /
        
        
          
            T
            
              /
            
            288.15
          
        
        .
      
    
    {\displaystyle N/{\sqrt {\theta }}=N/{\sqrt {T/288.15}}.}
  Corrected speed is often abbreviated to 
  
    
      
        
          N
          
            c
          
        
      
    
    {\displaystyle N_{c}}
   or 
  
    
      
        
          N
          
            r
          
        
      
    
    {\displaystyle N_{r}}
   (for referred speed).
So-called non-dimensional speed 
  
    
      
        N
        
          /
        
        
          
            T
          
        
      
    
    {\displaystyle N/{\sqrt {T}}}
   is proportional to corrected speed:

  
    
      
        N
        
          /
        
        
          
            T
          
        
        =
        (
        N
        
          /
        
        
          
            θ
          
        
        )
        
          /
        
        
          
            288.15
          
        
      
    
    {\displaystyle N/{\sqrt {T}}=(N/{\sqrt {\theta }})/{\sqrt {288.15}}}
  Nomenclature:

  
    
      
        T
      
    
    {\displaystyle T}
   – stagnation (or total) temperature (in kelvins),

  
    
      
        N
      
    
    {\displaystyle N}
   – real shaft speed,

  
    
      
        θ
      
    
    {\displaystyle \theta }
   – referred temperature.In relative form, corrected speed, referred speed and non-dimensional speed are all measures of peripheral Mach number.


== See also ==
Compressor map
Turbine map
Corrected flow","pandas(index=41, _1=41, text='corrected speed is the speed a component would rotate at if the inlet temperature corresponded to ambient conditions at sea level, on a standard day (i.e. 288.15 k). corrected speed    n  /    θ      – referred temperature.in relative form, corrected speed, referred speed and non-dimensional speed are all measures of peripheral mach number.   == see also == compressor map turbine map corrected flow')"
42,"A constant speed drive (CSD) is a type of transmission that takes an input shaft rotating at a wide range of speeds, delivering this power to an output shaft that rotates at a constant speed, despite the varying input. They are used to drive mechanisms, typically electrical generators, that require a constant input speed.
The term is most commonly applied to hydraulic transmissions found on the accessory drives of gas turbine engines, such as aircraft jet engines. On modern aircraft, the CSD is often combined with a generator into a single unit known as an integrated drive generator (IDG).


== Mechanism ==
CSDs are mainly used on airliner and military aircraft jet engines to drive the alternating current (AC) electrical generator. In order to produce the proper voltage at a constant AC frequency, usually three-phase 115 VAC at 400 Hz, an alternator needs to spin at a constant specific speed (typically 6,000 RPM for air-cooled generators). Since the jet engine gearbox speed varies from idle to full power, this creates the need for a constant speed drive (CSD).  The CSD takes the variable speed output of the accessory drive gearbox and hydro-mechanically produces a constant output RPM.Different systems have been used to control the alternator speed. Modern designs are mostly hydrokinetic, but early designs often took advantage of the bleed air available from the engines. Some of these were mostly mechanically powered, with an air turbine to provide a vernier speed adjustment. Others were purely turbine-driven.


== Integrated drive generator ==
On aircraft such as the Airbus A310, Airbus A320 family, Airbus A320neo, Airbus A330, Airbus A330neo, Airbus A340, Boeing 737 Next Generation, 747, 757, 767 and 777, an integrated drive generator (IDG) is used. This unit is simply a CSD and an oil cooled generator inside the same case. Troubleshooting is simplified as this unit is the line-replaceable electrical generation unit on the engine.


== Manufacturers ==
Collins Aerospace (formerly UTC Aerospace Systems (formerly Hamilton Sundstrand)) is an American manufacturer of CSD and IDG units.


== Alternatives ==
A variable-speed constant-frequency (VSCF) generator can be used to provide AC power using an electronic tap converter.
Variable Frequency Starter Generator (VFSG) used primarily on the Boeing 787 are both used for electric start and electric generation.


== See also ==
Centrifugal governor
Continuously variable transmission


== References ==","pandas(index=42, _1=42, text='a constant speed drive (csd) is a type of transmission that takes an input shaft rotating at a wide range of speeds, delivering this power to an output shaft that rotates at a constant speed, despite the varying input. they are used to drive mechanisms, typically electrical generators, that require a constant input speed. the term is most commonly applied to hydraulic transmissions found on the accessory drives of gas turbine engines, such as aircraft jet engines. on modern aircraft, the csd is often combined with a generator into a single unit known as an integrated drive generator (idg).   == mechanism == csds are mainly used on airliner and military aircraft jet engines to drive the alternating current (ac) electrical generator. in order to produce the proper voltage at a constant ac frequency, usually three-phase 115 vac at 400 hz, an alternator needs to spin at a constant specific speed (typically 6,000 rpm for air-cooled generators). since the jet engine gearbox speed varies from idle to full power, this creates the need for a constant speed drive (csd).  the csd takes the variable speed output of the accessory drive gearbox and hydro-mechanically produces a constant output rpm.different systems have been used to control the alternator speed. modern designs are mostly hydrokinetic, but early designs often took advantage of the bleed air available from the engines. some of these were mostly mechanically powered, with an air turbine to provide a vernier speed adjustment. others were purely turbine-driven.   == integrated drive generator == on aircraft such as the airbus a310, airbus a320 family, airbus a320neo, airbus a330, airbus a330neo, airbus a340, boeing 737 next generation, 747, 757, 767 and 777, an integrated drive generator (idg) is used. this unit is simply a csd and an oil cooled generator inside the same case. troubleshooting is simplified as this unit is the line-replaceable electrical generation unit on the engine.   == manufacturers == collins aerospace (formerly utc aerospace systems (formerly hamilton sundstrand)) is an american manufacturer of csd and idg units.   == alternatives == a variable-speed constant-frequency (vscf) generator can be used to provide ac power using an electronic tap converter. variable frequency starter generator (vfsg) used primarily on the boeing 787 are both used for electric start and electric generation.   == see also == centrifugal governor continuously variable transmission   == references ==')"
43,"GIOVE ([ˈdʒɔːve]), or Galileo In-Orbit Validation Element, is the name for two satellites built for the European Space Agency (ESA) to test technology in orbit for the Galileo positioning system.Giove is the Italian word for ""Jupiter"". The name was chosen as a tribute to Galileo Galilei, who discovered the first four natural satellites of Jupiter, and later discovered that they could be used as a universal clock to obtain the longitude of a point on the Earth's surface.
The GIOVE satellites are operated by the GIOVE Mission  (GIOVE-M) segment in the frame of the risk mitigation for the In Orbit Validation (IOV) of the Galileo positioning system.


== Purpose ==
These validation satellites were previously known as the Galileo System Testbed (GSTB) version 2 (GSTB-V2). In 2004 the Galileo System Test Bed Version 1 (GSTB-V1) project validated the on-ground algorithms for Orbit Determination and Time Synchronization (OD&TS). This project, led by ESA and European Satellite Navigation Industries, has provided industry with fundamental knowledge to develop the mission segment of the Galileo positioning system.GIOVE satellites transmitted multifrequency ranging signals equivalent to the signals of future Galileo: L1BC, L1A, E6BC, E6A, E5a, E5b. The main purpose of the GIOVE mission was to test and validate the reception and performance of novel code modulations designed for Galileo including new signals based on the use of the BOC (Binary Offset Carrier) technique, in particular the high-performance E5AltBOC signal.


== Satellites ==


=== GIOVE-A ===
Previously known as GSTB-V2/A, this satellite was constructed by Surrey Satellite Technology Ltd (SSTL).
Its mission has the main goal of claiming the frequencies allocated to Galileo by the ITU. It has two independently developed Galileo signal generation chains and also tests the design of two on-board rubidium atomic clocks and the orbital characteristics of the intermediate circular orbit for future satellites.
GIOVE-A is the first spacecraft whose design is based upon SSTL's new Geostationary Minisatellite Platform (GMP) satellite bus, intended for geostationary orbit. GIOVE-A is also SSTL's first satellite outside low Earth orbit, operating in medium Earth orbit), and is SSTL's first satellite to use deployable Sun-tracking solar arrays. Previous SSTL satellites use body-mounted solar arrays, which generate less power per unit area as they do not face the Sun directly.


==== Launched on 28 December 2005 ====
It was launched at 05:19 UTC on December 28, 2005 on a Soyuz-FG/Fregat from the Baikonur Cosmodrome in Kazakhstan.


==== First Galileo transmissions ====
It began communicating as planned at 09:01 UTC while circling the Earth at a height of 23,222 km. The satellite successfully transmitted its first navigation signals at 17:25 GMT on 12 January 2006. These signals were received at Chilbolton Observatory in Hampshire, UK and the ESA Station at Redu in Belgium. Teams from SSTL and ESA have measured the signal generated by GIOVE-A to ensure it meets the frequency-filing allocation and reservation requirements for the International Telecommunication Union (ITU), a process that was required to be complete by June 2006.


==== Technical details ====
The GIOVE-A signal in space is fully representative of the Galileo signal from the point of view of frequencies and modulations, chip rates, and data rates. However, GIOVE-A can only transmit at two frequency bands at a time (i.e., L1+E5 or L1+E6).
GIOVE-A codes are different from Galileo codes. The GIOVE-A navigation message is not representative from the structure and contents viewpoint (demonstration only purpose). The generation of pseudorange measurements and detailed analysis of the tracking noise and multipath performance of GIOVE-A ranging signals have been performed with the use of the GETR (Galileo Experimental Test Receiver) designed by Septentrio.There has been some public controversy about the open source nature of some of the Pseudo-Random Noise (PRN) codes.  In the early part of 2006, researchers at Cornell monitored the GIOVE-A signal and extracted the PRN codes. The methods used and the codes which were found were published in the June 2006 issue of GPS World. ESA has now made the codes public.


==== Retirement ====
GIOVE A was retired (but not decommissioned) in 30 June 2012, after being raised in altitude to make way for an operational satellite. It remains under command by SSTL.


=== GIOVE-B ===
GIOVE-B (previously called GSTB-V2/B), has a similar mission, but has greatly improved signal generation hardware.
It was originally built by satellite consortium European Satellite Navigation Industries, but following re-organization of the project in 2007, the satellite prime contractor responsibility was passed to Astrium.
GIOVE-B also has MEO environment characterization objectives, as well as signal-in-space and receiver experimentation objectives. GIOVE-B carries three atomic clocks: two rubidium standards and the first space-qualified passive hydrogen maser.


==== Launched on 27 April 2008 ====

The launch was delayed due to various technical problems, and took place on 27 April 2008 at 04:16 Baikonur time (22:16 UTC Saturday) aboard a Soyuz-FG/Fregat rocket provided by Starsem. The Fregat stage was ignited three times to place the satellite into orbit. Giove-B reached its projected orbit after 02:00 UTC and successfully deployed its solar panels.


==== First Galileo navigation transmissions ====
GIOVE-B started transmitting navigation signals on May 7, 2008. The reception of the signals by GETR receivers and other means has been confirmed at a few ESA facilities.


==== Technical details ====
According to ESA, this is ""a truly historic step for satellite navigation since GIOVE-B is now, for the first time, transmitting the GPS-Galileo common signal using a specific optimised waveform, MBOC (multiplexed binary offset carrier), in accordance with the agreement drawn up in July 2007 by the EU and the US for their respective systems, Galileo and the future GPS III"".
“Now with GIOVE-B broadcasting its highly accurate signal in space we have a true representation of what Galileo will offer to provide the most advanced satellite positioning services, while ensuring compatibility and interoperability with GPS”, said ESA Galileo Project Manager, Javier Benedicto.
After launch, early orbit operations and platform commissioning, GIOVE-B's navigation payload was switched on and signal transmission commenced on May 7 and the quality of these signals is now being checked. Several facilities are involved in this process, including the GIOVE-B Control Centre at Telespazio's facilities in Fucino, Italy, the Galileo Processing Centre at ESA's European Space Research and Technology Centre (ESTEC), in the Netherlands, the ESA ground station at Redu, Belgium, and the Rutherford Appleton Laboratory (RAL) Chilbolton Observatory in the United Kingdom.
Chilbolton's 25-metre antenna makes it possible to analyse the characteristics of GIOVE-B signals with great accuracy and verify that they conform to the Galileo system's design specification. Each time the satellite is visible from Redu and Chilbolton, the large antennas are activated and track the satellite. GIOVE-B is orbiting at an altitude of 23 173 kilometres, making a complete journey around the Earth in 14 hours and 3 minutes.
The quality of the signals transmitted by GIOVE-B will have an important influence on the accuracy of the positioning information that will be provided by the user receivers on the ground. On board, GIOVE-B carries a passive hydrogen maser atomic clock, which is expected to deliver unprecedented stability performance.
The signal quality can be affected by the environment of the satellite in its orbit and by the propagation path of the signals travelling from space to ground. Additionally, the satellite signals must not create interference with services operating in adjacent frequency bands, and this is also being checked.
Galileo teams within ESA and industry have the means to observe and record the spectrum of the signals transmitted by GIOVE-B in real time. Several measurements are performed relating to transmitted signal power, centre frequency and bandwidth, as well as the format of the navigation signals generated on board. This allows the analysis of the satellite transmissions in the three frequency bands reserved for it.
The GIOVE-B mission also represents an opportunity for validating in-orbit critical satellite technologies, characterising the Medium Earth Orbit (MEO) radiation environment, and to test a key element of the future Galileo system - the user receivers.


==== Retirement ====
GIOVE B was retired (but not decommissioned) in 23 July 2012.


=== GIOVE-A2 ===
With the delays of GIOVE-B, the European Space Agency again contracted with SSTL for a second satellite, to ensure that the Galileo programme continues without any interruptions that could lead to loss of frequency allocations. Construction of GIOVE-A2 was terminated due to the successful launch and in-orbit operation of GIOVE-B.


== Mission segment ==
The GIOVE Mission segment, or GIOVE-M, is the name of a project dedicated to the exploitation and experimentation of the GIOVE satellites. The GIOVE Mission was intended to ensure risk mitigation of the In Orbit Validation (IOV) phase of the Galileo positioning system.


== GIOVE Mission history ==
The GIOVE Mission Segment began in October 2005 with the purpose of providing experimental results based on real data to be used for risk mitigation throughout the overall Galileo In Orbit Validation (IOV) phase of the Galileo positioning system. 
The GIOVE Mission segment infrastructure was based on evolution of the Galileo System Test Bed Version 1 (GSTB-V1) infrastructure conceived to process data from the GIOVE-A and GIOVE-B satellites. The GIOVE Mission segment was composed of a central processing facility called the Giove Processing Center (GPC) and a network of thirteen experimental Giove Sensor Stations (GESS).
The main objectives of the GIOVE Mission Segment experimentation were in the areas of:

On-board clock characterisation
Navigation message generation
Orbit modelling


== References ==


== External links ==
ESA GIOVE-B launch pages
GIOVE Mission Processing Centre website
eoPortal description of GIOVE
blog of GIOVE-A launch and press releases from Ballard Communications Management, used by SSTL.
Technical papers on GIOVE-A and B missions
GIOVE Mission Processing Centre - Website
eoPortal description of GIOVE","pandas(index=43, _1=43, text='giove ([ˈdʒɔːve]), or galileo in-orbit validation element, is the name for two satellites built for the european space agency (esa) to test technology in orbit for the galileo positioning system.giove is the italian word for ""jupiter"". the name was chosen as a tribute to galileo galilei, who discovered the first four natural satellites of jupiter, and later discovered that they could be used as a universal clock to obtain the longitude of a point on the earth\'s surface. the giove satellites are operated by the giove mission  (giove-m) segment in the frame of the risk mitigation for the in orbit validation (iov) of the galileo positioning system.   == purpose == these validation satellites were previously known as the galileo system testbed (gstb) version 2 (gstb-v2). in 2004 the galileo system test bed version 1 (gstb-v1) project validated the on-ground algorithms for orbit determination and time synchronization (od&ts). this project, led by esa and european satellite navigation industries, has provided industry with fundamental knowledge to develop the mission segment of the galileo positioning system.giove satellites transmitted multifrequency ranging signals equivalent to the signals of future galileo: l1bc, l1a, e6bc, e6a, e5a, e5b. the main purpose of the giove mission was to test and validate the reception and performance of novel code modulations designed for galileo including new signals based on the use of the boc (binary offset carrier) technique, in particular the high-performance e5altboc signal.   == satellites == with the delays of giove-b, the european space agency again contracted with sstl for a second satellite, to ensure that the galileo programme continues without any interruptions that could lead to loss of frequency allocations. construction of giove-a2 was terminated due to the successful launch and in-orbit operation of giove-b.   == mission segment == the giove mission segment, or giove-m, is the name of a project dedicated to the exploitation and experimentation of the giove satellites. the giove mission was intended to ensure risk mitigation of the in orbit validation (iov) phase of the galileo positioning system.   == giove mission history == the giove mission segment began in october 2005 with the purpose of providing experimental results based on real data to be used for risk mitigation throughout the overall galileo in orbit validation (iov) phase of the galileo positioning system. the giove mission segment infrastructure was based on evolution of the galileo system test bed version 1 (gstb-v1) infrastructure conceived to process data from the giove-a and giove-b satellites. the giove mission segment was composed of a central processing facility called the giove processing center (gpc) and a network of thirteen experimental giove sensor stations (gess). the main objectives of the giove mission segment experimentation were in the areas of:  on-board clock characterisation navigation message generation orbit modelling   == references ==   == external links == esa giove-b launch pages giove mission processing centre website eoportal description of giove blog of giove-a launch and press releases from ballard communications management, used by sstl. technical papers on giove-a and b missions giove mission processing centre - website eoportal description of giove')"
44,"A pilot chute is a small auxiliary parachute used to deploy the main or reserve parachute. The pilot chute is connected to the deployment bag containing the parachute by a bridle. Pilot chutes are a critical component of all modern skydiving and BASE jumping gear. Pilot chutes are also used as a component of spacecraft such as NASA's Orion.


== Deployment methods ==


=== Spring-loaded ===
The spring-loaded pilot chute is used in conjunction with a ripcord. When the user pulls the ripcord, the container opens, allowing the pilot chute compressed inside and loaded with a large spring inside it to jump out. Spring-loaded pilot chutes are mainly used to deploy reserve parachutes.  They are often also used to deploy the main parachute on skydiving students' parachute equipment. They are also commonly used in drogue parachute in cars or in planes such as the B52 Bomber.


=== Pull-out ===
The pull-out and throw-out pilot chutes are identical in construction; the difference is in their connection to the handle and the bridle, and in the way they are packed.
With the pull-out system, the pilot chute is packed inside the container. The activation handle is attached to a lanyard, which in turn is attached to the closing pin. The lanyard is also attached to base of the pilot chute, at the point of connection to the bridle. When the user pulls the handle, the closing pin is pulled, opening the container. Continuing the pull, the user pulls the pilot chute out of the container and into the airstream, at which point the pilot chute inflates and pulls the main parachute out of the container.


=== Throw-out ===
The throw-out pilot chute is the most popular type in use today. The pilot chute is packed in a pouch at the bottom of the container (often called BOC for short). The handle is attached to the apex of the pilot chute. When the user grabs the handle and throws the pilot chute into the airstream, the bridle extends, pulling the closing pin and opening the container, as the pilot chute continues in the airstream it extracts the deployment bag containing the main parachute from the container.  The pull-out pilot chute and the throw-out pilot chute were both invented by Bill Booth.


=== Drogues ===
Drogues used on tandem-systems are basically large throw-out pilot chutes, but the bridle is anchored on the container with a release system. When the user throws the drogue, the drogue inflates and the bridle extends. The deployed drogue slows down the free-fall speed of the tandem pair. When the user wants to open the parachute, he pulls a ripcord, releasing the bridle and allowing the drogue to open the main container.


== Types ==


=== Collapsible ===
With the advent of smaller higher performance canopies, the drag induced by trailing a pilot chute behind a canopy has become a significant concern. To reduce this drag some pilot chute designs of the Pull-out and Throw-out variety are collapsible. Once deployment of the parachute has occurred a kill line running up the center of the pilot chute bridle becomes loaded. This kill line pulls down on the apex of the pilot chute collapsing it and greatly reducing its drag on the canopy.Some designs replace the kill line with a fixed length of shock cord, which stretches when the pilot chute is moving quickly, allowing it to inflate. When the pilot slows down (after opening a canopy, for example) the shock cord retracts, killing the pilot chute. While this avoids the possibility of pilot-in-tow malfunction due to an un-cocked pilot, it has the disadvantage of requiring significant airspeed to operate. This could cause a delayed deployment if used for a BASE or balloon jump, or any other jump with a low speed deployment. This type may also begin to re-inflate behind a highly loaded, fast moving canopy, negating the usefulness of a collapsible pilot chute.


=== Vented ===
Pilot chutes for BASE jumping gear are typically larger than skydiving pilot chutes, and often include air vents on the surface. Research on the development of early round parachutes showed that vents can increase stability and reduce oscillation of the parachute. BASE jumpers often use pilot chutes with either apex vents, or ring vents.


== References ==","pandas(index=44, _1=44, text=""a pilot chute is a small auxiliary parachute used to deploy the main or reserve parachute. the pilot chute is connected to the deployment bag containing the parachute by a bridle. pilot chutes are a critical component of all modern skydiving and base jumping gear. pilot chutes are also used as a component of spacecraft such as nasa's orion.   == deployment methods == pilot chutes for base jumping gear are typically larger than skydiving pilot chutes, and often include air vents on the surface. research on the development of early round parachutes showed that vents can increase stability and reduce oscillation of the parachute. base jumpers often use pilot chutes with either apex vents, or ring vents.   == references =="")"
45,"Aerospace architecture is broadly defined to encompass architectural design of non-habitable and habitable structures and living and working environments in aerospace-related facilities, habitats, and vehicles. These environments include, but are not limited to: science platform aircraft and aircraft-deployable systems; space vehicles, space stations, habitats and lunar and planetary surface construction bases; and Earth-based control, experiment, launch, logistics, payload, simulation and test facilities. Earth analogs to space applications may include Antarctic, desert, high altitude, underground, undersea environments and closed ecological systems.
The American Institute of Aeronautics and Astronautics (AIAA) Design Engineering Technical Committee (DETC) meets several times a year to discuss policy, education, standards, and practice issues pertaining to aerospace architecture. See http://www.spacearchitect.org/


== The role of Appearance in Aerospace architecture ==
""The role of design creates and develops concepts and specifications that seek to simultaneously and synergistically optimize function, production, value and appearance."" In connection with, and with respect to, human presence and interactions, appearance is a component of human factors and includes considerations of human characteristics, needs and interests.
Appearance in this context refers to all visual aspects – the statics and dynamics of form(s), color(s), patterns, and textures in respect to all products, systems, services, and experiences. Appearance/esthetics affects humans both psychologically and physiologically and can effect/improving both human efficiency, attitude, and well-being. 
In reference to non-habitable design the influence of appearance is minimal if not non-existent. However, as the industry of aerospace continues to rapidly grow, and missions to put humans on Mars and back to the Moon are being announced. The role that appearance/esthetics to maintain crew well-being and health of multi-month or year missions becomes a monumental factor in mission success. 


== Habitable Structures within Earth's Atmosphere ==


=== Appearance/esthetics ===
Appearance/esthetics in aerospace design must at least co-exist, if not be synergistic, with the overall/societal fundamentals/metrics of aerospace engineering design. These metrics, for atmospheric flight consist of overall/societal factors directed toward productivity, safety, environmental issues such as noise/emissions and accessibly/ affordability. Furthermore, technological parameters such as space, weight and drag minimization and propulsion efficiency highly dictate and restrain the boundaries of appearance/esthetic design. Major factors that need to be considered in atmospheric flight design include producability, maintainability, reliability, flyability, inspectability, flexibility, repairability, operability, durability, and airport compatibility. 


== Habitable Structures outside of Low-Earth Orbit (LEO) ==
What is different concerning space in reference to human-centered design thinking is the nearly complete lack of human presence. Human-centered design influence wholly operates within the context of human interactions; how operations/ missions are ran (operability) or how products, systems, services, or experiences (PSSE’s) affect end users (usability). Currently the human presence involves the space station and the relatively few international rocket systems.


=== Human-Centered Design ===
Due to the large space boom and technological advancements, over the past decade numerous countries and companies have released statements that human expeditions to our solar system are far from done. With long duration confinement in limited interior space in micro-g with little-to-no real variability in environment, attention towards user [crew] subjects well-being, and mental alertness will pose complex human-centered design issues. Mars transit vehicles and surface habitats will constitute highly confined, technical settings characterized by social, emotional and physical deprivation while affording little opportunity to experience privacy and environmental variation. And esthetic/appearance measures for human exploration will emphasize upon “naturalistic countermeasures” to the innate/multitudinous stresses of such expeditions.
Although human wants, needs, and limitations both physically and mentally need to be evaluated and address when designing for space. Design decisions must at least co-exist, if not be synergistic, with the overall metrics of aerospace engineering design. Ex. The International Space Station Toilet. 


=== Past Examples ===
A study conducted in 1989 (reference 2) found that when given multiple photographs and paintings as potential decoration of the international space station. Test (crew) subjects all individually preferred those with naturalistic, irrespective themes, and a large depth of field. Other examples of human-centered design is using pastel paints on the International Space Station (ISS) to contrast and provide “up/down” cues in micro-g environments or  the concept of dynamically and spatially adjusting lighting color and intensities to conform to daily and even seasonal biorhythms similar to earth to mitigate the societal separation effects experienced in space.


== See also ==
Airborne observatory
Atmosphere of Venus
High Altitude Venus Operational Concept (HAVOC)
Colonization of Venus
Floating city (science fiction)


== External links ==
American Institute of Aeronautics and Astronautics
[1]
Design Engineering Technical Committee of the AIAA
Spacearchitect.org
Sasakawa International Center for Space Architecture (SICSA)
MOTHER Aerospace Architecture consultancy
Architecture and Vision, Design Studio specializing on Aerospace Architecture and Technology Transfer
LIQUIFER Systems Group, interdisciplinary design team developing architecture, design and systems for Earth and Space
Synthesis, a fundamental design collaborative with experts from Space Architecture, Engineering and Industrial Design
Earth2Orbit, Satellite & Launch Services, Human Space Systems, Robotic Systems, Infrastructure and High-Tech Facilities, Consulting
The Galactic Suite Space Hotel
Galactic Suite Design Aerospace Architecture and Experiences","pandas(index=45, _1=45, text='aerospace architecture is broadly defined to encompass architectural design of non-habitable and habitable structures and living and working environments in aerospace-related facilities, habitats, and vehicles. these environments include, but are not limited to: science platform aircraft and aircraft-deployable systems; space vehicles, space stations, habitats and lunar and planetary surface construction bases; and earth-based control, experiment, launch, logistics, payload, simulation and test facilities. earth analogs to space applications may include antarctic, desert, high altitude, underground, undersea environments and closed ecological systems. the american institute of aeronautics and astronautics (aiaa) design engineering technical committee (detc) meets several times a year to discuss policy, education, standards, and practice issues pertaining to aerospace architecture. see http://www.spacearchitect.org/   == the role of appearance in aerospace architecture == ""the role of design creates and develops concepts and specifications that seek to simultaneously and synergistically optimize function, production, value and appearance."" in connection with, and with respect to, human presence and interactions, appearance is a component of human factors and includes considerations of human characteristics, needs and interests. appearance in this context refers to all visual aspects – the statics and dynamics of form(s), color(s), patterns, and textures in respect to all products, systems, services, and experiences. appearance/esthetics affects humans both psychologically and physiologically and can effect/improving both human efficiency, attitude, and well-being. in reference to non-habitable design the influence of appearance is minimal if not non-existent. however, as the industry of aerospace continues to rapidly grow, and missions to put humans on mars and back to the moon are being announced. the role that appearance/esthetics to maintain crew well-being and health of multi-month or year missions becomes a monumental factor in mission success.   == habitable structures within earth\'s atmosphere == a study conducted in 1989 (reference 2) found that when given multiple photographs and paintings as potential decoration of the international space station. test (crew) subjects all individually preferred those with naturalistic, irrespective themes, and a large depth of field. other examples of human-centered design is using pastel paints on the international space station (iss) to contrast and provide “up/down” cues in micro-g environments or  the concept of dynamically and spatially adjusting lighting color and intensities to conform to daily and even seasonal biorhythms similar to earth to mitigate the societal separation effects experienced in space.   == see also == airborne observatory atmosphere of venus high altitude venus operational concept (havoc) colonization of venus floating city (science fiction)   == external links == american institute of aeronautics and astronautics [1] design engineering technical committee of the aiaa spacearchitect.org sasakawa international center for space architecture (sicsa) mother aerospace architecture consultancy architecture and vision, design studio specializing on aerospace architecture and technology transfer liquifer systems group, interdisciplinary design team developing architecture, design and systems for earth and space synthesis, a fundamental design collaborative with experts from space architecture, engineering and industrial design earth2orbit, satellite & launch services, human space systems, robotic systems, infrastructure and high-tech facilities, consulting the galactic suite space hotel galactic suite design aerospace architecture and experiences')"
46,"A trijet is a jet aircraft powered by three jet engines. In general, passenger airline trijets are considered to be second-generation jet airliners, due to their innovative engine locations, in addition to the advancement of turbofan technology.
Other variations of three-engine designs are trimotors, which are aircraft with three propellers (driven by piston engines or turboprops).


== Design ==

One issue with trijets is positioning the central engine. This is mostly accomplished by placing the engine along the centerline, but this still poses difficulties.
The most common configuration is having the central engine located in the rear fuselage and supplied with air by an S-shaped duct; this is used on the Hawker Siddeley Trident, Boeing 727, Tupolev Tu-154, Lockheed L-1011 TriStar, and, more recently, the Dassault Falcon 7X. The S-duct has low drag, and since the third engine is mounted closer to the centerline, the aircraft will normally be easy to handle in the event of an engine failure. However, S-duct designs are extremely complex and costly. Furthermore, the central engine bay would require structural changes in the event of a major re-engining. For example, the 727's central bay was only wide enough to fit a low-bypass turbofan and not the newer high-bypass turbofans which were quieter and more powerful. Boeing decided that a redesign was too expensive and ended its production instead of pursuing further development. The Lockheed Tristar's tail section was too short to fit an existing two-spool engine as it was designed only to accommodate the new three-spool Rolls-Royce RB211 engine, and delays in the RB211's development, in turn, pushed back the TriStar's entry into service which affected sales.The McDonnell Douglas DC-10 and related MD-11 use an alternative ""straight-through"" layout, which allows for easier engine installation, modification, and access. It also has the additional benefit of being much easier to re-engine. However, this sacrifices aerodynamics compared to the S-duct. Also, as the engine is located much higher up than the wing-mounted engines, engine failure will produce a greater pitching moment, making it more difficult to control.
The placement of the remaining two engines varies. Most smaller aircraft, like the Hawker Siddeley Trident, the Boeing 727 and the Tupolev Tu-154 have two side-mount engine pylons in a T-tail configuration. The larger widebody Lockheed TriStar and DC-10/MD-11 mount an engine underneath each wing.  Preliminary studies were done on the TriStar to reuse the fuselage and wing for a twinjet design though these never materialized due to Lockheed's lack of funds. Additionally in the late-1990s Boeing, which had taken over McDonnell Douglas, considered removing the tail engine from the MD-11 to make it a twinjet but ending up instead ending its production altogether, as the 767 and 777 would have cannibalized it.


== Advantages and drawbacks ==
One major advantage of the trijet design is that the wings can be located further aft on the fuselage, compared to twinjets and quadjets with all wing-mounted engines, allowing main cabin exit and entry doors to be more centrally located for quicker boarding and deplaning, ensuring shorter turnaround times. The rear-mounted engine and wings shift the aircraft's center of gravity rearwards, improving fuel efficiency, although this will also make the plane slightly less stable and more difficult to handle during takeoff and landing. (The McDonnell Douglas DC-9 twinjet and its derivatives, whose engines are mounted on pylons near the rear empennage, have similar advantages/disadvantages of the trijet design, such as the wings located further aft and a more rearward center of gravity.)
Trijets are more efficient and cheaper than four-engine aircraft, as the engines are the most expensive part of the plane and having more engines consumes more fuel, particularly if quadjets and trijets share engines of similar power, making the trijet configuration more suited to a mid-size airliner compared to larger quadjets. However, higher purchase prices, primarily due to the difficulty and complexity of mounting the third engine through the tail, will somewhat negate this advantage.
Due to their added thrust, trijets will have slightly improved takeoff performance compared to twinjets if an engine fails. Because takeoff performance for aircraft is usually calculated to include an extra margin to account for a possible engine failure, trijets are better able to take off from hot and high airports or those where terrain clearance near the runway is an issue.
Unlike twinjets, trijets are not required to land immediately at the nearest suitable airport if one engine fails (this advantage is also shared with quadjets). This is advantageous if the aircraft is not near one of the operator's maintenance bases, as the pilots may then continue the flight and land at an airport where it is more suitable to perform repairs. Additionally, for trijets on the ground with one engine inoperative, approval can be granted to perform two-engine ferry flights. Prior to the introduction of ETOPS, only trijets and quadjets were able to perform long international flights over areas without any diversion airports. However, this advantage has largely disappeared in recent years as ETOPS-certified twin-engined aircraft are able to do so as well.
The biggest obstacle trijets face today is operating costs, primarily fuel efficiency, as a three-engine design almost certainly consumes more fuel than a comparable two-engine design. This also greatly increases the difficulty of marketing a new trijet aircraft today, especially for passenger service. However this was worth the trade-off between 1970 and the 1990s when trijets and twinjets shared engines of similar output, such as when the DC-10, MD-11, Boeing's 767, and Airbus's A300, A310, and A330 were all powered by the General Electric CF6, and the additional power from the third engine gave the DC-10/MD-11 advantages in longer range and/or heavier payload over the A300/A330 twinjet. Since the 1990s, with further advancements in high-bypass turbofan technology, large twinjets have been equipped with purpose-designed engines like the Boeing 777's General Electric GE90, allowing twinjets to perform the same tasks as most trijets and even many quadjets but more efficiently.


== History ==

The first trijet design to fly was the Tupolev Tu-73 bomber prototype, first flown in 1947. The first commercial trijets were the Hawker Siddeley Trident (1962) and the Boeing 727 (1963). Both were compromises to meet airline requirements; in the case of the Trident, it was to meet BEA's changing needs, while the 727 had to be acceptable for three different airlines. Although collaboration between the manufacturers was considered, it did not come about.Early American twinjet designs were limited by the FAA's 60-minute rule, whereby the flight path of twin-engine jetliners was restricted to within 60 minutes' flying time from a suitable airport, in case of engine failure. In 1964, this rule was lifted for trijet designs, as they had a greater safety margin.
For second-generation jet airliners, with the innovations of the high-bypass turbofan for greater efficiency and reduced noise, and the wide-body (twin-aisle) for greater passenger/cargo capacity, the trijet design was seen as the optimal configuration for the medium wide-body jet airliner, sitting in terms of size, range, and cost between quadjets (four-engine aircraft) and twinjets, and this led to a flurry of trijet designs. The four-engine Boeing 747 was popular for transoceanic flights due to its long-range and large size, but it was expensive and not all routes were able to fill its seating capacity, while the original models of the Airbus A300 twinjet were limited to short- to medium-range distances. During this period, different jet airliners shared engines of similar output, such as when the McDonnell Douglas DC-10, Airbus A300, and Boeing 767 were powered by the General Electric CF6, the additional power from the third engine gave the DC-10 advantages in longer range and/or heavier payload over the A300 and 767 twinjets. Thus trijet designs such as the DC-10 and L-1011 TriStar represented the best compromise with medium- to long-range and medium size that US airlines sought for their domestic and transatlantic routes. As a result of these trijet wide-bodies, as well as the popularity of the Boeing 727, in their heyday of the 1980s trijets made up a majority of all such US jet airliners.
From 1985 to 2003 the number of such planes in service had sunk from 1488 to 602. The number of twinjets, on the other hand, had more than quadrupled in the same period. Both Lockheed and McDonnell Douglas were financially weakened competing in the widebody market, which led to Lockheed ending production of the L-1011 in 1984 after producing only half the units needed to break even, while a number of fatal DC-10 crashes also slowed its sales. In 1984 Boeing ended production of the 727, as its central engine bay would require an extremely expensive redesign to accommodate quieter high-bypass turbofans, and it was soon supplanted by Airbus with their A320 and Boeing with their 737 and 757. Further advancements in high-bypass turbofan technology and subsequent relaxation in airline safety rules made the trijet and even the quadjet nearly obsolete for passenger services, as their range and payload could be covered more efficiently with large twinjets powered with purpose-designed engines such as the 777's General Electric GE90.
During the 1980s, McDonnell Douglas was the only Western manufacturer to continue development of the trijet design with an update to the DC-10, the MD-11, which initially held a range and payload advantage over its closest medium wide-body competitors which were twinjets, the in-production Boeing 767 and upcoming Airbus A330. McDonnell Douglas had planned a new trijet called the MD-XX, which were lengthened versions of the MD-11. The MD-XX Long Range aircraft would have been capable of traveling distances up to 8,320 nautical miles (15,410 km) and had a wingspan of 65 metres (213 ft). The project was canceled in 1996, one year before McDonnell Douglas was acquired by Boeing. Boeing ended production of the MD-11 after filling remaining customer orders since the MD-11 would have competed with the 767 and 777. A study to remove the MD-11's tail-mounted engine (which would have made it a twinjet) never came to fruition as it would have been very expensive, and the MD-11 had very little in common in terms of design or type rating with other Boeing airliners. In contrast to McDonnell Douglas sticking with their existing trijet configuration, Airbus (which never produced a trijet aircraft) and Boeing worked on new widebody twinjet designs that would become the A330 and 777, respectively. The MD-11's long-range advantage was brief as it soon was threatened by the A330's four-engine derivative, the A340, and the 777. The only other notable trijet development during the 1980s was in the Soviet Union, where the Tupolev Tu-154 was re-engined with the Soloviev D-30 engine as well as a new wing design and entered serial production from 1984 as the Tu-154M.
With the exception of the Dassault Falcon 7X, Falcon 8x, and Falcon 900, no manufacturer now produces three-engine airliners.


=== Current status ===
Modern engines have extremely low failure rates and can generate much higher shaft power and thrust than early types. This makes twinjets more suitable than they were before for long-haul trans-oceanic operations, resulting in eased ETOPS restrictions; modern wide-body twin-engine jets usually have an ETOPS 180 or even (in the case of the Boeing 777 and 787) ETOPS 330 rating. As such, having more than two engines is no longer considered necessary, except for very large or heavy aircraft such as the Boeing 747, Airbus A380 (over 400 seats in a mixed-class configuration), Antonov An-124, and An-225, or for flights through the Southern Hemisphere, primarily to and from Australia (which has not yet adopted the ETOPS 330 standard), where the most direct route for some flights is over Antarctica.Today, both narrow-body and wide-body trijet production has ceased for almost all commercial aircraft, being replaced by twinjets. As of 2016, the Falcon 7X, 8X, and 900 business jets, all of which use S-ducts, are the only trijets in production. Some old models, such as the 727, Tu-154, DC-10, and MD-11, have found second careers as cargo aircraft, as well as limited charter, governmental, and military service. The most widely used trijets are the DC-10 and the MD-11, mostly operated by UPS Airlines and FedEx Express in cargo service.
For private and corporate operators, where fuel efficiency is often less important than for airlines, trijets may still be of interest due to their immunity from ETOPS and the ability to take off from shorter runways. As a result, a sizeable number of trijets, such as 727s and newly built Dassault Falcons, are in use by private operators and corporate flight departments.


=== Future of trijets ===
Airbus filed a patent in 2008 for a new, twin-tail trijet design, whose tail engine appears to use a ""straight"" layout similar to the MD-11, but it is unknown if and when this will be developed or produced. However, the proposed Boeing X-48 blended wing body design, Lockheed's N+2 design study, and Aerion AS2 supersonic business jet also have three engines. The AS2 is currently taking orders and a wooden mockup has been constructed.Boom Technology's planned Overture supersonic transport (SST) airliner is planned to use three engines, with the third engine installed in the tail with a Y-shaped duct and air intakes on both sides of the rear.


== Examples ==
Boeing 727
Boeing X-48
Dassault Falcon 50
Dassault Falcon 900
Dassault Falcon 7X
Dassault Falcon 8X
Hawker Siddeley Trident
Lockheed L-1011 TriStar
Martin XB-51
McDonnell Douglas DC-10
McDonnell Douglas MD-11
Tupolev Tu-154
Yakovlev Yak-40
Yakovlev Yak-42


== Proposed or suspended trijet developments ==
Boeing 747-300 Trijet – downsized 747 to compete with the DC-10 and L-1011, changed to four engines
Blended Wing Body Trijet – proposed design based on the Boeing X-48
McDonnell Douglas MD-XX – stretched derivative of the DC-10, project shelved
North American NR-349 – proposed interceptor derivative of the A-5 Vigilante, cancelled
Airbus twin-tail trijet, – status unknown
Dassault Supersonic Business Jet – suspended
Aerion AS2
Sukhoi-Gulfstream S-21
Boom Technology Overture
Boeing 777 – Originally envisioned as a trijet 767 in the 1970s to compete with the DC-10 and the L-1011; later became a new twin-engine design.


== See also ==
Quadjet
S-duct
Trimotor
Twinjet
Wide-body aircraft


== References ==

Modern Commercial Aircraft Willian Green, Gordon Swanborough and John Mowinski, 1987


== External links ==
Stanford University Aircraft Aerodynamics and Design Group Engine Placement Accessed 2007-03-13
Undeveloped MD-11/MD-12 models page
Patent for a triple engine fighter
Patent for a triple engine fighter
NR-349 interceptor proposal","pandas(index=46, _1=46, text='a trijet is a jet aircraft powered by three jet engines. in general, passenger airline trijets are considered to be second-generation jet airliners, due to their innovative engine locations, in addition to the advancement of turbofan technology. other variations of three-engine designs are trimotors, which are aircraft with three propellers (driven by piston engines or turboprops).   == design ==  one issue with trijets is positioning the central engine. this is mostly accomplished by placing the engine along the centerline, but this still poses difficulties. the most common configuration is having the central engine located in the rear fuselage and supplied with air by an s-shaped duct; this is used on the hawker siddeley trident, boeing 727, tupolev tu-154, lockheed l-1011 tristar, and, more recently, the dassault falcon 7x. the s-duct has low drag, and since the third engine is mounted closer to the centerline, the aircraft will normally be easy to handle in the event of an engine failure. however, s-duct designs are extremely complex and costly. furthermore, the central engine bay would require structural changes in the event of a major re-engining. for example, the 727\'s central bay was only wide enough to fit a low-bypass turbofan and not the newer high-bypass turbofans which were quieter and more powerful. boeing decided that a redesign was too expensive and ended its production instead of pursuing further development. the lockheed tristar\'s tail section was too short to fit an existing two-spool engine as it was designed only to accommodate the new three-spool rolls-royce rb211 engine, and delays in the rb211\'s development, in turn, pushed back the tristar\'s entry into service which affected sales.the mcdonnell douglas dc-10 and related md-11 use an alternative ""straight-through"" layout, which allows for easier engine installation, modification, and access. it also has the additional benefit of being much easier to re-engine. however, this sacrifices aerodynamics compared to the s-duct. also, as the engine is located much higher up than the wing-mounted engines, engine failure will produce a greater pitching moment, making it more difficult to control. the placement of the remaining two engines varies. most smaller aircraft, like the hawker siddeley trident, the boeing 727 and the tupolev tu-154 have two side-mount engine pylons in a t-tail configuration. the larger widebody lockheed tristar and dc-10/md-11 mount an engine underneath each wing.  preliminary studies were done on the tristar to reuse the fuselage and wing for a twinjet design though these never materialized due to lockheed\'s lack of funds. additionally in the late-1990s boeing, which had taken over mcdonnell douglas, considered removing the tail engine from the md-11 to make it a twinjet but ending up instead ending its production altogether, as the 767 and 777 would have cannibalized it.   == advantages and drawbacks == one major advantage of the trijet design is that the wings can be located further aft on the fuselage, compared to twinjets and quadjets with all wing-mounted engines, allowing main cabin exit and entry doors to be more centrally located for quicker boarding and deplaning, ensuring shorter turnaround times. the rear-mounted engine and wings shift the aircraft\'s center of gravity rearwards, improving fuel efficiency, although this will also make the plane slightly less stable and more difficult to handle during takeoff and landing. (the mcdonnell douglas dc-9 twinjet and its derivatives, whose engines are mounted on pylons near the rear empennage, have similar advantages/disadvantages of the trijet design, such as the wings located further aft and a more rearward center of gravity.) trijets are more efficient and cheaper than four-engine aircraft, as the engines are the most expensive part of the plane and having more engines consumes more fuel, particularly if quadjets and trijets share engines of similar power, making the trijet configuration more suited to a mid-size airliner compared to larger quadjets. however, higher purchase prices, primarily due to the difficulty and complexity of mounting the third engine through the tail, will somewhat negate this advantage. due to their added thrust, trijets will have slightly improved takeoff performance compared to twinjets if an engine fails. because takeoff performance for aircraft is usually calculated to include an extra margin to account for a possible engine failure, trijets are better able to take off from hot and high airports or those where terrain clearance near the runway is an issue. unlike twinjets, trijets are not required to land immediately at the nearest suitable airport if one engine fails (this advantage is also shared with quadjets). this is advantageous if the aircraft is not near one of the operator\'s maintenance bases, as the pilots may then continue the flight and land at an airport where it is more suitable to perform repairs. additionally, for trijets on the ground with one engine inoperative, approval can be granted to perform two-engine ferry flights. prior to the introduction of etops, only trijets and quadjets were able to perform long international flights over areas without any diversion airports. however, this advantage has largely disappeared in recent years as etops-certified twin-engined aircraft are able to do so as well. the biggest obstacle trijets face today is operating costs, primarily fuel efficiency, as a three-engine design almost certainly consumes more fuel than a comparable two-engine design. this also greatly increases the difficulty of marketing a new trijet aircraft today, especially for passenger service. however this was worth the trade-off between 1970 and the 1990s when trijets and twinjets shared engines of similar output, such as when the dc-10, md-11, boeing\'s 767, and airbus\'s a300, a310, and a330 were all powered by the general electric cf6, and the additional power from the third engine gave the dc-10/md-11 advantages in longer range and/or heavier payload over the a300/a330 twinjet. since the 1990s, with further advancements in high-bypass turbofan technology, large twinjets have been equipped with purpose-designed engines like the boeing 777\'s general electric ge90, allowing twinjets to perform the same tasks as most trijets and even many quadjets but more efficiently.   == history ==  the first trijet design to fly was the tupolev tu-73 bomber prototype, first flown in 1947. the first commercial trijets were the hawker siddeley trident (1962) and the boeing 727 (1963). both were compromises to meet airline requirements; in the case of the trident, it was to meet bea\'s changing needs, while the 727 had to be acceptable for three different airlines. although collaboration between the manufacturers was considered, it did not come about.early american twinjet designs were limited by the faa\'s 60-minute rule, whereby the flight path of twin-engine jetliners was restricted to within 60 minutes\' flying time from a suitable airport, in case of engine failure. in 1964, this rule was lifted for trijet designs, as they had a greater safety margin. for second-generation jet airliners, with the innovations of the high-bypass turbofan for greater efficiency and reduced noise, and the wide-body (twin-aisle) for greater passenger/cargo capacity, the trijet design was seen as the optimal configuration for the medium wide-body jet airliner, sitting in terms of size, range, and cost between quadjets (four-engine aircraft) and twinjets, and this led to a flurry of trijet designs. the four-engine boeing 747 was popular for transoceanic flights due to its long-range and large size, but it was expensive and not all routes were able to fill its seating capacity, while the original models of the airbus a300 twinjet were limited to short- to medium-range distances. during this period, different jet airliners shared engines of similar output, such as when the mcdonnell douglas dc-10, airbus a300, and boeing 767 were powered by the general electric cf6, the additional power from the third engine gave the dc-10 advantages in longer range and/or heavier payload over the a300 and 767 twinjets. thus trijet designs such as the dc-10 and l-1011 tristar represented the best compromise with medium- to long-range and medium size that us airlines sought for their domestic and transatlantic routes. as a result of these trijet wide-bodies, as well as the popularity of the boeing 727, in their heyday of the 1980s trijets made up a majority of all such us jet airliners. from 1985 to 2003 the number of such planes in service had sunk from 1488 to 602. the number of twinjets, on the other hand, had more than quadrupled in the same period. both lockheed and mcdonnell douglas were financially weakened competing in the widebody market, which led to lockheed ending production of the l-1011 in 1984 after producing only half the units needed to break even, while a number of fatal dc-10 crashes also slowed its sales. in 1984 boeing ended production of the 727, as its central engine bay would require an extremely expensive redesign to accommodate quieter high-bypass turbofans, and it was soon supplanted by airbus with their a320 and boeing with their 737 and 757. further advancements in high-bypass turbofan technology and subsequent relaxation in airline safety rules made the trijet and even the quadjet nearly obsolete for passenger services, as their range and payload could be covered more efficiently with large twinjets powered with purpose-designed engines such as the 777\'s general electric ge90. during the 1980s, mcdonnell douglas was the only western manufacturer to continue development of the trijet design with an update to the dc-10, the md-11, which initially held a range and payload advantage over its closest medium wide-body competitors which were twinjets, the in-production boeing 767 and upcoming airbus a330. mcdonnell douglas had planned a new trijet called the md-xx, which were lengthened versions of the md-11. the md-xx long range aircraft would have been capable of traveling distances up to 8,320 nautical miles (15,410 km) and had a wingspan of 65 metres (213 ft). the project was canceled in 1996, one year before mcdonnell douglas was acquired by boeing. boeing ended production of the md-11 after filling remaining customer orders since the md-11 would have competed with the 767 and 777. a study to remove the md-11\'s tail-mounted engine (which would have made it a twinjet) never came to fruition as it would have been very expensive, and the md-11 had very little in common in terms of design or type rating with other boeing airliners. in contrast to mcdonnell douglas sticking with their existing trijet configuration, airbus (which never produced a trijet aircraft) and boeing worked on new widebody twinjet designs that would become the a330 and 777, respectively. the md-11\'s long-range advantage was brief as it soon was threatened by the a330\'s four-engine derivative, the a340, and the 777. the only other notable trijet development during the 1980s was in the soviet union, where the tupolev tu-154 was re-engined with the soloviev d-30 engine as well as a new wing design and entered serial production from 1984 as the tu-154m. with the exception of the dassault falcon 7x, falcon 8x, and falcon 900, no manufacturer now produces three-engine airliners. airbus filed a patent in 2008 for a new, twin-tail trijet design, whose tail engine appears to use a ""straight"" layout similar to the md-11, but it is unknown if and when this will be developed or produced. however, the proposed boeing x-48 blended wing body design, lockheed\'s n2 design study, and aerion as2 supersonic business jet also have three engines. the as2 is currently taking orders and a wooden mockup has been constructed.boom technology\'s planned overture supersonic transport (sst) airliner is planned to use three engines, with the third engine installed in the tail with a y-shaped duct and air intakes on both sides of the rear.   == examples == boeing 727 boeing x-48 dassault falcon 50 dassault falcon 900 dassault falcon 7x dassault falcon 8x hawker siddeley trident lockheed l-1011 tristar martin xb-51 mcdonnell douglas dc-10 mcdonnell douglas md-11 tupolev tu-154 yakovlev yak-40 yakovlev yak-42   == proposed or suspended trijet developments == boeing 747-300 trijet – downsized 747 to compete with the dc-10 and l-1011, changed to four engines blended wing body trijet – proposed design based on the boeing x-48 mcdonnell douglas md-xx – stretched derivative of the dc-10, project shelved north american nr-349 – proposed interceptor derivative of the a-5 vigilante, cancelled airbus twin-tail trijet, – status unknown dassault supersonic business jet – suspended aerion as2 sukhoi-gulfstream s-21 boom technology overture boeing 777 – originally envisioned as a trijet 767 in the 1970s to compete with the dc-10 and the l-1011; later became a new twin-engine design.   == see also == quadjet s-duct trimotor twinjet wide-body aircraft   == references ==  modern commercial aircraft willian green, gordon swanborough and john mowinski, 1987   == external links == stanford university aircraft aerodynamics and design group engine placement accessed 2007-03-13 undeveloped md-11/md-12 models page patent for a triple engine fighter patent for a triple engine fighter nr-349 interceptor proposal')"
47,"Nadcap (formerly NADCAP, the National Aerospace and Defense Contractors Accreditation Program) is a global cooperative accreditation program for aerospace engineering, defense and related industries.


== History of Nadcap ==
The Nadcap program is administered by the Performance Review Institute (PRI). Nadcap was established in 1990 by SAE International. Nadcap's membership consists of ""prime contractors"" who coordinate with aerospace accredited suppliers to develop industry-wide audit criteria for special processes and products. Through PRI, Nadcap provides independent certification of manufacturing processes for the industry. PRI has its headquarters in Warrendale, Pennsylvania with branch offices for Nadcap located in London, Beijing, and Nagoya.


== Fields of Nadcap activities ==
The Nadcap program provides accreditation for special processes in the aerospace and defense industry.
These include:

Aerospace Quality Systems (AQS)
Aero Structure Assembly (ASA)
Chemical Processing (CP)
Coatings (CT)
Composites (COMP)
Conventional Machining as a Special Process (CMSP)
Elastomer Seals (SEAL)
Electronics (ETG)
Fluids Distribution (FLU)
Heat Treating (HT)
Materials Testing Laboratories (MTL)
Measurement & Inspection (M&I)
Metallic Materials Manufacturing (MMM)
Nonconventional Machining and Surface Enhancement (NMSE)
Nondestructive Testing (NDT)
Non Metallic Materials Manufacturing (NMMM)
Non Metallic Materials Testing (NMMT)
Sealants (SLT)
Welding (WLD)


== The Nadcap program and industry ==
PRI schedules an audit and assigns an industry approved auditor who will conduct the audit using an industry agreed checklist. At the end of the audit, any non-conformity issues will be raised through a non-conformance report. PRI will administer and close out the non-conformance reports with the Supplier. Upon completion PRI will present the audit pack to a 'special process Task Group’ made up of members from industry who will review it and vote on its acceptability for approval.
The Nadcap subscribers include:

309th Maintenance Wing-Hill AFB
Aerojet Rocketdyne
Airbus Group - Airbus
Airbus Group - Airbus Defence and Space
Airbus Group - Airbus Helicopters
Airbus Group - Premium AEROTEC GmbH
Airbus Group - Stelia Aerospace
Air Force
BAE Systems Military Air Information (MAI)
BAE Systems
The Boeing Company
Bombardier Inc.
COMAC
Defense Contract Management Agency (DCMA)
Eaton, Aerospace Group
Embraer S.A.
GE Aviation
GE Aviation - GE Avio S.r.l.
General Dynamics - Gulfstream
GKN Aerospace
GKN Aerospace Sweden AB
Harris Corporation
Heroux-Devtek Landing Gear Division Inc.
Honeywell Aerospace
Israel Aerospace Industries
Latécoère
Leonardo S.p.A. Divisione Velivoli
Leonardo S.p.A. – Helicopter Division
Liebherr-Aerospace SAS
Lockheed Martin Corporation
Lockheed Martin - Sikorsky Aircraft
Mitsubishi Aircraft Corporation
Mitsubishi Heavy Industries LTD
MTU Aero Engines AG
NASA
Northrop Grumman Corporation
Parker Aerospace Group
Raytheon Company
Rolls-Royce
SAFRAN Group
Singapore Technologies Aerospace
Sonaca
Spirit AeroSystems
Textron Inc. - Textron Aviation
Textron Inc. - Bell Helicopter
Thales Group
Triumph Group Inc.
Raytheon Technologies  - Goodrich
Raytheon Technologies  - Collins Aerospace (Hamilton Sundstrand)
Raytheon Technologies  - Pratt & Whitney
Raytheon Technologies - Pratt & Whitney Canada
Raytheon Technologies - Collins Aerospace (Rockwell Collins)
Zodiac Aerospace


== Nadcap Meetings ==
Nadcap meetings are held several times a year in different locations worldwide. For example, the 2017 meetings were held in New Orleans, LA, USA in February, Berlin (Germany) in June; and Pittsburgh (Pennsylvania). During these meetings there are open Task Group meetings and other workshops (with participation of Primes, Suppliers, and PRI staff). These meetings are used to discuss the program development and changes to audit criteria among other topics. Agendas and minutes are posted on the PRI website.


== Nadcap Training ==
During the Nadcap meetings, training classes are provided on different topics such as:

Root Cause Corrective Action - RCCA
Special processes, such as, NDT, chemical processing, etc.
Internal auditing
AS/EN/JISQ 9100
Problem Solving Tools
Nadcap Audit Preparation – Chemical Processing
Nadcap Audit Preparation – Heat Treating
Nadcap Audit Preparation – Metallic Material Testing Laboratories
Nadcap Audit Preparation – Non-Destructive Testing
Nadcap Audit Preparation – Welding


== References ==


== External links ==
Official website
Boeing official site
ADS Group official site
Aerospace Manufacturing
Quality Manufacturing Today","pandas(index=47, _1=47, text='nadcap (formerly nadcap, the national aerospace and defense contractors accreditation program) is a global cooperative accreditation program for aerospace engineering, defense and related industries.   == history of nadcap == the nadcap program is administered by the performance review institute (pri). nadcap was established in 1990 by sae international. nadcap\'s membership consists of ""prime contractors"" who coordinate with aerospace accredited suppliers to develop industry-wide audit criteria for special processes and products. through pri, nadcap provides independent certification of manufacturing processes for the industry. pri has its headquarters in warrendale, pennsylvania with branch offices for nadcap located in london, beijing, and nagoya.   == fields of nadcap activities == the nadcap program provides accreditation for special processes in the aerospace and defense industry. these include:  aerospace quality systems (aqs) aero structure assembly (asa) chemical processing (cp) coatings (ct) composites (comp) conventional machining as a special process (cmsp) elastomer seals (seal) electronics (etg) fluids distribution (flu) heat treating (ht) materials testing laboratories (mtl) measurement & inspection (m&i) metallic materials manufacturing (mmm) nonconventional machining and surface enhancement (nmse) nondestructive testing (ndt) non metallic materials manufacturing (nmmm) non metallic materials testing (nmmt) sealants (slt) welding (wld)   == the nadcap program and industry == pri schedules an audit and assigns an industry approved auditor who will conduct the audit using an industry agreed checklist. at the end of the audit, any non-conformity issues will be raised through a non-conformance report. pri will administer and close out the non-conformance reports with the supplier. upon completion pri will present the audit pack to a \'special process task group’ made up of members from industry who will review it and vote on its acceptability for approval. the nadcap subscribers include:  309th maintenance wing-hill afb aerojet rocketdyne airbus group - airbus airbus group - airbus defence and space airbus group - airbus helicopters airbus group - premium aerotec gmbh airbus group - stelia aerospace air force bae systems military air information (mai) bae systems the boeing company bombardier inc. comac defense contract management agency (dcma) eaton, aerospace group embraer s.a. ge aviation ge aviation - ge avio s.r.l. general dynamics - gulfstream gkn aerospace gkn aerospace sweden ab harris corporation heroux-devtek landing gear division inc. honeywell aerospace israel aerospace industries latécoère leonardo s.p.a. divisione velivoli leonardo s.p.a. – helicopter division liebherr-aerospace sas lockheed martin corporation lockheed martin - sikorsky aircraft mitsubishi aircraft corporation mitsubishi heavy industries ltd mtu aero engines ag nasa northrop grumman corporation parker aerospace group raytheon company rolls-royce safran group singapore technologies aerospace sonaca spirit aerosystems textron inc. - textron aviation textron inc. - bell helicopter thales group triumph group inc. raytheon technologies  - goodrich raytheon technologies  - collins aerospace (hamilton sundstrand) raytheon technologies  - pratt & whitney raytheon technologies - pratt & whitney canada raytheon technologies - collins aerospace (rockwell collins) zodiac aerospace   == nadcap meetings == nadcap meetings are held several times a year in different locations worldwide. for example, the 2017 meetings were held in new orleans, la, usa in february, berlin (germany) in june; and pittsburgh (pennsylvania). during these meetings there are open task group meetings and other workshops (with participation of primes, suppliers, and pri staff). these meetings are used to discuss the program development and changes to audit criteria among other topics. agendas and minutes are posted on the pri website.   == nadcap training == during the nadcap meetings, training classes are provided on different topics such as:  root cause corrective action - rcca special processes, such as, ndt, chemical processing, etc. internal auditing as/en/jisq 9100 problem solving tools nadcap audit preparation – chemical processing nadcap audit preparation – heat treating nadcap audit preparation – metallic material testing laboratories nadcap audit preparation – non-destructive testing nadcap audit preparation – welding   == references ==   == external links == official website boeing official site ads group official site aerospace manufacturing quality manufacturing today')"
48,"The max q condition is the point when an aerospace vehicle's atmospheric flight reaches maximum dynamic pressure. This is a significant factor in the design of such vehicles because the aerodynamic structural load on them is proportional to dynamic pressure. This may impose limits on the vehicle's flight envelope.
Dynamic pressure, q, is defined mathematically as

  
    
      
        q
        =
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        ,
      
    
    {\displaystyle q={\tfrac {1}{2}}\,\rho \,v^{2},}
  where ρ is the local air density, and v is the vehicle's velocity; the dynamic pressure can be thought of as the kinetic energy density of the air with respect to the vehicle.  For a launch of a rocket from the ground into space, dynamic pressure is

zero at lift-off, when the air density ρ is high but the vehicle's speed v = 0
zero outside the atmosphere, where the speed v is high, but the air density ρ = 0
always non-negative, given the quantities involvedTherefore, (by Rolle's theorem) there will always be a point where the dynamic pressure is maximum.
In other words, before reaching max q, the dynamic pressure change due to increasing velocity is greater than that due to decreasing air density so that the dynamic pressure (opposing kinetic energy) acting on the craft continues to increase. After passing max q, the opposite is true. The dynamic pressure acting against the craft decreases as the air density decreases, ultimately reaching 0 when the air density becomes zero.


== Rocket launch examples ==
During a normal Space Shuttle launch, for example, max q value of 0.32 atmospheres occurred at an altitude of approximately 11 km (36,000 ft). The three Space Shuttle Main Engines were throttled back to about 60–70% of their rated thrust (depending on payload) as the dynamic pressure approached max q; combined with the propellant grain design of the solid rocket boosters, which reduced the thrust at max q by one third after 50 seconds of burn, the total stresses on the vehicle were kept to a safe level.
During a typical Apollo mission, the max q (also just over 0.3 atmospheres) occurred between 13 and 14 kilometres (43,000–46,000 ft) of altitude; approximately the same values occur for the SpaceX Falcon 9.The point of max q is a key milestone during a rocket launch, as it is the point at which the airframe undergoes maximum mechanical stress.


== See also ==
Prandtl–Glauert singularity
Ideal gas law
Gravity turn
Gravity drag


== References ==","pandas(index=48, _1=48, text=""the max q condition is the point when an aerospace vehicle's atmospheric flight reaches maximum dynamic pressure. this is a significant factor in the design of such vehicles because the aerodynamic structural load on them is proportional to dynamic pressure. this may impose limits on the vehicle's flight envelope. dynamic pressure, q, is defined mathematically as     q =    1 2     ρ   v  2   ,    where ρ is the local air density, and v is the vehicle's velocity; the dynamic pressure can be thought of as the kinetic energy density of the air with respect to the vehicle.  for a launch of a rocket from the ground into space, dynamic pressure is  zero at lift-off, when the air density ρ is high but the vehicle's speed v = 0 zero outside the atmosphere, where the speed v is high, but the air density ρ = 0 always non-negative, given the quantities involvedtherefore, (by rolle's theorem) there will always be a point where the dynamic pressure is maximum. in other words, before reaching max q, the dynamic pressure change due to increasing velocity is greater than that due to decreasing air density so that the dynamic pressure (opposing kinetic energy) acting on the craft continues to increase. after passing max q, the opposite is true. the dynamic pressure acting against the craft decreases as the air density decreases, ultimately reaching 0 when the air density becomes zero.   == rocket launch examples == during a normal space shuttle launch, for example, max q value of 0.32 atmospheres occurred at an altitude of approximately 11 km (36,000 ft). the three space shuttle main engines were throttled back to about 60–70% of their rated thrust (depending on payload) as the dynamic pressure approached max q; combined with the propellant grain design of the solid rocket boosters, which reduced the thrust at max q by one third after 50 seconds of burn, the total stresses on the vehicle were kept to a safe level. during a typical apollo mission, the max q (also just over 0.3 atmospheres) occurred between 13 and 14 kilometres (43,000–46,000 ft) of altitude; approximately the same values occur for the spacex falcon 9.the point of max q is a key milestone during a rocket launch, as it is the point at which the airframe undergoes maximum mechanical stress.   == see also == prandtl–glauert singularity ideal gas law gravity turn gravity drag   == references =="")"
49,"Terrain Contour Matching, or TERCOM, is a navigation system used primarily by cruise missiles. It uses a pre-recorded contour map of the terrain that is compared with measurements made during flight by an on-board radar altimeter. A TERCOM system considerably increases the accuracy of a missile compared with inertial navigation systems (INS). The increased accuracy allows a TERCOM-equipped missile to fly closer to obstacles and generally lower altitudes, making it harder to detect by ground radar.


== Description ==


=== Optical contour matching ===
The Goodyear Aircraft Corporation ATRAN (Automatic Terrain Recognition And Navigation) system for the MGM-13 Mace was the earliest known TERCOM system. In August 1952, Air Materiel Command initiated the mating of the Goodyear ATRAN with the MGM-1 Matador.  This mating resulted in a production contract in June 1954. ATRAN was difficult to jam and was not range-limited by line-of sight, but its range was restricted by the availability of radar maps. In time, it became possible to construct radar maps from topographic maps.
Preparation of the maps required the route to be flown by an aircraft. A radar on the aircraft was set to a fixed angle and made horizontal scans of the land in front. The timing of the return signal indicated the range to the landform and produced an amplitude modulated (AM) signal. This was sent to a light source and recorded on 35 mm film, advancing the film and taking a picture at indicated times. The film could then be processed and copied for use in multiple missiles.
In the missile, a similar radar produced the same signal. A second system scanned the frames of film against a photocell and produced a similar AM signal. By comparing the points along the scan where the brightness changed rapidly, which could be picked out easily by simple electronics, the system could compare the left-right path of the missile compared with that of the pathfinding aircraft. Errors between the two signals drove corrections in the autopilot needed to bring the missile back onto its programmed flight path.


=== Altitude matching ===
Modern TERCOM systems use a different concept, based on the altitude of the ground the missile flies over and comparing that to measurements made by a radar altimeter. TERCOM ""maps"" consist of a series of squares of a selected size. Using a smaller number of larger squares saves memory, at the cost of decreasing accuracy. A series of such maps are produced, typically from data from radar mapping satellites. When flying over water, contour maps are replaced by magnetic field maps.
As a radar altimeter measures the distance between the missile and the terrain, not the absolute altitude compared to sea level, the important measure in the data is the change in altitude from square to square. The missile's radar altimeter feeds measurements into a small buffer that periodically ""gates"" the measurements over a period of time and averages them out to produce a single measurement. The series of such numbers held in the buffer produce a strip of measurements similar to those held in the maps. The series of changes in the buffer is then compared with the values in the map, looking for areas where the changes in altitude are identical. This produces a location and direction. The guidance system can then use this information to correct the flight path of the missile.
During the cruise portion of the flight to the target, the accuracy of the system has to be enough only to avoid terrain features. This allows the maps to be a relatively low resolution in these areas. Only the portion of the map for the terminal approach has to be higher resolution, and would normally be encoded at the highest resolutions available to the satellite mapping system.


=== TAINS ===
Due to the limited amount of memory available in mass storage devices of the 1960s and 70s, and their slow access times, the amount of terrain data that could be stored in a missile-sized package was far too small to encompass the entire flight. Instead, small patches of terrain information were stored and periodically used to update a conventional inertial platform. These systems, combining TERCOM and inertial navigation, are sometimes known as TAINS, for TERCOM-Aided Inertial Navigation System.


=== Advantages ===
TERCOM systems have the advantage of offering accuracy that is not based on the length of the flight; an inertial system slowly drifts after a ""fix"", and its accuracy is lower for longer distances. TERCOM systems receive constant fixes during the flight, and thus do not have any drift. Their absolute accuracy, however, is based on the accuracy of the radar mapping information, which is typically in the range of meters, and the ability of the processor to compare the altimeter data to the map quickly enough as the resolution increases. This generally limits first generation TERCOM systems to targets on the order of hundreds of meters, limiting them to the use of nuclear warheads. Use of conventional warheads requires further accuracy, which in turn demands additional terminal guidance systems.


=== Disadvantages ===
The limited data storage and computing systems of the time meant that the entire route had to be pre-planned, including its launch point. If the missile was launched from an unexpected location or flew too far off-course, it would never fly over the features included in the maps, and would become lost. The INS system can help, allowing it to fly to the general area of the first patch, but gross errors simply cannot be corrected. This made early TERCOM-based systems much less flexible than more modern systems like GPS, which can be set to attack any location from any location, and do not require pre-recorded information which means they can be given their targets immediately before launch.
Improvements in computing and memory, combined with the availability of global digital elevation maps, have reduced this problem, as TERCOM data is no longer limited to small patches, and the availability of side-looking radar allows much larger areas of landscape contour data to be acquired for comparison with the stored contour data.


== Comparison with other guidance systems ==


=== DSMAC ===

DSMAC was an early form of AI which could guide missiles in real time by using camera inputs to determine location. DSMAC was used in Tomahawk Block II onward, and proved itself successfully during the first Gulf War. The system worked by comparing camera inputs during flight to maps computed from spy satellite images. The DSMAC AI system computed contrast maps of images, which it then combined in a buffer and then averaged. It then compared the averages to stored maps computed beforehand by a large mainframe computer, which converted spy satellite pictures to simulate what routes and targets would look like from low level. Since the data were not identical and would change by season and from other unexpected changes and visual effects, the DSMAC system within the missiles had to be able to compare and determine if maps were the same, regardless of changes. It could successfully filter out differences in maps and use the remaining map data to determine its location. Due to its ability to visually identify targets instead of simply attacking estimated coordinates, its accuracy exceeded GPS guided weapons during the first Gulf War.The massive improvements in memory and processing power from the 1950s, when these scene comparison systems were first invented, to the 1980s, when TERCOM was widely deployed, changed the nature of the problem considerably. Modern systems can store numerous images of a target as seen from different directions, and often the imagery can be calculated using image synthesis techniques. Likewise, the complexity of the live imaging systems has been greatly reduced through the introduction of solid-state technologies like CCDs. The combination of these technologies produced the Digitized Scene-Mapping Area Correlator (DSMAC). DSMAC systems are often combined with TERCOM as a terminal guidance system, allowing point attack with conventional warheads.

MGM-31 Pershing II, SS-12 Scaleboard Temp-SM and OTR-23 Oka used an active radar homing version of DSMAC (digitized correlator unit DCU), which compared radar topographic maps taken by satellites or aircraft with information received from the onboard active radar regarding target topography, for terminal guidance.


=== Satellite navigation ===
Yet another way to navigate a cruise missile is by using a satellite positioning system as they are precise and cheap. Unfortunately, they rely on satellites. If the satellites are interfered with (e.g. destroyed) or if the satellite signal is interfered with (e.g. jammed), the satellite navigation system becomes inoperable. Therefore, the GPS-based (or GLONASS-based) navigation is useful in a conflict with a technologically unsophisticated adversary. On the other hand, to be ready for a conflict with a technologically advanced adversary, one needs missiles equipped with TAINS and DSMAC.


== Missiles that employ TERCOM navigation ==
The cruise missiles that employ a TERCOM system include:

Supersonic Low Altitude Missile (early version of TERCOM was slated to be used in this never-built missile)
AGM-86B (United States)
AGM-129 ACM (United States)
BGM-109 Tomahawk (some versions, United States)
C-602 Anti-ship & Land attack cruise missile (China)
Kh-55 Granat NATO reporting name AS-15 Kent (Soviet Union)
Newer Russian cruise missiles, such as Kh-101 and Kh-555 are likely to have TERCOM navigation, but little information is available about these missiles
C-802 or YJ-82 NATO reporting name CSS-N-8 Saccade (China) – it is unclear if this missile employs TERCOM navigation
Hyunmoo III (South Korea)
DH-10 (China)
Babur (Pakistan) Land Attack Cruise Missile
Ra'ad (Pakistan) Air Launched Cruise Missile
Naval Strike Missile (Anti ship and land attack missile, Norway)
SOM (missile) (air launched cruise missile, Turkey)
HongNiao 1/2/3 cruise missiles
9K720 Iskander (Short-range ballistic missile and cruise missile variants, Russia)


== See also ==
Missile guidance
TERPROM


== References ==


== External links ==
""Terrestrial Guidance Methods"", Section 16.5.3 of Fundamentals of Naval Weapons Systems
More info at fas.org
Info at aeronautics.ru","pandas(index=49, _1=49, text='terrain contour matching, or tercom, is a navigation system used primarily by cruise missiles. it uses a pre-recorded contour map of the terrain that is compared with measurements made during flight by an on-board radar altimeter. a tercom system considerably increases the accuracy of a missile compared with inertial navigation systems (ins). the increased accuracy allows a tercom-equipped missile to fly closer to obstacles and generally lower altitudes, making it harder to detect by ground radar.   == description == yet another way to navigate a cruise missile is by using a satellite positioning system as they are precise and cheap. unfortunately, they rely on satellites. if the satellites are interfered with (e.g. destroyed) or if the satellite signal is interfered with (e.g. jammed), the satellite navigation system becomes inoperable. therefore, the gps-based (or glonass-based) navigation is useful in a conflict with a technologically unsophisticated adversary. on the other hand, to be ready for a conflict with a technologically advanced adversary, one needs missiles equipped with tains and dsmac.   == missiles that employ tercom navigation == the cruise missiles that employ a tercom system include:  supersonic low altitude missile (early version of tercom was slated to be used in this never-built missile) agm-86b (united states) agm-129 acm (united states) bgm-109 tomahawk (some versions, united states) c-602 anti-ship & land attack cruise missile (china) kh-55 granat nato reporting name as-15 kent (soviet union) newer russian cruise missiles, such as kh-101 and kh-555 are likely to have tercom navigation, but little information is available about these missiles c-802 or yj-82 nato reporting name css-n-8 saccade (china) – it is unclear if this missile employs tercom navigation hyunmoo iii (south korea) dh-10 (china) babur (pakistan) land attack cruise missile ra\'ad (pakistan) air launched cruise missile naval strike missile (anti ship and land attack missile, norway) som (missile) (air launched cruise missile, turkey) hongniao 1/2/3 cruise missiles 9k720 iskander (short-range ballistic missile and cruise missile variants, russia)   == see also == missile guidance terprom   == references ==   == external links == ""terrestrial guidance methods"", section 16.5.3 of fundamentals of naval weapons systems more info at fas.org info at aeronautics.ru')"
50,"Weight distribution is the apportioning of weight within a vehicle, especially cars, airplanes, and trains. Typically, it is written in the form x/y, where x is the percentage of weight in the front, and y is the percentage in the back.
In a vehicle which relies on gravity in some way, weight distribution directly affects a variety of vehicle characteristics, including handling, acceleration, traction, and component life.  For this reason weight distribution varies with the vehicle's intended usage. For example, a drag car maximizes traction at the rear axle while countering the reactionary pitch-up torque.  It generates this counter-torque by placing a small amount of counterweight at a great distance forward of the rear axle.
In the airline industry, load balancing is used to evenly distribute the weight of passengers, cargo, and fuel throughout an aircraft, so as to keep the aircraft's center of gravity close to its center of pressure to avoid losing pitch control.  In military transport aircraft, it is common to have a loadmaster as a part of the crew; their responsibilities include calculating accurate load information for center of gravity calculations, and ensuring cargo is properly secured to prevent its shifting.
In large aircraft and ships, multiple fuel tanks and pumps are often used, so that as fuel is consumed, the remaining fuel can be positioned to keep the vehicle balanced, and to reduce stability problems associated with the free surface effect.
In the trucking industry, individual axle weight limits require balancing the cargo when the gross vehicle weight nears the legal limit.


== See also ==
Center of mass
Center of percussion
Load transfer
Mass distribution
Roll center
Tilt test
Weight transfer


== References ==


== External links ==
Weight Distribution Calculator","pandas(index=50, _1=50, text=""weight distribution is the apportioning of weight within a vehicle, especially cars, airplanes, and trains. typically, it is written in the form x/y, where x is the percentage of weight in the front, and y is the percentage in the back. in a vehicle which relies on gravity in some way, weight distribution directly affects a variety of vehicle characteristics, including handling, acceleration, traction, and component life.  for this reason weight distribution varies with the vehicle's intended usage. for example, a drag car maximizes traction at the rear axle while countering the reactionary pitch-up torque.  it generates this counter-torque by placing a small amount of counterweight at a great distance forward of the rear axle. in the airline industry, load balancing is used to evenly distribute the weight of passengers, cargo, and fuel throughout an aircraft, so as to keep the aircraft's center of gravity close to its center of pressure to avoid losing pitch control.  in military transport aircraft, it is common to have a loadmaster as a part of the crew; their responsibilities include calculating accurate load information for center of gravity calculations, and ensuring cargo is properly secured to prevent its shifting. in large aircraft and ships, multiple fuel tanks and pumps are often used, so that as fuel is consumed, the remaining fuel can be positioned to keep the vehicle balanced, and to reduce stability problems associated with the free surface effect. in the trucking industry, individual axle weight limits require balancing the cargo when the gross vehicle weight nears the legal limit.   == see also == center of mass center of percussion load transfer mass distribution roll center tilt test weight transfer   == references ==   == external links == weight distribution calculator"")"
51,"The torques or moments acting on an airfoil moving through a fluid can be accounted for by the net lift and net drag applied at some point on the airfoil, and a separate net pitching moment about that point whose magnitude varies with the choice of where the lift is chosen to be applied.  The Aerodynamic center is the point at which the pitching moment coefficient for the airfoil does not vary with lift coefficient (i.e. angle of attack), making analysis simpler.

  
    
      
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  L
                
              
            
          
        
        =
        0
      
    
    {\displaystyle {dC_{m} \over dC_{L}}=0}
   where 
  
    
      
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{L}}
   is the aircraft lift coefficient.The lift and drag forces can be applied at a single point, the center of pressure, about which they exert zero torque. However, the location of the center of pressure moves significantly with a change in angle of attack and is thus impractical for aerodynamic analysis. Instead the aerodynamic center is used and as a result the incremental lift and drag due to change in angle of attack acting at this point is sufficient to describe the aerodynamic forces acting on the given body.


== Theory ==
Within the assumptions embodied in thin airfoil theory, the aerodynamic center is located at the quarter-chord (25% chord position) on a symmetric airfoil while it is close but not exactly equal to the quarter-chord point on a cambered airfoil.
From thin airfoil theory:

  
    
      
         
        
          c
          
            l
          
        
        =
        2
        π
        α
      
    
    {\displaystyle \ c_{l}=2\pi \alpha }
  
where 
  
    
      
        
          c
          
            l
          
        
        
      
    
    {\displaystyle c_{l}\!}
   is the section lift coefficient,

  
    
      
        α
        
      
    
    {\displaystyle \alpha \!}
   is the angle of attack in radian, measured relative to the chord line.

  
    
      
         
        
          
            
              d
              
                c
                
                  m
                  ,
                  c
                  
                    /
                  
                  4
                
              
            
            
              d
              α
            
          
        
        =
        
          m
          
            0
          
        
      
    
    {\displaystyle \ {dc_{m,c/4} \over d\alpha }=m_{0}}
  
where 
  
    
      
         
        
          c
          
            m
            ,
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ c_{m,c/4}}
   is the moment taken at quarter-chord point and 
  
    
      
         
        
          m
          
            0
          
        
      
    
    {\displaystyle \ m_{0}}
   is a constant.

  
    
      
         
        
          M
          
            a
            c
          
        
        =
        L
        (
        c
        
          x
          
            a
            c
          
        
        −
        c
        
          /
        
        4
        )
        +
        
          M
          
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ M_{ac}=L(cx_{ac}-c/4)+M_{c/4}}
  

  
    
      
         
        
          c
          
            m
            ,
            a
            c
          
        
        =
        
          c
          
            l
          
        
        (
        
          x
          
            a
            c
          
        
        −
        0.25
        )
        +
        
          c
          
            m
            ,
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle \ c_{m,ac}=c_{l}(x_{ac}-0.25)+c_{m,c/4}}
  Differentiating with respect to angle of attack

  
    
      
         
        
          x
          
            a
            c
          
        
        =
        
          
            
              −
              
                m
                
                  0
                
              
            
            
              2
              π
            
          
        
        +
        0.25
      
    
    {\displaystyle \ x_{ac}={-m_{0} \over {2\pi }}+0.25}
  For symmetrical airfoils 
  
    
      
         
        
          m
          
            0
          
        
        =
        0
      
    
    {\displaystyle \ m_{0}=0}
  , so the aerodynamic center is at 25% of chord. But for cambered airfoils the aerodynamic center can be slightly less than 25% of the chord from the leading edge, which depends on the slope of the moment coefficient, 
  
    
      
         
        
          m
          
            0
          
        
      
    
    {\displaystyle \ m_{0}}
  . These results obtained are calculated using the thin airfoil theory so the use of the results are warranted only when the assumptions of thin airfoil theory are realistic. In precision experimentation with real airfoils and advanced analysis, the aerodynamic center is observed to change location slightly as angle of attack varies. In most literature however the aerodynamic center is assumed to be fixed at the 25% chord position.


== Role of aerodynamic center in aircraft stability ==
For longitudinal static stability: 
  
    
      
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              α
            
          
        
        <
        0
      
    
    {\displaystyle {dC_{m} \over d\alpha }<0}
       and    
  
    
      
        
          
            
              d
              
                C
                
                  z
                
              
            
            
              d
              α
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{z} \over d\alpha }>0}
  
For directional static stability:   
  
    
      
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              β
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{n} \over d\beta }>0}
       and    
  
    
      
        
          
            
              d
              
                C
                
                  y
                
              
            
            
              d
              β
            
          
        
        >
        0
      
    
    {\displaystyle {dC_{y} \over d\beta }>0}
  
Where:

  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
        cos
        ⁡
        (
        α
        )
        +
        
          C
          
            d
          
        
        sin
        ⁡
        (
        α
        )
      
    
    {\displaystyle C_{z}=C_{L}\cos(\alpha )+C_{d}\sin(\alpha )}
  

  
    
      
        
          C
          
            x
          
        
        =
        
          C
          
            L
          
        
        sin
        ⁡
        (
        α
        )
        −
        
          C
          
            d
          
        
        cos
        ⁡
        (
        α
        )
      
    
    {\displaystyle C_{x}=C_{L}\sin(\alpha )-C_{d}\cos(\alpha )}
  For a force acting away from the aerodynamic center, which is away from the reference point:

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{z}}}
  Which for small angles 
  
    
      
        cos
        ⁡
        (
        α
        )
        =
        1
      
    
    {\displaystyle \cos(\alpha )=1}
   and 
  
    
      
        sin
        ⁡
        (
        α
        )
        =
        α
      
    
    {\displaystyle \sin(\alpha )=\alpha }
  , 
  
    
      
        β
        =
        0
      
    
    {\displaystyle \beta =0}
  , 
  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
        −
        
          C
          
            d
          
        
        ∗
        α
      
    
    {\displaystyle C_{z}=C_{L}-C_{d}*\alpha }
  , 
  
    
      
        
          C
          
            z
          
        
        =
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{z}=C_{L}}
   simplifies to:

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  L
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{L}}}
  

  
    
      
        
          Y
          
            A
            C
          
        
        =
        
          Y
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle Y_{AC}=Y_{\mathrm {ref} }}
  

  
    
      
        
          Z
          
            A
            C
          
        
        =
        
          Z
          
            
              r
              e
              f
            
          
        
      
    
    {\displaystyle Z_{AC}=Z_{\mathrm {ref} }}
  General Case: From the definition of the AC it follows that

  
    
      
        
          X
          
            A
            C
          
        
        =
        
          X
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              
                C
                
                  y
                
              
            
          
        
      
    
    {\displaystyle X_{AC}=X_{\mathrm {ref} }+c{dC_{m} \over dC_{z}}+c{dC_{n} \over dC_{y}}}
  
.

  
    
      
        
          Y
          
            A
            C
          
        
        =
        
          Y
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  l
                
              
            
            
              d
              
                C
                
                  z
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  n
                
              
            
            
              d
              
                C
                
                  x
                
              
            
          
        
      
    
    {\displaystyle Y_{AC}=Y_{\mathrm {ref} }+c{dC_{l} \over dC_{z}}+c{dC_{n} \over dC_{x}}}
  
.

  
    
      
        
          Z
          
            A
            C
          
        
        =
        
          Z
          
            
              r
              e
              f
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  l
                
              
            
            
              d
              
                C
                
                  y
                
              
            
          
        
        +
        c
        
          
            
              d
              
                C
                
                  m
                
              
            
            
              d
              
                C
                
                  x
                
              
            
          
        
      
    
    {\displaystyle Z_{AC}=Z_{\mathrm {ref} }+c{dC_{l} \over dC_{y}}+c{dC_{m} \over dC_{x}}}
  The Static Margin can then be used to quantify the AC:

  
    
      
        S
        M
        =
        
          
            
              
                X
                
                  A
                  C
                
              
              −
              
                X
                
                  C
                  G
                
              
            
            c
          
        
      
    
    {\displaystyle SM={X_{AC}-X_{CG} \over c}}
  where:

  
    
      
        
          C
          
            n
          
        
      
    
    {\displaystyle C_{n}}
   = yawing moment coefficient

  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   = pitching moment coefficient

  
    
      
        
          C
          
            l
          
        
      
    
    {\displaystyle C_{l}}
   = rolling moment coefficient

  
    
      
        
          C
          
            x
          
        
      
    
    {\displaystyle C_{x}}
   = X-force ~= Drag

  
    
      
        
          C
          
            y
          
        
      
    
    {\displaystyle C_{y}}
   = Y-force ~= Side Force

  
    
      
        
          C
          
            z
          
        
      
    
    {\displaystyle C_{z}}
   = Z-force ~= Lift
ref = reference point (about which moments were taken)
c   = reference length
S   = reference area
q   = dynamic pressure

  
    
      
        α
      
    
    {\displaystyle \alpha }
     = angle of attack

  
    
      
        β
      
    
    {\displaystyle \beta }
     = sideslip angleSM   = Static Margin


== See also ==
Aircraft flight mechanics
Flight dynamics
Longitudinal static stability
Thin-airfoil theory
Joukowsky transform


== References ==","pandas(index=51, _1=51, text='the torques or moments acting on an airfoil moving through a fluid can be accounted for by the net lift and net drag applied at some point on the airfoil, and a separate net pitching moment about that point whose magnitude varies with the choice of where the lift is chosen to be applied.  the aerodynamic center is the point at which the pitching moment coefficient for the airfoil does not vary with lift coefficient (i.e. angle of attack), making analysis simpler.        d  c  m     d  c  l      = 0    = sideslip anglesm   = static margin   == see also == aircraft flight mechanics flight dynamics longitudinal static stability thin-airfoil theory joukowsky transform   == references ==')"
52,"In aerodynamics, the pitching moment on an airfoil is the moment (or torque) produced by the aerodynamic force on the airfoil if that aerodynamic force is considered to be applied, not at the center of pressure, but at the aerodynamic center of the airfoil.  The pitching moment on the wing of an airplane is part of the total moment that must be balanced using the lift on the horizontal stabilizer. More generally, a pitching moment is any moment acting on the pitch axis of a moving body.
The lift on an airfoil is a distributed force that can be said to act at a point called the center of pressure.  However, as angle of attack changes on a cambered airfoil, there is movement of the center of pressure forward and aft.  This makes analysis difficult when attempting to use the concept of the center of pressure.  One of the remarkable properties of a cambered airfoil is that, even though the center of pressure moves forward and aft, if the lift is imagined to act at a point called the aerodynamic center. The moment of the lift force changes in proportion to the square of the airspeed.  If the moment is divided by the dynamic pressure, the area and chord of the airfoil, the result is known as the pitching moment coefficient. This coefficient changes only a little over the operating range of angle of attack of the airfoil but the change in moment slope against the AOA shown in figure below seems very steep so this should be of change in pitching moment of wing about CG rather than about AC.  The combination of the two concepts of aerodynamic center and pitching moment coefficient make it relatively simple to analyse some of the flight characteristics of an aircraft.


== Measurement ==
The aerodynamic center of an airfoil is usually close to 25% of the chord behind the leading edge of the airfoil.  When making tests on a model airfoil, such as in a wind-tunnel, if the force sensor is not aligned with the quarter-chord of the airfoil, but offset by a distance x, the pitching moment about the quarter-chord point, 
  
    
      
        
          M
          
            c
            
              /
            
            4
          
        
      
    
    {\displaystyle M_{c/4}}
   is given by

  
    
      
        
          M
          
            c
            
              /
            
            4
          
        
        =
        
          M
          
            indicated
          
        
        +
        
          x
        
        ×
        (
        
          D
          
            indicated
          
        
        ,
        
          L
          
            indicated
          
        
        )
      
    
    {\displaystyle M_{c/4}=M_{\text{indicated}}+\mathbf {x} \times (D_{\text{indicated}},L_{\text{indicated}})}
  where the indicated values of D and L are the drag and lift on the model, as measured by the force sensor.


== Coefficient ==
The pitching moment coefficient is important in the study of the longitudinal static stability of aircraft and missiles.
The pitching moment coefficient 
  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   is defined as follows

  
    
      
        
          C
          
            m
          
        
        =
        
          
            M
            
              q
              S
              c
            
          
        
      
    
    {\displaystyle C_{m}={\frac {M}{qSc}}}
  where M is the pitching moment, q is the dynamic pressure, S is the wing area, and c is the length of the chord of the airfoil.

  
    
      
        
          C
          
            m
          
        
      
    
    {\displaystyle C_{m}}
   is a dimensionless coefficient so consistent units must be used for M, q, S and c.
Pitching moment coefficient is fundamental to the definition of aerodynamic center of an airfoil.  The aerodynamic center is defined to be the point on the chord line of the airfoil at which the pitching moment coefficient does not vary with angle of attack, or at least does not vary significantly over the operating range of angle of attack of the airfoil.
In the case of a symmetric airfoil, the lift force acts through one point for all angles of attack, and the center of pressure does not move as it does in a cambered airfoil.  Consequently, the pitching moment coefficient for a symmetric airfoil is zero.
The pitching moment is, by convention, considered to be positive when it acts to pitch the airfoil in the nose-up direction.  Conventional cambered airfoils supported at the aerodynamic center pitch nose-down so the pitching moment coefficient of these airfoils is negative.


== References ==
L. J. Clancy (1975), Aerodynamics, Pitman Publishing Limited, London, ISBN 0-273-01120-0
Piercy, N.A.V (1943) Aerodynamics, pages 384–386, English Universities Press. London
Low-Speed Stability Retrieved on 2008-07-18


=== Notes ===


== See also ==

Aircraft flight mechanics
Flight dynamics
Longitudinal static stability
Neutral point
Lift coefficient
Drag coefficient","pandas(index=52, _1=52, text='in aerodynamics, the pitching moment on an airfoil is the moment (or torque) produced by the aerodynamic force on the airfoil if that aerodynamic force is considered to be applied, not at the center of pressure, but at the aerodynamic center of the airfoil.  the pitching moment on the wing of an airplane is part of the total moment that must be balanced using the lift on the horizontal stabilizer. more generally, a pitching moment is any moment acting on the pitch axis of a moving body. the lift on an airfoil is a distributed force that can be said to act at a point called the center of pressure.  however, as angle of attack changes on a cambered airfoil, there is movement of the center of pressure forward and aft.  this makes analysis difficult when attempting to use the concept of the center of pressure.  one of the remarkable properties of a cambered airfoil is that, even though the center of pressure moves forward and aft, if the lift is imagined to act at a point called the aerodynamic center. the moment of the lift force changes in proportion to the square of the airspeed.  if the moment is divided by the dynamic pressure, the area and chord of the airfoil, the result is known as the pitching moment coefficient. this coefficient changes only a little over the operating range of angle of attack of the airfoil but the change in moment slope against the aoa shown in figure below seems very steep so this should be of change in pitching moment of wing about cg rather than about ac.  the combination of the two concepts of aerodynamic center and pitching moment coefficient make it relatively simple to analyse some of the flight characteristics of an aircraft.   == measurement == the aerodynamic center of an airfoil is usually close to 25% of the chord behind the leading edge of the airfoil.  when making tests on a model airfoil, such as in a wind-tunnel, if the force sensor is not aligned with the quarter-chord of the airfoil, but offset by a distance x, the pitching moment about the quarter-chord point,     m  c  /  4      is a dimensionless coefficient so consistent units must be used for m, q, s and c. pitching moment coefficient is fundamental to the definition of aerodynamic center of an airfoil.  the aerodynamic center is defined to be the point on the chord line of the airfoil at which the pitching moment coefficient does not vary with angle of attack, or at least does not vary significantly over the operating range of angle of attack of the airfoil. in the case of a symmetric airfoil, the lift force acts through one point for all angles of attack, and the center of pressure does not move as it does in a cambered airfoil.  consequently, the pitching moment coefficient for a symmetric airfoil is zero. the pitching moment is, by convention, considered to be positive when it acts to pitch the airfoil in the nose-up direction.  conventional cambered airfoils supported at the aerodynamic center pitch nose-down so the pitching moment coefficient of these airfoils is negative.   == references == l. j. clancy (1975), aerodynamics, pitman publishing limited, london, isbn 0-273-01120-0 piercy, n.a.v (1943) aerodynamics, pages 384–386, english universities press. london low-speed stability retrieved on 2008-07-18 == see also ==  aircraft flight mechanics flight dynamics longitudinal static stability neutral point lift coefficient drag coefficient')"
53,"Tokyo tanks were internally mounted self-sealing fuel tanks used in the Boeing B-17 Flying Fortress and Consolidated B-24 Liberator bombers during World War II. Although nicknamed ""Tokyo"" tanks to dramatically illustrate the significant range they added to the B-17 (approximately 40% greater with combat weights), it was also an exaggeration in that no B-17 ever had the range to bomb Japan from any base in World War II.


== Description ==
These fuel tanks consisted of eighteen removable containers made of a rubberized compound, called cells, installed inside the wings of the airplane, nine to each side. The wings of the B-17 consisted of an ""inboard wing"" structure mounted to the fuselage which held the engines and flaps, and an ""outboard wing"" structure joined to the inboard wing and carrying the ailerons. The Tokyo tanks were installed on either side of the joint (a load-bearing point) where the two wing portions were connected. Five cells, totaling 270 US gallons (1,000 L) capacity, sat side by side in the outboard wing and were joined by a fuel line to the main tank delivering fuel to the outboard engine. The sixth cell was located in the space where the wing sections joined, and the remaining three cells were located side-by-side in the inboard wing; these four cells delivered 270 US gallons (1,000 L) of fuel to the feeder tank for the inboard engine. The same arrangement was repeated on the opposite wing. The Tokyo tanks added 1,080 US gallons (4,100 L) of fuel to the 1,700 US gallons (6,400 L) already carried in the six regular wing tanks and the 820 US gallons (3,100 L) that could be carried in an auxiliary tank that could be mounted in the bomb bay, for a combined total of 3,600 US gallons (14,000 L).
All B-17F aircraft built by Boeing from Block 80, by Douglas from Block 25, and by Vega from Block 30 were equipped with Tokyo tanks, and the entire run of B-17Gs by all three manufacturers had Tokyo tanks. B-17s with factory-mounted Tokyo tanks were first introduced to the Eighth Air Force in England in April 1943 with the arrival of the 94th and 95th Bomb Groups, equipped with new aircraft. By June 1943, aircraft that were so equipped began to appear in greater numbers as replacements, and from the beginning of July 1943, all replacement aircraft that did not have the tanks already installed were equipped before issue.
Although the tanks were removable, this could only be done by first removing the wing panels, and so was not a routine maintenance task. A drawback to the tanks was that there was no means of measuring remaining fuel quantity within the cells. Fuel was moved from the cells to the engine tanks by opening control valves within the bomb bay so that the fuel drained by gravity. Although the tanks were specified as self-sealing, vapor buildup within partially drained tanks made them explosive hazards in combat.


== References ==

Bishop, Cliff T. Fortresses of the Big Triangle First (1986), pp. 50–51. ISBN 1-869987-00-4","pandas(index=53, _1=53, text='tokyo tanks were internally mounted self-sealing fuel tanks used in the boeing b-17 flying fortress and consolidated b-24 liberator bombers during world war ii. although nicknamed ""tokyo"" tanks to dramatically illustrate the significant range they added to the b-17 (approximately 40% greater with combat weights), it was also an exaggeration in that no b-17 ever had the range to bomb japan from any base in world war ii.   == description == these fuel tanks consisted of eighteen removable containers made of a rubberized compound, called cells, installed inside the wings of the airplane, nine to each side. the wings of the b-17 consisted of an ""inboard wing"" structure mounted to the fuselage which held the engines and flaps, and an ""outboard wing"" structure joined to the inboard wing and carrying the ailerons. the tokyo tanks were installed on either side of the joint (a load-bearing point) where the two wing portions were connected. five cells, totaling 270 us gallons (1,000 l) capacity, sat side by side in the outboard wing and were joined by a fuel line to the main tank delivering fuel to the outboard engine. the sixth cell was located in the space where the wing sections joined, and the remaining three cells were located side-by-side in the inboard wing; these four cells delivered 270 us gallons (1,000 l) of fuel to the feeder tank for the inboard engine. the same arrangement was repeated on the opposite wing. the tokyo tanks added 1,080 us gallons (4,100 l) of fuel to the 1,700 us gallons (6,400 l) already carried in the six regular wing tanks and the 820 us gallons (3,100 l) that could be carried in an auxiliary tank that could be mounted in the bomb bay, for a combined total of 3,600 us gallons (14,000 l). all b-17f aircraft built by boeing from block 80, by douglas from block 25, and by vega from block 30 were equipped with tokyo tanks, and the entire run of b-17gs by all three manufacturers had tokyo tanks. b-17s with factory-mounted tokyo tanks were first introduced to the eighth air force in england in april 1943 with the arrival of the 94th and 95th bomb groups, equipped with new aircraft. by june 1943, aircraft that were so equipped began to appear in greater numbers as replacements, and from the beginning of july 1943, all replacement aircraft that did not have the tanks already installed were equipped before issue. although the tanks were removable, this could only be done by first removing the wing panels, and so was not a routine maintenance task. a drawback to the tanks was that there was no means of measuring remaining fuel quantity within the cells. fuel was moved from the cells to the engine tanks by opening control valves within the bomb bay so that the fuel drained by gravity. although the tanks were specified as self-sealing, vapor buildup within partially drained tanks made them explosive hazards in combat.   == references ==  bishop, cliff t. fortresses of the big triangle first (1986), pp. 50–51. isbn 1-869987-00-4')"
54,"A cambered aerofoil generates no lift when it is moving parallel to an axis called the zero-lift axis (or the zero-lift line.)  When the angle of attack on an aerofoil is measured relative to the zero-lift axis it is true to say the lift coefficient is zero when the angle of attack is zero.  For this reason, on a cambered aerofoil the zero-lift line is better than the chord line when describing the angle of attack.When symmetric aerofoils are moving parallel to the chord line of the aerofoil, zero lift is generated.  However, when cambered aerofoils are moving parallel to the chord line, lift is generated.  (See diagram at right.) For symmetric aerofoils, the chord line and the zero lift line are the same.


== See also ==
Angle of attack
Aerobatics
Aerobatic maneuver


== References ==
Anderson, John D. Jr (2005), Introduction to Flight, Section 7.4 (fifth edition), McGraw-Hill ISBN 0-07-282569-3
L. J. Clancy (1975), Aerodynamics, Sections 5.6 and 5.7, Pitman Publishing, London.  ISBN 0-273-01120-0
Kermode, A.C. (1972), Mechanics of Flight, Chapter 3, (p. 76, eighth edition), Pitman Publishing ISBN 0-273-31623-0


== Notes ==","pandas(index=54, _1=54, text='a cambered aerofoil generates no lift when it is moving parallel to an axis called the zero-lift axis (or the zero-lift line.)  when the angle of attack on an aerofoil is measured relative to the zero-lift axis it is true to say the lift coefficient is zero when the angle of attack is zero.  for this reason, on a cambered aerofoil the zero-lift line is better than the chord line when describing the angle of attack.when symmetric aerofoils are moving parallel to the chord line of the aerofoil, zero lift is generated.  however, when cambered aerofoils are moving parallel to the chord line, lift is generated.  (see diagram at right.) for symmetric aerofoils, the chord line and the zero lift line are the same.   == see also == angle of attack aerobatics aerobatic maneuver   == references == anderson, john d. jr (2005), introduction to flight, section 7.4 (fifth edition), mcgraw-hill isbn 0-07-282569-3 l. j. clancy (1975), aerodynamics, sections 5.6 and 5.7, pitman publishing, london.  isbn 0-273-01120-0 kermode, a.c. (1972), mechanics of flight, chapter 3, (p. 76, eighth edition), pitman publishing isbn 0-273-31623-0   == notes ==')"
55,"Clean configuration is the flight configuration of a fixed-wing aircraft when its external equipment is retracted to minimize drag, and thus maximize airspeed for a given power setting.
For most airplanes, clean configuration means simply that the wing flaps and landing gear are retracted, as these are the cause of drag due to the lack of streamlined shape. On more complex airplanes, it also means that other devices on the wings (such as slats, spoilers, and leading edge flaps) are retracted. Clean configuration is used for normal cruising at altitude during which lift, or rise in altitude, is not needed.
In military aviation, a clean configuration is generally without external stores which reduce maximum performance both due to increased weight and even more so due to increased drag.


== References ==","pandas(index=55, _1=55, text='clean configuration is the flight configuration of a fixed-wing aircraft when its external equipment is retracted to minimize drag, and thus maximize airspeed for a given power setting. for most airplanes, clean configuration means simply that the wing flaps and landing gear are retracted, as these are the cause of drag due to the lack of streamlined shape. on more complex airplanes, it also means that other devices on the wings (such as slats, spoilers, and leading edge flaps) are retracted. clean configuration is used for normal cruising at altitude during which lift, or rise in altitude, is not needed. in military aviation, a clean configuration is generally without external stores which reduce maximum performance both due to increased weight and even more so due to increased drag.   == references ==')"
56,"Parts Manufacturer Approval (PMA) is an approval granted by the United States Federal Aviation Administration (FAA) to a manufacturer of aircraft parts.


== Approval ==
It is generally illegal in the United States to install replacement or modification parts on a certificated aircraft without an airworthiness release such as a Supplemental Type Certificate (STC) or Parts Manufacturing Approval (PMA).  There are a number of other methods of compliance, including parts manufactured to government or industry standards, parts manufactured under technical standard order authorization [TSO], owner-/operator-produced parts, experimental aircraft, field approvals, etc.PMA-holding manufacturers are permitted to make replacement parts for aircraft, even though they are not the original manufacturer of the aircraft.  The process is analogous to 'after-market' parts for automobiles, except that the United States aircraft parts production market remains tightly regulated by the FAA.
An applicant for a PMA applies for approval from the FAA. The FAA prioritizes its review of a new application based on its internal process called Project Prioritization.The FAA Order covering the application for PMA is Order 8110.42 revision D.  This document is worded as instructions to the FAA reviewing personnel.  An accompanying Advisory Circular (AC) 21.303-4 is intended to address the applicant.  8110.42C addressed both the applicant and the reviewer.  Per the order, application for a PMA can be made per the following ways:  Identicality in which the applicant attempts to convince the FAA that the PMA part is identical to the OAH (Original Approval Holder) part.  Identicality by Licensure is accomplished by providing evidence to the FAA that the applicant has licensed the part data from the OAH.  This evidence is usually in the form of an Assist Letter provided to the applicant by the OAH.  PMA may also be granted based upon prior approval of an STC .  As an example:  If an STC were granted to alter an existing aircraft design then that approval would also apply to the parts needed to make that modification.  A PMA would be required, however, to manufacture the parts.  The last method to obtain a PMA is Test & Computation.  This approach consist of one or a combination of both of the following methods:  General Analysis and Comparative Analysis.  General analysis compares the proposed part to the functional requirements of that part when installed.  Comparative Analysis compares the function of the proposed part to the OAH part.  As an example:  If a PMA application for flight control cables were to show that the PMA part exceeds the pull strength requirements of the aircraft system it is meant for, that is general analysis.  To show that it exceeds that of the OAH part is comparative analysis.  The modern trend is to use a variety of techniques in combination in order to obtain approval of complicated parts - relying on the techniques that are most accurate and best able to provide the proof of airworthiness desired.  The cognizant regional FAA Aircraft Certification Office (ACO) determines if the applicant has shown compliance with all relevant airworthiness regulations and is thus entitled to design approval.
The second step in the application process is to apply to the FAA Manufacturing Inspection Divisional Office (MIDO) to obtain approval of the manufacturing quality assurance system (known as production approval).  Production approval will be granted when the FAA is satisfied that the system will not permit parts to leave the system until the parts have been verified to meet the requirements of the approved design, and the system otherwise meets the requirements of the FAA quality system regulations.  A Production Approval Holder (PAH) will typically already have satisfied this requirement before PMA application is made.
PMA applications based upon licensure or STC do not require ACO approval (since the data has already been approved) and can go straight to the MIDO.


== History ==
Under the Civil Air Regulations (CARs), the government had the authority to approve aircraft parts in a predecessor to the PMA rules.  This authority was found in each of the sets of airworthiness standards published in the Civil Air Regulations.  CAR 3.31, for example, permitted the Administrator to approve aircraft parts as early as 1947.In 1952, the Civil Aeronautics Board adjusted the location of the parts production authority from the "".31"" regulations to the "".18"" regulations.  For example, the CAR 3 authority for modification and replacement parts could be found in section 3.18 after 1952.
In 1955, the Civil Aeronautics Board separated the parts authority out of the airworthiness standards, and placed it in a more general location so that one standard would apply to replacement and modification parts for all different forms of aircraft.In 1965 CAR 1.55 became Federal Aviation Regulation section 21.303.The 1965 regulatory change also imposed specific obligations on the PMA holder related to the Fabrication Inspection System.Amendment 21-38 of Part 21 was published May 26, 1972.  This was the next rule change to affect PMAs.  This rule eliminated the incorporation by reference of type certification requirements in favor of PMA-specific data submission requirements.  This change established the separate process and separate requirements for data that must be submitted by an applicant for a PMA (prior to this there was no explicit distinction between the application data requirements for type certificated products and the data requirements for PMAed articles).The aircraft parts aftermarket expanded greatly in the 1980s as airlines sought to reduce the costs of spares by finding alternative sources of parts. During this time period, though, many manufacturers failed to obtain PMA approvals from the FAA.
In the 1990s, the FAA engaged in an ""Enhanced Enforcement"" program that educated the industry about the importance of approval and as a consequence a huge number of parts were approved through formal FAA mechanisms.   Under this program, companies that had previously manufactured aircraft parts without PMAs could apply for PMAs in order to bring their manufacturing operations into full compliance with the regulations.  This movement brought an explosion of PMA parts to the marketplace.


== 2009 Rule Change ==
The FAA published a significant revision to the U.S. manufacturing regulations on October 16, 2009.  This new rule eliminates some of the legal distinctions between forms of production approval issued by the FAA, which should have the effect of further demonstrating the FAA's support of the quality systems implemented by PMA manufacturers.  Specifically, instead of having a separate body of regulations for a PMA Fabrication Inspection System (FIS), as was the case in prior regulations, the PMA regulations now include a cross reference to the 14 C.F.R. § 21.137, which is the regulation defining the elements of a quality system for all production approval holders.  In practice, all production approval holders were held to the same production quality standards before the rule change - this will now be more obvious in the FAA's regulations.  Accomplishing this harmonization of standards was an important goal of the Modification and Replacement Parts Association (MARPA).
The new rule became effective April 16, 2011.  The  FAA's FAQ on Part 21 stated that PMA quality systems would be evaluated for compliance by the FAA during certificate management activity after the compliance date of the rule.  Today, all FAA production approvals - whether for complete aircraft or for piece parts - rely on a common set of quality assurance system elements.  E.g. 14 C.F.R. §§ 21.137 (quality system requirements for production certificates), 21.307 (requiring PMA holders to establish a quality system that meets the requirements of § 21.137), 21.607 (requiring TSOA holders to establish a quality system that meets the requirements of § 21.137).


== Relationship to repair ==
The FAA is also working on new policies concerning parts fabricated in the course of repair.  This practice has historically been confused with PMA manufacturing, although the two are actually quite different practices supported by different FAA regulations.  Today, FAA Advisory Circular 43.18 provides guidance for the fabrication of parts to be consumed purely during a maintenance operation, and additional guidance is expected to be released in the near future.  One of the key features of FAC 43.18 is that it recommends implementation of a quality assurance system quite similar to the fabrication inspection systems that PMA manufacturers are required to have.


== Industry association ==
The trade association representing the PMA industry is the Modification and Replacement Parts Association (MARPA).  MARPA works closely with the FAA and other agencies to promote PMA safety.  MARPA maintains a website at http://www.pmaparts.org.


== Developments Outside the United States ==
The United States has Bilateral Aviation Safety Agreements (BASA) with most of its major trading partners, and the standard language of these BASAs requires the trading partner to treat FAA-PMA as an importable aircraft part that is airworthy and eligible for installation on aircraft registered in the importing jurisdiction.  This process has been facilitated by the International Air Transport Association (IATA) which has published a book on accepting PMA parts. Although the PMA industry began in the United States, several countries have begun promoting production of approved aircraft parts within their own borders.  These jurisdictions include:

Australia
China
The European Union (which produces them as ""EPA Parts"")Other jurisdictions have established PMA regulations and are working with trading partners to achieve acceptance of their PMA industries, and thus should be expected to enter the PMA marketplace in the near future.  For example, Japan has PMA regulations and has secured a bilateral agreement with the United States that authorizes the export of these parts to the United States as airworthy aircraft parts.  


== References ==


== External links ==
MARPA
YouTube Video: ""What is PMA?""","pandas(index=56, _1=56, text='parts manufacturer approval (pma) is an approval granted by the united states federal aviation administration (faa) to a manufacturer of aircraft parts.   == approval == it is generally illegal in the united states to install replacement or modification parts on a certificated aircraft without an airworthiness release such as a supplemental type certificate (stc) or parts manufacturing approval (pma).  there are a number of other methods of compliance, including parts manufactured to government or industry standards, parts manufactured under technical standard order authorization [tso], owner-/operator-produced parts, experimental aircraft, field approvals, etc.pma-holding manufacturers are permitted to make replacement parts for aircraft, even though they are not the original manufacturer of the aircraft.  the process is analogous to \'after-market\' parts for automobiles, except that the united states aircraft parts production market remains tightly regulated by the faa. an applicant for a pma applies for approval from the faa. the faa prioritizes its review of a new application based on its internal process called project prioritization.the faa order covering the application for pma is order 8110.42 revision d.  this document is worded as instructions to the faa reviewing personnel.  an accompanying advisory circular (ac) 21.303-4 is intended to address the applicant.  8110.42c addressed both the applicant and the reviewer.  per the order, application for a pma can be made per the following ways:  identicality in which the applicant attempts to convince the faa that the pma part is identical to the oah (original approval holder) part.  identicality by licensure is accomplished by providing evidence to the faa that the applicant has licensed the part data from the oah.  this evidence is usually in the form of an assist letter provided to the applicant by the oah.  pma may also be granted based upon prior approval of an stc .  as an example:  if an stc were granted to alter an existing aircraft design then that approval would also apply to the parts needed to make that modification.  a pma would be required, however, to manufacture the parts.  the last method to obtain a pma is test & computation.  this approach consist of one or a combination of both of the following methods:  general analysis and comparative analysis.  general analysis compares the proposed part to the functional requirements of that part when installed.  comparative analysis compares the function of the proposed part to the oah part.  as an example:  if a pma application for flight control cables were to show that the pma part exceeds the pull strength requirements of the aircraft system it is meant for, that is general analysis.  to show that it exceeds that of the oah part is comparative analysis.  the modern trend is to use a variety of techniques in combination in order to obtain approval of complicated parts - relying on the techniques that are most accurate and best able to provide the proof of airworthiness desired.  the cognizant regional faa aircraft certification office (aco) determines if the applicant has shown compliance with all relevant airworthiness regulations and is thus entitled to design approval. the second step in the application process is to apply to the faa manufacturing inspection divisional office (mido) to obtain approval of the manufacturing quality assurance system (known as production approval).  production approval will be granted when the faa is satisfied that the system will not permit parts to leave the system until the parts have been verified to meet the requirements of the approved design, and the system otherwise meets the requirements of the faa quality system regulations.  a production approval holder (pah) will typically already have satisfied this requirement before pma application is made. pma applications based upon licensure or stc do not require aco approval (since the data has already been approved) and can go straight to the mido.   == history == under the civil air regulations (cars), the government had the authority to approve aircraft parts in a predecessor to the pma rules.  this authority was found in each of the sets of airworthiness standards published in the civil air regulations.  car 3.31, for example, permitted the administrator to approve aircraft parts as early as 1947.in 1952, the civil aeronautics board adjusted the location of the parts production authority from the "".31"" regulations to the "".18"" regulations.  for example, the car 3 authority for modification and replacement parts could be found in section 3.18 after 1952. in 1955, the civil aeronautics board separated the parts authority out of the airworthiness standards, and placed it in a more general location so that one standard would apply to replacement and modification parts for all different forms of aircraft.in 1965 car 1.55 became federal aviation regulation section 21.303.the 1965 regulatory change also imposed specific obligations on the pma holder related to the fabrication inspection system.amendment 21-38 of part 21 was published may 26, 1972.  this was the next rule change to affect pmas.  this rule eliminated the incorporation by reference of type certification requirements in favor of pma-specific data submission requirements.  this change established the separate process and separate requirements for data that must be submitted by an applicant for a pma (prior to this there was no explicit distinction between the application data requirements for type certificated products and the data requirements for pmaed articles).the aircraft parts aftermarket expanded greatly in the 1980s as airlines sought to reduce the costs of spares by finding alternative sources of parts. during this time period, though, many manufacturers failed to obtain pma approvals from the faa. in the 1990s, the faa engaged in an ""enhanced enforcement"" program that educated the industry about the importance of approval and as a consequence a huge number of parts were approved through formal faa mechanisms.   under this program, companies that had previously manufactured aircraft parts without pmas could apply for pmas in order to bring their manufacturing operations into full compliance with the regulations.  this movement brought an explosion of pma parts to the marketplace.   == 2009 rule change == the faa published a significant revision to the u.s. manufacturing regulations on october 16, 2009.  this new rule eliminates some of the legal distinctions between forms of production approval issued by the faa, which should have the effect of further demonstrating the faa\'s support of the quality systems implemented by pma manufacturers.  specifically, instead of having a separate body of regulations for a pma fabrication inspection system (fis), as was the case in prior regulations, the pma regulations now include a cross reference to the 14 c.f.r. § 21.137, which is the regulation defining the elements of a quality system for all production approval holders.  in practice, all production approval holders were held to the same production quality standards before the rule change - this will now be more obvious in the faa\'s regulations.  accomplishing this harmonization of standards was an important goal of the modification and replacement parts association (marpa). the new rule became effective april 16, 2011.  the  faa\'s faq on part 21 stated that pma quality systems would be evaluated for compliance by the faa during certificate management activity after the compliance date of the rule.  today, all faa production approvals - whether for complete aircraft or for piece parts - rely on a common set of quality assurance system elements.  e.g. 14 c.f.r. §§ 21.137 (quality system requirements for production certificates), 21.307 (requiring pma holders to establish a quality system that meets the requirements of § 21.137), 21.607 (requiring tsoa holders to establish a quality system that meets the requirements of § 21.137).   == relationship to repair == the faa is also working on new policies concerning parts fabricated in the course of repair.  this practice has historically been confused with pma manufacturing, although the two are actually quite different practices supported by different faa regulations.  today, faa advisory circular 43.18 provides guidance for the fabrication of parts to be consumed purely during a maintenance operation, and additional guidance is expected to be released in the near future.  one of the key features of fac 43.18 is that it recommends implementation of a quality assurance system quite similar to the fabrication inspection systems that pma manufacturers are required to have.   == industry association == the trade association representing the pma industry is the modification and replacement parts association (marpa).  marpa works closely with the faa and other agencies to promote pma safety.  marpa maintains a website at http://www.pmaparts.org.   == developments outside the united states == the united states has bilateral aviation safety agreements (basa) with most of its major trading partners, and the standard language of these basas requires the trading partner to treat faa-pma as an importable aircraft part that is airworthy and eligible for installation on aircraft registered in the importing jurisdiction.  this process has been facilitated by the international air transport association (iata) which has published a book on accepting pma parts. although the pma industry began in the united states, several countries have begun promoting production of approved aircraft parts within their own borders.  these jurisdictions include:  australia china the european union (which produces them as ""epa parts"")other jurisdictions have established pma regulations and are working with trading partners to achieve acceptance of their pma industries, and thus should be expected to enter the pma marketplace in the near future.  for example, japan has pma regulations and has secured a bilateral agreement with the united states that authorizes the export of these parts to the united states as airworthy aircraft parts.   == references ==   == external links == marpa youtube video: ""what is pma?""')"
57,"Decalage on a fixed-wing aircraft is the angle difference between the upper and lower wings of a biplane, i.e. the acute angle contained between the chords of the wings in question.
Decalage is said to be positive when the upper wing has a higher angle of incidence than the lower wing, and negative when the lower wing's incidence is greater than that of the upper wing. Positive decalage results in greater lift from the upper wing than the lower wing, the difference increasing with the amount of decalage.In a survey of representative biplanes, real-life design decalage is typically zero, with both wings having equal incidence.  A notable exception is the Stearman PT-17, which has 4° of incidence in the lower wing, and 3° in the upper wing.  Considered from an aerodynamic perspective, it is desirable to have the forward-most wing stall first, which will induce a pitch-down moment, aiding in stall recovery.  Biplane designers may use incidence to control stalling behavior, but may also use airfoil selection or other means to accomplish correct behavior.
Decalage angle can also refer to the difference in angle of the chord line of the wing and the chord line of the horizontal stabilizer. This is different from the angle of incidence, which refers to the angle of the wing chord to the longitudinal axis of the fuselage, without reference to the horizontal stabilizer.


== References ==


== External links ==
National Advisory Committee for Aeronautics test reports accessed through the Cranfield University AERADE website.","pandas(index=57, _1=57, text=""decalage on a fixed-wing aircraft is the angle difference between the upper and lower wings of a biplane, i.e. the acute angle contained between the chords of the wings in question. decalage is said to be positive when the upper wing has a higher angle of incidence than the lower wing, and negative when the lower wing's incidence is greater than that of the upper wing. positive decalage results in greater lift from the upper wing than the lower wing, the difference increasing with the amount of decalage.in a survey of representative biplanes, real-life design decalage is typically zero, with both wings having equal incidence.  a notable exception is the stearman pt-17, which has 4° of incidence in the lower wing, and 3° in the upper wing.  considered from an aerodynamic perspective, it is desirable to have the forward-most wing stall first, which will induce a pitch-down moment, aiding in stall recovery.  biplane designers may use incidence to control stalling behavior, but may also use airfoil selection or other means to accomplish correct behavior. decalage angle can also refer to the difference in angle of the chord line of the wing and the chord line of the horizontal stabilizer. this is different from the angle of incidence, which refers to the angle of the wing chord to the longitudinal axis of the fuselage, without reference to the horizontal stabilizer.   == references ==   == external links == national advisory committee for aeronautics test reports accessed through the cranfield university aerade website."")"
58,"Flying qualities is one of the three principal regimes in the science of flight test, which also includes performance and systems. Flying qualities involves the study and evaluation of the stability and control characteristics of an aircraft. They have a critical bearing on the safety of flight and on the ease of controlling an airplane in steady flight and in maneuvers.


== Relation to stability ==
To understand the discipline of flying qualities, the concept of stability should be understood. Stability can be defined only when the vehicle is in trim; that is, there are no unbalanced forces or moments acting on the vehicle to cause it to deviate from steady flight. If this condition exists, and if the vehicle is disturbed, stability refers to the tendency of the vehicle to return to the trimmed condition. If the vehicle initially tends to return to a trimmed condition, it is said to be statically stable. If it continues to approach the trimmed condition without overshooting, the motion is called a subsidence. If the motion causes the vehicle to overshoot the trimmed condition, it may oscillate back and forth. If this oscillation damps out, the motion is called a damped oscillation and the vehicle is said to be dynamically stable. On the other hand, if the motion increases in amplitude, the vehicle is said to be dynamically unstable.
The theory of stability of airplanes was worked out by G. H. Bryan in England in 1904. This theory is essentially equivalent to the theory taught to aeronautical students today and was a remarkable intellectual achievement considering that at the time Bryan developed the theory, he had not even heard of the Wright brothers' first flight. Because of the complication of the theory and the tedious computations required in its use, it was rarely applied by airplane designers. Obviously, to fly successfully, pilotless airplanes had to be dynamically stable. The airplane flown by the Wright brothers, and most airplanes flown thereafter, were not stable, but by trial and error, designers developed a few planes that had satisfactory flying qualities. Many other airplanes, however, had poor flying qualities, which sometimes resulted in crashes.
Handling qualities are those characteristics of a flight vehicle that govern the ease and precision with which a pilot is able to perform a flying task. This includes the human-machine interface.  The way in which particular vehicle factors affect flying qualities has been studied in aircraft for decades, and reference standards for the flying qualities of both fixed-wing aircraft and rotary-wing aircraft have been developed and are now in common use. These standards define a subset of the dynamics and control design space that provides good handling qualities for a given vehicle type and flying task.


== Historical development ==
Bryan showed that the stability characteristics of airplanes could be separated into longitudinal and lateral groups with the corresponding motions called modes of motion. These modes of motion were either aperiodic, which means that the airplane steadily approaches or diverges from a trimmed condition, or oscillatory, which means that the airplane oscillates about the trim condition. The longitudinal modes of a statically stable airplane following a disturbance were shown to consist of a long-period oscillation called the phugoid oscillation, usually with a period in seconds about one-quarter of the airspeed in miles per hour and a short-period oscillation with a period of only a few seconds. The lateral motion had three modes of motion: an aperiodic mode called the spiral mode that could be a divergence or subsidence, a heavily damped aperiodic mode called the roll subsidence, and a short-period oscillation, usually poorly damped, called the Dutch roll mode.
Some early airplane designers attempted to make airplanes that were dynamically stable, but it was found that the requirements for stability conflicted with those for satisfactory flying qualities. Meanwhile, no information was available to guide the designer as to just what characteristics should be incorporated to provide satisfactory flying qualities.
By the 1930s, there was a general feeling that airplanes should be dynamically stable, but some aeronautical engineers were starting to recognize the conflict between the requirements for stability and flying qualities. To resolve this question, Edward Warner, who was working as a consultant to the Douglas Aircraft Company on the design of the DC-4, a large four-engine transport airplane, made the first effort in the United States to write a set of requirements for satisfactory flying qualities. Dr. Warner, a member of the main committee of the NACA, also requested that a flight study be made to determine the flying qualities of an airplane along the lines of the suggested requirements. This study was conducted by Hartley A. Soulé of Langley. Entitled Preliminary Investigation of the Flying Qualities of Airplanes, Soulé's report showed several areas in which the suggested requirements needed revision and showed the need for more research on other types of airplanes. As a result, a program was started by Robert R. Gilruth with Melvin N. Gough as the chief test pilot.


== Evaluation of flying qualities ==
The technique for the study of flying qualities requirements used by Gilruth was first to install instruments to record relevant quantities such as control positions and forces, airplane angular velocities, linear accelerations, airspeed, and altitude. Then a program of specified flight conditions and maneuvers was flown by an experienced test pilot. After the flight, data were transcribed from the records and the results were correlated with pilot opinion. This approach would be considered routine today, but it was a notable original contribution by Gilruth that took advantage of the flight recording instruments already available at Langley and the variety of airplanes available for tests under comparable conditions.
An important quantity in flying qualities measurements in turns or pull-ups is the variation of control force on the control stick or wheel with the value of acceleration normal to the flight direction expressed in g units. This quantity is usually called the force per g.


== Relation to Spacecraft ==
A new generation of spacecraft now under development by NASA to replace the Space Shuttle and return astronauts to the Moon will have a manual control capability for several mission tasks, and the ease and precision with which pilots can execute these tasks will have an important effect on performance, mission risk and training costs. No reference standards currently exist for flying qualities of piloted spacecraft.


== See also ==
Flight test
Cooper-Harper rating scale
Pilot-induced oscillation
Longitudinal static stability
Flight envelope


== References ==


== External links ==
William Hewitt Phillips. ""Flying Qualities"". Journey in Aeornautical Research: A Career at NASA Langley Research Center. Retrieved 2010-07-31.
Airplane Stability and Control by Malcolm L. Abzug
Stengel R F: Flight Dynamics. Princeton University Press 2004, ISBN 0-691-11407-2.","pandas(index=58, _1=58, text='flying qualities is one of the three principal regimes in the science of flight test, which also includes performance and systems. flying qualities involves the study and evaluation of the stability and control characteristics of an aircraft. they have a critical bearing on the safety of flight and on the ease of controlling an airplane in steady flight and in maneuvers.   == relation to stability == to understand the discipline of flying qualities, the concept of stability should be understood. stability can be defined only when the vehicle is in trim; that is, there are no unbalanced forces or moments acting on the vehicle to cause it to deviate from steady flight. if this condition exists, and if the vehicle is disturbed, stability refers to the tendency of the vehicle to return to the trimmed condition. if the vehicle initially tends to return to a trimmed condition, it is said to be statically stable. if it continues to approach the trimmed condition without overshooting, the motion is called a subsidence. if the motion causes the vehicle to overshoot the trimmed condition, it may oscillate back and forth. if this oscillation damps out, the motion is called a damped oscillation and the vehicle is said to be dynamically stable. on the other hand, if the motion increases in amplitude, the vehicle is said to be dynamically unstable. the theory of stability of airplanes was worked out by g. h. bryan in england in 1904. this theory is essentially equivalent to the theory taught to aeronautical students today and was a remarkable intellectual achievement considering that at the time bryan developed the theory, he had not even heard of the wright brothers\' first flight. because of the complication of the theory and the tedious computations required in its use, it was rarely applied by airplane designers. obviously, to fly successfully, pilotless airplanes had to be dynamically stable. the airplane flown by the wright brothers, and most airplanes flown thereafter, were not stable, but by trial and error, designers developed a few planes that had satisfactory flying qualities. many other airplanes, however, had poor flying qualities, which sometimes resulted in crashes. handling qualities are those characteristics of a flight vehicle that govern the ease and precision with which a pilot is able to perform a flying task. this includes the human-machine interface.  the way in which particular vehicle factors affect flying qualities has been studied in aircraft for decades, and reference standards for the flying qualities of both fixed-wing aircraft and rotary-wing aircraft have been developed and are now in common use. these standards define a subset of the dynamics and control design space that provides good handling qualities for a given vehicle type and flying task.   == historical development == bryan showed that the stability characteristics of airplanes could be separated into longitudinal and lateral groups with the corresponding motions called modes of motion. these modes of motion were either aperiodic, which means that the airplane steadily approaches or diverges from a trimmed condition, or oscillatory, which means that the airplane oscillates about the trim condition. the longitudinal modes of a statically stable airplane following a disturbance were shown to consist of a long-period oscillation called the phugoid oscillation, usually with a period in seconds about one-quarter of the airspeed in miles per hour and a short-period oscillation with a period of only a few seconds. the lateral motion had three modes of motion: an aperiodic mode called the spiral mode that could be a divergence or subsidence, a heavily damped aperiodic mode called the roll subsidence, and a short-period oscillation, usually poorly damped, called the dutch roll mode. some early airplane designers attempted to make airplanes that were dynamically stable, but it was found that the requirements for stability conflicted with those for satisfactory flying qualities. meanwhile, no information was available to guide the designer as to just what characteristics should be incorporated to provide satisfactory flying qualities. by the 1930s, there was a general feeling that airplanes should be dynamically stable, but some aeronautical engineers were starting to recognize the conflict between the requirements for stability and flying qualities. to resolve this question, edward warner, who was working as a consultant to the douglas aircraft company on the design of the dc-4, a large four-engine transport airplane, made the first effort in the united states to write a set of requirements for satisfactory flying qualities. dr. warner, a member of the main committee of the naca, also requested that a flight study be made to determine the flying qualities of an airplane along the lines of the suggested requirements. this study was conducted by hartley a. soulé of langley. entitled preliminary investigation of the flying qualities of airplanes, soulé\'s report showed several areas in which the suggested requirements needed revision and showed the need for more research on other types of airplanes. as a result, a program was started by robert r. gilruth with melvin n. gough as the chief test pilot.   == evaluation of flying qualities == the technique for the study of flying qualities requirements used by gilruth was first to install instruments to record relevant quantities such as control positions and forces, airplane angular velocities, linear accelerations, airspeed, and altitude. then a program of specified flight conditions and maneuvers was flown by an experienced test pilot. after the flight, data were transcribed from the records and the results were correlated with pilot opinion. this approach would be considered routine today, but it was a notable original contribution by gilruth that took advantage of the flight recording instruments already available at langley and the variety of airplanes available for tests under comparable conditions. an important quantity in flying qualities measurements in turns or pull-ups is the variation of control force on the control stick or wheel with the value of acceleration normal to the flight direction expressed in g units. this quantity is usually called the force per g.   == relation to spacecraft == a new generation of spacecraft now under development by nasa to replace the space shuttle and return astronauts to the moon will have a manual control capability for several mission tasks, and the ease and precision with which pilots can execute these tasks will have an important effect on performance, mission risk and training costs. no reference standards currently exist for flying qualities of piloted spacecraft.   == see also == flight test cooper-harper rating scale pilot-induced oscillation longitudinal static stability flight envelope   == references ==   == external links == william hewitt phillips. ""flying qualities"". journey in aeornautical research: a career at nasa langley research center. retrieved 2010-07-31. airplane stability and control by malcolm l. abzug stengel r f: flight dynamics. princeton university press 2004, isbn 0-691-11407-2.')"
59,"The Parker variable wing is a wing configuration in biplane or triplane aircraft designed by H.F. Parker in 1920. His design allows a supplement in lift while landing or taking-off.
The system is depicted in the figure. The figure shows the biplane configuration. The lower airfoil is rigid. The upper airfoil is flexible. At high angle of attack the flow over the lower airfoil will cause the airflow to bend up and create an upward force at the lower surface of the upper airfoil. This upward force will cause the flexible section to be
pushed upward. The flexible wing section is held at points A and B. The trailing edge is rigid and can rotate about point B. Due to this effect the camber of the airfoil is increased, and hence the lift it creates is increased.


== See also ==
Aeroelasticity
X-53 Active Aeroelastic Wing
Adaptive compliant wing


== References ==","pandas(index=59, _1=59, text='the parker variable wing is a wing configuration in biplane or triplane aircraft designed by h.f. parker in 1920. his design allows a supplement in lift while landing or taking-off. the system is depicted in the figure. the figure shows the biplane configuration. the lower airfoil is rigid. the upper airfoil is flexible. at high angle of attack the flow over the lower airfoil will cause the airflow to bend up and create an upward force at the lower surface of the upper airfoil. this upward force will cause the flexible section to be pushed upward. the flexible wing section is held at points a and b. the trailing edge is rigid and can rotate about point b. due to this effect the camber of the airfoil is increased, and hence the lift it creates is increased.   == see also == aeroelasticity x-53 active aeroelastic wing adaptive compliant wing   == references ==')"
60,"A swept wing is a wing that angles either backward or occasionally forward from its root rather than in a straight sideways direction.
Swept wings have been flown since the pioneer days of aviation. Wing sweep at high speeds was first investigated in Germany as early as 1935 by Albert Betz and Adolph Busemann, finding application just before the end of the Second World War. It has the effect of delaying the shock waves and accompanying aerodynamic drag rise caused by fluid compressibility near the speed of sound, improving performance. Swept wings are therefore almost always used on jet aircraft designed to fly at these speeds. Swept wings are also sometimes used for other reasons, such as low drag, low observability, structural convenience or pilot visibility.
The term ""swept wing"" is normally used to mean ""swept back"", but variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back. The delta wing is also aerodynamically a form of swept wing.


== Design characteristics ==
For a wing of given span, sweeping it increases the length of the spars running along it from root to tip. This tends to increase weight and reduce stiffness. If the fore-aft chord of the wing also remains the same, the distance between leading and trailing edges reduces, reducing its ability to resist twisting (torsion) forces. A swept wing of given span and chord must therefore be strengthened and will be heavier than the equivalent unswept wing.
A swept wing typically angles backward from its root rather than forwards. Because wings are made as light as possible, they tend to flex under load. This aeroelasticity under aerodynamic load causes the tips to bend upwards in normal flight. Backwards sweep causes the tips to reduce their angle of attack as they bend, reducing their lift and limiting the effect. Forward sweep causes the tips to increase their angle of attack as they bend. This increases their lift causing further bending and hence yet more lift in a cycle which can cause a runaway structural failure. For this reason forward sweep is rare and the wing must be unusually rigid.
The characteristic ""sweep angle"" is normally measured by drawing a line from root to tip, typically 25% of the way back from the leading edge, and comparing that with the perpendicular to the longitudinal axis of the aircraft.  Typical sweep angles vary from 0 for a straight-wing aircraft, to 45 degrees or more for fighters and other high-speed designs.


== Aerodynamics ==


=== Subsonic and transonic flight ===

As an aircraft enters the transonic speeds just below the speed of sound, the pressure waves associated with subsonic flight converge and begin to impinge on the aircraft. As the pressure waves converge the air in front of the aircraft begins to compress. This creates a force known as wave drag. This wave drag increases steeply until the whole aircraft is supersonic and then reduces.
However, shock waves can form on some parts of an aircraft moving at less than the speed of sound. Low-pressure regions around an aircraft cause the flow to accelerate, and at transonic speeds this local acceleration can exceed Mach 1. Localized supersonic flow must return to the freestream conditions around the rest of the aircraft, and as the flow enters an adverse pressure gradient in the aft section of the wing, a discontinuity emerges in the form of a shock wave as the air is forced to rapidly slow and return to ambient pressure.
With objects where there is a sudden reduction in profile/thickness and the local air expands rapidly to fill the space taken by the solid object or where a rapid angular change is imparted to the airflow causing a momentary increase of volume/decrease in density, an oblique shock wave is generated. This is why shock waves are often associated with the part of a fighter aircraft cockpit canopy with the highest local curvature, appearing immediately behind this point.
At the point where the density drops, the local speed of sound correspondingly drops and a shock wave can form. This is why in conventional wings, shock waves form first after the maximum Thickness/Chord and why all airliners designed for cruising in the transonic range (above M0.8) have supercritical wings that are flatter on top resulting in minimized angular change of flow to upper surface air. The angular change to the air that is normally part of lift generation is decreased and this lift reduction is compensated for by deeper curved lower surfaces accompanied by a reflex curve at the trailing edge. This results in a much weaker standing shock wave towards the rear of the upper wing surface and a corresponding increase in critical mach number.
Shock waves require energy to form. This energy is taken out of the aircraft, which has to supply extra thrust to make up for this energy loss. Thus the shocks are seen as a form of drag. Since the shocks form when the local air velocity reaches supersonic speeds, there is a certain ""critical mach"" speed where sonic flow first appears on the wing. There is a following point called the drag divergence mach number where the effect of the drag from the shocks becomes noticeable.  This is normally when the shocks start generating over the wing, which on most aircraft is the largest continually curved surface, and therefore the largest contributor to this effect.
Sweeping the wing has the effect of reducing the curvature of the body as seen from the airflow, by the cosine of the angle of sweep. For instance, a wing with a 45 degree sweep will see a reduction in effective curvature to about 70% of its straight-wing value. This has the effect of increasing the critical Mach by 30%. When applied to large areas of the aircraft, like the wings and empennage, this allows the aircraft to reach speeds closer to Mach 1.
One of the simplest and best explanations of how the swept wing works was offered by Robert T. Jones:
""Suppose a cylindrical wing (constant chord, incidence, etc.) is placed in an airstream at an angle of yaw – i.e., it is swept back. Now, even if the local speed of the air on the upper surface of the wing becomes supersonic, a shock wave cannot form there because it would have to be a sweptback shock – swept at the same angle as the wing – i.e., it would be an oblique shock. Such an oblique shock cannot form until the velocity component normal to it becomes supersonic.""One limiting factor in swept wing design is the so-called ""middle effect"". If a swept wing is continuous – an oblique swept wing, the pressure iso-bars will be swept at a continuous angle from tip to tip. However, if the left and right halves are swept back equally, as is common practice, the pressure iso-bars on the left wing in theory will meet the pressure iso-bars of the right wing on the centerline at a large angle.  As the iso-bars cannot meet in such a fashion, they will tend to curve on each side as they near the centerline, so that the iso-bars cross the centerline at right angles to the centerline. This causes an ""unsweeping"" of the iso-bars in the wing root region.  To combat this unsweeping, German aerodynamicist Dietrich Küchemann proposed and had tested a local indentation of the fuselage above and below the wing root. This proved to not be very effective. During the development of the Douglas DC-8 airliner, uncambered airfoils were used in the wing root area to combat the unsweeping. Similarly, a decambered wing root glove was added to the Boeing 707 wing to create the Boeing 720.


=== Supersonic flight ===

Airflow at supersonic speeds generates lift through the formation of shock waves, as opposed to the patterns of airflow over and under the wing. These shock waves, as in the transonic case, generate large amounts of drag. One of these shock waves is created by the leading edge of the wing, but contributes little to the lift. In order to minimize the strength of this shock it needs to remain ""attached"" to the front of the wing, which demands a very sharp leading edge. To better shape the shocks that will contribute to lift, the rest of an ideal supersonic airfoil is roughly diamond-shaped in cross-section. For low-speed lift these same airfoils are very inefficient, leading to poor handling and very high landing speeds.One way to avoid the need for a dedicated supersonic wing is to use a highly swept subsonic design. Airflow behind the shock waves of a moving body are reduced to subsonic speeds. This effect is used within the intakes of engines meant to operate in the supersonic, as jet engines are generally incapable of ingesting supersonic air directly. This can also be used to reduce the speed of the air as seen by the wing, using the shocks generated by the nose of the aircraft. As long as the wing lies behind the cone-shaped shock wave, it will ""see"" subsonic airflow and work as normal. The angle needed to lie behind the cone increases with increasing speed, at Mach 1.3 the angle is about 45 degrees, at Mach 2.0 it is 60 degrees. For instance, at Mach 1.3 the angle of the Mach cone formed off the body of the aircraft will be at about sinμ = 1/M (μ is the sweep angle of the Mach cone)Generally it is not possible to arrange the wing so it will lie entirely outside the supersonic airflow and still have good subsonic performance. Some aircraft, like the English Electric Lightning are tuned almost entirely for high-speed flight and feature highly swept wings that make little to no compromise for the low-speed problems that such a profile creates. In other cases the use of variable geometry wings, as on the Grumman F-14 Tomcat and Panavia Tornado, allows an aircraft to move the wing to keep it at the most efficient angle regardless of speed, although the drawbacks incurred of increased complexity and weight have led to this being a rare feature.Most high-speed aircraft have a wing that spends at least some of its time in the supersonic airflow. But since the shock cone moves towards the fuselage with increased speed (that is, the cone becomes narrower), the portion of the wing in the supersonic flow also changes with speed. Since these wings are swept, as the shock cone moves inward, the lift vector moves forward as the outer, rearward portions of the wing are generating less lift. This results in powerful pitching moments and their associated required trim changes.


=== Disadvantages ===

When a swept wing travels at high speed, the airflow has little time to react and simply flows over the wing almost straight from front to back. At lower speeds the air does have time to react, and is pushed spanwise by the angled leading edge, towards the wing tip. At the wing root, by the fuselage, this has little noticeable effect, but as one moves towards the wingtip the airflow is pushed spanwise not only by the leading edge, but the spanwise moving air beside it. At the tip the airflow is moving along the wing instead of over it, a problem known as spanwise flow.
The lift from a wing is generated by the airflow over it from front to rear. With increasing span-wise flow the boundary layers on the surface of the wing have longer to travel, and so are thicker and more susceptible to transition to turbulence or flow separation, also the effective aspect ratio of the wing is less and so air ""leaks"" around the wing tips reducing their effectiveness. The spanwise flow on swept wings produces airflow that moves the stagnation point on the leading edge of any individual wing segment further beneath the leading edge, increasing effective angle of attack of wing segments relative to its neighbouring forward segment. The result is that wing segments farther towards the rear operate at increasingly higher angles of attack promoting early stall of those segments. This promotes tip stall on back swept wings, as the tips are most rearward, while delaying tip stall for forward swept wings, where the tips are forward. With both forward and back swept wings, the rear of the wing will stall first. This creates a nose-up pressure on the aircraft. If this is not corrected by the pilot it causes the plane to pitch up, leading to more of the wing stalling, leading to more pitch up, and so on. This problem came to be known as the Sabre dance in reference to the number of North American F-100 Super Sabres that crashed on landing as a result.The solution to this problem took on many forms. One was the addition of a fin known as a wing fence on the upper surface of the wing to redirect the flow to the rear ; the MiG-15 was one example of an aircraft fitted with wing fences. Another closely related design was addition of a dogtooth notch to the leading edge, as present on the Avro Arrow interceptor. Other designs took a more radical approach, including the Republic XF-91 Thunderceptor's wing that grew wider towards the tip to provide more lift at the tip. The Handley Page Victor was equipped with a crescent wing, featuring substantial sweep-back near the wing root where the wing was thickest, and progressively reducing sweep along the span as the wing thickness reduced towards the tip.Modern solutions to the problem no longer require ""custom"" designs such as these. The addition of leading-edge slats and large compound flaps to the wings has largely resolved the issue. On fighter designs, the addition of leading-edge extensions, which are typically included to achieve a high level of maneuverability, also serve to add lift during landing and reduce the problem.The swept wing also has several more problems. One is that for any given length of wing, the actual span from tip-to-tip is shorter than the same wing that is not swept. Low speed drag is strongly correlated with the aspect ratio, the span compared to chord, so a swept wing always has more drag at lower speeds. Another concern is the torque applied by the wing to the fuselage, as much of the wing's lift lies behind the point where the wing root connects to the plane. Finally, while it is fairly easy to run the main spars of the wing right through the fuselage in a straight wing design to use a single continuous piece of metal, this is not possible on the swept wing because the spars will meet at an angle.


=== Sweep theory ===
Sweep theory is an aeronautical engineering description of the behavior of airflow over a wing when the wing's leading edge encounters the airflow at an oblique angle. The development of sweep theory resulted in the swept wing design used by most modern jet aircraft, as this design performs more effectively at transonic and supersonic speeds. In its advanced form, sweep theory led to the experimental oblique wing concept.
Adolf Busemann introduced the concept of the swept wing and presented this in 1935 at the 5. Volta-Congress in Rome.  Sweep theory in general was a subject of development and investigation throughout the 1930s and 1940s, but the breakthrough mathematical definition of sweep theory is generally credited to NACA's Robert T. Jones in 1945. Sweep theory builds on other wing lift theories. Lifting line theory describes lift generated by a straight wing (a wing in which the leading edge is perpendicular to the airflow). Weissinger theory describes the distribution of lift for a swept wing, but does not have the capability to include chordwise pressure distribution. There are other methods that do describe chordwise distributions, but they have other limitations. Jones' sweep theory provides a simple, comprehensive analysis of swept wing performance.
To visualize the basic concept of simple sweep theory, consider a straight, non-swept wing of infinite length, which meets the airflow at a perpendicular angle. The resulting air pressure distribution is equivalent to the length of the wing's chord (the distance from the leading edge to the trailing edge). If we were to begin to slide the wing sideways (spanwise), the sideways motion of the wing relative to the air would be added to the previously perpendicular airflow, resulting in an airflow over the wing at an angle to the leading edge. This angle results in airflow traveling a greater distance from leading edge to trailing edge, and thus the air pressure is distributed over a greater distance (and consequently lessened at any particular point on the surface).
This scenario is identical to the airflow experienced by a swept wing as it travels through the air. The airflow over a swept wing encounters the wing at an angle. That angle can be broken down into two vectors, one perpendicular to the wing, and one parallel to the wing. The flow parallel to the wing has no effect on it, and since the perpendicular vector is shorter (meaning slower) than the actual airflow, it consequently exerts less pressure on the wing. In other words, the wing experiences airflow that is slower - and at lower pressures - than the actual speed of the aircraft.
One of the factors that must be taken into account when designing a high-speed wing is compressibility, which is the effect that acts upon a wing as it approaches and passes through the speed of sound. The significant negative effects of compressibility made it a prime issue with aeronautical engineers. Sweep theory helps mitigate the effects of compressibility in transonic and supersonic aircraft because of the reduced pressures. This allows the mach number of an aircraft to be higher than that actually experienced by the wing.
There is also a negative aspect to sweep theory. The lift produced by a wing is directly related to the speed of the air over the wing. Since the airflow speed experienced by a swept wing is lower than what the actual aircraft speed is, this becomes a problem during slow-flight phases, such as takeoff and landing. There have been various ways of addressing the problem, including the variable-incidence wing design on the Vought F-8 Crusader, and swing wings on aircraft such as the F-14, F-111, and the Panavia Tornado.


== Variant designs ==
The term ""swept wing"" is normally used to mean ""swept back"", but other swept variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back.  The delta wing also incorporates the same advantages as part of its layout.


=== Forward sweep ===

Sweeping a wing forward has approximately the same effect as rearward in terms of drag reduction, but has other advantages in terms of low-speed handling where tip stall problems simply go away. In this case the low-speed air flows towards the fuselage, which acts as a very large wing fence. Additionally, wings are generally larger at the root anyway, which allows them to have better low-speed lift.
However, this arrangement also has serious stability problems. The rearmost section of the wing will stall first causing a pitch-up moment pushing the aircraft further into stall similar to a swept back wing design. Thus swept-forward wings are unstable in a fashion similar to the low-speed problems of a conventional swept wing. However unlike swept back wings, the tips on a forward swept design will stall last, maintaining roll control.
Forward-swept wings can also experience dangerous flexing effects compared to aft-swept wings that can negate the tip stall advantage if the wing is not sufficiently stiff. In aft-swept designs, when the airplane maneuvers at high load factor the wing loading and geometry twists the wing in such a way as to create washout (tip twists leading edge down). This reduces the angle of attack at the tip, thus reducing the bending moment on the wing, as well as somewhat reducing the chance of tip stall.  However, the same effect on forward-swept wings produces a wash-in effect that increases the angle of attack promoting tip stall.
Small amounts of sweep do not cause serious problems, and had been used on a variety of aircraft to move the spar into a convenient location, as on the Junkers Ju 287 or HFB 320 Hansa Jet. However, larger sweep suitable for high-speed aircraft, like fighters, was generally impossible until the introduction of fly by wire systems that could react quickly enough to damp out these instabilities. The Grumman X-29 was an experimental technology demonstration project designed to test the forward swept wing for enhanced maneuverability during the 1980s. The Sukhoi Su-47 Berkut is another notable demonstrator aircraft implementing this technology to achieve high levels of agility. To date, no highly swept-forward design has entered production.


== History ==


=== Early history ===
The first successful aeroplanes adhered to the basic design of rectangular wings at right angles to the body of the machine, but there were experimentalists who explored other geometries to achieve better aerodynamic results. The swept wing geometry appeared before World War I, and was conceived as a means of permitting the design of safe and stable aeroplanes. The best of these designs imposed ""self-damping"" inherent stability upon a tailless swept wing. These inspired several flying wing gliders and some powered aircraft during the interwar years.

The first to achieve stability was British designer J. W. Dunne who was obsessed with achieving inherent stability in flight. He successfully employed swept wings in his tailless aircraft (which, crucially, used washout) as a means of creating positive longitudinal static stability. For a low-speed aircraft, swept wings may be used to resolve problems with the center of gravity, to move the wing spar into a more convenient location, or to improve the sideways view from the pilot's position. By 1905, Dunne had already built a model glider with swept wings and by 1913 he had constructed successful powered variants that were able to cross the English Channel. The Dunne D.5 was exceptionally aerodynamically stable for the time, and the D.8 was sold to the Royal Flying Corps; it was also manufactured under licence by Starling Burgess to the United States Navy amongst other customers.Dunne's work ceased with the onset of war in 1914, but afterwards the idea was taken up by G. T. R. Hill in England who designed a series of gliders and aircraft to Dunne's guidelines, notably the Westland-Hill Pterodactyl series. However, Dunne's theories met with little acceptance amongst the leading aircraft designers and aviation companies at the time.


=== German developments ===

The idea of using swept wings to reduce high-speed drag was developed in Germany in the 1930s. At a Volta Conference meeting in 1935 in Italy, Dr. Adolf Busemann suggested the use of swept wings for supersonic flight. He noted that the airspeed over the wing was dominated by the normal component of the airflow, not the freestream velocity, so by setting the wing at an angle the forward velocity at which the shock waves would form would be higher (the same had been noted by Max Munk in 1924, although not in the context of high-speed flight). Albert Betz immediately suggested the same effect would be equally useful in the transonic. After the presentation the host of the meeting, Arturo Crocco, jokingly sketched ""Busemann's airplane of the future"" on the back of a menu while they all dined. Crocco's sketch showed a classic 1950's fighter design, with swept wings and tail surfaces, although he also sketched a swept propeller powering it.At the time, however, there was no way to power an aircraft to these sorts of speeds, and even the fastest aircraft of the era were only approaching 400 km/h (249 mph).The presentation was largely of academic interest, and soon forgotten. Even notable attendees including Theodore von Kármán and Eastman Jacobs did not recall the presentation 10 years later when it was re-introduced to them.Hubert Ludwieg of the High-Speed Aerodynamics Branch at the AVA Göttingen in 1939 conducted the first wind tunnel tests to investigate Busemann's theory. Two wings, one with no sweep, and one with 45 degrees of sweep were tested at Mach numbers of 0.7 and 0.9 in the 11 x 13 cm wind tunnel.  The results of these tests confirmed the drag reduction offered by swept wings at transonic speeds.  The results of the tests were communicated to Albert Betz who then passed them on to Willy Messerschmitt in December 1939.  The tests were expanded in 1940 to include wings with 15, 30 and -45 degrees of sweep and Mach numbers as high as 1.21.With the introduction of jets in the later half of the Second World War, the swept wing became increasingly applicable to optimally satisfying aerodynamic needs. The German jet-powered Messerschmitt Me 262 and rocket-powered Messerschmitt Me 163 suffered from compressibility effects that made both aircraft very difficult to control at high speeds. In addition, the speeds put them into the wave drag regime, and anything that could reduce this drag would increase the performance of their aircraft, notably the notoriously short flight times measured in minutes. This resulted in a crash program to introduce new swept wing designs, both for fighters as well as bombers. The Blohm & Voss P 215 was designed to take full advantage of the swept wing's aerodynamic properties; however, an order for three prototypes was received only weeks before the war ended and no examples were ever built. The Focke-Wulf Ta 183 was another swept wing fighter design, but was also not produced before the war's end. In the post-war era, Kurt Tank developed the Ta 183 into the IAe Pulqui II, but this proved unsuccessful.A prototype test aircraft, the Messerschmitt Me P.1101, was built to research the tradeoffs of the design and develop general rules about what angle of sweep to use. When it was 80% complete, the P.1101 was captured by US forces and returned to the United States, where two additional copies with US-built engines carried on the research as the Bell X-5. Germany's wartime experience with the swept wings and its high value for supersonic flight stood in strong contract to the prevailing views of Allied experts of the era, who commonly espoused their belief in the impossibility of manned vehicles travelling at such speeds.


=== Postwar advancements ===

During the immediate post-war era, several nations were conducting research into high speed aircraft. In the United Kingdom, work commenced during 1943 on the Miles M-52, a high-speed experimental aircraft equipped with a straight wing that was developed in conjunction with Frank Whittle's Power Jets company, the Royal Aircraft Establishment (RAE) in Farnborough, and the National Physical Laboratory. Despite being envisioned to be capable of achieving 1,000 miles per hour (1,600 km/h) in level flight, thus enabling the aircraft to potentially be the first to exceed the speed of sound in the world, in February 1946, the programme was abrupted discontinued for unclear reasons. It has since been widely recognised that the cancellation of the M.52 was a major setback in British progress in the field of supersonic design.Another, more successful, programme was the US's Bell X-1, which also was equipped with a straight wing. According to Miles Chief Aerodynamicist Dennis Bancroft, the Bell Aircraft company was given access to the drawings and research on the M.52. On 14 October 1947, the Bell X-1 performed the first manned supersonic flight, piloted by Captain Charles ""Chuck"" Yeager, having been drop launched from the bomb bay of a Boeing B-29 Superfortress and attained a record-breaking speed of Mach 1.06 (700 miles per hour (1,100 km/h; 610 kn)). The news of a successful straight-wing supersonic aircraft surprised many aeronautical experts on both sides of the Atlantic, as it was increasingly believed that a swept-wing design not only highly beneficial but also necessary to break the sound barrier.

During the final years of the Second World War, aircraft designer Sir Geoffrey de Havilland commenced development on the de Havilland Comet, which would become the world's first jet airliner. An early design consideration was whether to apply the new swept-wing configuration. Thus, an experimental aircraft to explore the technology, the de Havilland DH 108, was developed by the firm in 1944, headed by project engineer John Carver Meadows Frost with a team of 8–10 draughtsmen and engineers. The DH 108 primarily consisted of the pairing of the front fuselage of the de Havilland Vampire to a swept wing and compact stubby vertical tail; it was the first British swept wing jet, unofficially known as the ""Swallow"". It first flew on 15 May 1946, a mere eight months after the project's go-ahead. Company test pilot and son of the builder, Geoffrey de Havilland Jr., flew the first of three aircraft and found it extremely fast – fast enough to try for a world speed record. On 12 April 1948, a D.H.108 did set a world's speed record at 973.65 km/h (605 mph), it subsequently became the first jet aircraft to exceed the speed of sound.Around this same timeframe, the Air Ministry introduced a program of experimental aircraft to examine the effects of swept wings, as well as the delta wing configuration. Furthermore, the Royal Air Force (RAF) identified a pair of proposed fighter aircraft equipped with swept wings from Hawker Aircraft and Supermarine, the Hawker Hunter and Supermarine Swift respectively, and successfully pressed for orders to be placed 'off the drawing board' in 1950. On 7 September 1953, the sole Hunter Mk 3 (the modified first prototype, WB 188) flown by Neville Duke broke the world air speed record for jet-powered aircraft, attaining a speed of 727.63 mph (1,171.01 km/h) over Littlehampton, West Sussex. This world record stood for less than three weeks before being broken on 25 September 1953 by the Hunter's early rival, the Supermarine Swift, being flown by Michael Lithgow.In February 1945, NACA engineer Robert T. Jones started looking at highly swept delta wings and V shapes, and discovered the same effects as Busemann. He finished a detailed report on the concept in April, but found his work was heavily criticised by other members of NACA Langley, notably Theodore Theodorsen, who referred to it as ""hocus-pocus"" and demanded some ""real mathematics"". However, Jones had already secured some time for free-flight models under the direction of Robert Gilruth, whose reports were presented at the end of May and showed a fourfold decrease in drag at high speeds. All of this was compiled into a report published on June 21, 1945, which was sent out to the industry three weeks later. Ironically, by this point Busemann's work had already been passed around.

On May 1945, the American Operation Paperclip reached Braunschweig, where US personnel discovered a number of swept wing models and a mass of technical data from the wind tunnels. One member of the US team was George S. Schairer, who was at that time working at the Boeing company. He immediately forwarded a letter to Ben Cohn at Boeing, communicating the value of the swept wing concept. He also told Cohn to distribute the letter to other companies as well, although only Boeing and North American made immediate use of it.Boeing was in the midst of designing the B-47 Stratojet, and the initial Model 424 was a straight-wing design similar to the B-45, B-46 and B-48 it competed with. Analysis by Boeing engineer Vic Ganzer suggested an optimum sweepback angle of about 35 degrees. By September 1945, the Braunschweig data had been worked into the design, which re-emerged as the Model 448, a larger six-engine design with more robust wings swept at 35 degrees. Another re-work moved the engines into strut-mounted pods under the wings due to concerns of the uncontained failure of an internal engine could potentially destroy the aircraft via either fire or vibration. The resulting B-47 was hailed as the fastest of its class in the world during the late 1940s, and trounced the straight-winged competition. Boeing's jet-transport formula of swept wings and pylon-mounted engines has since been universally adopted.In fighters, North American Aviation was in the midst of working on a straight-wing jet-powered naval fighter, then known as the FJ-1; it was later submitted to the United States Air Force as the XP-86. Larry Green, who could read German, studied the Busemann reports and convinced management to allow a redesign starting in August 1945. The performance of the F-86A allowed it set the first of several official world speed records, attaining 671 miles per hour (1,080 km/h) on 15 September 1948, flown by Major Richard L. Johnson. With the appearance of the MiG-15, the F-86 was rushed into combat, while straight-wing jets like the Lockheed P-80 Shooting Star and Republic F-84 Thunderjet were quickly relegated to ground attack missions. Some, such as the F-84 and Grumman F-9 Cougar, were later redesigned with swept wings from straight-winged aircraft. Later planes, such as the North American F-100 Super Sabre, would be designed with swept wings from the start, though additional innovations such as the afterburner, area-rule and new control surfaces would be necessary to master supersonic flight.

The Soviet Union was also intrigued about the idea of swept wings on aircraft, when their ""captured aviation technology"" counterparts to the western Allies spread out across the defeated Third Reich. Artem Mikoyan was asked by the Soviet government's TsAGI aviation research department to develop a test-bed aircraft to research the swept wing idea — the result was the late 1945-flown, unusual MiG-8 Utka pusher canard layout aircraft, with its rearwards-located wings being swept back for this type of research. The swept wing was applied to the MiG-15, an early jet-powered fighter, its maximum speed of 1,075 km/h (668 mph) outclassed the straight-winged American jets and piston-engined fighters initially deployed during the Korean War. The MiG-15 is believed to have been one of the most produced jet aircraft; in excess of 13,000 would ultimately be manufactured.The MiG-15, which could not safely exceed Mach 0.92, served as the basis for the MiG-17, which was designed to be controllable at higher Mach numbers. Its wing featured a ""sickle sweep"" compound shape, somewhat similar to the F-100 Super Sabre, with a 45° angle near the fuselage and a 42° angle for the outboard part of the wings. A further derivative of the design, designated MiG-19, featured a relatively thin wing suited to supersonic flight that was designed at TsAGI, the Soviet Central Aerohydrodynamic Institute; swept back at an angle of 55 degrees, this wing featured a single wing fence on each side. A specialist high-altitude variant, the Mig-19SV, featured, amongst other changes, flap adjusted to generate greater lift at higher altitudes, helping to increase the aircraft's ceiling from 17,500 m (57,400 ft) to 18,500 m (60,700 ft).Germany's swept wing research also made its way to the Swedish aircraft manufacturer SAAB, allegedly via a group of ex-Messerschmitt engineers that had fled to Switzerland during late 1945. At the time, SAAB was eager to make aeronautic advances, particularly in the new field of jet propulsion. The company incorporated both the jet engine and the swept wing to produce the Saab 29 Tunnan fighter; on 1 September 1948, the first prototype conducted its maiden flight, flown by the English test pilot S/L Robert A. 'Bob' Moore, DFC and bar, Although not well known outside Sweden, the Tunnan was the first Western European fighter to be introduced with such a wing configuration. In parallel, SAAB also developed another swept wing aircraft, the Saab 32 Lansen, primarily to serve as Sweden's standard attack aircraft. Its wing, which had a 10 per cent laminar profile and a 35° sweep, featured triangular fences near the wing roots in order to improve airflow when the aircraft was being flown at a high angle of attack. On 25 October 1953, a SAAB 32 Lansen attained a Mach number of at least 1.12 while in a shallow dive, exceeding the sound barrier.

The dramatic successes of aircraft such as Hawker Hunter, the B-47, and F-86 embodied the widespread acceptance of the swept wing research acquired from Germany. Eventually, almost all advanced design efforts would incorporate a swept wing configuration. The classic Boeing B-52, designed in the 1950s, continues in service as a high-subsonic long-range heavy bomber despite the development of the triple-sonic North American B-70 Valkyrie, supersonic swing-wing Rockwell B-1 Lancer, and flying wing designs. While the Soviets never matched the performance of the Boeing B-52 Stratofortress with a jet aircraft, the intercontinental range Tupolev Tu-95 turboprop bomber with its near-jet class top speed of 920 km/h, combining swept wings with propeller propulsion, also remains in service today, being the fastest propeller-powered production aircraft. In Britain, a range of swept-wing bombers were designed, these being the Vickers Valiant (1951), the Avro Vulcan (1952), and the Handley Page Victor (1952).By the early 1950s, nearly every new fighter was either rebuilt or designed from scratch with a swept wing. By the 1960s, most civilian jets also adopted swept wings. The Douglas A-4 Skyhawk and Douglas F4D Skyray were examples of delta wings that also have swept leading edges with or without a tail. Most early transonic and supersonic designs such as the MiG-19 and F-100 used long, highly swept wings. Swept wings would reach Mach 2 in the arrow-winged BAC Lightning, and stubby winged Republic F-105 Thunderchief, which was found to be wanting in turning ability in Vietnam combat. By the late 1960s, the F-4 Phantom and Mikoyan-Gurevich MiG-21 that both used variants on tailed delta wings came to dominate front line air forces. Variable geometry wings were employed on the American F-111, Grumman F-14 Tomcat and Soviet Mikoyan MiG-27, although the idea would be abandoned for the American SST design. After the 1970s, most newer generation fighters optimized for maneuvering air combat since the USAF F-15 and Soviet Mikoyan MiG-29 have employed relatively short-span fixed wings with relatively large wing area.


== See also ==
Delta wing
Theodore von Kármán, first to recognize the importance of the swept wing
Trapezoidal wing
Wing configuration


== References ==


=== Citations ===


=== Bibliography ===


== Further reading ==
""The High-speed Shape: Pitch-up and palliatives adopted on swept-wing aircraft"", Flight International, 2 January 1964


== External links ==
Swept Wings and Effective Dihedral
The development of swept wings
Simple sweep theory math
Advanced math of swept and oblique wings
The L-39 and swept wing research
Sweep theory in a 3D environment
CFD results showing the three-dimensional supersonic bubble over the wing of an A 320. Another CFD result showing the MDXX and how the shock vanishes close to the fuselage where the aerofoil is more slender","pandas(index=60, _1=60, text='a swept wing is a wing that angles either backward or occasionally forward from its root rather than in a straight sideways direction. swept wings have been flown since the pioneer days of aviation. wing sweep at high speeds was first investigated in germany as early as 1935 by albert betz and adolph busemann, finding application just before the end of the second world war. it has the effect of delaying the shock waves and accompanying aerodynamic drag rise caused by fluid compressibility near the speed of sound, improving performance. swept wings are therefore almost always used on jet aircraft designed to fly at these speeds. swept wings are also sometimes used for other reasons, such as low drag, low observability, structural convenience or pilot visibility. the term ""swept wing"" is normally used to mean ""swept back"", but variants include forward sweep, variable sweep wings and oblique wings in which one side sweeps forward and the other back. the delta wing is also aerodynamically a form of swept wing.   == design characteristics == for a wing of given span, sweeping it increases the length of the spars running along it from root to tip. this tends to increase weight and reduce stiffness. if the fore-aft chord of the wing also remains the same, the distance between leading and trailing edges reduces, reducing its ability to resist twisting (torsion) forces. a swept wing of given span and chord must therefore be strengthened and will be heavier than the equivalent unswept wing. a swept wing typically angles backward from its root rather than forwards. because wings are made as light as possible, they tend to flex under load. this aeroelasticity under aerodynamic load causes the tips to bend upwards in normal flight. backwards sweep causes the tips to reduce their angle of attack as they bend, reducing their lift and limiting the effect. forward sweep causes the tips to increase their angle of attack as they bend. this increases their lift causing further bending and hence yet more lift in a cycle which can cause a runaway structural failure. for this reason forward sweep is rare and the wing must be unusually rigid. the characteristic ""sweep angle"" is normally measured by drawing a line from root to tip, typically 25% of the way back from the leading edge, and comparing that with the perpendicular to the longitudinal axis of the aircraft.  typical sweep angles vary from 0 for a straight-wing aircraft, to 45 degrees or more for fighters and other high-speed designs.   == aerodynamics == == further reading == ""the high-speed shape: pitch-up and palliatives adopted on swept-wing aircraft"", flight international, 2 january 1964   == external links == swept wings and effective dihedral the development of swept wings simple sweep theory math advanced math of swept and oblique wings the l-39 and swept wing research sweep theory in a 3d environment cfd results showing the three-dimensional supersonic bubble over the wing of an a 320. another cfd result showing the mdxx and how the shock vanishes close to the fuselage where the aerofoil is more slender')"
61,"Space Power Facility (SPF) is a NASA facility used to test spaceflight hardware under simulated launch and spaceflight conditions. The SPF is part of NASA's Plum Brook Station, which in turn is part of the Glenn Research Center. The Plum Brook Station and the SPF are located near Sandusky, Ohio (Oxford Township, Erie County, Ohio).
The SPF is able to simulate a spacecraft's launch environment, as well as in-space environments. NASA has developed these capabilities under one roof to optimize testing of spaceflight hardware while minimizing transportation issues. Space Power Facility has become a ""One Stop Shop"" to qualify flight hardware for manned space flight. This facility provides the capability to perform the following environmental testing:

Thermal-vacuum testing
Reverberation acoustic testing
Mechanical vibration testing
Modal testing
Electromagnetic interference and compatibility testing


== Thermal-Vacuum Test Chamber ==
The Space Power Facility (SPF) is a vacuum chamber built by NASA in 1969. It stands 122 feet (37 m) high and 100 feet (30 m) in diameter, enclosing a bullet-shaped space. It is the world's largest thermal vacuum chamber. It was originally commissioned for nuclear-electric power studies under vacuum conditions, but was later decommissioned. Recently, it was recommissioned for use in testing spacecraft propulsion systems. Recent uses include testing the airbag landing systems for the Mars Pathfinder and the Mars Exploration Rovers, Spirit and Opportunity, under simulated Mars atmospheric conditions.
The facility was designed and constructed to test both nuclear and non-nuclear space hardware in a simulated Low-Earth-Orbiting environment. Although the facility was designed for testing nuclear hardware, only non-nuclear tests have been performed throughout its history. Some of the test programs that have been performed at the facility include high-energy experiments, rocket-fairing separation tests, Mars Lander system tests, deployable Solar Sail tests and International Space Station hardware tests. The SPF is located at the NASA Glenn Research Center at the Plum Brook site.
The facility can sustain a high vacuum (10−6 torr, 130 μPa); simulate solar radiation via a 4 MW quartz heat lamp array, solar spectrum by a 400 kW arc lamp, and cold environments (−320 °F (−195.6 °C)) with a variable geometry cryogenic cold shroud.
The facility is available on a full-cost reimbursable basis to government, universities, and the private sector.
In Spring 2013 SpaceX conducted a fairing separation test in the vacuum chamber.


=== Aluminum Test Chamber ===
The Aluminum Test Chamber is a vacuum-tight aluminum plate vessel that is 100 feet (30 m) in diameter and 122 feet (37 m) high. Designed for an external pressure of 2.5 psi (17 kPa) and internal pressure of 5 psi (34 kPa), the chamber is constructed of Type 5083 aluminum which is a clad on the interior surface with a 1⁄8 in (3.2 mm) thick type 3003 aluminum for corrosion resistance. This material was selected because of its low neutron absorption cross-section. The floor plate and vertical shell are 1 inch (25 mm) (total) thick, while the dome shell is 1 3⁄8 in (35 mm). Welded circumferentially to the exterior surface is aluminum structural T-section members that are 3 feet (0.9 m) deep and 2 feet (0.6 m) wide. The doors of the test chamber are 50 by 50 feet (15 by 15 m) in size and have double door seals to prevent leakage. The chamber floor was designed for a load of 300 tons.


=== Concrete Chamber Enclosure ===
The concrete chamber enclosure serves not only as a radiological shield but also as a primary vacuum barrier from atmospheric pressure. 130 feet (40 m) in diameter and 150 feet (46 m) in height, the chamber was designed to withstand atmospheric pressure outside of the chamber at the same time vacuum conditions are occurring within. The concrete thickness varies from 6 to 8 feet (1.8 to 2.4 m) and contains a leak-tight steel containment barrier embedded within. The chamber's doors are 50 by 50 feet (15 by 15 m) and have inflatable seals. The space between the concrete enclosure and the aluminum test chamber is pumped down to a pressure of 20 torrs (2.7 kPa) during a test.

		
		
		
		
Brian Cox of the BBC's Human Universe filmed a rock and feather drop episode at the Space Power Facility.  Below is a YouTube clip:
Rock and Feather Drop at NASA's Space Power Facility


=== Electromagnetic Interference/Compatibility (EMI/EMC) functionality ===
Designed specifically as a large-scale thermal-vacuum test chamber for qualification testing of vehicles and equipment in outer-space conditions, it was discovered in the late 2000s that the unique construction of the SPF interior aluminum vacuum chamber also makes it an extremely large and electrically complex RF cavity with excellent reverberant RF characteristics. In 2009 these characteristics were measured by NIST and others after which the facility was understood to be, not only the world's largest Vacuum chamber, but also the world's largest EMI/EMC test facility. In 2011 NASA GRC successfully performed a calibration of the aluminum vacuum chamber using IEC 61000-4-21 methodologies. As a result of these activities, the SPF is capable of performing radiated susceptibility EMI tests for vehicles and equipment per MIL-STD-461 and able to achieve MIL-STD-461F limits above approximately 80 MHz. In the spring of 2017 the low-power characterizations and calibrations from 2009 and 2011 were proven correct in a series of high-power tests performed in the chamber to validate its capabilities. The SPF chamber is currently being prepared for EMI radiated susceptibility testing of the crew module for the Artemis 1 of NASA's Orion spacecraft.

		


== Reverberant Acoustic Test Facility ==
The Reverberant Acoustic Test Facility has 36 nitrogen-driven horns to simulate the high noise levels that will be experienced during a space vehicle launch and supersonic ascent conditions. The RATF is capable of an overall sound pressure level of 163 dB within a 101,500-cubic-foot (2,870 m3) chamber.

		
		
		


== Mechanical Vibration Test Facility ==

The Mechanical Vibration Test Facility (MVF), is a three-axis vibration system. It will apply vibration in each of the three orthogonal axes (not simultaneously) with one direction in parallel to the Earth-launch thrust axis (X) at 5–150 Hz, 0-1.25 g-pk vertical, and 5–150 Hz 0-1.0 g-pk for the horizontal axes. 
Vertical, or the thrust axis, shaking is accomplished by using 16 vertical actuators manufactured by TEAM Corporation, each capable of 30,000 lbf (130 kN). The 16 vertical actuators allow for testing of up to a 75,000 lb (34,000 kg) article at the previously stated frequency and amplitude limits.
Horizontal shaking is accomplished through use of 4 TEAM Corporation Horizontal Actuators. The horizontal actuators are used during Vertical testing to counteract cross axis forces and overturning moments.

		
		
		
		
NASA's Space Power Facility Vibro-Acoustic Construction


== Modal Test Facility ==
In addition to the sine vibe table, a fixed-base Modal floor sufficient for the 20 ft (6.1 m) diameter test article is available. The fixed based Modal Test Facility is a 6 in (150 mm) thick steel floor on top of 19 ft (5.8 m) of concrete, that is tied to the earth using 50 ft (15 m) deep tensioned rock anchors.
There were over 21,000,000 pounds (9,500 t) of rock anchors, and 6,000,000 pounds (2,700 t) of concrete used in the construction of the fixed-base modal test facility and mechanical vibration test facility.

		
		


== Assembly Area ==
The SPF Facility layout is ideal for performing multiple test programs. The facility has two large high bay areas adjacent to either side of the vacuum chamber. The advantage of having both areas available is that it allows for two complex tests to be prepared simultaneously. One test can be prepared in a high bay while another test is being conducted in the vacuum chamber. Large chamber doors provide access to the test chamber from either high bay.

NASA's Space Power Facility Vibro-Acoustic Construction


== References ==


== External links ==
Skylab Shroud in Plum Brook Space Power Facility
NASA image gallery, featuring the SPF
Detailed facility capabilities","pandas(index=61, _1=61, text='space power facility (spf) is a nasa facility used to test spaceflight hardware under simulated launch and spaceflight conditions. the spf is part of nasa\'s plum brook station, which in turn is part of the glenn research center. the plum brook station and the spf are located near sandusky, ohio (oxford township, erie county, ohio). the spf is able to simulate a spacecraft\'s launch environment, as well as in-space environments. nasa has developed these capabilities under one roof to optimize testing of spaceflight hardware while minimizing transportation issues. space power facility has become a ""one stop shop"" to qualify flight hardware for manned space flight. this facility provides the capability to perform the following environmental testing:  thermal-vacuum testing reverberation acoustic testing mechanical vibration testing modal testing electromagnetic interference and compatibility testing   == thermal-vacuum test chamber == the space power facility (spf) is a vacuum chamber built by nasa in 1969. it stands 122 feet (37 m) high and 100 feet (30 m) in diameter, enclosing a bullet-shaped space. it is the world\'s largest thermal vacuum chamber. it was originally commissioned for nuclear-electric power studies under vacuum conditions, but was later decommissioned. recently, it was recommissioned for use in testing spacecraft propulsion systems. recent uses include testing the airbag landing systems for the mars pathfinder and the mars exploration rovers, spirit and opportunity, under simulated mars atmospheric conditions. the facility was designed and constructed to test both nuclear and non-nuclear space hardware in a simulated low-earth-orbiting environment. although the facility was designed for testing nuclear hardware, only non-nuclear tests have been performed throughout its history. some of the test programs that have been performed at the facility include high-energy experiments, rocket-fairing separation tests, mars lander system tests, deployable solar sail tests and international space station hardware tests. the spf is located at the nasa glenn research center at the plum brook site. the facility can sustain a high vacuum (10−6 torr, 130 μpa); simulate solar radiation via a 4 mw quartz heat lamp array, solar spectrum by a 400 kw arc lamp, and cold environments (−320 °f (−195.6 °c)) with a variable geometry cryogenic cold shroud. the facility is available on a full-cost reimbursable basis to government, universities, and the private sector. in spring 2013 spacex conducted a fairing separation test in the vacuum chamber. designed specifically as a large-scale thermal-vacuum test chamber for qualification testing of vehicles and equipment in outer-space conditions, it was discovered in the late 2000s that the unique construction of the spf interior aluminum vacuum chamber also makes it an extremely large and electrically complex rf cavity with excellent reverberant rf characteristics. in 2009 these characteristics were measured by nist and others after which the facility was understood to be, not only the world\'s largest vacuum chamber, but also the world\'s largest emi/emc test facility. in 2011 nasa grc successfully performed a calibration of the aluminum vacuum chamber using iec 61000-4-21 methodologies. as a result of these activities, the spf is capable of performing radiated susceptibility emi tests for vehicles and equipment per mil-std-461 and able to achieve mil-std-461f limits above approximately 80 mhz. in the spring of 2017 the low-power characterizations and calibrations from 2009 and 2011 were proven correct in a series of high-power tests performed in the chamber to validate its capabilities. the spf chamber is currently being prepared for emi radiated susceptibility testing of the crew module for the artemis 1 of nasa\'s orion spacecraft.       == reverberant acoustic test facility == the reverberant acoustic test facility has 36 nitrogen-driven horns to simulate the high noise levels that will be experienced during a space vehicle launch and supersonic ascent conditions. the ratf is capable of an overall sound pressure level of 163 db within a 101,500-cubic-foot (2,870 m3) chamber.             == mechanical vibration test facility ==  the mechanical vibration test facility (mvf), is a three-axis vibration system. it will apply vibration in each of the three orthogonal axes (not simultaneously) with one direction in parallel to the earth-launch thrust axis (x) at 5–150 hz, 0-1.25 g-pk vertical, and 5–150 hz 0-1.0 g-pk for the horizontal axes. vertical, or the thrust axis, shaking is accomplished by using 16 vertical actuators manufactured by team corporation, each capable of 30,000 lbf (130 kn). the 16 vertical actuators allow for testing of up to a 75,000 lb (34,000 kg) article at the previously stated frequency and amplitude limits. horizontal shaking is accomplished through use of 4 team corporation horizontal actuators. the horizontal actuators are used during vertical testing to counteract cross axis forces and overturning moments.              nasa\'s space power facility vibro-acoustic construction   == modal test facility == in addition to the sine vibe table, a fixed-base modal floor sufficient for the 20 ft (6.1 m) diameter test article is available. the fixed based modal test facility is a 6 in (150 mm) thick steel floor on top of 19 ft (5.8 m) of concrete, that is tied to the earth using 50 ft (15 m) deep tensioned rock anchors. there were over 21,000,000 pounds (9,500 t) of rock anchors, and 6,000,000 pounds (2,700 t) of concrete used in the construction of the fixed-base modal test facility and mechanical vibration test facility.          == assembly area == the spf facility layout is ideal for performing multiple test programs. the facility has two large high bay areas adjacent to either side of the vacuum chamber. the advantage of having both areas available is that it allows for two complex tests to be prepared simultaneously. one test can be prepared in a high bay while another test is being conducted in the vacuum chamber. large chamber doors provide access to the test chamber from either high bay.  nasa\'s space power facility vibro-acoustic construction   == references ==   == external links == skylab shroud in plum brook space power facility nasa image gallery, featuring the spf detailed facility capabilities')"
62,"An obturator ring was a type of piston ring used in World War I aero engines for improved sealing in the presence of cylinder distortion.


== Purpose ==
The cylinders of rotary aircraft engines of World War I (engines with the crankshaft fixed to the airframe and rotating cylinders) were notoriously difficult to keep cool leading to  thermal distortion. To keep the weight down they had very thin-wall (1.5 mm) steel cylinders. Obturator rings, made of bronze in the early Gnome engines, could flex to the shape of the cylinder. Wear on the rings was considerable. Engines needed to be overhauled about every 20 hours. The reliability of Gnome engines license-built by The British Gnome and Le Rhone Engine Co. gave an overhaul life of about 80 hours mainly as a result using a special tool to roll the 'L' section obturator rings. The problem of thermal distortion was effectively cured on the Bentley BR1 engine by using aluminium cylinders, for good thermal conductivity, with cast iron liners shrunk in.An 'L' section obturator ring is shown in Patent US 1378109A - ""Obturator ring"".


== See also ==
Piston ring


== References ==","pandas(index=62, _1=62, text='an obturator ring was a type of piston ring used in world war i aero engines for improved sealing in the presence of cylinder distortion.   == purpose == the cylinders of rotary aircraft engines of world war i (engines with the crankshaft fixed to the airframe and rotating cylinders) were notoriously difficult to keep cool leading to  thermal distortion. to keep the weight down they had very thin-wall (1.5 mm) steel cylinders. obturator rings, made of bronze in the early gnome engines, could flex to the shape of the cylinder. wear on the rings was considerable. engines needed to be overhauled about every 20 hours. the reliability of gnome engines license-built by the british gnome and le rhone engine co. gave an overhaul life of about 80 hours mainly as a result using a special tool to roll the \'l\' section obturator rings. the problem of thermal distortion was effectively cured on the bentley br1 engine by using aluminium cylinders, for good thermal conductivity, with cast iron liners shrunk in.an \'l\' section obturator ring is shown in patent us 1378109a - ""obturator ring"".   == see also == piston ring   == references ==')"
63,"An Air Data Inertial Reference Unit (ADIRU) is a key component of the integrated Air Data Inertial Reference System (ADIRS), which supplies air data (airspeed, angle of attack and altitude) and inertial reference (position and attitude) information to the pilots' electronic flight instrument system displays as well as other systems on the aircraft such as the engines, autopilot, aircraft flight control system and landing gear systems. An ADIRU acts as a single, fault tolerant source of navigational data for both pilots of an aircraft. It may be complemented by a secondary attitude air data reference unit (SAARU), as in the Boeing 777 design.This device is used on various military aircraft as well as civilian airliners starting with the Airbus A320 and Boeing 777.


== Description ==
An ADIRS consists of up to three fault tolerant ADIRUs located in the aircraft electronic rack, an associated control and display unit (CDU) in the cockpit and remotely mounted air data modules (ADMs). The No 3 ADIRU is a redundant unit that may be selected to supply data to either the commander's or the co-pilot's displays in the event of a partial or complete failure of either the No 1 or No 2 ADIRU. There is no cross-channel redundancy between the Nos 1 and 2 ADIRUs, as No 3 ADIRU is the only alternate source of air and inertial reference data. An inertial reference (IR) fault in ADIRU No 1 or 2 will cause a loss of attitude and navigation information on their associated primary flight display (PFD) and navigation display (ND) screens. An air data reference (ADR) fault will cause the loss of airspeed and altitude information on the affected display. In either case the information can only be restored by selecting the No 3 ADIRU.Each ADIRU comprises an ADR and an inertial reference (IR) component.


=== Air data reference ===

The air data reference (ADR) component of an ADIRU provides airspeed, Mach number, angle of attack, temperature and barometric altitude data. Ram air pressure and static pressures used in calculating airspeed are measured by small ADMs located as close as possible to the respective pitot and static pressure sensors. ADMs transmit their pressures to the ADIRUs through ARINC 429 data buses.


=== Inertial reference ===
The IR component of an ADIRU gives attitude, flight path vector, ground speed and positional data. The ring laser gyroscope is a core enabling technology in the system, and is used together with accelerometers, GPS and other sensors to provide raw data. The primary benefits of a ring laser over older mechanical gyroscopes are that there are no moving parts, it is rugged and lightweight, frictionless and does not resist a change in precession.


== Complexity in redundancy ==
Analysis of complex systems is itself so difficult as to be subject to errors in the certification process. Complex interactions between flight computers and ADIRU's can lead to counter-intuitive behaviour for the crew in the event of a failure. In the case of Qantas Flight 72, the captain switched the source of IR data from ADIRU1 to ADIRU3 following a failure of ADIRU1; however ADIRU1 continued to supply ADR data to the captain's primary flight display. In addition, the master flight control computer (PRIM1) was switched from PRIM1 to PRIM2, then PRIM2 back to PRIM1, thereby creating a situation of uncertainty for the crew who did not know which redundant systems they were relying upon.Reliance on redundancy of aircraft systems can also lead to delays in executing needed repairs, as airline operators rely on the redundancy to keep the aircraft system working without having to repair faults immediately.


== Failures and directives ==


=== FAA Airworthiness directive 2000-07-27 ===
On May 3, 2000, the FAA issued airworthiness directive 2000-07-27, addressing dual critical failures during flight, attributed to power supply issues affecting early Honeywell HG2030 and HG2050 ADIRU ring laser gyros used on several Boeing 737, 757, Airbus A319, A320, A321, A330, and A340 models.


=== Airworthiness directive 2003-26-03 ===
On 27 January 2004 the FAA issued airworthiness directive 2003-26-03 (later superseded by AD 2008-17-12) which called for modification to the mounting of ADIRU3 in Airbus A320 family aircraft to prevent failure and loss of critical attitude and airspeed data.


=== Alitalia A320 ===
On 25 June 2005, an Alitalia Airbus A320-200 registered as I-BIKE departed Milan with a defective ADIRU as permitted by the Minimum Equipment List. While approaching London Heathrow Airport during deteriorating weather another ADIRU failed, leaving only one operable. In the subsequent confusion the third was inadvertently reset, losing its reference heading and disabling several automatic functions. The crew was able to effect a safe landing after declaring a Pan-pan.


=== Malaysia Airlines Flight 124 ===
On 1 August 2005, a serious incident involving Malaysia Airlines Flight 124 occurred when an ADIRU fault in a Boeing 777-2H6ER (9M-MRG) flying from Perth to Kuala Lumpur International caused the aircraft to act on false indications, resulting in uncommanded manoeuvres. In that incident the incorrect data impacted all planes of movement while the aircraft was climbing through 38,000 feet (11,600 m). The aircraft pitched up and climbed to around 41,000 feet (12,500 m), with the stall warning activated. The pilots recovered the aircraft with the autopilot disengaged and requested a return to Perth. During the return to Perth, both the left and right autopilots were briefly activated by the crew, but in both instances the aircraft pitched down and banked to the right. The aircraft was flown manually for the remainder of the flight and landed safely in Perth. There were no injuries and no damage to the aircraft. The ATSB found that the main probable cause of this incident was a latent software error which allowed the ADIRU to use data from a failed accelerometer.The US Federal Aviation Administration issued Emergency Airworthiness Directive (AD) 2005-18-51 requiring all 777 operators to install upgraded software to resolve the error.


=== Qantas Flight 68 ===
On 12 September 2006, Qantas Flight 68, Airbus A330 registration VH-QPA, from Singapore to Perth exhibited ADIRU problems but without causing any disruption to the flight. At 41,000 feet (12,000 m) and estimated position 530 nautical miles (980 km) north of Learmonth, Western Australia, NAV IR1 FAULT then, 30 minutes later, NAV ADR 1 FAULT notifications were received on the ECAM identifying navigation system faults in Inertial Reference Unit 1, then in ADR 1 respectively. The crew reported to the later Qantas Flight 72 investigation involving the same airframe and ADIRU that they had received numerous warning and caution messages which changed too quickly to be dealt with. While investigating the problem, the crew noticed a weak and intermittent ADR 1 FAULT light and elected to switch off ADR 1, after which they experienced no further problems. There was no impact on the flight controls throughout the event. The ADIRU manufacturer's recommended maintenance procedures were carried out after the flight and system testing found no further fault.


=== Jetstar Flight 7 ===
On 7 February 2008, a similar aircraft (VH-EBC) operated by Qantas subsidiary Jetstar Airways was involved in a similar occurrence while conducting the JQ7 service from Sydney to Ho Chi Minh City, Vietnam. In this event - which occurred 1,760 nautical miles (3,260 km) east of Learmonth - many of the same errors occurred in the ADIRU unit. The crew followed the relevant procedure applicable at the time and the flight continued without problems.


=== Airworthiness directive 2008-17-12 ===
On 6 August 2008, the FAA issued airworthiness directive 2008-17-12 expanding on the requirements of the earlier AD 2003-26-03 which had been determined to be an insufficient remedy. In some cases it called for replacement of ADIRUs with newer models, but allowed 46 months from October 2008 to implement the directive.The ATSB has yet to confirm if this event is related to the other Airbus A330 ADIRU occurrences.


=== Qantas Flight 72 ===
On 7 October 2008, Qantas Flight 72, using the same aircraft involved in the Flight 68 incident, departed Singapore for Perth. Some time into the flight, while cruising at 37,000 ft, a failure in the No.1 ADIRU led to the autopilot automatically disengaging followed by two sudden uncommanded pitch down manoeuvres, according to the Australian Transport Safety Bureau (ATSB). The accident injured up to 74 passengers and crew, ranging from minor to serious injuries. The aircraft was able to make an emergency landing without further injuries. The aircraft was equipped with a Northrop Grumman made ADIRS, which investigators sent to the manufacturer for further testing.


=== Qantas Flight 71 ===
On 27 December 2008, Qantas Flight 71 from Perth to Singapore, a different Qantas A330-300 with registration VH-QPG was involved in an incident at 36,000 feet approximately 260 nautical miles (480 km) north-west of Perth and 350 nautical miles (650 km) south of Learmonth Airport at 1729 WST. The autopilot disconnected and the crew received an alert indicating a problem with ADIRU Number 1.


=== Emergency Airworthiness Directive No 2009-0012-E ===
On 15 January 2009, the European Aviation Safety Agency issued Emergency Airworthiness Directive No 2009-0012-E to address the above A330 and A340 Northrop-Grumman ADIRU problem of incorrectly responding to a defective inertial reference. In the event of a NAV IR fault the directed crew response is now to ""select OFF the relevant IR, select OFF the relevant ADR, and then turn the IR rotary mode selector to the OFF position."" The effect is to ensure that the faulted IR is powered off so that it no longer can send erroneous data to other systems.


=== Air France Flight 447 ===
On 1 June 2009, Air France Flight 447, an Airbus A330 en route from Rio de Janeiro to Paris, crashed in the Atlantic Ocean after transmitting automated messages indicating faults with various equipment, including the ADIRU. While examining possibly related events of weather-related loss of ADIRS, the NTSB decided to investigate two similar cases on cruising A330s. On a 21 May 2009 Miami-Sao Paulo TAM Flight 8091 registered as PT-MVB, and on a 23 June 2009 Hong Kong-Tokyo Northwest Airlines Flight 8 registered as N805NW each saw sudden loss of airspeed data at cruise altitude and consequent loss of ADIRS control.


=== Ryanair Flight 6606 ===
On 9 October 2018, the Boeing 737-800 operating the flight from Porto Airport to Edinburgh Airport suffered a left ADIRU failure that resulted in the aircraft pitching up and climbing 600 feet. The left ADIRU was put in ATT (attitude-only) mode in accordance with the Quick Reference Handbook, but it continued to display erroneous attitude information to the captain. The remainder of the flight was flown manually with an uneventful landing. The UK's AAIB released the final report on 31 October 2019, with the following recommendation:It is recommended that Boeing Commercial Aircraft amend the Boeing 737 Quick Reference Handbook to include a non-normal checklist for situations when pitch and roll comparator annunciations appear on the attitude display.


== See also ==
Acronyms and abbreviations in avionics


== References ==


== Further reading ==
Dave Carbaugh; Doug Forsythe; Melville McIntyre. ""Erroneous flight instrumenent information"". Aero Magazine. Boeing. Archived from the original on 6 September 2008. Retrieved 2008-10-16.
Melville Duncan W. McIntyre, Boeing (2003-11-25). ""US Patent 6654685 - Apparatus and method for navigation of an aircraft"". United States Patent Office. Retrieved 2008-10-16.","pandas(index=63, _1=63, text='an air data inertial reference unit (adiru) is a key component of the integrated air data inertial reference system (adirs), which supplies air data (airspeed, angle of attack and altitude) and inertial reference (position and attitude) information to the pilots\' electronic flight instrument system displays as well as other systems on the aircraft such as the engines, autopilot, aircraft flight control system and landing gear systems. an adiru acts as a single, fault tolerant source of navigational data for both pilots of an aircraft. it may be complemented by a secondary attitude air data reference unit (saaru), as in the boeing 777 design.this device is used on various military aircraft as well as civilian airliners starting with the airbus a320 and boeing 777.   == description == an adirs consists of up to three fault tolerant adirus located in the aircraft electronic rack, an associated control and display unit (cdu) in the cockpit and remotely mounted air data modules (adms). the no 3 adiru is a redundant unit that may be selected to supply data to either the commander\'s or the co-pilot\'s displays in the event of a partial or complete failure of either the no 1 or no 2 adiru. there is no cross-channel redundancy between the nos 1 and 2 adirus, as no 3 adiru is the only alternate source of air and inertial reference data. an inertial reference (ir) fault in adiru no 1 or 2 will cause a loss of attitude and navigation information on their associated primary flight display (pfd) and navigation display (nd) screens. an air data reference (adr) fault will cause the loss of airspeed and altitude information on the affected display. in either case the information can only be restored by selecting the no 3 adiru.each adiru comprises an adr and an inertial reference (ir) component. on 9 october 2018, the boeing 737-800 operating the flight from porto airport to edinburgh airport suffered a left adiru failure that resulted in the aircraft pitching up and climbing 600 feet. the left adiru was put in att (attitude-only) mode in accordance with the quick reference handbook, but it continued to display erroneous attitude information to the captain. the remainder of the flight was flown manually with an uneventful landing. the uk\'s aaib released the final report on 31 october 2019, with the following recommendation:it is recommended that boeing commercial aircraft amend the boeing 737 quick reference handbook to include a non-normal checklist for situations when pitch and roll comparator annunciations appear on the attitude display.   == see also == acronyms and abbreviations in avionics   == references ==   == further reading == dave carbaugh; doug forsythe; melville mcintyre. ""erroneous flight instrumenent information"". aero magazine. boeing. archived from the original on 6 september 2008. retrieved 2008-10-16. melville duncan w. mcintyre, boeing (2003-11-25). ""us patent 6654685 - apparatus and method for navigation of an aircraft"". united states patent office. retrieved 2008-10-16.')"
64,"The planet Mars has been explored remotely by spacecraft. Probes sent from Earth, beginning in the late 20th century, have yielded a large increase in knowledge about the Martian system, focused primarily on understanding its geology and habitability potential. Engineering interplanetary journeys is complicated and the exploration of Mars has experienced a high failure rate, especially the early attempts. Roughly sixty percent of all spacecraft destined for Mars failed before completing their missions and some failed before their observations could begin. Some missions have met with unexpected success, such as the twin Mars Exploration Rovers, Spirit and Opportunity which operated for years beyond their specification.


== Current status ==

As of February 2021, there are two operational rovers on the surface of Mars, the Curiosity and Perseverance rovers, both operated by the United States of America space agency NASA. A third rover, part of the Tianwen-1 mission, is currently attatched to its orbiter, and is planned to land in May 2021. There are eight orbiters surveying the planet: Mars Odyssey, Mars Express, Mars Reconnaissance Orbiter, Mars Orbiter Mission, MAVEN, the Trace Gas Orbiter, the Tianwen-1 orbiter, and the Hope Mars Mission, which have contributed massive amounts of information about Mars. The stationary lander InSight is investigating the deep interior of Mars. No sample return missions have been attempted for Mars and an attempted return mission for Mars' moon Phobos (Fobos-Grunt) failed at launch in 2011. In all, there are 11 probes currently surveying Mars, with a 12th, the Tianwen-1 rover, that is in Martian orbit but has not landed yet. 
The next missions expected to arrive at Mars are:

The joint ExoMars program of Roscosmos and ESA has delayed the launch of the Kazachok landing platform, which will carry the Rosalind Franklin rover, until 2022.
Mars Orbiter Mission 2 by India, planned launch in 2024.


== Martian system ==

Mars has long been the subject of human interest. Early telescopic observations revealed color changes on the surface that were attributed to seasonal vegetation and apparent linear features were ascribed to intelligent design. Further telescopic observations found two moons, Phobos and Deimos, polar ice caps and the feature now known as Olympus Mons, the Solar System's second tallest mountain. The discoveries piqued further interest in the study and exploration of the red planet. Mars is a rocky planet, like Earth, that formed around the same time, yet with only half the diameter of Earth, and a far thinner atmosphere; it has a cold and desert-like surface.One way the surface of Mars has been categorized, is by thirty ""quadrangles"", with each quadrangle named for a prominent physiographic feature within that quadrangle.


== Launch windows ==

The minimum-energy launch windows for a Martian expedition occur at intervals of approximately two years and two months (specifically 780 days, the planet's synodic period with respect to Earth). In addition, the lowest available transfer energy varies on a roughly 16-year cycle. For example, a minimum occurred in the 1969 and 1971 launch windows, rising to a peak in the late 1970s, and hitting another low in 1986 and 1988.


== Past and current missions ==

Starting in 1960, the Soviets launched a series of probes to Mars including the first intended flybys and hard (impact) landing (Mars 1962B). The first successful flyby of Mars was on 14–15 July 1965, by NASA's Mariner 4. On November 14, 1971, Mariner 9 became the first space probe to orbit another planet when it entered into orbit around Mars. The amount of data returned by probes increased dramatically as technology improved.The first to contact the surface were two Soviet probes: Mars 2 lander on November 27 and Mars 3 lander on December 2, 1971—Mars 2 failed during descent and Mars 3 about twenty seconds after the first Martian soft landing. Mars 6 failed during descent but did return some corrupted atmospheric data in 1974. The 1975 NASA launches of the Viking program consisted of two orbiters, each with a lander that successfully soft landed in 1976. Viking 1 remained operational for six years, Viking 2 for three. The Viking landers relayed the first color panoramas of Mars.The Soviet probes Phobos 1 and 2 were sent to Mars in 1988 to study Mars and its two moons, with a focus on Phobos. Phobos 1 lost contact on the way to Mars. Phobos 2, while successfully photographing Mars and Phobos, failed before it was set to release two landers to the surface of Phobos.Mars has a reputation as a difficult space exploration target; just 25 of 55 missions through 2019, or 45.5%, have been fully successful, with a further three partially successful and partially failures. However, of the sixteen missions since 2001, twelve have been successful and eight of these are still operational.
Missions that ended prematurely after Phobos 1 and 2 (1988) include (see Probing difficulties section for more details):

Mars Observer (launched in 1992)
Mars 96 (1996)
Mars Climate Orbiter (1999)
Mars Polar Lander with Deep Space 2 (1999)
Nozomi (2003)
Beagle 2 (2003)
Fobos-Grunt with Yinghuo-1 (2011)
Schiaparelli lander (2016)Following the 1993 failure of the Mars Observer orbiter, the NASA Mars Global Surveyor achieved Mars orbit in 1997. This mission was a complete success, having finished its primary mapping mission in early 2001. Contact was lost with the probe in November 2006 during its third extended program, spending exactly 10 operational years in space. The NASA Mars Pathfinder, carrying a robotic exploration vehicle Sojourner, landed in the Ares Vallis on Mars in the summer of 1997, returning many images.

NASA's Mars Odyssey orbiter entered Mars orbit in 2001. Odyssey's Gamma Ray Spectrometer detected significant amounts of hydrogen in the upper metre or so of regolith on Mars. This hydrogen is thought to be contained in large deposits of water ice.The Mars Express mission of the European Space Agency (ESA) reached Mars in 2003. It carried the Beagle 2 lander, which was not heard from after being released and was declared lost in February 2004.  Beagle 2 was located in January 2015 by HiRise camera on NASA's Mars Reconnaissance Orbiter (MRO) having landed safely but failed to fully deploy its solar panels and antenna. In early 2004, the Mars Express Planetary Fourier Spectrometer team announced the orbiter had detected methane in the Martian atmosphere, a potential biosignature. ESA announced in June 2006 the discovery of aurorae on Mars by the Mars Express.

In January 2004, the NASA twin Mars Exploration Rovers named Spirit (MER-A) and Opportunity (MER-B) landed on the surface of Mars. Both have met and exceeded all their science objectives. Among the most significant scientific returns has been conclusive evidence that liquid water existed at some time in the past at both landing sites. Martian dust devils and windstorms have occasionally cleaned both rovers' solar panels, and thus increased their lifespan. Spirit rover (MER-A) was active until 2010, when it stopped sending data because it got stuck in a sand dune and was unable to reorient itself to recharge its batteries.On 10 March 2006, NASA's Mars Reconnaissance Orbiter (MRO) probe arrived in orbit to conduct a two-year science survey. The orbiter began mapping the Martian terrain and weather to find suitable landing sites for upcoming lander missions. The MRO captured the first image of a series of active avalanches near the planet's north pole in 2008.Rosetta came within 250 km of Mars during its 2007 flyby. Dawn flew by Mars in February 2009 for a gravity assist on its way to investigate Vesta and Ceres.Phoenix landed on the north polar region of Mars on May 25, 2008. Its robotic arm dug into the Martian soil and the presence of water ice was confirmed on June 20, 2008. The mission concluded on November 10, 2008 after contact was lost. In 2008, the price of transporting material from the surface of Earth to the surface of Mars was approximately US$309,000 per kilogram.The Mars Science Laboratory mission was launched on November 26, 2011 and it delivered the Curiosity rover on the surface of Mars on August 6, 2012 UTC. It is larger and more advanced than the Mars Exploration Rovers, with a velocity of up to 90 meters per hour (295 feet per hour). Experiments include a laser chemical sampler that can deduce the composition of rocks at a distance of 7 meters.

MAVEN orbiter was launched on 18 November 2013, and on 22 September 2014, it was injected into an areocentric elliptic orbit 6,200 km (3,900 mi) by 150 km (93 mi) above the planet's surface to study its atmosphere. Mission goals include determining how the planet's atmosphere and water, presumed to have once been substantial, were lost over time.The Indian Space Research Organisation (ISRO) launched their Mars Orbiter Mission (MOM) on November 5, 2013, and it was inserted into Mars orbit on September 24, 2014. India's ISRO is the fourth space agency to reach Mars, after the Soviet space program, NASA and ESA. India successfully placed a spacecraft into Mars orbit, and became the first country to do so in its maiden attempt.The ExoMars Trace Gas Orbiter arrived at Mars in 2016 and deployed the Schiaparelli EDM lander, a test lander. Schiaparelli crashed on surface, but it transmitted key data during its parachute descent, so the test was declared a partial success.


=== Overview of missions ===
The following entails a brief overview of Mars exploration, oriented towards orbiters and flybys; see also Mars landing and Mars rover.


==== Early Soviet missions ====


===== 1960s =====

Between 1960 and 1969, the Soviet Union launched nine probes intended to reach Mars. They all failed: three at launch; three failed to reach near-Earth orbit; one during the burn to put the spacecraft into trans-Mars trajectory; and two during the interplanetary orbit.
The Mars 1M programs (sometimes dubbed Marsnik in Western media) was the first Soviet unmanned spacecraft interplanetary exploration program, which consisted of two flyby probes launched towards Mars in October 1960, Mars 1960A and Mars 1960B (also known as Korabl 4 and Korabl 5 respectively). After launch, the third stage pumps on both launchers were unable to develop enough pressure to commence ignition, so Earth parking orbit was not achieved. The spacecraft reached an altitude of 120 km before reentry.
Mars 1962A was a Mars flyby mission, launched on October 24, 1962 and Mars 1962B an intended first Mars lander mission, launched in late December of the same year (1962). Both failed from either breaking up as they were going into Earth orbit or having the upper stage explode in orbit during the burn to put the spacecraft into trans-Mars trajectory.


====== The first success ======
Mars 1 (1962 Beta Nu 1), an automatic interplanetary spacecraft launched to Mars on November 1, 1962, was the first probe of the Soviet Mars probe program to achieve interplanetary orbit. Mars 1 was intended to fly by the planet at a distance of about 11,000 km and take images of the surface as well as send back data on cosmic radiation, micrometeoroid impacts and Mars' magnetic field, radiation environment, atmospheric structure, and possible organic compounds. Sixty-one radio transmissions were held, initially at 2-day intervals and later at 5-day intervals, from which a large amount of interplanetary data was collected. On 21 March 1963, when the spacecraft was at a distance of 106,760,000 km from Earth, on its way to Mars, communications ceased due to failure of its antenna orientation system.In 1964, both Soviet probe launches, of Zond 1964A on June 4, and Zond 2 on November 30, (part of the Zond program), resulted in failures. Zond 1964A had a failure at launch, while communication was lost with Zond 2 en route to Mars after a mid-course maneuver, in early May 1965.In 1969, and as part of the Mars probe program, the Soviet Union prepared two identical 5-ton orbiters called M-69, dubbed by NASA as Mars 1969A and Mars 1969B.  Both probes were lost in launch-related complications with the newly developed Proton rocket.


===== 1970s =====
The USSR intended to have the first artificial satellite of Mars beating the planned American Mariner 8 and Mariner 9 Mars orbiters. In May 1971, one day after Mariner 8 malfunctioned at launch and failed to reach orbit, Cosmos 419 (Mars 1971C), a heavy probe of the Soviet Mars program M-71, also failed to launch. This spacecraft was designed as an orbiter only, while the next two probes of project M-71, Mars 2 and Mars 3, were multipurpose combinations of an orbiter and a lander with small skis-walking rovers that would be the first planet rovers outside the Moon. They were successfully launched in mid-May 1971 and reached Mars about seven months later. On November 27, 1971 the lander of Mars 2 crash-landed due to an on-board computer malfunction and became the first man-made object to reach the surface of Mars. On 2 December 1971, the Mars 3 lander became the first spacecraft to achieve a soft landing, but its transmission was interrupted after 14.5 seconds.The Mars 2 and 3 orbiters sent back a relatively large volume of data covering the period from December 1971 to March 1972, although transmissions continued through to August. By 22 August 1972, after sending back data and a total of 60 pictures, Mars 2 and 3 concluded their missions. The images and data enabled creation of surface relief maps, and gave information on the Martian gravity and magnetic fields.In 1973, the Soviet Union sent four more probes to Mars: the Mars 4 and Mars 5 orbiters and the Mars 6 and Mars 7 flyby/lander combinations. All missions except Mars 7 sent back data, with Mars 5 being most successful. Mars 5 transmitted just 60 images before a loss of pressurization in the transmitter housing ended the mission. Mars 6 lander transmitted data during descent, but failed upon impact. Mars 4 flew by the planet at a range of 2200 km returning one swath of pictures and radio occultation data, which constituted the first detection of the nightside ionosphere on Mars. Mars 7 probe separated prematurely from the carrying vehicle due to a problem in the operation of one of the onboard systems (attitude control or retro-rockets) and missed the planet by 1,300 kilometres (8.7×10−6 au).


==== Mariner program ====

In 1964, NASA's Jet Propulsion Laboratory made two attempts at reaching Mars. Mariner 3 and Mariner 4 were identical spacecraft designed to carry out the first flybys of Mars. Mariner 3 was launched on November 5, 1964, but the shroud encasing the spacecraft atop its rocket failed to open properly, dooming the mission. Three weeks later, on November 28, 1964, Mariner 4 was launched successfully on a 7½-month voyage to Mars.Mariner 4 flew past Mars on July 14, 1965, providing the first close-up photographs of another planet. The pictures, gradually played back to Earth from a small tape recorder on the probe, showed impact craters.  It provided radically more accurate data about the planet; a surface atmospheric pressure of about 1% of Earth's and daytime temperatures of −100 °C (−148 °F) were estimated. No magnetic field or Martian radiation belts were detected. The new data meant redesigns for then planned Martian landers, and showed life would have a more difficult time surviving there than previously anticipated.

NASA continued the Mariner program with another pair of Mars flyby probes, Mariner 6 and 7. They were sent at the next launch window, and reached the planet in 1969. During the following launch window the Mariner program again suffered the loss of one of a pair of probes. Mariner 9 successfully entered orbit about Mars, the first spacecraft ever to do so, after the launch time failure of its sister ship, Mariner 8.  When Mariner 9 reached Mars in 1971, it and two Soviet orbiters (Mars 2 and Mars 3) found that a planet-wide dust storm was in progress.  The mission controllers used the time spent waiting for the storm to clear to have the probe rendezvous with, and photograph, Phobos.  When the storm cleared sufficiently for Mars' surface to be photographed by Mariner 9, the pictures returned represented a substantial advance over previous missions. These pictures were the first to offer more detailed evidence that liquid water might at one time have flowed on the planetary surface. They also finally discerned the true nature of many Martian albedo features. For example, Nix Olympica was one of only a few features that could be seen during the planetary duststorm, revealing it to be the highest mountain (volcano, to be exact) on any planet in the entire Solar System, and leading to its reclassification as Olympus Mons.


==== Viking program ====

The Viking program launched Viking 1 and Viking 2 spacecraft to Mars in 1975; The program consisted of two orbiters and two landers – these were the second and third spacecraft to successfully land on Mars.

The primary scientific objectives of the lander mission were to search for biosignatures and observe meteorologic, seismic and magnetic properties of Mars. The results of the biological experiments on board the Viking landers remain inconclusive, with a reanalysis of the Viking data published in 2012 suggesting signs of microbial life on Mars.
The Viking orbiters revealed that large floods of water carved deep valleys, eroded grooves into bedrock, and traveled thousands of kilometers. Areas of branched streams, in the southern hemisphere, suggest that rain once fell.


==== Mars Pathfinder ====

Mars Pathfinder was a U.S. spacecraft that landed a base station with a roving probe on Mars on July 4, 1997. It consisted of a lander and a small 10.6 kilograms (23 lb) wheeled robotic rover named Sojourner, which was the first rover to operate on the surface of Mars.  In addition to scientific objectives, the Mars Pathfinder mission was also a ""proof-of-concept"" for various technologies, such as an airbag landing system and automated obstacle avoidance, both later exploited by the Mars Exploration Rovers.


==== Mars Global Surveyor ====

After the 1992 failure of NASA's Mars Observer orbiter, NASA retooled and launched Mars Global Surveyor (MGS).  Mars Global Surveyor launched on November 7, 1996, and entered orbit on September 12, 1997. After a year and a half trimming its orbit from a looping ellipse to a circular track around the planet, the spacecraft began its primary mapping mission in March 1999. It observed the planet from a low-altitude, nearly polar orbit over the course of one complete Martian year, the equivalent of nearly two Earth years. Mars Global Surveyor completed its primary mission on January 31, 2001, and completed several extended mission phases.The mission studied the entire Martian surface, atmosphere, and interior, and returned more data about the red planet than all previous Mars missions combined. The data has been archived and remains available publicly.

Among key scientific findings, Global Surveyor took pictures of gullies and debris flow features that suggest there may be current sources of liquid water, similar to an aquifer, at or near the surface of the planet.  Similar channels on Earth are formed by flowing water, but on Mars the temperature is normally too cold and the atmosphere too thin to sustain liquid water. Nevertheless, many scientists hypothesize that liquid groundwater can sometimes surface on Mars, erode gullies and channels, and pool at the bottom before freezing and evaporating.Magnetometer readings showed that the planet's magnetic field is not globally generated in the planet's core, but is localized in particular areas of the crust. New temperature data and closeup images of the Martian moon Phobos showed that its surface is composed of powdery material at least 1 metre (3 feet) thick, caused by millions of years of meteoroid impacts. Data from the spacecraft's laser altimeter gave scientists their first 3-D views of Mars' north polar ice cap.Faulty software uploaded to the vehicle in June 2006 caused the spacecraft to orient its solar panels incorrectly several months later, resulting in battery overheating and subsequent failure. On November 5, 2006 MGS lost contact with Earth.  NASA ended efforts to restore communication on January 28, 2007.


==== Mars Odyssey and Mars Express ====

In 2001, NASA's Mars Odyssey orbiter arrived at Mars. Its mission is to use spectrometers and imagers to hunt for evidence of past or present water and volcanic activity on Mars. In 2002, it was announced that the probe's gamma-ray spectrometer and neutron spectrometer had detected large amounts of hydrogen, indicating that there are vast deposits of water ice in the upper three meters of Mars' soil within 60° latitude of the south pole.On June 2, 2003, the European Space Agency's Mars Express set off from Baikonur Cosmodrome to Mars. The Mars Express craft consists of the Mars Express Orbiter and the stationary lander Beagle 2. The lander carried a digging device and the smallest mass spectrometer created to date, as well as a range of other devices, on a robotic arm in order to accurately analyze soil beneath the dusty surface to look for biosignatures and biomolecules.The orbiter entered Mars orbit on December 25, 2003, and Beagle 2 entered Mars' atmosphere the same day. However, attempts to contact the lander failed. Communications attempts continued throughout January, but Beagle 2 was declared lost in mid-February, and a joint inquiry was launched by the UK and ESA. The Mars Express Orbiter confirmed the presence of water ice and carbon dioxide ice at the planet's south pole, while NASA had previously confirmed their presence at the north pole of Mars.The lander's fate remained a mystery until it was located intact on the surface of Mars in a series of images from the Mars Reconnaissance Orbiter. The images suggest that two of the spacecraft's four solar panels failed to deploy, blocking the spacecraft's communications antenna. Beagle 2 is the first British and first European probe to achieve a soft landing on Mars.


==== MER and Phoenix ====

NASA's Mars Exploration Rover Mission (MER), started in 2003, was a robotic space mission involving two rovers, Spirit (MER-A) and Opportunity, (MER-B) that explored the Martian surface geology. The mission's scientific objective was to search for and characterize a wide range of rocks and soils that hold clues to past water activity on Mars. The mission was part of NASA's Mars Exploration Program, which includes three previous successful landers: the two Viking program landers in 1976; and Mars Pathfinder probe in 1997.


==== Mars Reconnaissance Orbiter ====

The Mars Reconnaissance Orbiter (MRO) is a multipurpose spacecraft designed to conduct reconnaissance and exploration of Mars from orbit. The US$720 million spacecraft was built by Lockheed Martin under the supervision of the Jet Propulsion Laboratory, launched August 12, 2005, and entered Mars orbit on March 10, 2006.The MRO contains a host of scientific instruments such as the HiRISE camera, CTX camera, CRISM, and SHARAD. The HiRISE camera is used to analyze Martian landforms, whereas CRISM and SHARAD can detect water, ice, and minerals on and below the surface.  Additionally, MRO is paving the way for upcoming generations of spacecraft through daily monitoring of Martian weather and surface conditions, searching for future landing sites, and testing a new telecommunications system that enable it to send and receive information at an unprecedented bitrate, compared to previous Mars spacecraft. Data transfer to and from the spacecraft occurs faster than all previous interplanetary missions combined and allows it to serve as an important relay satellite for other missions.


==== Rosetta and Dawn swingbys ====

The ESA Rosetta space probe mission to the comet 67P/Churyumov-Gerasimenko flew within 250 km of Mars on February 25, 2007, in a gravitational slingshot designed to slow and redirect the spacecraft.The NASA Dawn spacecraft used the gravity of Mars in 2009 to change direction and velocity on its way to Vesta, and tested out Dawn's cameras and other instruments on Mars.


==== Fobos-Grunt ====

On November 8, 2011, Russia's Roscosmos launched an ambitious mission called Fobos-Grunt. It consisted of a lander aimed to retrieve a sample back to Earth from Mars' moon Phobos, and place the Chinese Yinghuo-1 probe in Mars' orbit. The Fobos-Grunt mission suffered a complete control and communications failure shortly after launch and was left stranded in low Earth orbit, later falling back to Earth. The Yinghuo-1 satellite and Fobos-Grunt underwent destructive re-entry on January 15, 2012, finally disintegrating over the Pacific Ocean.


==== Curiosity rover ====

The NASA Mars Science Laboratory mission with its rover named Curiosity, was launched on November 26, 2011, and landed on Mars on August 6, 2012 on Aeolis Palus in Gale Crater. The rover carries instruments designed to look for past or present conditions relevant to the past or present habitability of Mars.


==== MAVEN ====
NASA's MAVEN is an orbiter mission to study the upper atmosphere of Mars. It will also serve as a communications relay satellite for robotic landers and rovers on the surface of Mars. MAVEN was launched 18 November 2013 and reached Mars on 22 September 2014.


==== Mars Orbiter Mission ====
The Mars Orbiter Mission, also called Mangalyaan, was launched on 5 November 2013 by the Indian Space Research Organisation (ISRO). It was successfully inserted into Martian orbit on 24 September 2014.  The mission is a technology demonstrator, and as secondary objective, it will also study the Martian atmosphere. This is India's first mission to Mars, and with it, ISRO became the fourth space agency to successfully reach Mars after the Soviet Union, NASA (USA) and ESA (Europe). It also made ISRO the second space agency to reach Mars orbit on its first attempt (the first national one, after the international ESA), and also the first Asian country to successfully send an orbiter to Mars. It was completed in a record low budget of $71 million, making it the least-expensive Mars mission to date.


==== Trace Gas Orbiter and EDM ====

The ExoMars Trace Gas Orbiter is an atmospheric research orbiter built in collaboration between ESA and Roscosmos. It was injected into Mars orbit on 19 October 2016 to gain a better understanding of methane (CH4) and other trace gases present in the Martian atmosphere that could be evidence for possible biological or geological activity. The Schiaparelli EDM lander was destroyed when trying to land on the surface of Mars.


==== InSight and MarCO ====

In August 2012, NASA selected InSight, a $425 million lander mission with a heat flow probe and seismometer, to determine the deep interior structure of Mars. Two flyby CubeSats called MarCO were launched with InSight on 5 May 2018 to provide real-time telemetry during the entry and landing of InSight. The CubeSats separated from the Atlas V booster 1.5 hours after launch and traveled their own trajectories to Mars. InSight landed successfully on Mars on 26 November 2018.


==== Hope ====
The United Arab Emirates launched the Hope Mars Mission, in July 2020 on the Japanese H-IIA booster. It was successfully placed into orbit on 9 February 2021. It is studying the Martian atmosphere and weather.


==== Tianwen-1 ====
Tianwen-1 is a Chinese mission, launched on 23 July 2020. It includes an orbiter, a lander and a small rover. The orbiter was placed into orbit on 10 February 2021. The lander and rover are currently planned to land in May 2021.


==== Mars 2020 ====

The Mars 2020 mission by NASA was launched on 30 July 2020 on a United Launch Alliance Atlas V rocket from Cape Canaveral. It is based on the Mars Science Laboratory design. The scientific payload is focused on astrobiology. It includes Perseverance rover and Mars Helicopter Ingenuity. Unlike older rovers that relied on solar power, Perseverance is nuclear powered, to survive longer than its predecessors in this harsh, dusty environment. The car-size rover weighs about 1 ton, with a robotic arm that reaches about 7 feet, zoom cameras, a chemical analyzer and a rock drill.After traveling 293 million miles to reach Mars over the course of more than six months, Perseverance successfully landed on February 18, 2021. Its initial mission is set for at least one Martian year, or 687 Earth days. It will search for signs of ancient life and explore the red planet's surface.


== Future missions ==

As part of the ExoMars program, ESA and the Roscosmos plan to send the Rosalind Franklin rover in 2022 to search for evidence of past or present microscopic life on Mars. The lander to deliver the rover is called Kazachok, and it will perform scientific studies for about 2 years.
India's ISRO plans to send a follow-up mission to its Mars Orbiter Mission in 2024; it is called Mars Orbiter Mission 2 (MOM-2) and it will consist of an orbiter, and probably a rover.


=== Proposals ===
The Finnish-Russian Mars MetNet concept would use multiple small meteorological stations on Mars to establish a widespread observation network to investigate the planet's atmospheric structure, physics and meteorology. The MetNet precursor or demonstrator was considered for a piggyback launch on Fobos-Grunt, and on the two proposed to fly on the 2016 and 2020 ExoMars spacecraft.
The Mars-Grunt is a Russian mission concept to bring a sample of Martian soil to Earth.
A ESA-NASA team produced a three-launch architecture concept for a Mars sample return, which uses a rover to cache small samples, a Mars ascent stage to send it into orbit, and an orbiter to rendezvous with it above Mars and take it to Earth. Solar-electric propulsion could allow a one launch sample return instead of three.
The Mars Scout Program's SCIM would involve a probe grazing the upper atmosphere of Mars to collect dust and air for return to Earth.
JAXA is working on a mission concept called MELOS rover that would look for biosignatures of extant life on Mars.Other future mission concepts include polar probes, Martian aircraft and a network of small meteorological stations. Longterm areas of study may include Martian lava tubes, resource utilization, and electronic charge carriers in rocks. Micromissions are another possibility, such as piggybacking a small spacecraft on an Ariane 5 rocket and using a lunar gravity assist to get to Mars.


== Human mission proposals ==

The human exploration of Mars has been an aspiration since the earliest days of modern rocketry; Robert H. Goddard credits the idea of reaching Mars as his own inspiration to study the physics and engineering of space flight.  Proposals for human exploration of Mars have been made throughout the history of space exploration; currently there are multiple active plans and programs to put humans on Mars within the next ten to thirty years, both governmental and private, some of which are listed below.


=== NASA ===

Human exploration by the United States was identified as a long-term goal in the Vision for Space Exploration announced in 2004 by then US President George W. Bush. The planned Orion spacecraft would be used to send a human expedition to Earth's moon by 2020 as a stepping stone to a Mars expedition. On September 28, 2007, NASA administrator Michael D. Griffin stated that NASA aims to put a person on Mars by 2037.On December 2, 2014, NASA's Advanced Human Exploration Systems and Operations Mission Director Jason Crusan and Deputy Associate Administrator for Programs James Reuthner announced tentative support for the Boeing ""Affordable Mars Mission Design"" including radiation shielding, centrifugal artificial gravity, in-transit consumable resupply, and a lander which can return. Reuthner suggested that if adequate funding was forthcoming, the proposed mission would be expected in the early 2030s.On October 8, 2015, NASA published its official plan for human exploration and colonization of Mars. They called it ""Journey to Mars"". The plan operates through three distinct phases leading up to fully sustained colonization.
The first stage, already underway, is the ""Earth Reliant"" phase.  This phase continues utilizing the International Space Station until 2024; validating deep space technologies and studying the effects of long duration space missions on the human body.
The second stage, ""Proving Ground,"" moves away from Earth reliance and ventures into cislunar space for most of its tasks.  This is when NASA plans to capture an asteroid (planned for 2020), test deep space habitation facilities, and validate capabilities required for human exploration of Mars.  Finally, phase three is the transition to independence from Earth resources.
The last stage, the ""Earth Independent"" phase, includes long term missions on the lunar surface which leverage surface habitats that only require routine maintenance, and the harvesting of Martian resources for fuel, water, and building materials.  NASA is still aiming for human missions to Mars in the 2030s, though Earth independence could take decades longer.
On August 28, 2015, NASA funded a year long simulation to study the effects of a year long Mars mission on six scientists. The scientists lived in a bio dome on a Mauna Loa mountain in Hawaii with limited connection to the outside world and were only allowed outside if they were wearing spacesuits.NASAs human Mars exploration plans have evolved through the NASA Mars Design Reference Missions, a series of design studies for human exploration of Mars.
In 2017 the focus of NASA shifted to a return to the Moon by 2024 with the Artemis program, a flight to Mars could follow after this project.


=== SpaceX ===
The long-term goal of the private corporation SpaceX is the establishment of routine flights to Mars to enable colonization. To this end, the company is developing Starship, a spacecraft capable of crew transportation to Mars and other celestial bodies, along with its booster Super Heavy. In 2017 SpaceX announced plans to send two uncrewed Starships to Mars by 2022, followed by two more uncrewed flights and two crewed flights in 2024. Starship is planned to have a payload of at least 100 tonnes. Starship is designed to use a combination of aerobraking and propulsive descent, utilizing fuel produced from a Mars (in situ resource utilization) facility.  As of early 2021, the Starship development program has seen successful testing of several Starship prototypes. Most notably, Starship SN8, which performed its first test flight in December 2020, was a partial success. Although major objectives were achieved, including stable ascent, descent and flip maneuver, it crashed upon landing due to pressurization issues in a fuel tank. A similar approach is intended to be used on Mars.


=== Zubrin ===
Mars Direct, a low-cost human mission proposed by Robert Zubrin, founder of the Mars Society, would use heavy-lift Saturn V class rockets, such as the Ares V, to skip orbital construction, LEO rendezvous, and lunar fuel depots. A modified proposal, called ""Mars to Stay"", involves not returning the first immigrant explorers immediately, if ever (see Colonization of Mars).


== Probing difficulties ==

The challenge, complexity and length of Mars missions have led to many mission failures. The high failure rate of missions attempting to explore Mars is informally called the ""Mars Curse"" or ""Martian Curse"".  The phrase ""Galactic Ghoul"" or ""Great Galactic Ghoul"", referring to a fictitious space monster that subsists on a diet of Mars probes, and is sometimes facetiously used to ""explain"" the recurring difficulties.Two Soviet probes were sent to Mars in 1988 as part of the Phobos program. Phobos 1 operated normally until an expected communications session on 2 September 1988 failed to occur. The problem was traced to a software error, which deactivated Phobos 1's attitude thrusters, causing the spacecraft's solar arrays to no longer point at the Sun, depleting Phobos 1's batteries. Phobos 2 operated normally throughout its cruise and Mars orbital insertion phases on January 29, 1989, gathering data on the Sun, interplanetary medium, Mars, and Phobos. Shortly before the final phase of the mission – during which the spacecraft was to approach within 50 m of Phobos' surface and release two landers, one a mobile 'hopper', the other a stationary platform – contact with Phobos 2 was lost. The mission ended when the spacecraft signal failed to be successfully reacquired on March 27, 1989. The cause of the failure was determined to be a malfunction of the on-board computer.Just a few years later in 1992 Mars Observer, launched by NASA, failed as it approached Mars. Mars 96, an orbiter launched on November 16, 1996 by Russia failed, when the planned second burn of the Block D-2 fourth stage did not occur.Following the success of Global Surveyor and Pathfinder, another spate of failures occurred in 1998 and 1999, with the Japanese Nozomi orbiter and NASA's Mars Climate Orbiter, Mars Polar Lander, and Deep Space 2 penetrators all suffering various fatal errors. The Mars Climate Orbiter was noted for mixing up U.S. customary units with metric units, causing the orbiter to burn up while entering Mars' atmosphere.The European Space Agency has also attempted to land two probes on the Martian surface; Beagle 2, a British-built lander that failed to deploy its solar arrays properly after touchdown in December 2003, and Schiaparelli, which was flown along the ExoMars Trace Gas Orbiter. Contact with the Schiaparelli EDM lander was lost 50 seconds before touchdown. It was later confirmed that the lander struck the surface at a high velocity, possibly exploding.


== See also ==

Mars
General


== References ==


== Bibliography ==
Mars – A Warmer, Wetter Planet by Jeffrey S. Kargel (published July 2004; ISBN 978-1-85233-568-7)
The Compact NASA Atlas of the Solar System by Ronald Greeley and Raymond Batson (published January 2002; ISBN 0-521-80633-X)
Mars: The NASA Mission Reports / edited by Robert Godwin (2000) ISBN 1-896522-62-9


== External links ==
NASA Mars exploration website
Mars Exploration Scientific American Maps and Articles
Next on Mars (Bruce Moomaw, Space Daily, 9 March 2005): An extensive overview of NASA's Mars exploration plans
Catalog of Soviet Mars images Collection of Russian Mars probes' images.
Simplified study of orbits to land on Mars and return to Earth (High School level)
Planetary Society Mars page


== Notes ==
^α  The diagram includes missions that are active on the surface, such as operational rovers and landers, as well as probes in Mars orbit. The diagram does not include missions that are en route to Mars, or probes that performed a fly-by of Mars and moved on.","pandas(index=64, _1=64, text='the planet mars has been explored remotely by spacecraft. probes sent from earth, beginning in the late 20th century, have yielded a large increase in knowledge about the martian system, focused primarily on understanding its geology and habitability potential. engineering interplanetary journeys is complicated and the exploration of mars has experienced a high failure rate, especially the early attempts. roughly sixty percent of all spacecraft destined for mars failed before completing their missions and some failed before their observations could begin. some missions have met with unexpected success, such as the twin mars exploration rovers, spirit and opportunity which operated for years beyond their specification.   == current status ==  as of february 2021, there are two operational rovers on the surface of mars, the curiosity and perseverance rovers, both operated by the united states of america space agency nasa. a third rover, part of the tianwen-1 mission, is currently attatched to its orbiter, and is planned to land in may 2021. there are eight orbiters surveying the planet: mars odyssey, mars express, mars reconnaissance orbiter, mars orbiter mission, maven, the trace gas orbiter, the tianwen-1 orbiter, and the hope mars mission, which have contributed massive amounts of information about mars. the stationary lander insight is investigating the deep interior of mars. no sample return missions have been attempted for mars and an attempted return mission for mars\' moon phobos (fobos-grunt) failed at launch in 2011. in all, there are 11 probes currently surveying mars, with a 12th, the tianwen-1 rover, that is in martian orbit but has not landed yet. the next missions expected to arrive at mars are:  the joint exomars program of roscosmos and esa has delayed the launch of the kazachok landing platform, which will carry the rosalind franklin rover, until 2022. mars orbiter mission 2 by india, planned launch in 2024.   == martian system ==  mars has long been the subject of human interest. early telescopic observations revealed color changes on the surface that were attributed to seasonal vegetation and apparent linear features were ascribed to intelligent design. further telescopic observations found two moons, phobos and deimos, polar ice caps and the feature now known as olympus mons, the solar system\'s second tallest mountain. the discoveries piqued further interest in the study and exploration of the red planet. mars is a rocky planet, like earth, that formed around the same time, yet with only half the diameter of earth, and a far thinner atmosphere; it has a cold and desert-like surface.one way the surface of mars has been categorized, is by thirty ""quadrangles"", with each quadrangle named for a prominent physiographic feature within that quadrangle.   == launch windows ==  the minimum-energy launch windows for a martian expedition occur at intervals of approximately two years and two months (specifically 780 days, the planet\'s synodic period with respect to earth). in addition, the lowest available transfer energy varies on a roughly 16-year cycle. for example, a minimum occurred in the 1969 and 1971 launch windows, rising to a peak in the late 1970s, and hitting another low in 1986 and 1988.   == past and current missions ==  starting in 1960, the soviets launched a series of probes to mars including the first intended flybys and hard (impact) landing (mars 1962b). the first successful flyby of mars was on 14–15 july 1965, by nasa\'s mariner 4. on november 14, 1971, mariner 9 became the first space probe to orbit another planet when it entered into orbit around mars. the amount of data returned by probes increased dramatically as technology improved.the first to contact the surface were two soviet probes: mars 2 lander on november 27 and mars 3 lander on december 2, 1971—mars 2 failed during descent and mars 3 about twenty seconds after the first martian soft landing. mars 6 failed during descent but did return some corrupted atmospheric data in 1974. the 1975 nasa launches of the viking program consisted of two orbiters, each with a lander that successfully soft landed in 1976. viking 1 remained operational for six years, viking 2 for three. the viking landers relayed the first color panoramas of mars.the soviet probes phobos 1 and 2 were sent to mars in 1988 to study mars and its two moons, with a focus on phobos. phobos 1 lost contact on the way to mars. phobos 2, while successfully photographing mars and phobos, failed before it was set to release two landers to the surface of phobos.mars has a reputation as a difficult space exploration target; just 25 of 55 missions through 2019, or 45.5%, have been fully successful, with a further three partially successful and partially failures. however, of the sixteen missions since 2001, twelve have been successful and eight of these are still operational. missions that ended prematurely after phobos 1 and 2 (1988) include (see probing difficulties section for more details):  mars observer (launched in 1992) mars 96 (1996) mars climate orbiter (1999) mars polar lander with deep space 2 (1999) nozomi (2003) beagle 2 (2003) fobos-grunt with yinghuo-1 (2011) schiaparelli lander (2016)following the 1993 failure of the mars observer orbiter, the nasa mars global surveyor achieved mars orbit in 1997. this mission was a complete success, having finished its primary mapping mission in early 2001. contact was lost with the probe in november 2006 during its third extended program, spending exactly 10 operational years in space. the nasa mars pathfinder, carrying a robotic exploration vehicle sojourner, landed in the ares vallis on mars in the summer of 1997, returning many images.  nasa\'s mars odyssey orbiter entered mars orbit in 2001. odyssey\'s gamma ray spectrometer detected significant amounts of hydrogen in the upper metre or so of regolith on mars. this hydrogen is thought to be contained in large deposits of water ice.the mars express mission of the european space agency (esa) reached mars in 2003. it carried the beagle 2 lander, which was not heard from after being released and was declared lost in february 2004.  beagle 2 was located in january 2015 by hirise camera on nasa\'s mars reconnaissance orbiter (mro) having landed safely but failed to fully deploy its solar panels and antenna. in early 2004, the mars express planetary fourier spectrometer team announced the orbiter had detected methane in the martian atmosphere, a potential biosignature. esa announced in june 2006 the discovery of aurorae on mars by the mars express.  in january 2004, the nasa twin mars exploration rovers named spirit (mer-a) and opportunity (mer-b) landed on the surface of mars. both have met and exceeded all their science objectives. among the most significant scientific returns has been conclusive evidence that liquid water existed at some time in the past at both landing sites. martian dust devils and windstorms have occasionally cleaned both rovers\' solar panels, and thus increased their lifespan. spirit rover (mer-a) was active until 2010, when it stopped sending data because it got stuck in a sand dune and was unable to reorient itself to recharge its batteries.on 10 march 2006, nasa\'s mars reconnaissance orbiter (mro) probe arrived in orbit to conduct a two-year science survey. the orbiter began mapping the martian terrain and weather to find suitable landing sites for upcoming lander missions. the mro captured the first image of a series of active avalanches near the planet\'s north pole in 2008.rosetta came within 250 km of mars during its 2007 flyby. dawn flew by mars in february 2009 for a gravity assist on its way to investigate vesta and ceres.phoenix landed on the north polar region of mars on may 25, 2008. its robotic arm dug into the martian soil and the presence of water ice was confirmed on june 20, 2008. the mission concluded on november 10, 2008 after contact was lost. in 2008, the price of transporting material from the surface of earth to the surface of mars was approximately us$309,000 per kilogram.the mars science laboratory mission was launched on november 26, 2011 and it delivered the curiosity rover on the surface of mars on august 6, 2012 utc. it is larger and more advanced than the mars exploration rovers, with a velocity of up to 90 meters per hour (295 feet per hour). experiments include a laser chemical sampler that can deduce the composition of rocks at a distance of 7 meters.  maven orbiter was launched on 18 november 2013, and on 22 september 2014, it was injected into an areocentric elliptic orbit 6,200 km (3,900 mi) by 150 km (93 mi) above the planet\'s surface to study its atmosphere. mission goals include determining how the planet\'s atmosphere and water, presumed to have once been substantial, were lost over time.the indian space research organisation (isro) launched their mars orbiter mission (mom) on november 5, 2013, and it was inserted into mars orbit on september 24, 2014. india\'s isro is the fourth space agency to reach mars, after the soviet space program, nasa and esa. india successfully placed a spacecraft into mars orbit, and became the first country to do so in its maiden attempt.the exomars trace gas orbiter arrived at mars in 2016 and deployed the schiaparelli edm lander, a test lander. schiaparelli crashed on surface, but it transmitted key data during its parachute descent, so the test was declared a partial success. mars direct, a low-cost human mission proposed by robert zubrin, founder of the mars society, would use heavy-lift saturn v class rockets, such as the ares v, to skip orbital construction, leo rendezvous, and lunar fuel depots. a modified proposal, called ""mars to stay"", involves not returning the first immigrant explorers immediately, if ever (see colonization of mars).   == probing difficulties ==  the challenge, complexity and length of mars missions have led to many mission failures. the high failure rate of missions attempting to explore mars is informally called the ""mars curse"" or ""martian curse"".  the phrase ""galactic ghoul"" or ""great galactic ghoul"", referring to a fictitious space monster that subsists on a diet of mars probes, and is sometimes facetiously used to ""explain"" the recurring difficulties.two soviet probes were sent to mars in 1988 as part of the phobos program. phobos 1 operated normally until an expected communications session on 2 september 1988 failed to occur. the problem was traced to a software error, which deactivated phobos 1\'s attitude thrusters, causing the spacecraft\'s solar arrays to no longer point at the sun, depleting phobos 1\'s batteries. phobos 2 operated normally throughout its cruise and mars orbital insertion phases on january 29, 1989, gathering data on the sun, interplanetary medium, mars, and phobos. shortly before the final phase of the mission – during which the spacecraft was to approach within 50 m of phobos\' surface and release two landers, one a mobile \'hopper\', the other a stationary platform – contact with phobos 2 was lost. the mission ended when the spacecraft signal failed to be successfully reacquired on march 27, 1989. the cause of the failure was determined to be a malfunction of the on-board computer.just a few years later in 1992 mars observer, launched by nasa, failed as it approached mars. mars 96, an orbiter launched on november 16, 1996 by russia failed, when the planned second burn of the block d-2 fourth stage did not occur.following the success of global surveyor and pathfinder, another spate of failures occurred in 1998 and 1999, with the japanese nozomi orbiter and nasa\'s mars climate orbiter, mars polar lander, and deep space 2 penetrators all suffering various fatal errors. the mars climate orbiter was noted for mixing up u.s. customary units with metric units, causing the orbiter to burn up while entering mars\' atmosphere.the european space agency has also attempted to land two probes on the martian surface; beagle 2, a british-built lander that failed to deploy its solar arrays properly after touchdown in december 2003, and schiaparelli, which was flown along the exomars trace gas orbiter. contact with the schiaparelli edm lander was lost 50 seconds before touchdown. it was later confirmed that the lander struck the surface at a high velocity, possibly exploding.   == see also ==  mars general   == references ==   == bibliography == mars – a warmer, wetter planet by jeffrey s. kargel (published july 2004; isbn 978-1-85233-568-7) the compact nasa atlas of the solar system by ronald greeley and raymond batson (published january 2002; isbn 0-521-80633-x) mars: the nasa mission reports / edited by robert godwin (2000) isbn 1-896522-62-9   == external links == nasa mars exploration website mars exploration scientific american maps and articles next on mars (bruce moomaw, space daily, 9 march 2005): an extensive overview of nasa\'s mars exploration plans catalog of soviet mars images collection of russian mars probes\' images. simplified study of orbits to land on mars and return to earth (high school level) planetary society mars page   == notes == ^α  the diagram includes missions that are active on the surface, such as operational rovers and landers, as well as probes in mars orbit. the diagram does not include missions that are en route to mars, or probes that performed a fly-by of mars and moved on.')"
65,"An aerostructure is a component of an aircraft's airframe. This may include all or part of the fuselage, wings, or flight control surfaces. Companies that specialize in constructing these components are referred to as ""aerostructures manufacturers"", though many larger aerospace firms with a more diversified product portfolio also build aerostructures.
Mechanical testing of the individual components or complete structure is carried out on a Universal Testing Machine.  Test carried out include tensile, compression, flexure, fatigue, impact, compression after impact. Before testing the component, aerospace engineers build finite element models to simulate the reality.


== Civilian ==
Airplanes designed for civilian use are often cheaper than military aircraft. Smaller passenger airplanes are used for short distance, transcontinental transport. It is more cost efficient for airlines and there is less demand for aircraft transportation at these distances as people can, while inconvenient, drive these distances. While bigger airplanes are manufactured for intercontinental transport, so more passengers can be carried at one time, money can be saved on fuel, and airliners do not have to pay as many pilots. Cargo planes are usually built to be bigger than the average jet. They have a lot of space and large dimensions, so they can carry a lot of weight and a large volume of cargo in one trip. They have large wingspans, a very large cargo hold, and a very tall vertical fin. They are not built to accommodate passengers except for the pilots, so the use of the cargo hold is much more efficient. There does not need to be room for seats and food and bathrooms for everybody, so the companies made a design that optimizes the space in the aircraft.


== Military ==
The YC-14 Prototype was a prototype plane that was being designed by Boeing specifically for the US Air Force. There were a lot of different designs that were considered and different technologies that were used specifically for carrying tanks and paratroopers. There was a computer that was installed and a very powerful vertical wing that could keep the plane flying at a set altitude, so they could drop whatever they needed to in the battlefield without any complications. This allowed for precise troop placement which could be the difference between victory and defeat in a battle. It also talks about different cheaper materials for the prototype which were heavier and used a honeycomb pattern. The cheaper materials were too heavy, and the Air Force was not happy that Boeing did not meet the Air Force's expectations on the prototype even though the Air Force was aware that they would be using different materials in the production of the actual aircraft.The Apache helicopter that Boeing makes is designed so the front of the helicopter is very narrow.  Not only does it create less drag, but it is a smaller target for infantry units to hit the helicopter. They have also designed the F-15 fighter jet, which has two engines instead of one for maximum speed. This particular aircraft can reach speeds of Mach 2.5. It also happens to be the 8th fastest aircraft ever built. The Boeing C-17 Globemaster 3 uses size and a very large design to carry cargo. It has 4 powerful engines and a special T-tail designed by Boeing for precise control of the unusually large aircraft.


== Research ==
There is a new aircraft material that is 20% lighter than other conventional aircraft materials. However FSW aluminum-alloy which is much heavier than this new material, is more advantageous as opposed to using the new CFRP black constructions. The aluminum is more understood and can be crafted to almost exact precision as opposed to the CFRP, which is very hard to shape. The weight of the aircraft is important, but the precision of the measurements of the aircraft is also important. The new methods and testing require a wide variety of material properties, even though weight is very important when choosing a material.Additionally, there is a new method for research, called Thermography, that uses infrared light to look at computer simulated damage to the material and the structure of an aircraft to see how it holds up. They can use this to look at materials and evaluate the integrity of the actual design of an aircraft. It is very accurate, and it will increase the development of materials as the test is much faster than traditional testing methods. It can also be used to predict the behavior of materials under certain stressful conditions that might make it fail while in use.Boeing Australia is creating big new plants that will help them research and develop materials for aircraft faster than anybody else. Their goal is to be the most innovative company and be the most innovative company at the highest speed. As a result, they are making investments in robots to get the job done. They have decided not to use cheap labor, but high cost, quality labor and a high amount of faculty to maintain these robots and ensure that the plant is running well. They will be paying a high amount of very well qualified candidates to research and keep Boeing going. The age of aircraft is moving toward expensive plants to be able to build the aircraft that is so precisely designed to the exact measurements that is needed for optimal performance and reliability. Aircraft are advanced machines that have only been around for a little more than one hundred years.


== Examples ==
Aero Vodochody
Alcoa's Howmet division
Collins Aerospace, currently a subsidiary of Raytheon Technologies
D-J Engineering Inc.
FACC
GKN
Goodrich Aerostructures Group, currently a part of Collins Aerospace
Mitsubishi Heavy Industries Aerospace
Messier-Bugatti-Dowty
Indonesian Aerospace
Premium AEROTEC
Exelis Inc.
Groupe Latécoère
Spirit AeroSystems
Stelia Aerospace
Vought


== References ==","pandas(index=65, _1=65, text='an aerostructure is a component of an aircraft\'s airframe. this may include all or part of the fuselage, wings, or flight control surfaces. companies that specialize in constructing these components are referred to as ""aerostructures manufacturers"", though many larger aerospace firms with a more diversified product portfolio also build aerostructures. mechanical testing of the individual components or complete structure is carried out on a universal testing machine.  test carried out include tensile, compression, flexure, fatigue, impact, compression after impact. before testing the component, aerospace engineers build finite element models to simulate the reality.   == civilian == airplanes designed for civilian use are often cheaper than military aircraft. smaller passenger airplanes are used for short distance, transcontinental transport. it is more cost efficient for airlines and there is less demand for aircraft transportation at these distances as people can, while inconvenient, drive these distances. while bigger airplanes are manufactured for intercontinental transport, so more passengers can be carried at one time, money can be saved on fuel, and airliners do not have to pay as many pilots. cargo planes are usually built to be bigger than the average jet. they have a lot of space and large dimensions, so they can carry a lot of weight and a large volume of cargo in one trip. they have large wingspans, a very large cargo hold, and a very tall vertical fin. they are not built to accommodate passengers except for the pilots, so the use of the cargo hold is much more efficient. there does not need to be room for seats and food and bathrooms for everybody, so the companies made a design that optimizes the space in the aircraft.   == military == the yc-14 prototype was a prototype plane that was being designed by boeing specifically for the us air force. there were a lot of different designs that were considered and different technologies that were used specifically for carrying tanks and paratroopers. there was a computer that was installed and a very powerful vertical wing that could keep the plane flying at a set altitude, so they could drop whatever they needed to in the battlefield without any complications. this allowed for precise troop placement which could be the difference between victory and defeat in a battle. it also talks about different cheaper materials for the prototype which were heavier and used a honeycomb pattern. the cheaper materials were too heavy, and the air force was not happy that boeing did not meet the air force\'s expectations on the prototype even though the air force was aware that they would be using different materials in the production of the actual aircraft.the apache helicopter that boeing makes is designed so the front of the helicopter is very narrow.  not only does it create less drag, but it is a smaller target for infantry units to hit the helicopter. they have also designed the f-15 fighter jet, which has two engines instead of one for maximum speed. this particular aircraft can reach speeds of mach 2.5. it also happens to be the 8th fastest aircraft ever built. the boeing c-17 globemaster 3 uses size and a very large design to carry cargo. it has 4 powerful engines and a special t-tail designed by boeing for precise control of the unusually large aircraft.   == research == there is a new aircraft material that is 20% lighter than other conventional aircraft materials. however fsw aluminum-alloy which is much heavier than this new material, is more advantageous as opposed to using the new cfrp black constructions. the aluminum is more understood and can be crafted to almost exact precision as opposed to the cfrp, which is very hard to shape. the weight of the aircraft is important, but the precision of the measurements of the aircraft is also important. the new methods and testing require a wide variety of material properties, even though weight is very important when choosing a material.additionally, there is a new method for research, called thermography, that uses infrared light to look at computer simulated damage to the material and the structure of an aircraft to see how it holds up. they can use this to look at materials and evaluate the integrity of the actual design of an aircraft. it is very accurate, and it will increase the development of materials as the test is much faster than traditional testing methods. it can also be used to predict the behavior of materials under certain stressful conditions that might make it fail while in use.boeing australia is creating big new plants that will help them research and develop materials for aircraft faster than anybody else. their goal is to be the most innovative company and be the most innovative company at the highest speed. as a result, they are making investments in robots to get the job done. they have decided not to use cheap labor, but high cost, quality labor and a high amount of faculty to maintain these robots and ensure that the plant is running well. they will be paying a high amount of very well qualified candidates to research and keep boeing going. the age of aircraft is moving toward expensive plants to be able to build the aircraft that is so precisely designed to the exact measurements that is needed for optimal performance and reliability. aircraft are advanced machines that have only been around for a little more than one hundred years.   == examples == aero vodochody alcoa\'s howmet division collins aerospace, currently a subsidiary of raytheon technologies d-j engineering inc. facc gkn goodrich aerostructures group, currently a part of collins aerospace mitsubishi heavy industries aerospace messier-bugatti-dowty indonesian aerospace premium aerotec exelis inc. groupe latécoère spirit aerosystems stelia aerospace vought   == references ==')"
66,"The West Wing of the White House houses the offices of the president of the United States.  The West Wing contains the Oval Office, the Cabinet Room, the Situation Room, and the Roosevelt Room.The West Wing's four floors contain offices for the vice president, White House chief of staff, the counselor to the president, the senior advisor to the president, the White House press secretary, and their support staffs. Adjoining the press secretary's office, in the colonnade between the West Wing and the Executive Residence is the James S. Brady Press Briefing Room along with workspace for the White House press corps.


== History ==

Before the construction of the West Wing, presidential staff worked on the western end of the second floor of what is now the Executive Residence. However, when Theodore Roosevelt became president, he found that the existing offices in the mansion were insufficient to accommodate his family of six children as well as his staff.
A year later in 1902, First Lady Edith Roosevelt hired McKim, Mead & White to separate the living quarters from the offices, to enlarge and modernize the public rooms, to re-do the landscaping, and to redecorate the interior.  Congress approved over half a million dollars for the renovation.The West Wing was originally intended as a temporary office structure, built on the site of the extensive greenhouses and stables. The President's Office and the Cabinet Room took up the eastern third of the building closest to the Residence and attached colonnaded terrace. Roosevelt's rectangular office with adjacent Cabinet Room through a set of double doors which was located approximately where the Roosevelt Room is now near the centre.In 1909, William Howard Taft expanded the building southward, covering the tennis court. He placed the first Oval Office at the centre of the addition's south facade, reminiscent of the oval rooms on the three floors of the White House. Later, at the outset of his presidency, Herbert Hoover rebuilt the West Wing, excavating a partial basement, and supporting it with structural steel. The completed building, however, lasted less than seven months. On December 24, 1929, the West Wing was significantly damaged by an electrical fire. Hoover rebuilt it, and added air-conditioning.
The fourth and final major reorganization was undertaken less than three years later by Franklin D. Roosevelt. Dissatisfied with the size and layout of President Hoover's West Wing, he engaged New York architect Eric Gugler to redesign it in 1933. To create additional space without increasing the apparent size of the building, Gugler excavated a full basement, added a set of subterranean offices under the adjacent lawn, and built an unobtrusive ""penthouse"" story. The directive to wring the most office space out of the existing building was responsible for its narrow corridors and cramped staff offices. Gugler's most notable change was the addition to the east side containing a new Cabinet Room, Secretary's Office, and Oval Office. The new office's location gave presidents greater privacy, allowing them to slip back and forth between the White House and the West Wing without being in full view of the staff.As the size of the president's staff grew over the latter half of the 20th century, the West Wing generally came to be seen as too small for its modern governmental functions. Today, most of the staff members of the Executive Office of the President are located in the adjacent Eisenhower Executive Office Building.

		
		
		
		


== First floor ==


=== Oval Office ===


=== Cabinet Room ===


=== Roosevelt Room ===

Richard Nixon also renamed the room, previously called by Franklin Roosevelt the ""Fish Room"" (where he kept aquariums, and where John F. Kennedy displayed trophy fish), in honour of the two presidents Roosevelt: Theodore, who first built the West Wing, and Franklin, who built the current Oval Office. By tradition, a portrait of Franklin Roosevelt hangs over the mantel of the Roosevelt Room during the administration of a president from the Democratic Party and a portrait of Theodore Roosevelt hangs during the administration of a Republican president (although Bill Clinton chose to retain the portrait of Theodore Roosevelt above the mantel). In the past, the portrait not hanging over the mantel hung on the opposite wall. However, during the first term of George W. Bush, an audio-visual cabinet was placed on the opposite wall providing secure audio and visual conference capabilities across the hall from the Oval Office.


=== Press Briefing Room ===

During the 1930s, the March of Dimes constructed a swimming pool so that Franklin Roosevelt could exercise, as therapy for his polio-related disability. Richard Nixon had the swimming pool covered over to create the Press Briefing Room, where the White House Press Secretary gives daily briefings.


=== White House press corps ===

The journalists, correspondents, and others who are part of the White House press corps have offices near the press briefing room.

		
		
		
		


== Ground floor ==


=== Situation Room ===


=== White House Mess ===
The West Wing ground floor is also the site of a small restaurant operated by the Presidential Food Service and staffed by Naval culinary specialists and called the White House Mess. It is located underneath the Oval Office, and was established by President Truman on June 11, 1951.

		
		


== Second floor ==


== Depiction on The West Wing TV series ==

In 1999, The West Wing television series brought greater public attention to the workings of the presidential staff, as well as to the location of those working in the West Wing. The show followed the working lives of a fictional Democratic U.S. president, Josiah Bartlet, and his senior staff. When asked whether the show accurately captured the working environment in 2003, Press Secretary Scott McClellan commented that the show portrayed more foot traffic and larger rooms than in the real West Wing.


== References ==


== External links ==
White House Museum: West Wing, with floorplan and historical images
West Wing Interactive, from National Journal Magazine","pandas(index=66, _1=66, text='the west wing of the white house houses the offices of the president of the united states.  the west wing contains the oval office, the cabinet room, the situation room, and the roosevelt room.the west wing\'s four floors contain offices for the vice president, white house chief of staff, the counselor to the president, the senior advisor to the president, the white house press secretary, and their support staffs. adjoining the press secretary\'s office, in the colonnade between the west wing and the executive residence is the james s. brady press briefing room along with workspace for the white house press corps.   == history ==  before the construction of the west wing, presidential staff worked on the western end of the second floor of what is now the executive residence. however, when theodore roosevelt became president, he found that the existing offices in the mansion were insufficient to accommodate his family of six children as well as his staff. a year later in 1902, first lady edith roosevelt hired mckim, mead & white to separate the living quarters from the offices, to enlarge and modernize the public rooms, to re-do the landscaping, and to redecorate the interior.  congress approved over half a million dollars for the renovation.the west wing was originally intended as a temporary office structure, built on the site of the extensive greenhouses and stables. the president\'s office and the cabinet room took up the eastern third of the building closest to the residence and attached colonnaded terrace. roosevelt\'s rectangular office with adjacent cabinet room through a set of double doors which was located approximately where the roosevelt room is now near the centre.in 1909, william howard taft expanded the building southward, covering the tennis court. he placed the first oval office at the centre of the addition\'s south facade, reminiscent of the oval rooms on the three floors of the white house. later, at the outset of his presidency, herbert hoover rebuilt the west wing, excavating a partial basement, and supporting it with structural steel. the completed building, however, lasted less than seven months. on december 24, 1929, the west wing was significantly damaged by an electrical fire. hoover rebuilt it, and added air-conditioning. the fourth and final major reorganization was undertaken less than three years later by franklin d. roosevelt. dissatisfied with the size and layout of president hoover\'s west wing, he engaged new york architect eric gugler to redesign it in 1933. to create additional space without increasing the apparent size of the building, gugler excavated a full basement, added a set of subterranean offices under the adjacent lawn, and built an unobtrusive ""penthouse"" story. the directive to wring the most office space out of the existing building was responsible for its narrow corridors and cramped staff offices. gugler\'s most notable change was the addition to the east side containing a new cabinet room, secretary\'s office, and oval office. the new office\'s location gave presidents greater privacy, allowing them to slip back and forth between the white house and the west wing without being in full view of the staff.as the size of the president\'s staff grew over the latter half of the 20th century, the west wing generally came to be seen as too small for its modern governmental functions. today, most of the staff members of the executive office of the president are located in the adjacent eisenhower executive office building.                == first floor == the west wing ground floor is also the site of a small restaurant operated by the presidential food service and staffed by naval culinary specialists and called the white house mess. it is located underneath the oval office, and was established by president truman on june 11, 1951.          == second floor ==   == depiction on the west wing tv series ==  in 1999, the west wing television series brought greater public attention to the workings of the presidential staff, as well as to the location of those working in the west wing. the show followed the working lives of a fictional democratic u.s. president, josiah bartlet, and his senior staff. when asked whether the show accurately captured the working environment in 2003, press secretary scott mcclellan commented that the show portrayed more foot traffic and larger rooms than in the real west wing.   == references ==   == external links == white house museum: west wing, with floorplan and historical images west wing interactive, from national journal magazine')"
67,"Aircraft maintenance is the performance of tasks required to ensure the continuing airworthiness of an aircraft or aircraft part, including overhaul, inspection, replacement, defect rectification, and the embodiment of modifications, compliance with airworthiness directives and repair.


== Regulation ==
The maintenance of aircraft is highly regulated, in order to ensure safe and correct functioning during flight. In civil aviation national regulations are coordinated under international standards, established by the International Civil Aviation Organization (ICAO). The ICAO standards have to be implemented by local airworthiness authorities to regulate the maintenance tasks, personnel and inspection system. Maintenance staff must be licensed for the tasks they carry out.


== Aircraft maintenance organization ==


=== Scheduled maintenance checks ===

Aircraft maintenance in civil aviation generally organized using a maintenance checks or blocks which are packages of maintenance tasks that have to be done on an aircraft after a certain amount of time or usage. Packages are constructed by dividing the maintenance tasks into convenient, bite-size chunks to minimize the time the aircraft is out of service, to keep the maintenance workload level, and to maximize the use of maintenance facilities. 


=== Power-by-the-Hour ===
A Power by the Hour program provides budget predictability, avoids installing a loaner during repairs when an aircraft part fails and enrolled aircraft may have a better value and liquidity. This concept of unscheduled maintenance was initially introduced for aircraft engines to mitigate engine failures. The term was coined by Bristol Siddeley in 1962 to support Vipers of the British Aerospace 125 business jets for a fixed sum per flying hour. A complete engine and accessory replacement service was provided, allowing the operator to accurately forecast this cost, and relieving him from purchasing stocks of engines and accessories.In the 1980s, Rolls-Royce plc reinstated the program to provide the operator with a fixed engine maintenance cost over an extended period of time. Operators are assured of an accurate cost projection and avoid the breakdowns costs; the term is trademarked by Rolls-Royce but is the common name in the industry. It is an option for operators of several Rolls-Royce aircraft engines. Other aircraft engine manufacturers such as General Electric and Pratt & Whitney offer similar programs.Jet Support Services provides hourly cost maintenance programs independently of the manufacturers. GEMCO also offers a similar program for piston engines in general aviation aircraft. Bombardier Aerospace offers its Smart Services program, covering parts and maintenance by the hour.


=== Maintenance release ===
At the completion of any maintenance task a person authorized by the national airworthiness authority signs a maintenance release stating that maintenance has been performed in accordance with the applicable airworthiness requirements. In the case of a certified aircraft this may be an Aircraft Maintenance Engineer or Aircraft Maintenance Technician, while for amateur-built aircraft this may be the owner or builder of the aircraft.
A maintenance release can be called a certificate of release to service (CRS).


== Maintenance personnel ==

The ICAO defines the licensed role of aircraft maintenance (technician/engineer/mechanic), noting that ""The terms in brackets are given as acceptable additions to the title of the license. Each Contracting State is expected to use in its own regulations the one it prefers."" Thus, aircraft maintenance technicians, engineers and mechanics all perform essentially the same role. However different countries use these terms in different ways to define their individual levels of qualification and responsibilities.In Americas licenses for aircraft maintenance personnel include:

Aircraft Maintenance Engineer (AME), also called Licensed Aircraft Maintenance Engineer (LAME or L-AME).
Aircraft Maintenance Technician (AMT), or colloquially Airframe and Powerplant (A&P).
Aircraft Maintenance Mechanic (AMM).As there will be 41,030 new airliners by 2036, Boeing expects 648,000 new commercial airline maintenance technicians from 2017 till then: 256,000 in Asia Pacific (39%), 118,000 in North America (19%) and 111,000 in Europe (17%).In Europe aircraft maintenance personnel must comply with Part 66, Certifying Staff, issued by the European Aviation Safety Agency (EASA). This regulation establishes four levels of authorization:

Level 1: General Familiarisation, Unlicensed
Level 2: Ramp and Transit, Category A
can only certify own work performed for tasks which he/she has received documented training
Level 3: Line Certifying Staff and Base Maintenance Supporting Staff, Category B1 (electromechanic) and/or B2(Avionics)
can certify all work performed on an aircraft/engine for which he/she is type rated excluding base maintenance (generally up to and including A-Check)
Level 4: Base Maintenance Certifying Staff, Category C
can certify all work performed on an aircraft/engine for which he/she is type rated, but only if it is base maintenance (additional level-3 staff necessary)
this authorization does not automatically include any level 2 or level 3 license.


== Market ==


=== Aircraft ===
The Maintenance, Repair, Overhaul (MRO) Market was US$135.1 Billion in 2015, three quarters of the $180.3 B aircraft production market. Of this, 60% is for civil aviation : air transport 48%, business and general aviation 9%, rotorcraft 3% ; and military aviation is 40% : fixed wing 27% and rotary 13%. Of the $64.3 Billion air transport MRO market, 40% is for engines, 22% for components, 17% for line, 14% for airframe and 7% for modifications. Its is projected to grow at 4.1% per annum till 2025 to $96B.Airliner MRO should reach $74.3 Billion in 2017 : 51% ($37.9B) single-aisles, 21% ($15.6B) long range twin-aisles, 8% ($5.9B) medium range twin-aisles, 7% ($5.2B) large aircraft, 6% ($4.5B) regional jets as turboprop regional airliners and 1% ($0.7B) short range twin-aisles.
Over the 2017–2026 decade, the worldwide market should reach over $900 billion, led by 23% in North America, 22% in Western Europe, and 19% in Asia Pacific.In 2017, of the $70 billion spent by airlines on maintenance, repair and overhaul (MRO), 31% were for engines, 27% for components, 24% for line maintenance, 10% for modifications and 8% for the airframe; 70% were for mature airliners (Airbus A320 and A330, Boeing 777 and 737NG), 23% were for “sunset” aircraft (MD-80, Boeing 737 Classic, B747 or B757) and 7% was spent on modern models (Boeing 787, Embraer E-Jet, Airbus A350XWB and A380).In 2018, the commercial aviation industry expended $88 billion for MRO, while military aircraft required $79.6 billion, including field maintenance.
Airliner MRO is forecast to reach $115 billion by 2028, a 4% compound annual growth rate from $77.4 billion in 2018.
Major airframe manufacturers Airbus, Boeing and Embraer entered the market, increasing concerns about intellectual property sharing. Shared data-supported predictive maintenance can reduce operational disruptions. Among other factors, prognostics helped Delta Air Lines reduce maintenance cancellations by 98% from 5,600 in 2010 to 78 in 2017.Insourced maintenance can be inefficient for small airlines with a fleet below 50–60 aircraft. They have to either outsource it or sell its MRO services to other carriers for better resource utilization.
For example, the maintenance on South African Comair's 26 Boeing 737s is outsourced to South African Airways' Technical Department.
Another example is Spain's Air Nostrum operates 45 CRJs and ATR72s and its 300-person maintenance department provides line, base maintenance and limited component repair for other airlines 20% of the time.Airframe heavy maintenance is worth $6 billion in 2019: $2.9 billion for C checks and $3.1 billion for D checks, Aviation Week forecasts a growth to $7.5 billion in 2028 – $3.1 billion C and $4.2 billion D – for $70 billion over 10 years, 10% of the overall market compared to 40% for the engines.


=== Engines ===

The commercial aviation engine MRO market is anticipated by Aviation Week to be $25.9 billion in 2018, a 2.5 billion increase from 2017, led by 21% for the Boeing 737NG' CFM56-7B and the A320's CFM56-5B and IAE V2500 (also on the MD-90) tied for second, followed by the mature widebody engines: the GE90 then the Trent 700.Over the 2017–2026 decade, the largest markets for turbofans will be the B737NG's CFM56-7 with 23%, the V2500-A5 with 21%, the GE90-115B with 13%, the A320's CFM56-5B with 13%, the PW1000G with 7%, the Trent 700 with 6%, the CF6-80C2 with 5%, the CFM LEAP with 5% and the CF34-8 with 4%.
Between 2018 and 2022, the largest MRO demand will be for CFM engines with 36%, followed by GE with 24%, Rolls with 13%, IAE with 12% and Pratt with 7%.As an aircraft gets older, a greater percentage of its value is represented by its engines.
Over the course of the engine life it is possible to put value back in by repair and overhaul, to sell it for its remaining useful time, or to disassemble it and sell the used parts, to extract its remaining value.
Its maintenance value includes the value of life-limited parts (LLPs) and the time before overhaul.
The core value is the value of its data plate and non-life-limited-parts.
Engine makers deeply discount their sales, up to 90%, to win the multi-year stream of spares and services, resembling the razor and blades model.Engines installed on a new aircraft are discounted by at least 40% while spare engine values closely follow list prices.
Accounting for 80% of a shop visit cost, LLP prices escalate to recoup the original discount, until engine availability increase with aircraft teardowns.
Between 2001 and 2018 for the Airbus A320 or the Boeing 737-800, their CFM56 value increased from 27–29% to 48–52% of the aircraft value.
The 777-200ER's PW4000 and the A330-300's Trent 700 engines rose from a share of 18–25% in 2001 to 29–40% in 2013.
For the A320neo and 737 MAX, between 52% and 57% of their value lies in their engines: this could rise to 80–90% after ten years, while new A350 or B787 engines are worth 36–40% of the aircraft.
After some time the maintenance reserves exceed the aircraft lease.In 2018, a full set of LLP for a B737-800's CFM56-7B list price is $3.6 million, like for the A320ceo's CFM56-5B for 20–30,000 cycles up from $2.0 million in 2009, while an IAE V2500 is priced at $3.9 million for 20,000 cycles but have a lower overhaul cost.
The LLP parts for and A320neo's PW1127G costs $4 million and its competitor $4.3 million for 20–30,000 cycles.
For an A330ceo, a GE CF6-80 LLP set is priced at $11 million for 15–20,000 cycles and $9 million for a PW4000, and $6 million for a Trent 700 but with a $9–10 million overhaul against $4–5 million for the others.
The LLP set for a B767-300ER's CF6 or PW4000 costs $7 million, and for a B787-8's Trent 1000 $7 million compared to $8.5 million for a GEnx.
An B777-300ER's GE90 LLP set is priced at $9 million while the A380's Trent 900 costs $7 million, both for 15,000 cycles.
Between 2019 and 2038, 5,200 spare airliner engines will be required with at least half leased.An engine overhaul for a B737-800 costs $3.1 million every 20,000 hours, or $3.4 million every 15,000 hours for earlier variants, while for a B757 powerplant it costs $4.5 million every 24,000 hours.
For an A330 turbofan, it costs $7 million every 24,000 hours, $8 million for an A350 or B787 engine, $9 million every 20,000 hours for a B777-200ER powerplant and $10 million every 25,000 hours for a B777-300ER engine.
It costs $4 million every 18,000 hours for each B747-400 turbofan and $7.5 million every 25,000 hours for an A380 engine.


== Future of aircraft maintenance ==


=== Aircraft health monitoring ===
Airbus has indicated that data diagnostics could put an end to aircraft unscheduled grounding for fault repairs around 2025, supported by big data and operational experience. Predictive maintenance, diagnostics and health monitoring could eliminate unscheduled groundings, by making maintenance schedule intervals more frequent to avoid AOGs and the associated operational interruptions, ultimately eliminating them. Data or monitoring can tell that some parts do not need a scheduled check, but a full transition to this model will need much greater experience. With more history, examples and regulatory confidence, the maintenance program and associated manuals could become a dynamic documents for each specific aircraft with maintenance schedule based on operational history of the aircraft.


=== Electric aircraft ===
In October 2018, consultant Roland Berger counted 134 electric propulsion projects: 70% electric engines with batteries recharged on ground and 30% hybrid-electric with a fuel generator, in parallel or in series; 45% are urban air taxis, 43% general aviation and 12% airliners.
All-electric is sometimes selected for sub-19 seats commuters, and more often for smaller 2-4-seat aircraft like urban air taxis or trainers.
Electric motors will probably require less maintenance than a fuel engine, while batteries and cables may need to be exchanged more often than fuel systems.The all-electric Pipistrel Alpha Electro two-seat trainer is already certified as a LSA in Europe, Australia and possibly the US. Redmond, Washington-based MagniX is integrating a 350 hp (260 kW) electric motor on its iron bird testbed before a first flight of a Cessna Caravan in 2019, with a 750 hp (560 kW) Magni500 replacing its PT6 single turboprop. MagniX expects to certify the Magni500 and the 375 hp (280 kW) Magni250 by 2020, and the Caravan conversion by 2022 with a range of 100–200 mi (160–320 km) as it is typically operated over less than 100 mi (160 km). A Britten-Norman Islander retrofitted with electric propulsion should be demonstrated by 2021 by Cranfield Aerospace before commercial service in 2023. Roland Berger expects a 50-seat hybrid-electric airliner in 2032 with a 340 km (210 mi) range.


=== Maintenance automation ===

Automated aircraft inspection systems have the potential to make aircraft maintenance safer and more reliable. Various solutions are currently developed: a collaborative mobile robot named Air-Cobot, and Unmanned aerial vehicles from Donecle or Easyjet.


== See also ==
Airworthiness
Groundcrew
Line-replaceable unit
Maintenance Resource Management
Maintenance (technical)
Professional Aviation Maintenance Association
RAMS
Shop-replaceable unit


== References ==


== External links ==
""Association for Women in Aviation Maintenance"".
Lindsay Bjerregaard, Lee Ann Shay (Oct 11, 2017). ""A Day In American's Line Operations At Chicago O'Hare"". Aviation Week network.
James Pozzi (Oct 17, 2017). ""Inside Iberia's Engine Shop"". Aviation Week network.
James Pozzi (Nov 4, 2017). ""A Look Around Lufthansa Technik Sofia's Expanded Facility"". Aviation Week network.
Lindsay Bjerregaard (Nov 13, 2017). ""On The Ground At JetBlue's JFK Hangar"". Aviation Week network.
GE Aviation. Maintenance Minute. Maintenance Minute videos are produced by GE Aviation's training team to help the aircraft maintainer with everyday engine maintenance tasks.","pandas(index=67, _1=67, text='aircraft maintenance is the performance of tasks required to ensure the continuing airworthiness of an aircraft or aircraft part, including overhaul, inspection, replacement, defect rectification, and the embodiment of modifications, compliance with airworthiness directives and repair.   == regulation == the maintenance of aircraft is highly regulated, in order to ensure safe and correct functioning during flight. in civil aviation national regulations are coordinated under international standards, established by the international civil aviation organization (icao). the icao standards have to be implemented by local airworthiness authorities to regulate the maintenance tasks, personnel and inspection system. maintenance staff must be licensed for the tasks they carry out.   == aircraft maintenance organization == automated aircraft inspection systems have the potential to make aircraft maintenance safer and more reliable. various solutions are currently developed: a collaborative mobile robot named air-cobot, and unmanned aerial vehicles from donecle or easyjet.   == see also == airworthiness groundcrew line-replaceable unit maintenance resource management maintenance (technical) professional aviation maintenance association rams shop-replaceable unit   == references ==   == external links == ""association for women in aviation maintenance"". lindsay bjerregaard, lee ann shay (oct 11, 2017). ""a day in american\'s line operations at chicago o\'hare"". aviation week network. james pozzi (oct 17, 2017). ""inside iberia\'s engine shop"". aviation week network. james pozzi (nov 4, 2017). ""a look around lufthansa technik sofia\'s expanded facility"". aviation week network. lindsay bjerregaard (nov 13, 2017). ""on the ground at jetblue\'s jfk hangar"". aviation week network. ge aviation. maintenance minute. maintenance minute videos are produced by ge aviation\'s training team to help the aircraft maintainer with everyday engine maintenance tasks.')"
68,"Cable lacing is a method for tying wiring harnesses and cable looms, traditionally used in telecommunication, naval, and aerospace applications.  This old cable management technique, taught to generations of lineworkers, is still used in some modern applications since it does not create obstructions along the length of the cable, avoiding the handling problems of cables groomed by plastic or hook-and-loop cable ties.
Cable lacing uses a thin cord, which is traditionally made of waxed linen, to bind together a group of cables using a series of running lockstitches.  Flat lacing tapes made of modern materials such as nylon, polyester, Teflon, fiberglass, and Nomex are also available with a variety of coatings to improve knot holding.


== Styles ==
The lacing begins and ends with a whipping or other knot to secure the free ends.  Wraps are spaced relative to the overall harness diameter to maintain the wiring in a tight, neat bundle, and the ends are then neatly trimmed.  In addition to continuous or running lacing, there are a variety of lacing patterns used in different circumstances.  In some cases stand-alone knots called spot ties are also used.  For lashing large cables and cable bundles to support structures in telecommunications applications, there are two named cable lacing styles: the ""Chicago stitch"" and ""Kansas City stitch"".Some organizations have in-house standards to which cable lacing must conform, for example NASA specifies its cable lacing techniques in chapter 9 of NASA-STD-8739.4.


== Examples ==

		
		
		


== Notes and references ==


== External links ==
NASA Technical Standard NASA-STD-8739.4 on Crimping, Interconnecting Cables, Harnesses, and Wiring
""Workmanship Standards Pictorial Reference for NASA-STD-8739"". NASA. Archived from the original on 2009-07-12.
Online excerpt from Electronic Installation Practices Manual (1951) , ""Chapter 9, Cabling""
Online excerpt from Workmanship and Design Practices for Electronic Equipment (1962)
Cable lacing tutorial using modern lacing tape
History, tools, and techniques
FAA Advisory Circular 43.13B paragraph 11-158","pandas(index=68, _1=68, text='cable lacing is a method for tying wiring harnesses and cable looms, traditionally used in telecommunication, naval, and aerospace applications.  this old cable management technique, taught to generations of lineworkers, is still used in some modern applications since it does not create obstructions along the length of the cable, avoiding the handling problems of cables groomed by plastic or hook-and-loop cable ties. cable lacing uses a thin cord, which is traditionally made of waxed linen, to bind together a group of cables using a series of running lockstitches.  flat lacing tapes made of modern materials such as nylon, polyester, teflon, fiberglass, and nomex are also available with a variety of coatings to improve knot holding.   == styles == the lacing begins and ends with a whipping or other knot to secure the free ends.  wraps are spaced relative to the overall harness diameter to maintain the wiring in a tight, neat bundle, and the ends are then neatly trimmed.  in addition to continuous or running lacing, there are a variety of lacing patterns used in different circumstances.  in some cases stand-alone knots called spot ties are also used.  for lashing large cables and cable bundles to support structures in telecommunications applications, there are two named cable lacing styles: the ""chicago stitch"" and ""kansas city stitch"".some organizations have in-house standards to which cable lacing must conform, for example nasa specifies its cable lacing techniques in chapter 9 of nasa-std-8739.4.   == examples ==             == notes and references ==   == external links == nasa technical standard nasa-std-8739.4 on crimping, interconnecting cables, harnesses, and wiring ""workmanship standards pictorial reference for nasa-std-8739"". nasa. archived from the original on 2009-07-12. online excerpt from electronic installation practices manual (1951) , ""chapter 9, cabling"" online excerpt from workmanship and design practices for electronic equipment (1962) cable lacing tutorial using modern lacing tape history, tools, and techniques faa advisory circular 43.13b paragraph 11-158')"
69,"A reaction engine is an engine or motor that produces thrust by expelling reaction mass, in accordance with Newton's third law of motion. This law of motion is most commonly paraphrased as: ""For every action force there is an equal, but opposite, reaction force.""
Examples include jet engines, rocket engines, pump-jet, and more uncommon variations such as Hall effect thrusters, ion drives, mass drivers, and nuclear pulse propulsion.


== Energy use ==


=== Propulsive efficiency ===

For all reaction engines that carry on-board propellant (such as rocket engines and electric propulsion drives) some energy must go into accelerating the reaction mass. Every engine wastes some energy, but even assuming 100% efficiency, the engine needs energy amounting to

  
    
      
        
          
            
              
                
                  
                    1
                    2
                  
                
              
            
          
        
        M
        
          V
          
            e
          
          
            2
          
        
      
    
    {\displaystyle {\begin{matrix}{\frac {1}{2}}\end{matrix}}MV_{e}^{2}}
  (where M is the mass of propellent expended and 
  
    
      
        
          V
          
            e
          
        
      
    
    {\displaystyle V_{e}}
   is the exhaust velocity), which is simply the energy to accelerate the exhaust.

Comparing the rocket equation (which shows how much energy ends up in the final vehicle) and the above equation (which shows the total energy required) shows that even with 100% engine efficiency, certainly not all energy supplied ends up in the vehicle – some of it, indeed usually most of it, ends up as kinetic energy of the exhaust.
If the specific impulse (
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
  ) is fixed, for a mission delta-v, there is a particular 
  
    
      
        
          I
          
            s
            p
          
        
      
    
    {\displaystyle I_{sp}}
   that minimises the overall energy used by the rocket. This comes to an exhaust velocity of about ⅔ of the mission delta-v (see the energy computed from the rocket equation). Drives with a specific impulse that is both high and fixed such as Ion thrusters have exhaust velocities that can be enormously higher than this ideal, and thus end up powersource limited and give very low thrust. Where the vehicle performance is power limited, e.g. if solar power or nuclear power is used, then in the case of a large 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   the maximum acceleration is inversely proportional to it. Hence the time to reach a required delta-v is proportional to 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
  . Thus the latter should not be too large.
On the other hand, if the exhaust velocity can be made to vary so that at each instant it is equal and opposite to the vehicle velocity then the absolute minimum energy usage is achieved. When this is achieved, the exhaust stops in space ^  and has no kinetic energy; and the propulsive efficiency is 100% all the energy ends up in the vehicle (in principle such a drive would be 100% efficient, in practice there would be thermal losses from within the drive system and residual heat in the exhaust). However, in most cases this uses an impractical quantity of propellant, but is a useful theoretical consideration.
Some drives (such as VASIMR or electrodeless plasma thruster) actually can significantly vary their exhaust velocity. This can help reduce propellant usage and improve acceleration at different stages of the flight. However the best energetic performance and acceleration is still obtained when the exhaust velocity is close to the vehicle speed. Proposed ion and plasma drives usually have exhaust velocities enormously higher than that ideal (in the case of VASIMR the lowest quoted speed is around 15 km/s compared to a mission delta-v from high Earth orbit to Mars of about 4 km/s).
For a mission, for example, when launching from or landing on a planet, the effects of gravitational attraction and any atmospheric drag must be overcome by using fuel. It is typical to combine the effects of these and other effects into an effective mission delta-v. For example, a launch mission to low Earth orbit requires about 9.3–10 km/s delta-v. These mission delta-vs are typically numerically integrated on a computer.


=== Cycle efficiency ===
All reaction engines lose some energy, mostly as heat.
Different reaction engines have different efficiencies and losses. For example, rocket engines can be up to 60–70% energy efficient in terms of accelerating the propellant. The rest is lost as heat and thermal radiation, primarily in the exhaust.


=== Oberth effect ===

Reaction engines are more energy efficient when they emit their reaction mass when the vehicle is travelling at high speed.
This is because the useful mechanical energy generated is simply force times distance, and when a thrust force is generated while the vehicle moves, then:

  
    
      
        E
        =
        F
        ×
        d
        
      
    
    {\displaystyle E=F\times d\;}
  where F is the force and d is the distance moved.
Dividing by length of time of motion we get:

  
    
      
        
          
            E
            t
          
        
        =
        P
        =
        
          
            
              F
              ×
              d
            
            t
          
        
        =
        F
        ×
        v
      
    
    {\displaystyle {\frac {E}{t}}=P={\frac {F\times d}{t}}=F\times v}
  Hence:

  
    
      
        P
        =
        F
        ×
        v
        
      
    
    {\displaystyle P=F\times v\;}
  where P is the useful power and v is the speed.
Hence, v should be as high as possible, and a stationary engine does no useful work.


=== Delta-v and propellant ===

Exhausting the entire usable propellant of a spacecraft through the engines in a straight line in free space would produce a net velocity change to the vehicle; this number is termed delta-v (
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  ).
If the exhaust velocity is constant then the total 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of a vehicle can be calculated using the rocket equation, where M is the mass of propellant, P is the mass of the payload (including the rocket structure), and 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   is the velocity of the rocket exhaust. This is known as the Tsiolkovsky rocket equation:

  
    
      
        Δ
        v
        =
        
          v
          
            e
          
        
        ln
        ⁡
        
          (
          
            
              
                M
                +
                P
              
              P
            
          
          )
        
        .
      
    
    {\displaystyle \Delta v=v_{e}\ln \left({\frac {M+P}{P}}\right).}
  For historical reasons, as discussed above, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
   is sometimes written as

  
    
      
        
          v
          
            e
          
        
        =
        
          I
          
            sp
          
        
        
          g
          
            0
          
        
      
    
    {\displaystyle v_{e}=I_{\text{sp}}g_{0}}
  where 
  
    
      
        
          I
          
            sp
          
        
      
    
    {\displaystyle I_{\text{sp}}}
   is the specific impulse of the rocket, measured in seconds, and 
  
    
      
        
          g
          
            0
          
        
      
    
    {\displaystyle g_{0}}
   is the gravitational acceleration at sea level.
For a high delta-v mission, the majority of the spacecraft's mass needs to be reaction mass. Because a rocket must carry all of its reaction mass, most of the initially-expended reaction mass goes towards accelerating reaction mass rather than payload. If the rocket has a payload of mass P, the spacecraft needs to change its velocity by 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  , and the rocket engine has exhaust velocity ve, then the reaction mass M which is needed can be calculated using the rocket equation and the formula for 
  
    
      
        
          I
          
            sp
          
        
      
    
    {\displaystyle I_{\text{sp}}}
  :

  
    
      
        M
        =
        P
        
          (
          
            
              e
              
                
                  
                    Δ
                    v
                  
                  
                    v
                    
                      e
                    
                  
                
              
            
            −
            1
          
          )
        
        .
      
    
    {\displaystyle M=P\left(e^{\frac {\Delta v}{v_{e}}}-1\right).}
  For 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   much smaller than ve, this equation is roughly linear, and little reaction mass is needed. If 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   is comparable to ve, then there needs to be about twice as much fuel as combined payload and structure (which includes engines, fuel tanks, and so on).  Beyond this, the growth is exponential; speeds much higher than the exhaust velocity require very high ratios of fuel mass to payload and structural mass.
For a mission, for example, when launching from or landing on a planet, the effects of gravitational attraction and any atmospheric drag must be overcome by using fuel. It is typical to combine the effects of these and other effects into an effective mission delta-v. For example, a launch mission to low Earth orbit requires about 9.3–10 km/s delta-v. These mission delta-vs are typically numerically integrated on a computer.
Some effects such as Oberth effect can only be significantly utilised by high thrust engines such as rockets; i.e., engines that can produce a high g-force (thrust per unit mass, equal to delta-v per unit time).


=== Energy ===

In the ideal case 
  
    
      
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{1}}
   is useful payload and 
  
    
      
        
          m
          
            0
          
        
        −
        
          m
          
            1
          
        
      
    
    {\displaystyle m_{0}-m_{1}}
   is reaction mass (this corresponds to empty tanks having no mass, etc.). The energy required can simply be computed as

  
    
      
        
          
            1
            2
          
        
        (
        
          m
          
            0
          
        
        −
        
          m
          
            1
          
        
        )
        
          v
          
            e
          
          
            2
          
        
      
    
    {\displaystyle {\frac {1}{2}}(m_{0}-m_{1})v_{\text{e}}^{2}}
  This corresponds to the kinetic energy the expelled reaction mass would have at a speed equal to the exhaust speed. If the reaction mass had to be accelerated from zero speed to the exhaust speed, all energy produced would go into the reaction mass and nothing would be left for kinetic energy gain by the rocket and payload. However, if the rocket already moves and accelerates (the reaction mass is expelled in the direction opposite to the direction in which the rocket moves) less kinetic energy is added to the reaction mass. To see this, if, for example, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{e}}
  =10 km/s and the speed of the rocket is 3 km/s, then the speed of a small amount of expended reaction mass changes from 3 km/s forwards to 7 km/s rearwards. Thus, although the energy required is 50 MJ per kg reaction mass, only 20 MJ is used for the increase in speed of the reaction mass. The remaining 30 MJ is the increase of the kinetic energy of the rocket and payload.
In general:

  
    
      
        d
        
          (
          
            
              
                1
                2
              
            
            
              v
              
                2
              
            
          
          )
        
        =
        v
        d
        v
        =
        v
        
          v
          
            e
          
        
        
          
            
              d
              m
            
            m
          
        
        =
        
          
            1
            2
          
        
        
          [
          
            
              v
              
                e
              
              
                2
              
            
            −
            
              
                (
                
                  v
                  −
                  
                    v
                    
                      e
                    
                  
                
                )
              
              
                2
              
            
            +
            
              v
              
                2
              
            
          
          ]
        
        
          
            
              d
              m
            
            m
          
        
      
    
    {\displaystyle d\left({\frac {1}{2}}v^{2}\right)=vdv=vv_{\text{e}}{\frac {dm}{m}}={\frac {1}{2}}\left[v_{\text{e}}^{2}-\left(v-v_{\text{e}}\right)^{2}+v^{2}\right]{\frac {dm}{m}}}
  Thus the specific energy gain of the rocket in any small time interval is the energy gain of the rocket including the remaining fuel, divided by its mass, where the energy gain is equal to the energy produced by the fuel minus the energy gain of the reaction mass. The larger the speed of the rocket, the smaller the energy gain of the reaction mass; if the rocket speed is more than half of the exhaust speed the reaction mass even loses energy on being expelled, to the benefit of the energy gain of the rocket; the larger the speed of the rocket, the larger the energy loss of the reaction mass.
We have

  
    
      
        Δ
        ϵ
        =
        ∫
        v
        
        d
        (
        Δ
        v
        )
      
    
    {\displaystyle \Delta \epsilon =\int v\,d(\Delta v)}
  where 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   is the specific energy of the rocket (potential plus kinetic energy) and 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   is a separate variable, not just the change in 
  
    
      
        v
      
    
    {\displaystyle v}
  . In the case of using the rocket for deceleration; i.e., expelling reaction mass in the direction of the velocity, 
  
    
      
        v
      
    
    {\displaystyle v}
   should be taken negative.
The formula is for the ideal case again, with no energy lost on heat, etc. The latter causes a reduction of thrust, so it is a disadvantage even when the objective is to lose energy (deceleration).
If the energy is produced by the mass itself, as in a chemical rocket, the fuel value has to be 
  
    
      
        
          
            
              v
              
                e
              
              
                2
              
            
            
              /
            
            2
          
        
      
    
    {\displaystyle \scriptstyle {v_{\text{e}}^{2}/2}}
  , where for the fuel value also the mass of the oxidizer has to be taken into account. A typical value is 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 4.5 km/s, corresponding to a fuel value of 10.1 MJ/kg. The actual fuel value is higher, but much of the energy is lost as waste heat in the exhaust that the nozzle was unable to extract.
The required energy 
  
    
      
        E
      
    
    {\displaystyle E}
   is

  
    
      
        E
        =
        
          
            1
            2
          
        
        
          m
          
            1
          
        
        
          (
          
            
              e
              
                
                  
                    Δ
                    v
                  
                  
                    v
                    
                      e
                    
                  
                
              
            
            −
            1
          
          )
        
        
          v
          
            e
          
          
            2
          
        
      
    
    {\displaystyle E={\frac {1}{2}}m_{1}\left(e^{\frac {\Delta v}{v_{\text{e}}}}-1\right)v_{\text{e}}^{2}}
  Conclusions:

for 
  
    
      
        Δ
        v
        ≪
        
          v
          
            e
          
        
      
    
    {\displaystyle \Delta v\ll v_{e}}
   we have 
  
    
      
        E
        ≈
        
          
            1
            2
          
        
        
          m
          
            1
          
        
        
          v
          
            e
          
        
        Δ
        v
      
    
    {\displaystyle E\approx {\frac {1}{2}}m_{1}v_{\text{e}}\Delta v}
  
for a given 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  , the minimum energy is needed if 
  
    
      
        
          v
          
            e
          
        
        =
        0.6275
        Δ
        v
      
    
    {\displaystyle v_{\text{e}}=0.6275\Delta v}
  , requiring an energy of
  
    
      
        E
        =
        0.772
        
          m
          
            1
          
        
        (
        Δ
        v
        
          )
          
            2
          
        
      
    
    {\displaystyle E=0.772m_{1}(\Delta v)^{2}}
  .
In the case of acceleration in a fixed direction, and starting from zero speed, and in the absence of other forces, this is 54.4% more than just the final kinetic energy of the payload. In this optimal case the initial mass is 4.92 times the final mass.These results apply for a fixed exhaust speed.
Due to the Oberth effect and starting from a nonzero speed, the required potential energy needed from the propellant may be less than the increase in energy in the vehicle and payload. This can be the case when the reaction mass has a lower speed after being expelled than before – rockets are able to liberate some or all of the initial kinetic energy of the propellant.
Also, for a given objective such as moving from one orbit to another, the required 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   may depend greatly on the rate at which the engine can produce 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   and maneuvers may even be impossible if that rate is too low. For example, a launch to Low Earth orbit (LEO) normally requires a 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of ca. 9.5 km/s (mostly for the speed to be acquired), but if the engine could produce 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   at a rate of only slightly more than g, it would be a slow launch requiring altogether a very large 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   (think of hovering without making any progress in speed or altitude, it would cost a 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   of 9.8 m/s each second). If the possible rate is only 
  
    
      
        g
      
    
    {\displaystyle g}
   or less, the maneuver can not be carried out at all with this engine.
The power is given by

  
    
      
        P
        =
        
          
            1
            2
          
        
        m
        a
        
          v
          
            e
          
        
        =
        
          
            1
            2
          
        
        F
        
          v
          
            e
          
        
      
    
    {\displaystyle P={\frac {1}{2}}mav_{\text{e}}={\frac {1}{2}}Fv_{\text{e}}}
  where 
  
    
      
        F
      
    
    {\displaystyle F}
   is the thrust and 
  
    
      
        a
      
    
    {\displaystyle a}
   the acceleration due to it. Thus the theoretically possible thrust per unit power is 2 divided by the specific impulse in m/s. The thrust efficiency is the actual thrust as percentage of this.
If, e.g., solar power is used, this restricts 
  
    
      
        a
      
    
    {\displaystyle a}
  ; in the case of a large 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   the possible acceleration is inversely proportional to it, hence the time to reach a required delta-v is proportional to 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
  ; with 100% efficiency:

for 
  
    
      
        Δ
        v
        ≪
        
          v
          
            e
          
        
      
    
    {\displaystyle \Delta v\ll v_{\text{e}}}
   we have 
  
    
      
        t
        ≈
        
          
            
              m
              
                v
                
                  e
                
              
              Δ
              v
            
            
              2
              P
            
          
        
      
    
    {\displaystyle t\approx {\frac {mv_{\text{e}}\Delta v}{2P}}}
  Examples:

power, 1000 W; mass, 100 kg; 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   = 5 km/s, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 16 km/s, takes 1.5 months.
power, 1000 W; mass, 100 kg; 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   = 5 km/s, 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   = 50 km/s, takes 5 months.Thus 
  
    
      
        
          v
          
            e
          
        
      
    
    {\displaystyle v_{\text{e}}}
   should not be too large.


=== Power to thrust ratio ===
The power to thrust ratio is simply:

  
    
      
        
          
            P
            F
          
        
        =
        
          
            
              
                
                  1
                  2
                
              
              
                
                  
                    
                      m
                      ˙
                    
                  
                
                
                  v
                  
                    2
                  
                
              
            
            
              
                
                  
                    m
                    ˙
                  
                
              
              v
            
          
        
        =
        
          
            1
            2
          
        
        v
      
    
    {\displaystyle {\frac {P}{F}}={\frac {{\frac {1}{2}}{{\dot {m}}v^{2}}}{{\dot {m}}v}}={\frac {1}{2}}v}
  Thus for any vehicle power P, the thrust that may be provided is:

  
    
      
        F
        =
        
          
            P
            
              
                
                  1
                  2
                
              
              v
            
          
        
        =
        
          
            
              2
              P
            
            v
          
        
      
    
    {\displaystyle F={\frac {P}{{\frac {1}{2}}v}}={\frac {2P}{v}}}
  


=== Example ===
Suppose a 10,000 kg space probe will be sent to Mars. The required 
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
   from LEO is approximately 3000 m/s, using a Hohmann transfer orbit. For the sake of argument, assume the following thrusters are options to be used:

Observe that the more fuel-efficient engines can use far less fuel; their mass is almost negligible (relative to the mass of the payload and the engine itself) for some of the engines. However, these require a large total amount of energy. For Earth launch, engines require a thrust to weight ratio of more than one. To do this with the ion or more theoretical electrical drives, the engine would have to be supplied with one to several gigawatts of power, equivalent to a major metropolitan generating station. From the table it can be seen that this is clearly impractical with current power sources.
Alternative approaches include some forms of laser propulsion, where the reaction mass does not provide the energy required to accelerate it, with the energy instead being provided from an external laser or other beam-powered propulsion system. Small models of some of these concepts have flown, although the engineering problems are complex and the ground-based power systems are not a solved problem.
Instead, a much smaller, less powerful generator may be included which will take much longer to generate the total energy needed. This lower power is only sufficient to accelerate a tiny amount of fuel per second, and would be insufficient for launching from Earth. However, over long periods in orbit where there is no friction, the velocity will be finally achieved. For example, it took the SMART-1 more than a year to reach the Moon, whereas with a chemical rocket it takes a few days. Because the ion drive needs much less fuel, the total launched mass is usually lower, which typically results in a lower overall cost, but the journey takes longer.
Mission planning therefore frequently involves adjusting and choosing the propulsion system so as to minimise the total cost of the project, and can involve trading off launch costs and mission duration against payload fraction.


== Types of reaction engines ==
Rocket-like
Rocket engine
Ion thruster
Airbreathing
Turbojet
Turbofan
Pulsejet
Ramjet
Scramjet
Liquid
Pump-jet
Rotary
Aeolipile
Solid exhaust
Mass driver


== See also ==
Internal combustion engine
Jet force
Jet propulsion
List of plasma physics articles
Thruster (disambiguation)


== Notes ==


== References ==


== External links ==
Popular Science May 1945","pandas(index=69, _1=69, text='a reaction engine is an engine or motor that produces thrust by expelling reaction mass, in accordance with newton\'s third law of motion. this law of motion is most commonly paraphrased as: ""for every action force there is an equal, but opposite, reaction force."" examples include jet engines, rocket engines, pump-jet, and more uncommon variations such as hall effect thrusters, ion drives, mass drivers, and nuclear pulse propulsion.   == energy use == for all reaction engines that carry on-board propellant (such as rocket engines and electric propulsion drives) some energy must go into accelerating the reaction mass. every engine wastes some energy, but even assuming 100% efficiency, the engine needs energy amounting to           1 2       m  v  e   2      from leo is approximately 3000 m/s, using a hohmann transfer orbit. for the sake of argument, assume the following thrusters are options to be used:  observe that the more fuel-efficient engines can use far less fuel; their mass is almost negligible (relative to the mass of the payload and the engine itself) for some of the engines. however, these require a large total amount of energy. for earth launch, engines require a thrust to weight ratio of more than one. to do this with the ion or more theoretical electrical drives, the engine would have to be supplied with one to several gigawatts of power, equivalent to a major metropolitan generating station. from the table it can be seen that this is clearly impractical with current power sources. alternative approaches include some forms of laser propulsion, where the reaction mass does not provide the energy required to accelerate it, with the energy instead being provided from an external laser or other beam-powered propulsion system. small models of some of these concepts have flown, although the engineering problems are complex and the ground-based power systems are not a solved problem. instead, a much smaller, less powerful generator may be included which will take much longer to generate the total energy needed. this lower power is only sufficient to accelerate a tiny amount of fuel per second, and would be insufficient for launching from earth. however, over long periods in orbit where there is no friction, the velocity will be finally achieved. for example, it took the smart-1 more than a year to reach the moon, whereas with a chemical rocket it takes a few days. because the ion drive needs much less fuel, the total launched mass is usually lower, which typically results in a lower overall cost, but the journey takes longer. mission planning therefore frequently involves adjusting and choosing the propulsion system so as to minimise the total cost of the project, and can involve trading off launch costs and mission duration against payload fraction.   == types of reaction engines == rocket-like rocket engine ion thruster airbreathing turbojet turbofan pulsejet ramjet scramjet liquid pump-jet rotary aeolipile solid exhaust mass driver   == see also == internal combustion engine jet force jet propulsion list of plasma physics articles thruster (disambiguation)   == notes ==   == references ==   == external links == popular science may 1945')"
70,"Redux  is the generic name of a family of phenol–formaldehyde/polyvinyl–formal adhesives developed by Aero Research Limited (ARL) at Duxford, UK, in the 1940s, subsequently produced by Ciba (ARL). The brand name is now also used for a range of epoxy and bismaleimide adhesives manufactured by Hexcel. The name is a contraction of REsearch at DUXford.


== History ==
Devised at ARL by Dr. Norman de Bruyne and George Newell in 1941 for use in the aircraft industry, the adhesive is used for the bonding of metal-to-metal and metal-to-wood structures. The adhesive system comprises a liquid phenolic resin and a PVF (PolyVinylFormal) thermoplastic powder.
The first formulation available was Redux Liquid E/Formvar, comprising a phenolic liquid (Redux Liquid E) and a PVF powder (Formvar), and after its initial non-aviation related application of bonding clutch plates on Churchill and Cromwell tanks, it was used by de Havilland from 1943 to the early 1960s, on, among other aircraft, the Hornet, the Comet and the derived Nimrod, and the Dove, Heron and Trident. It was also used by Vickers on the Viking and by Chance Vought on the F7U Cutlass.
Typically, Redux would be used to affix stiffening stringers and doublers to wing and fuselage panels, the resulting panel being both stronger and lighter than a riveted structure. In the case of the Hornet it was used to join the aluminium lower-wing skin to the wooden upper wing structure, and in the fabrication of the aluminium/wood main wing spar, both forms of composite construction made possible by the advent of Redux.
After initially supplying de Havilland only, ARL subsequently produced a refined form of Redux Liquid E/Formvar using a new liquid component known as Redux Liquid K6, and a finer-grade (smaller particle-size) PVF powder, and this was later made generally available to the wider aircraft industry as Redux Liquid 775/Powder 775, so-named because it was sold for aircraft use to specification DTD 775*. Available for general non-aerospace use it was called Redux Liquid K6/Powder C.
Redux Liquid 775/Powder 775 was joined in 1954 by the subsequent Redux Film 775 system, used from 1962 by de Havilland (later Hawker Siddeley and subsequently British Aerospace) on the DH.125 and DH.146. Other users included Bristol (on the Britannia), SAAB (on the Lansen & Draken), Fokker (on the F.27), Sud Aviation (on the Alouette II/III), Breguet and Fairchild, the film-form having the advantage of greater gap-filling ability with no loss of strength over Redux Liquid 775/Powder 775, allowing for wider tolerances in component-fit, as well as easier handling and use and controlled ratios of the liquid/powder components.   
Other Redux adhesives available included ""Redux 64"", a solution of the phenolic liquid and PVF powder, used worldwide for bonding linings to brake shoes, pads and clutches.
The Redux range was subsequently expanded to include the current range of adhesives, both in single and two part paste systems and film forms, for both aerospace and industrial uses.
* DTD = Directorate of Technical Development


== Usage ==
To use Redux in its liquid/powder form, a thin film of the phenolic liquid is applied to both mating surfaces and then dusted with or dipped in the PVF powder to give an approximate ratio by weight of 1 part liquid to 2 parts powder. The coated joints are then allowed to stand for not less than 30 minutes and not more than 72 hours before the components are brought together under elevated pressure and temperature. The curing process is by condensation and a typical figure for Redux Liquid 775/Powder 775 is 30 minutes at 145 °C (293 °F) under a pressure of 100 lbf/in2 (690 kPa). This is not critical and variations in curing-time and/or temperature may be used to increase shear and creep strength at temperatures above 60 °C (140 °F). Extending the curing cycle gives benefits in fatigue strength at some cost in the room-temperature peel strength, the practical limit for aluminium alloys being approx 170 °C (338 °F) for one hour, due to the possibility of affecting the alloy's mechanical properties.


== Performance (typical) Redux 775 ==
Lap shear strength at ambient temperature = 34.0 MPa (4,930 lbf/in2)
Young's Modulus (E) = 3.35 GPa (486,000 lbf/in2)
Shear modulus =  1.20 GPa (174,000 lbf/in2)Strength of bonds to materials other than aluminium:
Tensile shear of  0.5 inch (12.7 mm) lap joints at room temperature:

Bright mild steel of thickness 0.0625 in (1.6 mm) - mean failing stress = 4,980 lbf/in2 (33.7 MPa, 3.50 kgf/mm2)
Stainless steel of thickness 0.048 in (1.2 mm) - mean failing stress = 5,600 lbf/in2 (38.6 MPa, 3.94 kgf/mm2)
Magnesium alloy1 of thickness 0.063 in (1.6 mm) - mean failing stress = 3,210 lbf/in2 (22.1 MPa, 2.26 kgf/mm2)
Commercially-pure titanium2 of thickness 0.050 in (1.3 mm) - mean failing stress = 4,070 lbf/in2 (28.1 MPa, 2.86 kgf/mm2)1 = HK31A-H24
2 = ICI Titanium 130


== See also ==
Araldite
Aerolite
Tego film


== References ==

Project 3 – Environmental Durability of Adhesive Bonds – Report No. 9 – Forensic Studies of Adhesive Joints – Part 2 Bonded Aircraft Structure by A. Beevers. September 1995. [1]
Usage on the de Havilland Comet
The de Bruyne Medal
Bonding with Redux.(reprinted from The Aeroplane – Sept 1946)
Hexcel Redux film adhesive – 50th anniversary Press Release
Hexcel Redux 775 Product Data


== External links ==
A 1957 Aero Research advert for Redux
""Joint Economics"" a  short article on Redux by N. A de Bruyne in a 1953 issue of Flight","pandas(index=70, _1=70, text='redux  is the generic name of a family of phenol–formaldehyde/polyvinyl–formal adhesives developed by aero research limited (arl) at duxford, uk, in the 1940s, subsequently produced by ciba (arl). the brand name is now also used for a range of epoxy and bismaleimide adhesives manufactured by hexcel. the name is a contraction of research at duxford.   == history == devised at arl by dr. norman de bruyne and george newell in 1941 for use in the aircraft industry, the adhesive is used for the bonding of metal-to-metal and metal-to-wood structures. the adhesive system comprises a liquid phenolic resin and a pvf (polyvinylformal) thermoplastic powder. the first formulation available was redux liquid e/formvar, comprising a phenolic liquid (redux liquid e) and a pvf powder (formvar), and after its initial non-aviation related application of bonding clutch plates on churchill and cromwell tanks, it was used by de havilland from 1943 to the early 1960s, on, among other aircraft, the hornet, the comet and the derived nimrod, and the dove, heron and trident. it was also used by vickers on the viking and by chance vought on the f7u cutlass. typically, redux would be used to affix stiffening stringers and doublers to wing and fuselage panels, the resulting panel being both stronger and lighter than a riveted structure. in the case of the hornet it was used to join the aluminium lower-wing skin to the wooden upper wing structure, and in the fabrication of the aluminium/wood main wing spar, both forms of composite construction made possible by the advent of redux. after initially supplying de havilland only, arl subsequently produced a refined form of redux liquid e/formvar using a new liquid component known as redux liquid k6, and a finer-grade (smaller particle-size) pvf powder, and this was later made generally available to the wider aircraft industry as redux liquid 775/powder 775, so-named because it was sold for aircraft use to specification dtd 775*. available for general non-aerospace use it was called redux liquid k6/powder c. redux liquid 775/powder 775 was joined in 1954 by the subsequent redux film 775 system, used from 1962 by de havilland (later hawker siddeley and subsequently british aerospace) on the dh.125 and dh.146. other users included bristol (on the britannia), saab (on the lansen & draken), fokker (on the f.27), sud aviation (on the alouette ii/iii), breguet and fairchild, the film-form having the advantage of greater gap-filling ability with no loss of strength over redux liquid 775/powder 775, allowing for wider tolerances in component-fit, as well as easier handling and use and controlled ratios of the liquid/powder components. other redux adhesives available included ""redux 64"", a solution of the phenolic liquid and pvf powder, used worldwide for bonding linings to brake shoes, pads and clutches. the redux range was subsequently expanded to include the current range of adhesives, both in single and two part paste systems and film forms, for both aerospace and industrial uses. * dtd = directorate of technical development   == usage == to use redux in its liquid/powder form, a thin film of the phenolic liquid is applied to both mating surfaces and then dusted with or dipped in the pvf powder to give an approximate ratio by weight of 1 part liquid to 2 parts powder. the coated joints are then allowed to stand for not less than 30 minutes and not more than 72 hours before the components are brought together under elevated pressure and temperature. the curing process is by condensation and a typical figure for redux liquid 775/powder 775 is 30 minutes at 145 °c (293 °f) under a pressure of 100 lbf/in2 (690 kpa). this is not critical and variations in curing-time and/or temperature may be used to increase shear and creep strength at temperatures above 60 °c (140 °f). extending the curing cycle gives benefits in fatigue strength at some cost in the room-temperature peel strength, the practical limit for aluminium alloys being approx 170 °c (338 °f) for one hour, due to the possibility of affecting the alloy\'s mechanical properties.   == performance (typical) redux 775 == lap shear strength at ambient temperature = 34.0 mpa (4,930 lbf/in2) young\'s modulus (e) = 3.35 gpa (486,000 lbf/in2) shear modulus =  1.20 gpa (174,000 lbf/in2)strength of bonds to materials other than aluminium: tensile shear of  0.5 inch (12.7 mm) lap joints at room temperature:  bright mild steel of thickness 0.0625 in (1.6 mm) - mean failing stress = 4,980 lbf/in2 (33.7 mpa, 3.50 kgf/mm2) stainless steel of thickness 0.048 in (1.2 mm) - mean failing stress = 5,600 lbf/in2 (38.6 mpa, 3.94 kgf/mm2) magnesium alloy1 of thickness 0.063 in (1.6 mm) - mean failing stress = 3,210 lbf/in2 (22.1 mpa, 2.26 kgf/mm2) commercially-pure titanium2 of thickness 0.050 in (1.3 mm) - mean failing stress = 4,070 lbf/in2 (28.1 mpa, 2.86 kgf/mm2)1 = hk31a-h24 2 = ici titanium 130   == see also == araldite aerolite tego film   == references ==  project 3 – environmental durability of adhesive bonds – report no. 9 – forensic studies of adhesive joints – part 2 bonded aircraft structure by a. beevers. september 1995. [1] usage on the de havilland comet the de bruyne medal bonding with redux.(reprinted from the aeroplane – sept 1946) hexcel redux film adhesive – 50th anniversary press release hexcel redux 775 product data   == external links == a 1957 aero research advert for redux ""joint economics"" a  short article on redux by n. a de bruyne in a 1953 issue of flight')"
71,"In fluid dynamics, disk loading or disc loading is the average pressure change across an actuator disk, such as an airscrew. Airscrews with a relatively low disk loading are typically called rotors, including helicopter main rotors and tail rotors; propellers typically have a higher disk loading. 
The V-22 Osprey tiltrotor aircraft has a high disk loading relative to a helicopter in the hover mode, but a relatively low disk loading in fixed-wing mode compared to a turboprop aircraft.


== Rotors ==
Disc loading of a hovering helicopter 
is the ratio of its weight to the
total main rotor disc area. It is determined by dividing
the total helicopter weight by the rotor disc area,
which is the area swept by the blades of a rotor. Disc
area can be found by using the span of one rotor blade
as the radius of a circle and then determining the area
the blades encompass during a complete rotation. When a helicopter is being maneuvered, its disc loading changes.
The higher the loading, the more power needed to
maintain rotor speed. A low disc loading is a direct indicator of high lift thrust efficiency.Increasing the weight of a helicopter increases disk loading. For a given weight, a helicopter with shorter rotors will have higher disk loading, and will require more engine power to hover. A low disk loading improves autorotation performance in rotorcraft. Typically, an autogyro (or gyroplane) has a lower rotor disc loading than a helicopter, which provides a slower rate of descent in autorotation.


== Propellers ==
In reciprocating and propeller engines, disk loading can be defined as the ratio between propeller-induced velocity and freestream velocity. Lower disk loading will increase efficiency, so it is generally desirable to have larger propellers from an efficiency standpoint. Maximum efficiency is reduced as disk loading is increased due to the rotating slipstream; using contra-rotating propellers can alleviate this problem allowing high maximum efficiency even at relatively high disc loadings.The Airbus A400M fixed-wing aircraft will have a very high disk loading on its propellers.


== Theory ==
The momentum theory or disk actuator theory describes a mathematical model of an ideal actuator disk, developed by W.J.M. Rankine (1865), Alfred George Greenhill (1888) and R.E. Froude (1889). The helicopter rotor is modeled as an infinitely thin disc with an infinite number of blades that induce a constant pressure jump over the disk area and along the axis of rotation. For a helicopter that is hovering, the aerodynamic force is vertical and exactly balances the helicopter weight, with no lateral force.
The upward action on the helicopter results in a downward reaction on the air flowing through the rotor. The downward reaction produces a downward velocity on the air, increasing its kinetic energy. This energy transfer from the rotor to the air is the induced power loss of the rotary wing, which is analogous to the lift-induced drag of a fixed-wing aircraft.
Conservation of linear momentum relates the induced velocity downstream in the far wake field to the rotor thrust per unit of mass flow. Conservation of energy considers these parameters as well as the induced velocity at the rotor disk. Conservation of mass relates the mass flow to the induced velocity. The momentum theory applied to a helicopter gives the relationship between induced power loss and rotor thrust, which can be used to analyze the performance of the aircraft. Viscosity and compressibility of the air, frictional losses, and rotation of the slipstream in the wake are not considered.


=== Momentum theory ===
For an actuator disk of area 
  
    
      
        A
      
    
    {\displaystyle A}
  , with uniform induced velocity 
  
    
      
        v
      
    
    {\displaystyle v}
   at the rotor disk, and with 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   as the density of air, the mass flow rate 
  
    
      
        
          
          
            
              
                m
                ˙
              
            
          
        
      
    
    {\displaystyle ^{\dot {m}}}
   through the disk area is:

  
    
      
        
          
            
              m
              ˙
            
          
        
        =
        ρ
        
        A
        
        v
        .
      
    
    {\displaystyle {\dot {m}}=\rho \,A\,v.}
  By conservation of mass, the mass flow rate is constant across the slipstream both upstream and downstream of the disk (regardless of velocity). Since the flow far upstream of a helicopter in a level hover is at rest, the starting velocity, momentum, and energy are zero. If the homogeneous slipstream far downstream of the disk has velocity 
  
    
      
        w
      
    
    {\displaystyle w}
  , by conservation of momentum the total thrust 
  
    
      
        T
      
    
    {\displaystyle T}
   developed over the disk is equal to the rate of change of momentum, which assuming zero starting velocity is:

  
    
      
        T
        =
        
          
            
              m
              ˙
            
          
        
        
        w
        .
      
    
    {\displaystyle T={\dot {m}}\,w.}
  By conservation of energy, the work done by the rotor must equal the energy change in the slipstream:

  
    
      
        T
        
        v
        =
        
          
            
              1
              2
            
          
        
        
        
          
            
              m
              ˙
            
          
        
        
        
          
            w
            
              2
            
          
        
        .
      
    
    {\displaystyle T\,v={\tfrac {1}{2}}\,{\dot {m}}\,{w^{2}}.}
  Substituting for 
  
    
      
        T
      
    
    {\displaystyle T}
   and eliminating terms, we get:

  
    
      
        v
        =
        
          
            
              1
              2
            
          
        
        
        w
        .
      
    
    {\displaystyle v={\tfrac {1}{2}}\,w.}
  So the velocity of the slipstream far downstream of the disk is twice the velocity at the disk, which is the same result for an elliptically loaded fixed wing predicted by lifting-line theory.


=== Bernoulli's principle ===
To compute the disk loading using Bernoulli's principle, we assume the pressure in the slipstream far downstream is equal to the starting pressure 
  
    
      
        
          p
          
            0
          
        
      
    
    {\displaystyle p_{0}}
  , which is equal to the atmospheric pressure. From the starting point to the disk we have:

  
    
      
        
          p
          
            0
          
        
        =
        
        
          p
          
            1
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        .
      
    
    {\displaystyle p_{0}=\,p_{1}+\ {\tfrac {1}{2}}\,\rho \,v^{2}.}
  Between the disk and the distant wake, we have:

  
    
      
        
          p
          
            2
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
         
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
        .
      
    
    {\displaystyle p_{2}+\ {\tfrac {1}{2}}\,\rho \,v^{2}=\,p_{0}+\ {\tfrac {1}{2}}\,\rho \,w^{2}.}
  Combining equations, the disk loading 
  
    
      
        T
        
          /
        
        
        A
      
    
    {\displaystyle T/\,A}
   is:

  
    
      
        
          
            T
            A
          
        
        =
        
          p
          
            2
          
        
        −
        
        
          p
          
            1
          
        
        =
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
      
    
    {\displaystyle {\frac {T}{A}}=p_{2}-\,p_{1}={\tfrac {1}{2}}\,\rho \,w^{2}}
  The total pressure in the distant wake is:

  
    
      
        
          p
          
            0
          
        
        +
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          w
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}+{\tfrac {1}{2}}\,\rho \,w^{2}=\,p_{0}+{\frac {T}{A}}.}
  So the pressure change across the disk is equal to the disk loading. Above the disk the pressure change is:

  
    
      
        
          p
          
            0
          
        
        −
        
          
            
              1
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        −
        
        
          
            
              1
              4
            
          
        
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}-{\tfrac {1}{2}}\,\rho \,v^{2}=\,p_{0}-\,{\tfrac {1}{4}}{\frac {T}{A}}.}
  Below the disk, the pressure change is:

  
    
      
        
          p
          
            0
          
        
        +
        
          
            
              3
              2
            
          
        
        
        ρ
        
        
          v
          
            2
          
        
        =
        
        
          p
          
            0
          
        
        +
        
        
          
            
              3
              4
            
          
        
        
          
            T
            A
          
        
        .
      
    
    {\displaystyle p_{0}+{\tfrac {3}{2}}\,\rho \,v^{2}=\,p_{0}+\,{\tfrac {3}{4}}{\frac {T}{A}}.}
  The pressure along the slipstream is always falling downstream, except for the positive pressure jump across the disk.


=== Power required ===
From the momentum theory, thrust is:

  
    
      
        T
        =
        
          
            
              m
              ˙
            
          
        
        
        w
        =
        
          
            
              m
              ˙
            
          
        
        
        (
        2
        v
        )
        =
        2
        ρ
        
        A
        
        
          v
          
            2
          
        
        .
      
    
    {\displaystyle T={\dot {m}}\,w={\dot {m}}\,(2v)=2\rho \,A\,v^{2}.}
  The induced velocity is:

  
    
      
        v
        =
        
          
            
              
                T
                A
              
            
            ⋅
            
              
                1
                
                  2
                  ρ
                
              
            
          
        
        .
      
    
    {\displaystyle v={\sqrt {{\frac {T}{A}}\cdot {\frac {1}{2\rho }}}}.}
  Where 
  
    
      
        T
        
          /
        
        A
      
    
    {\displaystyle T/A}
   is the disk loading as before, and the power 
  
    
      
        P
      
    
    {\displaystyle P}
   required in hover (in the ideal case) is:

  
    
      
        P
        =
        T
        v
        =
        T
        
          
            
              
                T
                A
              
            
            ⋅
            
              
                1
                
                  2
                  ρ
                
              
            
          
        
        .
      
    
    {\displaystyle P=Tv=T{\sqrt {{\frac {T}{A}}\cdot {\frac {1}{2\rho }}}}.}
  Therefore, the induced velocity can be expressed as:

  
    
      
        v
        =
        
          
            P
            T
          
        
        =
        
          
            [
            
              
                T
                P
              
            
            ]
          
          
            −
            1
          
        
        .
      
    
    {\displaystyle v={\frac {P}{T}}=\left[{\frac {T}{P}}\right]^{-1}.}
  So, the induced velocity is inversely proportional to the power loading 
  
    
      
        T
        
          /
        
        P
      
    
    {\displaystyle T/P}
  .


== Examples ==


== See also ==
Wing loading


== References ==

 This article incorporates public domain material from the Federal Aviation Administration document: ""Rotorcraft Flying Handbook"" (PDF).","pandas(index=71, _1=71, text='in fluid dynamics, disk loading or disc loading is the average pressure change across an actuator disk, such as an airscrew. airscrews with a relatively low disk loading are typically called rotors, including helicopter main rotors and tail rotors; propellers typically have a higher disk loading. the v-22 osprey tiltrotor aircraft has a high disk loading relative to a helicopter in the hover mode, but a relatively low disk loading in fixed-wing mode compared to a turboprop aircraft.   == rotors == disc loading of a hovering helicopter is the ratio of its weight to the total main rotor disc area. it is determined by dividing the total helicopter weight by the rotor disc area, which is the area swept by the blades of a rotor. disc area can be found by using the span of one rotor blade as the radius of a circle and then determining the area the blades encompass during a complete rotation. when a helicopter is being maneuvered, its disc loading changes. the higher the loading, the more power needed to maintain rotor speed. a low disc loading is a direct indicator of high lift thrust efficiency.increasing the weight of a helicopter increases disk loading. for a given weight, a helicopter with shorter rotors will have higher disk loading, and will require more engine power to hover. a low disk loading improves autorotation performance in rotorcraft. typically, an autogyro (or gyroplane) has a lower rotor disc loading than a helicopter, which provides a slower rate of descent in autorotation.   == propellers == in reciprocating and propeller engines, disk loading can be defined as the ratio between propeller-induced velocity and freestream velocity. lower disk loading will increase efficiency, so it is generally desirable to have larger propellers from an efficiency standpoint. maximum efficiency is reduced as disk loading is increased due to the rotating slipstream; using contra-rotating propellers can alleviate this problem allowing high maximum efficiency even at relatively high disc loadings.the airbus a400m fixed-wing aircraft will have a very high disk loading on its propellers.   == theory == the momentum theory or disk actuator theory describes a mathematical model of an ideal actuator disk, developed by w.j.m. rankine (1865), alfred george greenhill (1888) and r.e. froude (1889). the helicopter rotor is modeled as an infinitely thin disc with an infinite number of blades that induce a constant pressure jump over the disk area and along the axis of rotation. for a helicopter that is hovering, the aerodynamic force is vertical and exactly balances the helicopter weight, with no lateral force. the upward action on the helicopter results in a downward reaction on the air flowing through the rotor. the downward reaction produces a downward velocity on the air, increasing its kinetic energy. this energy transfer from the rotor to the air is the induced power loss of the rotary wing, which is analogous to the lift-induced drag of a fixed-wing aircraft. conservation of linear momentum relates the induced velocity downstream in the far wake field to the rotor thrust per unit of mass flow. conservation of energy considers these parameters as well as the induced velocity at the rotor disk. conservation of mass relates the mass flow to the induced velocity. the momentum theory applied to a helicopter gives the relationship between induced power loss and rotor thrust, which can be used to analyze the performance of the aircraft. viscosity and compressibility of the air, frictional losses, and rotation of the slipstream in the wake are not considered. for an actuator disk of area    a    .   == examples ==   == see also == wing loading   == references ==  this article incorporates public domain material from the federal aviation administration document: ""rotorcraft flying handbook"" (pdf).')"
72,"A Bettsometer is a fabric degradation tester commonly used to measure or test the integrity of fabric coverings (and associated stitching) on aircraft and their wings.The Bettsometer comprises a pen-like instrument (which functions much like a spring balance) and a smooth round needle or pin. The needle is inserted into the fabric and then the instrument is pulled to exert a specific force on the fabric in order to test. A visual inspection is made to check for any rips or tears at the needle insertion point.
The Bettsometer test is often a requirement for the annual 'permit' renewal and is usually carried out by an aircraft inspector who will know the requirements of the test (i.e. the areas of sail and stitching to be tested and the force to be exerted).


== References ==","pandas(index=72, _1=72, text=""a bettsometer is a fabric degradation tester commonly used to measure or test the integrity of fabric coverings (and associated stitching) on aircraft and their wings.the bettsometer comprises a pen-like instrument (which functions much like a spring balance) and a smooth round needle or pin. the needle is inserted into the fabric and then the instrument is pulled to exert a specific force on the fabric in order to test. a visual inspection is made to check for any rips or tears at the needle insertion point. the bettsometer test is often a requirement for the annual 'permit' renewal and is usually carried out by an aircraft inspector who will know the requirements of the test (i.e. the areas of sail and stitching to be tested and the force to be exerted).   == references =="")"
73,"In aerospace engineering, an aircraft's fuel fraction, fuel weight fraction, or a spacecraft's propellant fraction, is the weight of the fuel or propellant divided by the gross take-off weight of the craft (including propellant):

  
    
      
         
        ζ
        =
        
          
            
              Δ
              W
            
            
              W
              
                1
              
            
          
        
      
    
    {\displaystyle \ \zeta ={\frac {\Delta W}{W_{1}}}}
  The fractional result of this mathematical division is often expressed as a percent. For aircraft with external drop tanks, the term internal fuel fraction is used to exclude the weight of external tanks and fuel. 
Fuel fraction is a key parameter in determining an aircraft's range, the distance it can fly without refueling.
Breguet’s aircraft range equation describes the relationship of range with airspeed, lift-to-drag ratio, specific fuel consumption, and the part of the total fuel fraction available for cruise, also known as the cruise fuel fraction, or  cruise fuel weight fraction.


== Fighter aircraft ==
At today’s state of the art for jet fighter aircraft, fuel fractions of 29 percent and below typically yield subcruisers; 33 percent provides a quasi–supercruiser; and 35 percent and above are needed for useful supercruising missions. The U.S. F-22 Raptor’s fuel fraction is 29 percent, Eurofighter is 31 percent, both similar to those of the subcruising F-4 Phantom II, F-15 Eagle and the Russian Mikoyan MiG-29 ""Fulcrum"". The Russian supersonic interceptor, the Mikoyan MiG-31 ""Foxhound"", has a fuel fraction of over 45 percent. The Panavia Tornado had a relatively low internal fuel fraction of 26 percent, and frequently carried drop tanks.


== Airliners ==
Airliners have a fuel fraction of less than half their takeoff weight, between 26% for medium-haul to 45% for long-haul:

The Concorde supersonic transport had a fuel fraction of 51%.


== General aviation ==
The Rutan Voyager took off on its 1986 around-the-world flight at 72 percent, the highest figure ever at the time. Steve Fossett's Virgin Atlantic GlobalFlyer could attain a fuel fraction of nearly 85 percent, meaning that it carried more than five times its empty weight in fuel.


== See also ==
Mass ratio


== References ==","pandas(index=73, _1=73, text='in aerospace engineering, an aircraft\'s fuel fraction, fuel weight fraction, or a spacecraft\'s propellant fraction, is the weight of the fuel or propellant divided by the gross take-off weight of the craft (including propellant):      ζ =    δ w   w  1        the fractional result of this mathematical division is often expressed as a percent. for aircraft with external drop tanks, the term internal fuel fraction is used to exclude the weight of external tanks and fuel. fuel fraction is a key parameter in determining an aircraft\'s range, the distance it can fly without refueling. breguet’s aircraft range equation describes the relationship of range with airspeed, lift-to-drag ratio, specific fuel consumption, and the part of the total fuel fraction available for cruise, also known as the cruise fuel fraction, or  cruise fuel weight fraction.   == fighter aircraft == at today’s state of the art for jet fighter aircraft, fuel fractions of 29 percent and below typically yield subcruisers; 33 percent provides a quasi–supercruiser; and 35 percent and above are needed for useful supercruising missions. the u.s. f-22 raptor’s fuel fraction is 29 percent, eurofighter is 31 percent, both similar to those of the subcruising f-4 phantom ii, f-15 eagle and the russian mikoyan mig-29 ""fulcrum"". the russian supersonic interceptor, the mikoyan mig-31 ""foxhound"", has a fuel fraction of over 45 percent. the panavia tornado had a relatively low internal fuel fraction of 26 percent, and frequently carried drop tanks.   == airliners == airliners have a fuel fraction of less than half their takeoff weight, between 26% for medium-haul to 45% for long-haul:  the concorde supersonic transport had a fuel fraction of 51%.   == general aviation == the rutan voyager took off on its 1986 around-the-world flight at 72 percent, the highest figure ever at the time. steve fossett\'s virgin atlantic globalflyer could attain a fuel fraction of nearly 85 percent, meaning that it carried more than five times its empty weight in fuel.   == see also == mass ratio   == references ==')"
74,"Electromagnetic formation flight (EMFF) investigates the concept of using electromagnets coupled with reaction wheels in place of more traditional propulsion systems to control the positions and attitudes of a number of spacecraft in close proximity.  Unlike traditional propulsion systems, which use exhaustible propellants that often limit lifetime, the EMFF system uses solar power to energize a magnetic field. The Massachusetts Institute of Technology Space Systems Laboratory is exploring this concept by developing dynamics and control models as well as an experimental testbed for their validation.


== How it works ==
The magnetic fields for EMFF are generated by sending current through coils of wire.  The interaction between the magnetic dipoles created is easily understood with a far field approximation where the separation distance between two vehicles is large compared to the physical size of the dipole.  By controlling the dipoles on various vehicles, attraction, repulsion, and shear forces can be created. Combined with reaction wheels, any desired maneuver can be performed as long as the formation’s center of mass is not required to change.


== Applications ==
The EMFF system is most applicable in cases where multiple spacecraft are free-flying relative to one another and there is no need to control the center of mass of the system. NASA’s Terrestrial Planet Finder (TPF) mission and space telescope assembly are just two such types of missions.  EMFF provides the foremost advantage of reduced dependence on consumables.  In addition, it eliminates thruster plumes and enhances the capability of replacing a failed element more economically.


== Testbed ==
The MIT-SSL constructed two EMFF testbed vehicles for demonstrating control of 2-D formations on a large flat floor.  Vehicles are suspended on a frictionless air carriage and are completely self-contained using RF communications, microprocessors, and a metrology system.  Liquid Nitrogen maintains cryogenic temperatures and batteries provide the power to the high-temperature superconductive (HTS) coils.  The testbed has demonstrated control of the relative degrees of freedom (DOF) in open loop and closed loop control using linearized controllers and a nonlinear sliding mode controller.


== Awards ==
Former Space Systems Lab associate director Dr. Raymond Sedwick (now at the University of Maryland, College Park) has been awarded the first Bepi Colombo Prize for a paper on electromagnetic formation flight. According to Aero-Astro Professor Manuel Martinez-Sanchez, who worked with Colombo and was a juror in the competition, ""The jury was unanimous in that Ray's paper best represented 'Bepi' Colombo's spirit of innovation and originality, combined with rigor.""


== Collaborators ==
Research on electromagnetic formation flight or similar projects is also ongoing at:

The Institute of Space and Astronautical Science / JAXA
Space Research Centre, Polish Academy of Sciences
Michigan Technogical University on Colomb Force Spacecraft


== Other journal articles ==
Elias, Laila M., Kwon, Daniel W., Sedwick, Raymond J., and Miller, David W., ""Electromagnetic Formation Flight Dynamics including Reaction Wheel Gyroscopic Stiffening Effects"" Journal of Guidance, Control, and Dynamics, Vol. 30, No. 2, Mar–Apr. 2007, pp. 499–511.


== References ==


== External links ==
SSL's EMFF web page
Other research at the MIT Space Systems Laboratory MSE
SPHERES","pandas(index=74, _1=74, text='electromagnetic formation flight (emff) investigates the concept of using electromagnets coupled with reaction wheels in place of more traditional propulsion systems to control the positions and attitudes of a number of spacecraft in close proximity.  unlike traditional propulsion systems, which use exhaustible propellants that often limit lifetime, the emff system uses solar power to energize a magnetic field. the massachusetts institute of technology space systems laboratory is exploring this concept by developing dynamics and control models as well as an experimental testbed for their validation.   == how it works == the magnetic fields for emff are generated by sending current through coils of wire.  the interaction between the magnetic dipoles created is easily understood with a far field approximation where the separation distance between two vehicles is large compared to the physical size of the dipole.  by controlling the dipoles on various vehicles, attraction, repulsion, and shear forces can be created. combined with reaction wheels, any desired maneuver can be performed as long as the formation’s center of mass is not required to change.   == applications == the emff system is most applicable in cases where multiple spacecraft are free-flying relative to one another and there is no need to control the center of mass of the system. nasa’s terrestrial planet finder (tpf) mission and space telescope assembly are just two such types of missions.  emff provides the foremost advantage of reduced dependence on consumables.  in addition, it eliminates thruster plumes and enhances the capability of replacing a failed element more economically.   == testbed == the mit-ssl constructed two emff testbed vehicles for demonstrating control of 2-d formations on a large flat floor.  vehicles are suspended on a frictionless air carriage and are completely self-contained using rf communications, microprocessors, and a metrology system.  liquid nitrogen maintains cryogenic temperatures and batteries provide the power to the high-temperature superconductive (hts) coils.  the testbed has demonstrated control of the relative degrees of freedom (dof) in open loop and closed loop control using linearized controllers and a nonlinear sliding mode controller.   == awards == former space systems lab associate director dr. raymond sedwick (now at the university of maryland, college park) has been awarded the first bepi colombo prize for a paper on electromagnetic formation flight. according to aero-astro professor manuel martinez-sanchez, who worked with colombo and was a juror in the competition, ""the jury was unanimous in that ray\'s paper best represented \'bepi\' colombo\'s spirit of innovation and originality, combined with rigor.""   == collaborators == research on electromagnetic formation flight or similar projects is also ongoing at:  the institute of space and astronautical science / jaxa space research centre, polish academy of sciences michigan technogical university on colomb force spacecraft   == other journal articles == elias, laila m., kwon, daniel w., sedwick, raymond j., and miller, david w., ""electromagnetic formation flight dynamics including reaction wheel gyroscopic stiffening effects"" journal of guidance, control, and dynamics, vol. 30, no. 2, mar–apr. 2007, pp. 499–511.   == references ==   == external links == ssl\'s emff web page other research at the mit space systems laboratory mse spheres')"
75,"In naval architecture and aerospace engineering, the fineness ratio is the ratio of the length of a body to its maximum width. Shapes that are short and wide have a low fineness ratio, those that are long and narrow have high fineness ratios. Aircraft that spend time at supersonic speeds, e.g. the Concorde, generally have high fineness ratios.
At speeds below critical mach, one of the primary forms of drag is skin friction. As the name implies, this is drag caused by the interaction of the airflow with the aircraft's skin.  To minimize this drag, the aircraft should be designed to minimize the exposed skin area, or ""wetted surface"". One solution to this problem is constructing an ""egg shaped"" fuselage, for example as used on the home-built Questair Venture.
Theoretical ideal fineness ratios in subsonic aircraft fuselages are typically found at about 6:1, however this may be compromised by other design considerations such as seating or freight size requirements. Because a higher fineness fuselage can have reduced tail surfaces, this ideal ratio can practically be increased to 8:1.Most aircraft have fineness ratios significantly greater than this, however. This is often due to the competing need to place the tail control surfaces at the end of a longer moment arm to increase their effectiveness. Reducing the length of the fuselage would require larger controls, which would offset the drag savings from using the ideal fineness ratio. An example of a high-performance design with an imperfect fineness ratio is the Lancair. In other cases, the designer is forced to use a non-ideal design due to outside factors such as seating arrangements or cargo pallet sizes. Modern airliners often have fineness ratios much higher than ideal, a side effect of their cylindrical cross-section which is selected for strength, as well as providing a single width to simplify seating layout and air cargo handling.
As an aircraft approaches the speed of sound, shock waves form on areas of greater curvature. These shock waves radiate away energy that the engines must supply, energy that does not go into making the aircraft go faster. This appears to be a new form of drag—referred to as wave drag—which peaks at about three times the drag at speeds even slightly below the critical mach. In order to minimize the wave drag, the curvature of the aircraft should be kept to a minimum, which implies much higher fineness ratios. This is why high-speed aircraft have long pointed noses and tails, and cockpit canopies that are flush to the fuselage line.
More technically, the best possible performance for a supersonic design is typified by two ""perfect shapes"", the Sears-Haack body which is pointed at both ends, or the von Kármán ogive, which has a blunt tail. Examples of the latter design include the Concorde, F-104 Starfighter and XB-70 Valkyrie, although to some degree practically every post-World War II interceptor aircraft featured such a design. Missile designers are even less interested in low-speed performance, and missiles generally have higher fineness ratios than most aircraft.
The introduction of aircraft with higher fineness ratios also introduced a new form of instability, inertial coupling. As the engines and cockpit moved away from the aircraft's center of mass, the roll inertia of these masses grew to be able to overwhelm the power of the aerodynamic surfaces. A variety of methods are used to combat this effect, including oversized controls and stability augmentation systems.


== References ==

Form Factor
Basic Fluid Dynamics","pandas(index=75, _1=75, text='in naval architecture and aerospace engineering, the fineness ratio is the ratio of the length of a body to its maximum width. shapes that are short and wide have a low fineness ratio, those that are long and narrow have high fineness ratios. aircraft that spend time at supersonic speeds, e.g. the concorde, generally have high fineness ratios. at speeds below critical mach, one of the primary forms of drag is skin friction. as the name implies, this is drag caused by the interaction of the airflow with the aircraft\'s skin.  to minimize this drag, the aircraft should be designed to minimize the exposed skin area, or ""wetted surface"". one solution to this problem is constructing an ""egg shaped"" fuselage, for example as used on the home-built questair venture. theoretical ideal fineness ratios in subsonic aircraft fuselages are typically found at about 6:1, however this may be compromised by other design considerations such as seating or freight size requirements. because a higher fineness fuselage can have reduced tail surfaces, this ideal ratio can practically be increased to 8:1.most aircraft have fineness ratios significantly greater than this, however. this is often due to the competing need to place the tail control surfaces at the end of a longer moment arm to increase their effectiveness. reducing the length of the fuselage would require larger controls, which would offset the drag savings from using the ideal fineness ratio. an example of a high-performance design with an imperfect fineness ratio is the lancair. in other cases, the designer is forced to use a non-ideal design due to outside factors such as seating arrangements or cargo pallet sizes. modern airliners often have fineness ratios much higher than ideal, a side effect of their cylindrical cross-section which is selected for strength, as well as providing a single width to simplify seating layout and air cargo handling. as an aircraft approaches the speed of sound, shock waves form on areas of greater curvature. these shock waves radiate away energy that the engines must supply, energy that does not go into making the aircraft go faster. this appears to be a new form of drag—referred to as wave drag—which peaks at about three times the drag at speeds even slightly below the critical mach. in order to minimize the wave drag, the curvature of the aircraft should be kept to a minimum, which implies much higher fineness ratios. this is why high-speed aircraft have long pointed noses and tails, and cockpit canopies that are flush to the fuselage line. more technically, the best possible performance for a supersonic design is typified by two ""perfect shapes"", the sears-haack body which is pointed at both ends, or the von kármán ogive, which has a blunt tail. examples of the latter design include the concorde, f-104 starfighter and xb-70 valkyrie, although to some degree practically every post-world war ii interceptor aircraft featured such a design. missile designers are even less interested in low-speed performance, and missiles generally have higher fineness ratios than most aircraft. the introduction of aircraft with higher fineness ratios also introduced a new form of instability, inertial coupling. as the engines and cockpit moved away from the aircraft\'s center of mass, the roll inertia of these masses grew to be able to overwhelm the power of the aerodynamic surfaces. a variety of methods are used to combat this effect, including oversized controls and stability augmentation systems.   == references ==  form factor basic fluid dynamics')"
76,"Human-rating certification, also known as man-rating or crew-rating, is the certification of a spacecraft or launch vehicle as capable of safely transporting humans. There is no one particular standard for human-rating a spacecraft or launch vehicle, and the various entities that launch or plan to launch such spacecraft specify requirements for their particular systems to be human-rated.


== NASA ==
One entity that applies human rating is the US government civilian space agency, NASA. NASA's human-rating requires not just that a system be designed to be tolerant of failure and to protect the crew even if an unrecoverable failure occurs, but also that astronauts aboard a human-rated spacecraft have some control over it. This set of technical requirements and the associated certification process for crewed space systems are in addition to the standards and requirements for all of NASA's space flight programs.The development of the Space Shuttle and the International Space Station pre-dates the current NASA human-rating requirements. After the Challenger and Columbia accidents, the criteria used by NASA for human-rating spacecraft were made more stringent.


=== Commercial Crew Program (CCP) ===

The NASA CCP human-rating standards require that the probability of a loss on ascent does not exceed 1 in 500, and that the probability of a loss on descent did not exceed 1 in 500. The overall mission loss risk, which includes vehicle risk from micrometeorites and orbital debris while in orbit for up to 210 days, is required to be no more than 1 in 270. Maximum sustained acceleration is limited to 3 g.The United Launch Alliance (ULA) published a paper submitted to AIAA detailing the modifications to its Delta IV and Atlas V launch vehicles that would be needed to conform to NASA Standard 8705.2B. ULA has since been awarded $6.7 million under NASA's Commercial Crew Development (CCDev) program for development of an Emergency Detection System, one of the final pieces that would be needed to make these launchers suitable for human spaceflight.SpaceX is using Dragon 2, launched on a Falcon 9 rocket, to deliver crew to the ISS. Dragon 2 made its first uncrewed test flight in March 2019 and has been conducting crewed flights since Demo-2 in May 2020.Boeing is developing the Starliner spacecraft as part of the Commercial Crew Program.


== Other space agencies ==
The Russian state corporation Roscosmos, Indian space agency ISRO, Chinese space agency CNSA, and each private spaceflight system builder typically sets up their own specific criteria to be met before carrying humans on any space transport system.


== See also ==
FAA
List of human spaceflight programs


== References ==","pandas(index=76, _1=76, text=""human-rating certification, also known as man-rating or crew-rating, is the certification of a spacecraft or launch vehicle as capable of safely transporting humans. there is no one particular standard for human-rating a spacecraft or launch vehicle, and the various entities that launch or plan to launch such spacecraft specify requirements for their particular systems to be human-rated.   == nasa == one entity that applies human rating is the us government civilian space agency, nasa. nasa's human-rating requires not just that a system be designed to be tolerant of failure and to protect the crew even if an unrecoverable failure occurs, but also that astronauts aboard a human-rated spacecraft have some control over it. this set of technical requirements and the associated certification process for crewed space systems are in addition to the standards and requirements for all of nasa's space flight programs.the development of the space shuttle and the international space station pre-dates the current nasa human-rating requirements. after the challenger and columbia accidents, the criteria used by nasa for human-rating spacecraft were made more stringent. the nasa ccp human-rating standards require that the probability of a loss on ascent does not exceed 1 in 500, and that the probability of a loss on descent did not exceed 1 in 500. the overall mission loss risk, which includes vehicle risk from micrometeorites and orbital debris while in orbit for up to 210 days, is required to be no more than 1 in 270. maximum sustained acceleration is limited to 3 g.the united launch alliance (ula) published a paper submitted to aiaa detailing the modifications to its delta iv and atlas v launch vehicles that would be needed to conform to nasa standard 8705.2b. ula has since been awarded $6.7 million under nasa's commercial crew development (ccdev) program for development of an emergency detection system, one of the final pieces that would be needed to make these launchers suitable for human spaceflight.spacex is using dragon 2, launched on a falcon 9 rocket, to deliver crew to the iss. dragon 2 made its first uncrewed test flight in march 2019 and has been conducting crewed flights since demo-2 in may 2020.boeing is developing the starliner spacecraft as part of the commercial crew program.   == other space agencies == the russian state corporation roscosmos, indian space agency isro, chinese space agency cnsa, and each private spaceflight system builder typically sets up their own specific criteria to be met before carrying humans on any space transport system.   == see also == faa list of human spaceflight programs   == references =="")"
77,"Design/Build/Fly, or DBF, is a radio-controlled aircraft competition sponsored by the American institute of Aeronautics and Astronautics (AIAA), Cessna Aircraft Company, and Raytheon Missile Systems. The Office of Naval Research was also a sponsor until 2006. The competition is intended to challenge the AIAA student branches of each university to design, build, and fly a remote controlled airplane that can complete specific ground and flight missions. Additionally, the teams are required to submit a comprehensive design report detailing the most important aspects of their designs.
The competition rules change every year. Usually, rules are published in late August and the competition fly-off is held in April. The rules define a mathematical formula used to determine the score for an entry. Recent competitions' formulas have used a combination of design report score, mission score determined by performance conducting one or more mission tasks at the fly-off, and Rated Aircraft Cost, a variable used to define the complexity of the design.

1. The 2020 fly-off was cancelled due to the COVID-19 pandemic. Scores and rankings were solely based on the report scores.


== List of Top 5 Finish by University ==


== External links ==
Home page","pandas(index=77, _1=77, text=""design/build/fly, or dbf, is a radio-controlled aircraft competition sponsored by the american institute of aeronautics and astronautics (aiaa), cessna aircraft company, and raytheon missile systems. the office of naval research was also a sponsor until 2006. the competition is intended to challenge the aiaa student branches of each university to design, build, and fly a remote controlled airplane that can complete specific ground and flight missions. additionally, the teams are required to submit a comprehensive design report detailing the most important aspects of their designs. the competition rules change every year. usually, rules are published in late august and the competition fly-off is held in april. the rules define a mathematical formula used to determine the score for an entry. recent competitions' formulas have used a combination of design report score, mission score determined by performance conducting one or more mission tasks at the fly-off, and rated aircraft cost, a variable used to define the complexity of the design.  1. the 2020 fly-off was cancelled due to the covid-19 pandemic. scores and rankings were solely based on the report scores.   == list of top 5 finish by university ==   == external links == home page"")"
78,"If an aircraft in flight suffers a disturbance in pitch that causes an increase (or decrease) in angle of attack, it is desirable that the aerodynamic forces on the aircraft cause a decrease (or increase) in angle of attack so that the disturbance does not cause a continuous increase (or decrease) in angle of attack.  This is longitudinal static stability.
Static margin is a concept used to characterize the static longitudinal stability and controllability of aircraft and missiles.
In aircraft analysis, static margin is defined as the distance between the center of gravity and the neutral point of the aircraft, expressed as a percentage of the mean aerodynamic chord of the wing.  The greater this distance and the narrower the wing, the more stable the aircraft.  Conventionally, the neutral point is aft of the c.g., although in rare cases (computer controlled fighter aircraft) it may be forward of the c.g., i.e. slightly unstable, to obtain quickness of response in combat.  Too great longitudinal stability makes the aircraft ""stiff"" in pitch, resulting in such undesirable features as difficulty in obtaining the necessary stalled nose-up pitch when landing.
The position of the neutral point is found by taking the algebraic net moment of all horizontal surfaces, measured from the nose of the aircraft, in the same manner as the c.g. is determined, i.e. the sum of all such moments divided by their total area.  The stabilizer and elevator dominate this result, but it is necessary to account for all surfaces such as fuselage, landing gear, prop-normal, etc.  It is also necessary to take account of the center of pressure of the wing, which can move a good deal fore and aft as angle of attack of a flat-bottom wing section (Clark Y) changes, or not at all in the case of self-stabilizing sections such as the M6.
The neutral point in conventional aircraft is a short distance behind the c.g. (""The feathers of the arrow must be at the back""); but in unconventional aircraft such as canards and those with dual-wings, such as the Quickie, this will not be so.  The overall rule stated above must hold, i.e. the neutral point must be aft of the c.g., wherever that may be. 
The position of the center of gravity is determined by factors such as the positions of  loads, e.g. passengers, fuel, weapons, etc.; whether such loads can vary, e.g. presence or absence of luggage, ammunition, etc.; and how fuel is consumed during flight.  Additional information regarding the usual position of the neutral point aft of the center of gravity is at  longitudinal static stability. (For an aircraft this may be described as positive static margin.)  The response of an aircraft or missile to an angular disturbance such as a pitch disturbance is determined by its static margin.
In missile analysis, static margin is defined as the distance between the center of gravity and the center of pressure.  Missiles are symmetric vehicles and if they have airfoils they too are symmetric.  
For missiles, positive static margin implies that the complete vehicle makes a restoring moment for any angle of attack from the trim position.  If the center of pressure is behind the center of gravity then the moment will be restoring.  For missiles with symmetric airfoils, the neutral point and the center of pressure are coincident and the term neutral point is not used.


== Relationship to aircraft and missile stability and control ==
If the center of gravity (CG) of an aircraft is forward of the neutral point, or the CG of a missile is forward of the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that returns the angle of attack of the vehicle towards the angle that existed prior to the disturbance.
If the CG of an aircraft is behind the neutral point, or the CG of a missile is behind the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that continues to drive the angle of attack of the vehicle further away from the starting position.The first condition above is positive static stability.  In missile analysis this is described as positive static margin.  (In aircraft analysis it may be described as negative static margin.)
The second condition above is negative static stability.  In missile analysis this is defined as negative static margin.  (In aircraft analysis it may be described as positive static margin.)  
Depending on the static margin, humans may not be able to use control inputs to the elevators to control the pitch of the vehicle.  Typically, computer based autopilots are required to control the vehicle when it has negative static stability - usually described as negative static margin.
The purpose of the reduced stability (low static margin) is to make an aircraft more responsive to pilot inputs.  An aircraft with a large static margin will be very stable and slow to respond to the pilot inputs.  The amount of static margin is an important factor in determining the handling qualities of an aircraft.  For an unguided rocket, the vehicle must have a large positive static margin so the rocket shows minimum tendency to diverge from the direction of flight given to it at launch.  In contrast, guided missiles usually have a negative static margin for increased maneuverability.


== See also ==
Aerodynamic center
Longitudinal static stability
Center of pressure


== References ==","pandas(index=78, _1=78, text='if an aircraft in flight suffers a disturbance in pitch that causes an increase (or decrease) in angle of attack, it is desirable that the aerodynamic forces on the aircraft cause a decrease (or increase) in angle of attack so that the disturbance does not cause a continuous increase (or decrease) in angle of attack.  this is longitudinal static stability. static margin is a concept used to characterize the static longitudinal stability and controllability of aircraft and missiles. in aircraft analysis, static margin is defined as the distance between the center of gravity and the neutral point of the aircraft, expressed as a percentage of the mean aerodynamic chord of the wing.  the greater this distance and the narrower the wing, the more stable the aircraft.  conventionally, the neutral point is aft of the c.g., although in rare cases (computer controlled fighter aircraft) it may be forward of the c.g., i.e. slightly unstable, to obtain quickness of response in combat.  too great longitudinal stability makes the aircraft ""stiff"" in pitch, resulting in such undesirable features as difficulty in obtaining the necessary stalled nose-up pitch when landing. the position of the neutral point is found by taking the algebraic net moment of all horizontal surfaces, measured from the nose of the aircraft, in the same manner as the c.g. is determined, i.e. the sum of all such moments divided by their total area.  the stabilizer and elevator dominate this result, but it is necessary to account for all surfaces such as fuselage, landing gear, prop-normal, etc.  it is also necessary to take account of the center of pressure of the wing, which can move a good deal fore and aft as angle of attack of a flat-bottom wing section (clark y) changes, or not at all in the case of self-stabilizing sections such as the m6. the neutral point in conventional aircraft is a short distance behind the c.g. (""the feathers of the arrow must be at the back""); but in unconventional aircraft such as canards and those with dual-wings, such as the quickie, this will not be so.  the overall rule stated above must hold, i.e. the neutral point must be aft of the c.g., wherever that may be. the position of the center of gravity is determined by factors such as the positions of  loads, e.g. passengers, fuel, weapons, etc.; whether such loads can vary, e.g. presence or absence of luggage, ammunition, etc.; and how fuel is consumed during flight.  additional information regarding the usual position of the neutral point aft of the center of gravity is at  longitudinal static stability. (for an aircraft this may be described as positive static margin.)  the response of an aircraft or missile to an angular disturbance such as a pitch disturbance is determined by its static margin. in missile analysis, static margin is defined as the distance between the center of gravity and the center of pressure.  missiles are symmetric vehicles and if they have airfoils they too are symmetric. for missiles, positive static margin implies that the complete vehicle makes a restoring moment for any angle of attack from the trim position.  if the center of pressure is behind the center of gravity then the moment will be restoring.  for missiles with symmetric airfoils, the neutral point and the center of pressure are coincident and the term neutral point is not used.   == relationship to aircraft and missile stability and control == if the center of gravity (cg) of an aircraft is forward of the neutral point, or the cg of a missile is forward of the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that returns the angle of attack of the vehicle towards the angle that existed prior to the disturbance. if the cg of an aircraft is behind the neutral point, or the cg of a missile is behind the center of pressure, the vehicle will respond to a disturbance by producing an aerodynamic moment that continues to drive the angle of attack of the vehicle further away from the starting position.the first condition above is positive static stability.  in missile analysis this is described as positive static margin.  (in aircraft analysis it may be described as negative static margin.) the second condition above is negative static stability.  in missile analysis this is defined as negative static margin.  (in aircraft analysis it may be described as positive static margin.) depending on the static margin, humans may not be able to use control inputs to the elevators to control the pitch of the vehicle.  typically, computer based autopilots are required to control the vehicle when it has negative static stability - usually described as negative static margin. the purpose of the reduced stability (low static margin) is to make an aircraft more responsive to pilot inputs.  an aircraft with a large static margin will be very stable and slow to respond to the pilot inputs.  the amount of static margin is an important factor in determining the handling qualities of an aircraft.  for an unguided rocket, the vehicle must have a large positive static margin so the rocket shows minimum tendency to diverge from the direction of flight given to it at launch.  in contrast, guided missiles usually have a negative static margin for increased maneuverability.   == see also == aerodynamic center longitudinal static stability center of pressure   == references ==')"
79,"In fluid dynamics, the Küssner effect describes the unsteady aerodynamic forces on an airfoil or hydrofoil caused by encountering a transverse gust. This is directly related to the Küssner function, used in describing the effect. Both the effect and function are named after Hans Georg Küssner (1900–1984), a German aerodynamics engineer.Küssner derived an approximate model for an airfoil encountering a sudden step-like change in the transverse gust velocity—or, equivalently, as seen from a frame of reference moving with the airfoil: a sudden change in the angle of attack. The airfoil is modelled as a flat plate in a potential flow, moving with constant horizontal velocity. For this case he derived the impulse response function—known as Küssner function—needed to compute the unsteady lift and moment exerted by the air on the airfoil.


== Notes ==


== References ==
H.G. Küssner (December 20, 1936), ""Zusammenfassender Bericht über den instationären Auftrieb von Flügeln (Summary report on the instationary lift of wings)"", Luftfahrtforschung (in German), 13 (12): 410–424
H.G. Küssner (1937), ""Flügel- und Leitwerkflattern"" (in German)
H.G. Küssner (1940), ""Der schwingende Flügel mit aerodynamisch ausgeglichenem Ruder"" (in German)
H.G. Küssner (1940), ""Allgemeine Tragflächentheorie"" (in German)
Ernst H. Hirschel; Horst Prem; Gero Madelung (2004), Aeronautical Research in Germany: From Lilienthal until Today, Springer, p. 287, ISBN 978-3-540-40645-7
Tuncer Cebeci (2005), Analysis of Low-speed Unsteady Airfoil Flows, Springer, pp. 15–16 & 52, ISBN 0-9668461-8-4
Raymond L. Bisplinghoff; Holt Ashley; Robert L. Halfman (1996), Aeroelasticity (revised ed.), Dover, pp. 281–286, ISBN 0-486-69189-6
John M. Eggleston (1956), Calculation of the forces and moments on a slender fuselage and vertical fin penetrating lateral gusts (PDF), NACA Technical Note 3805 Page 3
Beerinder Singh; Inderjit Chopra (September 2008), ""Insect-Based Hover-Capable Flapping Wings for Micro Air Vehicles: Experiments and Analysis"", AIAA Journal, 46 (9): 2115–2135, Bibcode:2008AIAAJ..46.2115S, doi:10.2514/1.28192
L.M. Laudanski (July 2000), ""Random disturbances, airplane loads and its fatigue life"", Probabilistic Engineering Mechanics, 15 (3): 233–240, doi:10.1016/S0266-8920(98)00020-4


== External links ==
E.C. Pike (Ed.) (1971), Manual on Aeroelasticity. Subject and author index (PDF), NATO AGARD Report 578CS1 maint: extra text: authors list (link) Page 13.
Küssner function, Georgia Institute of Technology, retrieved 10 March 2009","pandas(index=79, _1=79, text='in fluid dynamics, the küssner effect describes the unsteady aerodynamic forces on an airfoil or hydrofoil caused by encountering a transverse gust. this is directly related to the küssner function, used in describing the effect. both the effect and function are named after hans georg küssner (1900–1984), a german aerodynamics engineer.küssner derived an approximate model for an airfoil encountering a sudden step-like change in the transverse gust velocity—or, equivalently, as seen from a frame of reference moving with the airfoil: a sudden change in the angle of attack. the airfoil is modelled as a flat plate in a potential flow, moving with constant horizontal velocity. for this case he derived the impulse response function—known as küssner function—needed to compute the unsteady lift and moment exerted by the air on the airfoil.   == notes ==   == references == h.g. küssner (december 20, 1936), ""zusammenfassender bericht über den instationären auftrieb von flügeln (summary report on the instationary lift of wings)"", luftfahrtforschung (in german), 13 (12): 410–424 h.g. küssner (1937), ""flügel- und leitwerkflattern"" (in german) h.g. küssner (1940), ""der schwingende flügel mit aerodynamisch ausgeglichenem ruder"" (in german) h.g. küssner (1940), ""allgemeine tragflächentheorie"" (in german) ernst h. hirschel; horst prem; gero madelung (2004), aeronautical research in germany: from lilienthal until today, springer, p. 287, isbn 978-3-540-40645-7 tuncer cebeci (2005), analysis of low-speed unsteady airfoil flows, springer, pp. 15–16 & 52, isbn 0-9668461-8-4 raymond l. bisplinghoff; holt ashley; robert l. halfman (1996), aeroelasticity (revised ed.), dover, pp. 281–286, isbn 0-486-69189-6 john m. eggleston (1956), calculation of the forces and moments on a slender fuselage and vertical fin penetrating lateral gusts (pdf), naca technical note 3805 page 3 beerinder singh; inderjit chopra (september 2008), ""insect-based hover-capable flapping wings for micro air vehicles: experiments and analysis"", aiaa journal, 46 (9): 2115–2135, bibcode:2008aiaaj..46.2115s, doi:10.2514/1.28192 l.m. laudanski (july 2000), ""random disturbances, airplane loads and its fatigue life"", probabilistic engineering mechanics, 15 (3): 233–240, doi:10.1016/s0266-8920(98)00020-4   == external links == e.c. pike (ed.) (1971), manual on aeroelasticity. subject and author index (pdf), nato agard report 578cs1 maint: extra text: authors list (link) page 13. küssner function, georgia institute of technology, retrieved 10 march 2009')"
80,"In astronautics, a powered flyby, or Oberth maneuver, is a maneuver in which a spacecraft falls into a gravitational well and then uses its engines to further accelerate as it is falling, thereby achieving additional speed. The resulting maneuver is a more efficient way to gain kinetic energy than applying the same impulse outside of a gravitational well. The gain in efficiency is explained by the Oberth effect, wherein the use of a reaction engine at higher speeds generates a greater change in mechanical energy than its use at lower speeds. In practical terms, this means that the most energy-efficient method for a spacecraft to burn its fuel is at the lowest possible orbital periapsis, when its orbital velocity (and so, its kinetic energy) is greatest. In some cases, it is even worth spending fuel on slowing the spacecraft into a gravity well to take advantage of the efficiencies of the Oberth effect. The maneuver and effect are named after the person who first described them in 1927, Hermann Oberth, an Austro-Hungarian-born German physicist and a founder of modern rocketry.The Oberth effect is strongest at a point in orbit known as the periapsis, where the gravitational potential is lowest, and the speed is highest. This is because a given firing of a rocket engine at high speed causes a greater change in kinetic energy than when fired otherwise similarly at lower speed. 
Because the vehicle remains near periapsis only for a short time, for the Oberth maneuver to be most effective the vehicle must be able to generate as much impulse as possible in the shortest possible time. As a result the Oberth maneuver is much more useful for high-thrust rocket engines like liquid-propellant rockets, and less useful for low-thrust reaction engines such as ion drives, which take a long time to gain speed. The Oberth effect also can be used to understand the behavior of multi-stage rockets: the upper stage can generate much more usable kinetic energy than the total chemical energy of the propellants it carries.In terms of the energies involved, the Oberth effect is more effective at higher speeds because at high speed the propellant has significant kinetic energy in addition to its chemical potential energy. At higher speed the vehicle is able to employ the greater change (reduction) in kinetic energy of the propellant (as it is exhausted backwards and hence at reduced speed and hence reduced kinetic energy) to generate a greater increase in kinetic energy of the vehicle.


== Explanation in terms of momentum and kinetic energy ==
A rocket works by transferring momentum to its propellant. At a fixed exhaust velocity, this will be a fixed amount of momentum per unit of propellant. For a given mass of rocket (including remaining propellant), this implies a fixed change in velocity per unit of propellant. Because kinetic energy = 
  
    
      
        
          
            
              
                
                  
                    
                      1
                      2
                    
                  
                  m
                  
                    v
                    
                      2
                    
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{smallmatrix}{\frac {1}{2}}mv^{2}\end{smallmatrix}}}
  , this change in velocity imparts a greater increase in kinetic energy at a high velocity than it would at a low velocity. For example, considering a 2kg rocket: 

at 1 m/s, adding 1 m/s increases the  kinetic energy from 1J to 4J, for a gain of 3J.
at 10 m/s, starting with a kinetic energy of 100J, the rocket ends with 121J, for a net gain of 21J.This greater change in kinetic energy can then carry the rocket higher in the gravity well than if the propellant were burned at a lower speed.


== Description in terms of work ==
Rocket engines produce the same force regardless of their velocity. A rocket acting on a fixed object, as in a static firing, does no useful work at all; the rocket's stored energy is entirely expended on accelerating its propellant in the form of exhaust. But when the rocket moves, its thrust acts through the distance it moves. Force multiplied by distance is the definition of mechanical energy or work. So the farther the rocket and payload move during the burn (i.e. the faster they move), the greater the kinetic energy imparted to the rocket and its payload and the less to its exhaust.
This is shown as follows. The mechanical work done on the rocket (
  
    
      
        W
      
    
    {\displaystyle W}
  ) is defined as the dot product of the force of the engine's thrust (
  
    
      
        
          
            
              F
              →
            
          
        
      
    
    {\displaystyle {\vec {F}}}
  ) and the displacement it travels during the burn (
  
    
      
        
          
            
              s
              →
            
          
        
      
    
    {\displaystyle {\vec {s}}}
  ):

  
    
      
        W
        =
        
          
            
              F
              →
            
          
        
        ⋅
        
          
            
              s
              →
            
          
        
        .
      
    
    {\displaystyle W={\vec {F}}\cdot {\vec {s}}.}
  If the burn is made in the prograde direction, 
  
    
      
        
          
            
              F
              →
            
          
        
        ⋅
        
          
            
              s
              →
            
          
        
        =
        ‖
        F
        ‖
        ⋅
        ‖
        s
        ‖
        =
        F
        ⋅
        s
      
    
    {\displaystyle {\vec {F}}\cdot {\vec {s}}=\|F\|\cdot \|s\|=F\cdot s}
  . The work results in a change in kinetic energy

  
    
      
        Δ
        
          E
          
            k
          
        
        =
        F
        ⋅
        s
        .
      
    
    {\displaystyle \Delta E_{k}=F\cdot s.}
  Differentiating with respect to time, we obtain 

  
    
      
        
          
            
              
                d
              
              
                E
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        F
        ⋅
        
          
            
              
                d
              
              s
            
            
              
                d
              
              t
            
          
        
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} E_{k}}{\mathrm {d} t}}=F\cdot {\frac {\mathrm {d} s}{\mathrm {d} t}},}
  or

  
    
      
        
          
            
              
                d
              
              
                E
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        F
        ⋅
        v
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} E_{k}}{\mathrm {d} t}}=F\cdot v,}
  where 
  
    
      
        v
      
    
    {\displaystyle v}
   is the velocity. Dividing by the instantaneous mass 
  
    
      
        m
      
    
    {\displaystyle m}
   to express this in terms of specific energy (
  
    
      
        
          e
          
            k
          
        
      
    
    {\displaystyle e_{k}}
  ), we get 

  
    
      
        
          
            
              
                d
              
              
                e
                
                  k
                
              
            
            
              
                d
              
              t
            
          
        
        =
        
          
            F
            m
          
        
        ⋅
        v
        =
        a
        ⋅
        v
        ,
      
    
    {\displaystyle {\frac {\mathrm {d} e_{k}}{\mathrm {d} t}}={\frac {F}{m}}\cdot v=a\cdot v,}
  where 
  
    
      
        a
      
    
    {\displaystyle a}
   is the acceleration vector.
Thus it can be readily seen that the rate of gain of specific energy of every part of the rocket is proportional to speed and, given this, the equation can be integrated (numerically or otherwise) to calculate the overall increase in specific energy of the rocket.


== Impulsive burn ==
Integrating the above energy equation is often unnecessary if the burn duration is short. Short burns of chemical rocket engines close to periapsis or elsewhere are usually mathematically modelled as impulsive burns, where the force of the engine dominates any other forces that might change the vehicle's energy over the burn.
For example, as a vehicle falls towards periapsis in any orbit (closed or escape orbits) the velocity relative to the central body increases. Briefly burning the engine (an “impulsive burn”) prograde at periapsis increases the velocity by the same increment as at any other time (
  
    
      
        Δ
        v
      
    
    {\displaystyle \Delta v}
  ). However, since the vehicle's kinetic energy is related to the square of its velocity, this increase in velocity has a non-linear effect on the vehicle's kinetic energy, leaving it with higher energy than if the burn were achieved at any other time.


=== Oberth calculation for a parabolic orbit ===
If an impulsive burn of Δv is performed at periapsis in a parabolic orbit, then the velocity at periapsis before the burn is equal to the escape velocity (Vesc), and the specific kinetic energy after the burn is

  
    
      
        
          
            
              
                
                  e
                  
                    k
                  
                
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                
                  V
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                (
                
                  V
                  
                    esc
                  
                
                +
                Δ
                v
                
                  )
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      1
                      2
                    
                  
                
                
                  V
                  
                    esc
                  
                  
                    2
                  
                
                +
                Δ
                v
                
                  V
                  
                    esc
                  
                
                +
                
                  
                    
                      1
                      2
                    
                  
                
                Δ
                
                  v
                  
                    2
                  
                
                ,
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}e_{k}&={\tfrac {1}{2}}V^{2}\\&={\tfrac {1}{2}}(V_{\text{esc}}+\Delta v)^{2}\\&={\tfrac {1}{2}}V_{\text{esc}}^{2}+\Delta vV_{\text{esc}}+{\tfrac {1}{2}}\Delta v^{2},\end{aligned}}}
  where 
  
    
      
        V
        =
        
          V
          
            esc
          
        
        +
        Δ
        v
      
    
    {\displaystyle V=V_{\text{esc}}+\Delta v}
  .
When the vehicle leaves the gravity field, the loss of specific kinetic energy is

  
    
      
        
          
            
              1
              2
            
          
        
        
          V
          
            esc
          
          
            2
          
        
        ,
      
    
    {\displaystyle {\tfrac {1}{2}}V_{\text{esc}}^{2},}
  so it retains the energy

  
    
      
        Δ
        v
        
          V
          
            esc
          
        
        +
        
          
            
              1
              2
            
          
        
        Δ
        
          v
          
            2
          
        
        ,
      
    
    {\displaystyle \Delta vV_{\text{esc}}+{\tfrac {1}{2}}\Delta v^{2},}
  which is larger than the energy from a burn outside the gravitational field (
  
    
      
        
          
            
              1
              2
            
          
        
        Δ
        
          v
          
            2
          
        
      
    
    {\displaystyle {\tfrac {1}{2}}\Delta v^{2}}
  ) by

  
    
      
        Δ
        v
        
          V
          
            esc
          
        
        .
      
    
    {\displaystyle \Delta vV_{\text{esc}}.}
  When the vehicle has left the gravity well, it is travelling at a speed

  
    
      
        V
        =
        Δ
        v
        
          
            1
            +
            
              
                
                  2
                  
                    V
                    
                      esc
                    
                  
                
                
                  Δ
                  v
                
              
            
          
        
        .
      
    
    {\displaystyle V=\Delta v{\sqrt {1+{\frac {2V_{\text{esc}}}{\Delta v}}}}.}
  For the case where the added impulse  Δv is small compared to escape velocity, the 1 can be ignored, and the effective Δv of the impulsive burn can be seen to be multiplied by a factor of simply

  
    
      
        
          
            
              
                2
                
                  V
                  
                    esc
                  
                
              
              
                Δ
                v
              
            
          
        
      
    
    {\displaystyle {\sqrt {\frac {2V_{\text{esc}}}{\Delta v}}}}
  and one get 

  
    
      
        V
      
    
    {\displaystyle V}
   ≈ 
  
    
      
        
          
            
              2
              
                V
                
                  esc
                
              
            
            
              Δ
              v
            
          
        
        .
      
    
    {\displaystyle {\sqrt {{2V_{\text{esc}}}{\Delta v}}}.}
  Similar effects happen in closed and hyperbolic orbits.


=== Parabolic example ===
If the vehicle travels at velocity v at the start of a burn that changes the velocity by Δv, then the change in specific orbital energy (SOE) due to the new orbit is

  
    
      
        v
        
        Δ
        v
        +
        
          
            
              1
              2
            
          
        
        (
        Δ
        v
        
          )
          
            2
          
        
        .
      
    
    {\displaystyle v\,\Delta v+{\tfrac {1}{2}}(\Delta v)^{2}.}
  Once the spacecraft is far from the planet again, the SOE is entirely kinetic, since gravitational potential energy approaches zero. Therefore, the larger the v at the time of the burn, the greater the final kinetic energy, and the higher the final velocity.
The effect becomes more pronounced the closer to the central body, or more generally, the deeper in the gravitational field potential in which the burn occurs, since the velocity is higher there.
So if a spacecraft is on a parabolic flyby of Jupiter with a periapsis velocity of 50 km/s and performs a 5 km/s burn, it turns out that the final velocity change at great distance is 22.9 km/s, giving a multiplication of the burn by 4.58 times.


== Paradox ==
It may seem that the rocket is getting energy for free, which would violate conservation of energy. However, any gain to the rocket's kinetic energy is balanced by a relative decrease in the kinetic energy the exhaust is left with (the kinetic energy of the exhaust may still increase, but it does not increase as much). Contrast this to the situation of static firing, where the speed of the engine is fixed at zero. This means that its kinetic energy does not increase at all, and all the chemical energy released by the fuel is converted to the exhaust's kinetic energy (and heat).
At very high speeds the mechanical power imparted to the rocket can exceed the total power liberated in the combustion of the propellant; this may also seem to violate conservation of energy. But the propellants in a fast-moving rocket carry energy not only chemically, but also in their own kinetic energy, which at speeds above a few kilometres per second exceed the chemical component. When these propellants are burned, some of this kinetic energy is transferred to the rocket along with the chemical energy released by burning.The Oberth effect can therefore partly make up for what is extremely low efficiency early in the rocket's flight when it is moving only slowly. Most of the work done by a rocket early in flight is ""invested"" in the kinetic energy of the propellant not yet burned, part of which they will release later when they are burned.


== See also ==

Bi-elliptic transfer
Gravity assist
Propulsive efficiency


== References ==


== External links ==
Oberth effect
Explanation of the effect by Geoffrey Landis.Rocket propulsion, classical relativity, and the Oberth effectAnimation (MP4) of the Oberth effect in orbit from the Blanco and Mungan paper cited above.","pandas(index=80, _1=80, text='in astronautics, a powered flyby, or oberth maneuver, is a maneuver in which a spacecraft falls into a gravitational well and then uses its engines to further accelerate as it is falling, thereby achieving additional speed. the resulting maneuver is a more efficient way to gain kinetic energy than applying the same impulse outside of a gravitational well. the gain in efficiency is explained by the oberth effect, wherein the use of a reaction engine at higher speeds generates a greater change in mechanical energy than its use at lower speeds. in practical terms, this means that the most energy-efficient method for a spacecraft to burn its fuel is at the lowest possible orbital periapsis, when its orbital velocity (and so, its kinetic energy) is greatest. in some cases, it is even worth spending fuel on slowing the spacecraft into a gravity well to take advantage of the efficiencies of the oberth effect. the maneuver and effect are named after the person who first described them in 1927, hermann oberth, an austro-hungarian-born german physicist and a founder of modern rocketry.the oberth effect is strongest at a point in orbit known as the periapsis, where the gravitational potential is lowest, and the speed is highest. this is because a given firing of a rocket engine at high speed causes a greater change in kinetic energy than when fired otherwise similarly at lower speed. because the vehicle remains near periapsis only for a short time, for the oberth maneuver to be most effective the vehicle must be able to generate as much impulse as possible in the shortest possible time. as a result the oberth maneuver is much more useful for high-thrust rocket engines like liquid-propellant rockets, and less useful for low-thrust reaction engines such as ion drives, which take a long time to gain speed. the oberth effect also can be used to understand the behavior of multi-stage rockets: the upper stage can generate much more usable kinetic energy than the total chemical energy of the propellants it carries.in terms of the energies involved, the oberth effect is more effective at higher speeds because at high speed the propellant has significant kinetic energy in addition to its chemical potential energy. at higher speed the vehicle is able to employ the greater change (reduction) in kinetic energy of the propellant (as it is exhausted backwards and hence at reduced speed and hence reduced kinetic energy) to generate a greater increase in kinetic energy of the vehicle.   == explanation in terms of momentum and kinetic energy == a rocket works by transferring momentum to its propellant. at a fixed exhaust velocity, this will be a fixed amount of momentum per unit of propellant. for a given mass of rocket (including remaining propellant), this implies a fixed change in velocity per unit of propellant. because kinetic energy =           1 2   m  v  2           once the spacecraft is far from the planet again, the soe is entirely kinetic, since gravitational potential energy approaches zero. therefore, the larger the v at the time of the burn, the greater the final kinetic energy, and the higher the final velocity. the effect becomes more pronounced the closer to the central body, or more generally, the deeper in the gravitational field potential in which the burn occurs, since the velocity is higher there. so if a spacecraft is on a parabolic flyby of jupiter with a periapsis velocity of 50 km/s and performs a 5 km/s burn, it turns out that the final velocity change at great distance is 22.9 km/s, giving a multiplication of the burn by 4.58 times.   == paradox == it may seem that the rocket is getting energy for free, which would violate conservation of energy. however, any gain to the rocket\'s kinetic energy is balanced by a relative decrease in the kinetic energy the exhaust is left with (the kinetic energy of the exhaust may still increase, but it does not increase as much). contrast this to the situation of static firing, where the speed of the engine is fixed at zero. this means that its kinetic energy does not increase at all, and all the chemical energy released by the fuel is converted to the exhaust\'s kinetic energy (and heat). at very high speeds the mechanical power imparted to the rocket can exceed the total power liberated in the combustion of the propellant; this may also seem to violate conservation of energy. but the propellants in a fast-moving rocket carry energy not only chemically, but also in their own kinetic energy, which at speeds above a few kilometres per second exceed the chemical component. when these propellants are burned, some of this kinetic energy is transferred to the rocket along with the chemical energy released by burning.the oberth effect can therefore partly make up for what is extremely low efficiency early in the rocket\'s flight when it is moving only slowly. most of the work done by a rocket early in flight is ""invested"" in the kinetic energy of the propellant not yet burned, part of which they will release later when they are burned.   == see also ==  bi-elliptic transfer gravity assist propulsive efficiency   == references ==   == external links == oberth effect explanation of the effect by geoffrey landis.rocket propulsion, classical relativity, and the oberth effectanimation (mp4) of the oberth effect in orbit from the blanco and mungan paper cited above.')"
81,"GPS/INS is the use of GPS satellite signals to correct or calibrate a solution from an inertial navigation system (INS). The method is applicable for any GNSS/INS system.


== Overview ==


=== GPS/INS method ===
The GPS gives an absolute drift-free position value that can be used to reset the INS solution or can be blended with it by use of a mathematical algorithm, such as a Kalman filter. The angular orientation of the unit can be inferred from the series of position updates from the GPS. The change in the error in position relative to the GPS can be used to estimate the unknown angle error.
The benefits of using GPS with an INS are that the INS may be calibrated by the GPS signals and that the INS can provide position and angle updates at a quicker rate than GPS. For high dynamic vehicles, such as missiles and aircraft, INS fills in the gaps between GPS positions. Additionally, GPS may lose its signal and the INS can continue to compute the position and angle during the period of lost GPS signal. The two systems are complementary and are often employed together.


== Applications ==
GPS/INS is commonly used on aircraft for navigation purposes. Using GPS/INS allows for smoother position and velocity estimates that can be provided at a sampling rate faster than the GPS receiver.  This also allows for accurate estimation of the aircraft attitude (roll, pitch, and yaw) angles.  In general, GPS/INS sensor fusion is a nonlinear filtering problem, which is commonly approached using the extended Kalman filter (EKF) or the unscented Kalman filter (UKF).  The use of these two filters for GPS/INS has been compared in various sources, including a detailed sensitivity analysis. The EKF uses an analytical linearization approach using Jacobian matrices to linearize the system, while the UKF uses a statistical linearization approach called the unscented transform which uses a set of deterministically selected points to handle the nonlinearity. The UKF requires the calculation of a matrix square root of the state error covariance matrix, which is used to determine the spread of the sigma points for the unscented transform. There are various ways to calculate the matrix square root, which have been presented and compared within GPS/INS application. From this work it is recommended to use the Cholesky decomposition method.
In addition to aircraft applications, GPS/INS has also been studied for automobile applications such as autonomous navigation,  vehicle dynamics control, or sideslip, roll, and tire cornering stiffness estimation.


== See also ==
GNSS Augmentation


== References ==
US Patent No. 6900760","pandas(index=81, _1=81, text='gps/ins is the use of gps satellite signals to correct or calibrate a solution from an inertial navigation system (ins). the method is applicable for any gnss/ins system.   == overview == the gps gives an absolute drift-free position value that can be used to reset the ins solution or can be blended with it by use of a mathematical algorithm, such as a kalman filter. the angular orientation of the unit can be inferred from the series of position updates from the gps. the change in the error in position relative to the gps can be used to estimate the unknown angle error. the benefits of using gps with an ins are that the ins may be calibrated by the gps signals and that the ins can provide position and angle updates at a quicker rate than gps. for high dynamic vehicles, such as missiles and aircraft, ins fills in the gaps between gps positions. additionally, gps may lose its signal and the ins can continue to compute the position and angle during the period of lost gps signal. the two systems are complementary and are often employed together.   == applications == gps/ins is commonly used on aircraft for navigation purposes. using gps/ins allows for smoother position and velocity estimates that can be provided at a sampling rate faster than the gps receiver.  this also allows for accurate estimation of the aircraft attitude (roll, pitch, and yaw) angles.  in general, gps/ins sensor fusion is a nonlinear filtering problem, which is commonly approached using the extended kalman filter (ekf) or the unscented kalman filter (ukf).  the use of these two filters for gps/ins has been compared in various sources, including a detailed sensitivity analysis. the ekf uses an analytical linearization approach using jacobian matrices to linearize the system, while the ukf uses a statistical linearization approach called the unscented transform which uses a set of deterministically selected points to handle the nonlinearity. the ukf requires the calculation of a matrix square root of the state error covariance matrix, which is used to determine the spread of the sigma points for the unscented transform. there are various ways to calculate the matrix square root, which have been presented and compared within gps/ins application. from this work it is recommended to use the cholesky decomposition method. in addition to aircraft applications, gps/ins has also been studied for automobile applications such as autonomous navigation,  vehicle dynamics control, or sideslip, roll, and tire cornering stiffness estimation.   == see also == gnss augmentation   == references == us patent no. 6900760')"
82,"Hypersonic flight is flight through the atmosphere below about 90 km at speeds above Mach 5, a speed where dissociation of air begins to become significant and high heat loads exist.


== History ==
The first manufactured object to achieve hypersonic flight was the two-stage Bumper rocket, consisting of a WAC Corporal second stage set on top of a V-2 first stage. In February 1949, at White Sands, the rocket reached a speed of 8,288.12 km/h (5,150 mph), or approximately Mach 6.7. The vehicle, however, burned on atmospheric re-entry, and only charred remnants were found. In April 1961, Russian Major Yuri Gagarin became the first human to travel at hypersonic speed, during the world's first piloted orbital flight. Soon after, in May 1961, Alan Shepard became the first American and second person to achieve hypersonic flight when his capsule reentered the atmosphere at a speed above Mach 5 at the end of his suborbital flight over the Atlantic Ocean.In November 1961, Air Force Major Robert White flew the X-15 research airplane at speeds over Mach 6.
On 3 October 1967, in California, a X-15 reached Mach 6.7, but by the time the vehicle approached Edwards Air Force Base, intense heating associated with shock waves around the vehicle had partially melted the pylon that attached the ramjet engine to the fuselage.The reentry problem of a space vehicle was extensively studied. 
The NASA X-43A flew on scramjet for 10 seconds, and then glided for 10 minutes on its last flight in 2004.
The Boeing X-51 Waverider flew on scramjet for 210 seconds in 2013, finally reaching Mach 5.1 on its fourth flight test.
The hypersonic regime has since become the subject for further study during the 21st century, and strategic competition between China, India, Russia, and the U.S.


== Physics ==

The stagnation point of air flowing around a body is a point where its local velocity is zero. At this point the air flows around this location. A shock wave forms, which deflects the air from the stagnation point and insulates the flight body from the atmosphere. This can affect the lifting ability of a flight surface to counteract its drag and subsequent free fall. Ning describes a method for interrelating Reynolds number with Mach number.In order to maneuver in the atmosphere at faster speeds than supersonic, the forms of propulsion can still be airbreathing systems, but a ramjet no longer suffices for a system to attain Mach 5, as a ramjet slows down the airflow to subsonic. Some systems (waveriders) use a first stage rocket to boost a body into the hypersonic regime. Other systems (boost-glide vehicles) use scramjets after their initial boost, in which the speed of the air passing through the scramjet remains supersonic. Other systems (munitions) use a cannon for their initial boost.


=== High Temperature Effect ===
Hypersonic flow is a high energy flow. The ratio of kinetic energy to the internal energy of the gas increases as the square of the Mach number. When this flow enters a boundary layer, there are high viscous effects due to the friction between air and the high-speed object. In this case, the high kinetic energy is converted in part to internal energy and gas energy is proportional to the internal energy. Therefore, hypersonic boundary layers are high temperature regions due to the viscous dissipation of the flow's kinetic energy. 
Another region of high temperature flow is the shock layer behind the strong bow shock wave. In the case of the shock layer, the flows velocity decreases discontinuously as it passes through the shock wave. This results in a loss of kinetic energy and a gain of internal energy behind the shock wave. Due to high temperatures behind the shock wave, dissociation of molecules in the air becomes thermally active. For example, for air at T > 2000 K, dissociation of diatomic oxygen into oxygen radicals is active: O2 → 2OFor T > 4000 K, dissociation of diatomic nitrogen into N radicals is active: N2 → 2NConsequently, in this temperature range, molecular dissociation followed by recombination of oxygen and nitrogen radicals produces nitric oxide: N2 + O2 → 2NO, which then dissociates and recombines to form ions: N + O → NO+ + e−


=== Low Density Flow ===
At standard sea-level condition for air, the mean free path of air molecules is about 
  
    
      
        λ
        =
        68
        
          n
          m
        
      
    
    {\displaystyle \lambda =68\mathrm {nm} }
  . Low density air is much thinner. At an altitude of 104 km (342,000 ft) the mean free path is 
  
    
      
        λ
        =
        1
        
        f
        t
        =
        0.305
        
        m
      
    
    {\displaystyle \lambda =1\,ft=0.305\,m}
  . Because of this large free mean path aerodynamic concepts, equations, and results based on the assumption of a continuum begin to break down, therefore aerodynamics must be considered from kinetic theory. This regime of aerodynamics is called low-density flow.
For a given aerodynamic condition low-density effects depends on the value of a nondimensional parameter called the Knudsen number 
  
    
      
        
          K
          
            n
          
        
      
    
    {\displaystyle K_{n}}
  , defined as 
  
    
      
        
          K
          
            n
          
        
        =
        
          
            λ
            l
          
        
      
    
    {\displaystyle K_{n}={\frac {\lambda }{l}}}
   where 
  
    
      
        l
      
    
    {\displaystyle l}
   is the typical length scale of the object considered. The value of the Knudsen number based on nose radius, 
  
    
      
        
          K
          
            n
          
        
        =
        
          
            λ
            R
          
        
      
    
    {\displaystyle K_{n}={\frac {\lambda }{R}}}
  , can be near one.
Hypersonic vehicles frequently fly at very high altitudes and therefore encounter low-density conditions. Hence, the design and analysis of hypersonic vehicles sometimes require consideration of low-density flow. New generations of hypersonic airplanes may spend a considerable portion of their mission at high altitudes, and for these vehicles, low-density effects will become more significant.


=== Thin Shock Layer ===
The flow field between the shock wave and the body surface is called the shock layer. As the Mach number M increases, the angle of the resulting shock wave decreases. This Mach angle is described by the equation 
  
    
      
        μ
        =
        
          sin
          
            −
            1
          
        
        ⁡
        (
        a
        
          /
        
        v
        )
      
    
    {\displaystyle \mu =\sin ^{-1}(a/v)}
   where a is the speed of the sound wave and v is the flow velocity. Since M=v/a, the equation becomes 
  
    
      
        μ
        =
        
          sin
          
            −
            1
          
        
        ⁡
        (
        1
        
          /
        
        M
        )
      
    
    {\displaystyle \mu =\sin ^{-1}(1/M)}
  . Higher Mach numbers position the shock wave closer to the body surface, thus at hypersonic speeds, the shock wave lies extremely close to the body surface, resulting in a thin shock layer. At low Reynolds number, the boundary layer grows quite thick and merges with the shock wave, leading to a fully viscous shock layer.


=== Viscous Interaction ===
The compressible flow boundary layer increases proportionately to the square of the Mach number, and inversely to the square root of the Reynolds number.
At hypersonic speeds, this effect becomes much more pronounced, due to the exponential reliance on the Mach number. Since the boundary layer becomes so large, it interacts more viscously with the surrounding flow. The overall effect of this interaction is to create a much higher skin friction than normal, causing greater surface heat flow. Additionally, the surface pressure spikes, which results in a much larger aerodynamic drag coefficient. This effect is extreme at the leading edge and decreases as a function of length along the surface.


=== Entropy Layer ===
The entropy layer is a region of large velocity gradients caused by the strong curvature of the shock wave. The entropy layer begins at the nose of the aircraft and extends downstream close to the body surface. Downstream of the nose, the entropy layer interacts with the boundary layer which causes an increase in aerodynamic heating at the body surface. Although the shock wave at the nose at supersonic speeds is also curved, the entropy layer is only observed at hypersonic speeds because the magnitude of the curve is far greater at hypersonic speeds.


== Hypersonic weapons development ==

In the last year, China has tested more hypersonic weapons than we have in a decade. We've got to fix that.
Two main types of hypersonic weapons are hypersonic cruise missiles and hypersonic glide vehicles. Hypersonic weapons, by definition, travel five or more times the speed of sound. Hypersonic cruise missiles, which are powered by scramjet, are restricted below 100,000 feet; hypersonic glide vehicles can travel higher. Compared to a ballistic (parabolic) trajectory, a hypersonic vehicle would be capable of large-angle deviations from a parabolic trajectory. According to CNBC, Russia and China lead in hypersonic weapon development, trailed by the United States. India is also developing such weapons. France and Australia may also be pursuing the technology. Japan is acquiring both scramjet (Hypersonic Cruise Missile), and boost-glide weapons (Hyper Velocity Gliding Projectile).Waverider hypersonic weapons delivery is an avenue of development. 
China's XingKong-2 (星空二号, Starry-sky-2), a waverider, had its first flight 3 August 2018.In 2016, Russia is believed to have conducted two successful tests of Avangard, a hypersonic glide vehicle. The third known test, in 2017, failed. In 2018, an Avangard was launched at the Dombarovskiy missile base, reaching its target at the Kura shooting range, a distance of 3700 miles (5955 km).  Avangard uses new composite materials which are to withstand temperatures of up to 2,000 degrees Celsius (3,632 degrees Fahrenheit). The Avangard's environment at hypersonic speeds reaches such temperatures. Russia considered its carbon fiber solution to be unreliable, and replaced it with composite materials. Two Avangard hypersonic glide vehicles (HGVs) will first be mounted on SS-19 ICBMs; on 27 December 2019 the weapon was first fielded to the Yasnensky Missile Division, a unit in the Orenburg Oblast. In an earlier report, Franz-Stefan Gady named the unit as the 13th Regiment/Dombarovskiy Division (Strategic Missile Force).These tests have prompted US responses in weapons development per John Hyten's USSTRATCOM statement 05:03, 8 August 2018 (UTC). At least one vendor is developing ceramics to handle the temperatures of hypersonics systems. There are over a dozen US hypersonics projects as of 2018, notes the commander of USSTRATCOM; from which a future hypersonic cruise missile is sought, perhaps by Q4 FY2021. There are also privately developed hypersonic systems. 
DoD tested a Common Hypersonic Glide Body (C-HGB) in 2020.According to Air Force chief scientist, Dr. Greg Zacharias, the US anticipates having hypersonic weapons by the 2020s, hypersonic drones by the 2030s, and recoverable hypersonic drone aircraft by the 2040s. The focus of DoD development will be on air-breathing boost-glide hypersonics systems. Countering hypersonic weapons during their cruise phase will require radar with longer range, as well as space-based sensors, and systems for tracking and fire control.Rand Corporation (28 September 2017) estimates there is less than a decade to prevent Hypersonic Missile proliferation. 
In the same way that anti-ballistic missiles were developed as countermeasures to ballistic missiles, counter-countermeasures to hypersonics systems were not yet in development, as of 2019. But by 2019, $157.4 million was allocated in the FY2020 Pentagon budget for hypersonic defense, out of $2.6 billion for all hypersonic-related research. Both the US and Russia withdrew from the Intermediate-Range Nuclear Forces (INF) Treaty in February 2019. This will spur arms development, including hypersonic weapons, in FY2021 and forward.Australia and the US have begun joint development of air-launched hypersonic missiles, as announced by a Pentagon statement on 30 November 2020. The development will build on the $54 million Hypersonic International Flight Research Experimentation (HIFiRE) under which both nations collaborated on over a 15-year period. Small and large companies will all contribute to the development of these hypersonic missiles.


== Flown aircraft ==


=== Hypersonic aircraft ===
 Aerojet General X-8
 North American X-15 (crewed)
 Lockheed X-17
 NASA X-43
 Boeing X-51
 DF-ZF
 Avangard
 HSTDV


=== Spaceplanes ===
 Space Shuttle orbiter (crewed)
 Buran (human-rated, only flew without crew)
 RLV-TD
 Boeing X-37
 Shenlong
 IXV
 BOR-4
 Martin X-23 PRIME
 Martin X-24 (crewed)
 ASSET
 HYFLEX
 Chongfu Shiyong Shiyan Hangtian Qi (disputed)
 Jiageng-1


== Cancelled aircraft ==


=== Hypersonic aircraft ===
 Silbervogel (Sänger bomber)
 Keldysh bomber
 Tupolev Tu-360, follow-on to Tu-160
 Tupolev Tu-2000
 Lockheed L-301


=== Spaceplanes ===
 Boeing X-20 Dyna-Soar
 Rockwell X-30 (National Aerospace Plane)
 Orbital Sciences X-34
 Mikoyan-Gurevich MiG-105
 Tsien Spaceplane 1949
 HOPE-X
 XCOR Lynx
 Lockheed Martin X-33
 Hermes
 Prometheus
 HL-20 Personnel Launch System
 HL-42
 BAC Mustard
 Kliper
 HOTOL
 Valier Raketenschiff
 Rockwell C-1057


== Developing and proposed aircraft ==


=== Hypersonic aircraft ===
 I-Plane
 14-X
 Avatar (spacecraft)
 Advanced Technology Vehicle
 DARPA XS-1
 Dream Chaser
 NASA X-43
 HyperSoar
 HyperStar hypersonic passenger airliner
 Falcon HTV-2
 Boeing Commercial Airplanes hypersonic airliner Concept
 Lockheed Martin SR-72
 Tactical Boost Glide Vehicle
 Kholod
 Programme for Reusable In-orbit Demonstrator in Europe (PRIDE)
 Sänger II
 HyShot
 Hytex
 Horus
 SHEFEX
 Skylon
 Reaction Engines A2
 Spartan
 HEXAFLY
 SpaceLiner
 STRATOFLY
 Zero Emission Hyper Sonic Transport


=== Cruise missiles and warheads ===
 Advanced Hypersonic Weapon
 AGM-183A air launched rapid response weapon (ARRW, pronounced ""arrow"") Telemetry data has been successfully transmitted from ARRW —AGM-183A IMV-2 (Instrumented Measurement Vehicle) to the Point Mugu ground stations. Hundreds of ARRWs are being sought by the Air Force.
 Expendable Hypersonic Air-Breathing Multi-Mission Demonstrator (""Mayhem"") Based on HAWC and HSSW: ""solid rocket-boosted, air-breathing, hypersonic conventional cruise missile"", a follow-on to AGM-183A
 Hypersonic air-breathing weapon concept (HAWC, pronounced ""hawk"") It is easier to put a seeker on an air-breathing vehicle.
 Hypersonic conventional strike weapon (HCSW, pronounced ""hacksaw"") was cancelled 10 Feb 2020. HCSW was one of the boost-glide systems under development by the US.
 Kh-45 (cancelled)
 Avangard
 Kinzhal
 Zircon
 Hypersonic Technology Demonstrator Vehicle
HGV-202F Hypersonic glide vehicle

/ Brahmos-II
 DF-ZF


== See also ==
Hypersonic speed
Supersonic transport
Lifting body
Atmospheric entry
Boost-glide
Scramjet
Ramjet
List of X-planes
Thunderbird 1


== References ==


== External links ==
A comparative analysis of the performance of long-range hypervelocity vehicles
(2016) Joint Air Power Competence Centre (JAPCC)","pandas(index=82, _1=82, text='hypersonic flight is flight through the atmosphere below about 90 km at speeds above mach 5, a speed where dissociation of air begins to become significant and high heat loads exist.   == history == the first manufactured object to achieve hypersonic flight was the two-stage bumper rocket, consisting of a wac corporal second stage set on top of a v-2 first stage. in february 1949, at white sands, the rocket reached a speed of 8,288.12 km/h (5,150 mph), or approximately mach 6.7. the vehicle, however, burned on atmospheric re-entry, and only charred remnants were found. in april 1961, russian major yuri gagarin became the first human to travel at hypersonic speed, during the world\'s first piloted orbital flight. soon after, in may 1961, alan shepard became the first american and second person to achieve hypersonic flight when his capsule reentered the atmosphere at a speed above mach 5 at the end of his suborbital flight over the atlantic ocean.in november 1961, air force major robert white flew the x-15 research airplane at speeds over mach 6. on 3 october 1967, in california, a x-15 reached mach 6.7, but by the time the vehicle approached edwards air force base, intense heating associated with shock waves around the vehicle had partially melted the pylon that attached the ramjet engine to the fuselage.the reentry problem of a space vehicle was extensively studied. the nasa x-43a flew on scramjet for 10 seconds, and then glided for 10 minutes on its last flight in 2004. the boeing x-51 waverider flew on scramjet for 210 seconds in 2013, finally reaching mach 5.1 on its fourth flight test. the hypersonic regime has since become the subject for further study during the 21st century, and strategic competition between china, india, russia, and the u.s.   == physics ==  the stagnation point of air flowing around a body is a point where its local velocity is zero. at this point the air flows around this location. a shock wave forms, which deflects the air from the stagnation point and insulates the flight body from the atmosphere. this can affect the lifting ability of a flight surface to counteract its drag and subsequent free fall. ning describes a method for interrelating reynolds number with mach number.in order to maneuver in the atmosphere at faster speeds than supersonic, the forms of propulsion can still be airbreathing systems, but a ramjet no longer suffices for a system to attain mach 5, as a ramjet slows down the airflow to subsonic. some systems (waveriders) use a first stage rocket to boost a body into the hypersonic regime. other systems (boost-glide vehicles) use scramjets after their initial boost, in which the speed of the air passing through the scramjet remains supersonic. other systems (munitions) use a cannon for their initial boost. advanced hypersonic weapon agm-183a air launched rapid response weapon (arrw, pronounced ""arrow"") telemetry data has been successfully transmitted from arrw —agm-183a imv-2 (instrumented measurement vehicle) to the point mugu ground stations. hundreds of arrws are being sought by the air force. expendable hypersonic air-breathing multi-mission demonstrator (""mayhem"") based on hawc and hssw: ""solid rocket-boosted, air-breathing, hypersonic conventional cruise missile"", a follow-on to agm-183a hypersonic air-breathing weapon concept (hawc, pronounced ""hawk"") it is easier to put a seeker on an air-breathing vehicle. hypersonic conventional strike weapon (hcsw, pronounced ""hacksaw"") was cancelled 10 feb 2020. hcsw was one of the boost-glide systems under development by the us. kh-45 (cancelled) avangard kinzhal zircon hypersonic technology demonstrator vehicle hgv-202f hypersonic glide vehicle  / brahmos-ii df-zf   == see also == hypersonic speed supersonic transport lifting body atmospheric entry boost-glide scramjet ramjet list of x-planes thunderbird 1   == references ==   == external links == a comparative analysis of the performance of long-range hypervelocity vehicles (2016) joint air power competence centre (japcc)')"
83,"In aerodynamics, the normal shock tables are a series of tabulated data listing the various properties before and after the occurrence of a normal shock wave.  With a given upstream Mach number, the post-shock Mach number can be calculated along with the pressure, density, temperature, and stagnation pressure ratios.  Such tables are useful since the equations used to calculate the properties after a normal shock are cumbersome.
The tables below have been calculated using a heat capacity ratio, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , equal to 1.4.  The upstream Mach number, 
  
    
      
        
          M
          
            1
          
        
      
    
    {\displaystyle M_{1}}
  , begins at 1 and ends at 5.  Although the tables could be extended over any range of Mach numbers, stopping at Mach 5 is typical since assuming 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
   to be 1.4 over the entire Mach number range leads to errors over 10% beyond Mach 5.


== Normal shock table equations ==
Given an upstream Mach number, 
  
    
      
        
          M
          
            1
          
        
      
    
    {\displaystyle M_{1}}
  , and the ratio of specific heats, 
  
    
      
        γ
      
    
    {\displaystyle \gamma }
  , the post normal shock Mach number, 
  
    
      
        
          M
          
            2
          
        
      
    
    {\displaystyle M_{2}}
  , can be calculated using the equation below.

  
    
      
        
          M
          
            2
          
        
        =
        
          
            
              
                
                  M
                  
                    1
                  
                  
                    2
                  
                
                
                  (
                  
                    γ
                    −
                    1
                  
                  )
                
                +
                2
              
              
                2
                γ
                
                  M
                  
                    1
                  
                  
                    2
                  
                
                −
                
                  (
                  
                    γ
                    −
                    1
                  
                  )
                
              
            
          
        
      
    
    {\displaystyle M_{2}={\sqrt {\frac {M_{1}^{2}\left(\gamma -1\right)+2}{2\gamma M_{1}^{2}-\left(\gamma -1\right)}}}}
  The next equation shows the relationship between the post normal shock pressure, 
  
    
      
        
          p
          
            2
          
        
      
    
    {\displaystyle p_{2}}
  , and the upstream ambient pressure, 
  
    
      
        
          p
          
            1
          
        
      
    
    {\displaystyle p_{1}}
  .

  
    
      
        
          
            
              p
              
                2
              
            
            
              p
              
                1
              
            
          
        
        =
        
          
            
              2
              γ
              
                M
                
                  1
                
                
                  2
                
              
            
            
              γ
              +
              1
            
          
        
        −
        
          
            
              γ
              −
              1
            
            
              γ
              +
              1
            
          
        
      
    
    {\displaystyle {\frac {p_{2}}{p_{1}}}={\frac {2\gamma M_{1}^{2}}{\gamma +1}}-{\frac {\gamma -1}{\gamma +1}}}
  The relationship between the post normal shock density, 
  
    
      
        
          ρ
          
            2
          
        
      
    
    {\displaystyle \rho _{2}}
  , and the upstream ambient density, 
  
    
      
        
          ρ
          
            1
          
        
      
    
    {\displaystyle \rho _{1}}
   is shown next in the tables.

  
    
      
        
          
            
              ρ
              
                2
              
            
            
              ρ
              
                1
              
            
          
        
        =
        
          
            
              
                (
                
                  γ
                  +
                  1
                
                )
              
              
                M
                
                  1
                
                
                  2
                
              
            
            
              
                (
                
                  γ
                  −
                  1
                
                )
              
              
                M
                
                  1
                
                
                  2
                
              
              +
              2
            
          
        
      
    
    {\displaystyle {\frac {\rho _{2}}{\rho _{1}}}={\frac {\left(\gamma +1\right)M_{1}^{2}}{\left(\gamma -1\right)M_{1}^{2}+2}}}
  Next, the equation below shows the relationship between the post normal shock temperature, 
  
    
      
        
          T
          
            2
          
        
      
    
    {\displaystyle T_{2}}
  , and the upstream ambient temperature, 
  
    
      
        
          T
          
            1
          
        
      
    
    {\displaystyle T_{1}}
  .

  
    
      
        
          
            
              T
              
                2
              
            
            
              T
              
                1
              
            
          
        
        =
        
          
            
              
                (
                
                  1
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
                )
              
              
                (
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        −
                        1
                      
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                  −
                  1
                
                )
              
            
            
              
                M
                
                  1
                
                
                  2
                
              
              
                (
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        −
                        1
                      
                    
                  
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                
                )
              
            
          
        
      
    
    {\displaystyle {\frac {T_{2}}{T_{1}}}={\frac {\left(1+{\frac {\gamma -1}{2}}M_{1}^{2}\right)\left({\frac {2\gamma }{\gamma -1}}M_{1}^{2}-1\right)}{M_{1}^{2}\left({\frac {2\gamma }{\gamma -1}}+{\frac {\gamma -1}{2}}\right)}}}
  Finally, the ratio of stagnation pressures is shown below where 
  
    
      
        
          p
          
            01
          
        
      
    
    {\displaystyle p_{01}}
   is the upstream stagnation pressure and 
  
    
      
        
          p
          
            02
          
        
      
    
    {\displaystyle p_{02}}
   occurs after the normal shock.  The ratio of stagnation temperatures remains constant across a normal shock since the process is adiabatic.

  
    
      
        
          
            
              p
              
                02
              
            
            
              p
              
                01
              
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      
                        γ
                        +
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
                
                  1
                  +
                  
                    
                      
                        γ
                        −
                        1
                      
                      2
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                
              
            
            )
          
          
            
              γ
              
                γ
                −
                1
              
            
          
        
        
          
            (
            
              
                1
                
                  
                    
                      
                        2
                        γ
                      
                      
                        γ
                        +
                        1
                      
                    
                  
                  
                    M
                    
                      1
                    
                    
                      2
                    
                  
                  −
                  
                    
                      
                        γ
                        −
                        1
                      
                      
                        γ
                        +
                        1
                      
                    
                  
                
              
            
            )
          
          
            
              1
              
                γ
                −
                1
              
            
          
        
      
    
    {\displaystyle {\frac {p_{02}}{p_{01}}}=\left({\frac {{\frac {\gamma +1}{2}}M_{1}^{2}}{1+{\frac {\gamma -1}{2}}M_{1}^{2}}}\right)^{\frac {\gamma }{\gamma -1}}\left({\frac {1}{{\frac {2\gamma }{\gamma +1}}M_{1}^{2}-{\frac {\gamma -1}{\gamma +1}}}}\right)^{\frac {1}{\gamma -1}}}
  


== The normal shock tables (for γ = 1.4) ==


== References ==


== See also ==
Normal shock
Mach number
Compressible flow


== External links ==
University of Cincinnati shock relations calculator","pandas(index=83, _1=83, text='in aerodynamics, the normal shock tables are a series of tabulated data listing the various properties before and after the occurrence of a normal shock wave.  with a given upstream mach number, the post-shock mach number can be calculated along with the pressure, density, temperature, and stagnation pressure ratios.  such tables are useful since the equations used to calculate the properties after a normal shock are cumbersome. the tables below have been calculated using a heat capacity ratio,    γ       == the normal shock tables (for γ = 1.4) ==   == references ==   == see also == normal shock mach number compressible flow   == external links == university of cincinnati shock relations calculator')"
84,"In flight dynamics, longitudinal static stability is the stability of an aircraft in the longitudinal, or pitching, plane under steady flight conditions. This characteristic is important in determining whether a human pilot will be able to control the aircraft in the pitching plane without requiring excessive attention or excessive strength.


== Static stability ==

As any vehicle moves it will be subjected to minor changes in the forces that act on it, and in its speed.

If such a change causes further changes that tend to restore the vehicle to its original speed and orientation, without human or machine input, the vehicle is said to be statically stable. The aircraft has positive stability.
If such a change causes further changes that tend to drive the vehicle away from its original speed and orientation, the vehicle is said to be statically unstable.  The aircraft has negative stability.
If such a change causes no tendency for the vehicle to be restored to its original speed and orientation, and no tendency for the vehicle to be driven away from its original speed and orientation, the vehicle is said to be neutrally stable.  The aircraft has zero stability.For a vehicle to possess positive static stability it is not necessary for its speed and orientation to return to exactly the speed and orientation that existed before the minor change that caused the upset.  It is sufficient that the speed and orientation do not continue to diverge but undergo at least a small change back towards the original speed and orientation.


== Longitudinal stability ==
The longitudinal stability of an aircraft, also called pitch stability, refers to the aircraft's stability in its plane of symmetry, about the lateral axis (the axis along the wingspan). One important aspect of the handling qualities of the aircraft, it is one of the main factors determining the ease with which the pilot is able to maintain trim.If an aircraft is longitudinally stable, a small increase in angle of attack will create a negative (nose-down) pitching moment on the aircraft so that the angle of attack decreases.  Similarly, a small decrease in angle of attack will create a positive (nose-up) pitching moment so that the angle of attack increases.Unlike motion about the other two axes and in the other degrees of freedom of the aircraft (sideslip translation, rotation in roll, rotation in yaw), which are usually heavily coupled, motion in the longitudinal degrees of freedom is planar and can be treated as two-dimensional.


== The pilot's task ==
The pilot of an aircraft with positive longitudinal stability, whether it is a human pilot or an autopilot, has an easy task to fly the aircraft and maintain the desired pitch attitude which, in turn, makes it easy to control the speed, angle of attack and fuselage angle relative to the horizon.  The pilot of an aircraft with negative longitudinal stability has a more difficult task to fly the aircraft.  It will be necessary for the pilot devote more effort, make more frequent inputs to the elevator control, and make larger inputs, in an attempt to maintain the desired pitch attitude.Most successful aircraft have positive longitudinal stability, providing the aircraft's center of gravity lies within the approved range.  Some aerobatic and combat aircraft have low-positive or neutral stability to provide high maneuverability.  Some advanced aircraft have a form of low-negative stability called relaxed stability to provide extra-high maneuverability.


== Center of gravity ==
The longitudinal static stability of an aircraft is significantly influenced by the distance (moment arm or lever arm) between the centre of gravity (c.g.) and the aerodynamic centre of the airplane.  The c.g. is established by the design of the airplane and influenced by its loading, as by payload, passengers, etc.  The aerodynamic centre (a.c.) of the airplane can be located approximately by taking the algebraic sum of the plan-view areas fore and aft of the c.g. multiplied by their blended moment arms and divided by their areas, in a manner analogous to the method of locating the c.g. itself.  In conventional aircraft, this point is aft of, but close to, the one-quarter-chord point of the wing.  In unconventional aircraft, e.g. the Quickie, it is between the two wings because the aft wing is so large.  The pitching moment at the a.c. is typically negative and constant.
The a.c. of an airplane typically does not change with loading or other changes; but the c.g. does, as noted above.  If the c.g. moves forward, the airplane becomes more stable (greater moment arm between the a.c. and the c.g.), and if too far forward will cause the airplane to be difficult for the pilot to bring nose-up as for landing.  If the c.g. is too far aft, the moment arm between it and the a.c. diminishes, reducing the inherent stability of the airplane and in the extreme going negative and rendering the airplane longitudinally unstable; see the diagram below.
Accordingly, the operating handbook for every airplane specifies the range over which the c.g. is permitted to move.  Inside this range, the airplane is considered to be inherently stable, which is to say that it will self-correct longitudinal (pitch) disturbances without pilot input.


== Analysis ==
Near the cruise condition most of the lift force is generated by the wings, with ideally only a small amount generated by the fuselage and tail. We may analyse the longitudinal static stability by considering the aircraft in equilibrium under wing lift, tail force, and weight.  The moment equilibrium condition is called trim, and we are generally interested in the longitudinal stability of the aircraft about this trim condition.

Equating forces in the vertical direction:

  
    
      
        W
        =
        
          L
          
            w
          
        
        +
        
          L
          
            t
          
        
      
    
    {\displaystyle W=L_{w}+L_{t}}
  where W is the weight, 
  
    
      
        
          L
          
            w
          
        
      
    
    {\displaystyle L_{w}}
   is the wing lift and 
  
    
      
        
          L
          
            t
          
        
      
    
    {\displaystyle L_{t}}
   is the tail force.
For a thin airfoil at low angle of attack, the wing lift is proportional to the angle of attack:

  
    
      
        
          L
          
            w
          
        
        =
        q
        
          S
          
            w
          
        
        
          
            
              ∂
              
                C
                
                  L
                
              
            
            
              ∂
              α
            
          
        
        (
        α
        +
        
          α
          
            0
          
        
        )
      
    
    {\displaystyle L_{w}=qS_{w}{\frac {\partial C_{L}}{\partial \alpha }}(\alpha +\alpha _{0})}
  where 
  
    
      
        
          S
          
            w
          
        
      
    
    {\displaystyle S_{w}}
   is the wing area 
  
    
      
        
          C
          
            L
          
        
      
    
    {\displaystyle C_{L}}
   is the (wing) lift coefficient, 
  
    
      
        α
      
    
    {\displaystyle \alpha }
   is the angle of attack.  The term 
  
    
      
        
          α
          
            0
          
        
      
    
    {\displaystyle \alpha _{0}}
   is included to account for camber, which results in lift at zero angle of attack. Finally 
  
    
      
        q
      
    
    {\displaystyle q}
   is the dynamic pressure:

  
    
      
        q
        =
        
          
            1
            2
          
        
        ρ
        
          v
          
            2
          
        
      
    
    {\displaystyle q={\frac {1}{2}}\rho v^{2}}
  where 
  
    
      
        ρ
      
    
    {\displaystyle \rho }
   is the air density and 
  
    
      
        v
      
    
    {\displaystyle v}
   is the speed.


=== Trim ===
The force from the tailplane is proportional to its angle of attack, including the effects of any elevator deflection and any adjustment the pilot has made to trim-out any stick force.  In addition, the tail is located in the flow field of the main wing, and consequently experiences  downwash, reducing its angle of attack.
In a statically stable aircraft of conventional (tail in rear) configuration, the tailplane force may act upward or downward depending on the design and the flight conditions. In a typical canard aircraft both fore and aft planes are lifting surfaces. The fundamental requirement for static stability is that the aft surface must have greater authority (leverage) in restoring a disturbance than the forward surface has in exacerbating it.  This leverage is a product of moment arm from the center of mass and surface area. Correctly balanced in this way, the partial derivative of pitching moment with respect to changes in angle of attack will be negative: a momentary pitch up to a larger angle of attack makes the resultant pitching moment tend to pitch the aircraft back down. (Here, pitch is used casually for the angle between the nose and the direction of the airflow; angle of attack.) This is the ""stability derivative"" d(M)/d(alpha), described below.
The tail force is, therefore:

  
    
      
        
          L
          
            t
          
        
        =
        q
        
          S
          
            t
          
        
        
          (
          
            
              
                
                  ∂
                  
                    C
                    
                      l
                    
                  
                
                
                  ∂
                  α
                
              
            
            
              (
              
                α
                −
                
                  
                    
                      ∂
                      ϵ
                    
                    
                      ∂
                      α
                    
                  
                
                α
              
              )
            
            +
            
              
                
                  ∂
                  
                    C
                    
                      l
                    
                  
                
                
                  ∂
                  η
                
              
            
            η
          
          )
        
      
    
    {\displaystyle L_{t}=qS_{t}\left({\frac {\partial C_{l}}{\partial \alpha }}\left(\alpha -{\frac {\partial \epsilon }{\partial \alpha }}\alpha \right)+{\frac {\partial C_{l}}{\partial \eta }}\eta \right)}
  where 
  
    
      
        
          S
          
            t
          
        
        
      
    
    {\displaystyle S_{t}\!}
   is the tail area, 
  
    
      
        
          C
          
            l
          
        
        
      
    
    {\displaystyle C_{l}\!}
   is the tail force coefficient, 
  
    
      
        η
        
      
    
    {\displaystyle \eta \!}
   is the elevator deflection, and 
  
    
      
        ϵ
        
      
    
    {\displaystyle \epsilon \!}
   is the downwash angle.
A canard aircraft may have its foreplane rigged at a high angle of incidence, which can be seen in a canard catapult glider from a toy store; the design puts the c.g. well forward, requiring nose-up lift.
Violations of the basic principle are exploited in some high performance ""relaxed static stability"" combat aircraft to enhance agility; artificial stability is supplied by active electronic means.
There are a few classical cases where this favorable response was not achieved, notably in T-tail configurations. A T-tail airplane has a higher horizontal tail that passes through the wake of the wing later (at a higher angle of attack) than a lower tail would, and at this point the wing has already stalled and has a much larger separated wake. Inside the separated wake, the tail sees little to no freestream and loses effectiveness. Elevator control power is also heavily reduced or even lost, and the pilot is unable to easily escape the stall. This phenomenon is known as 'deep stall'.
Taking moments about the center of gravity, the net nose-up moment is:

  
    
      
        M
        =
        
          L
          
            w
          
        
        
          x
          
            g
          
        
        −
        (
        
          l
          
            t
          
        
        −
        
          x
          
            g
          
        
        )
        
          L
          
            t
          
        
        
      
    
    {\displaystyle M=L_{w}x_{g}-(l_{t}-x_{g})L_{t}\!}
  where 
  
    
      
        
          x
          
            g
          
        
        
      
    
    {\displaystyle x_{g}\!}
   is the location of the center of gravity behind the aerodynamic center of the main wing, 
  
    
      
        
          l
          
            t
          
        
        
      
    
    {\displaystyle l_{t}\!}
   is the tail moment arm.
For trim, this moment must be zero.  For a given maximum elevator deflection, there is a corresponding limit on center of gravity position at which the aircraft can be kept in equilibrium.  When limited by control deflection this is known as a 'trim limit'.  In principle trim limits could determine the permissible forwards and rearwards shift of the centre of gravity, but usually it is only the forward cg limit which is determined by the available control, the aft limit is usually dictated by stability.
In a missile context 'trim limit' more usually refers to the maximum angle of attack, and hence lateral acceleration which can be generated.


=== Static stability ===
The nature of stability may be examined by considering the increment in pitching moment with change in angle of attack at the trim condition.  If this is nose up, the aircraft is longitudinally unstable; if nose down it is stable.  Differentiating the moment equation with respect to 
  
    
      
        α
      
    
    {\displaystyle \alpha }
  :

  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
        =
        
          x
          
            g
          
        
        
          
            
              ∂
              
                L
                
                  w
                
              
            
            
              ∂
              α
            
          
        
        −
        (
        
          l
          
            t
          
        
        −
        
          x
          
            g
          
        
        )
        
          
            
              ∂
              
                L
                
                  t
                
              
            
            
              ∂
              α
            
          
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}=x_{g}{\frac {\partial L_{w}}{\partial \alpha }}-(l_{t}-x_{g}){\frac {\partial L_{t}}{\partial \alpha }}}
  Note: 
  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}}
   is a stability derivative.
It is convenient to treat total lift as acting at a distance h ahead of the centre of gravity, so that the moment equation may be written:

  
    
      
        M
        =
        h
        (
        
          L
          
            w
          
        
        +
        
          L
          
            t
          
        
        )
        
      
    
    {\displaystyle M=h(L_{w}+L_{t})\!}
  Applying the increment in angle of attack:

  
    
      
        
          
            
              ∂
              M
            
            
              ∂
              α
            
          
        
        =
        h
        
          (
          
            
              
                
                  ∂
                  
                    L
                    
                      w
                    
                  
                
                
                  ∂
                  α
                
              
            
            +
            
              
                
                  ∂
                  
                    L
                    
                      t
                    
                  
                
                
                  ∂
                  α
                
              
            
          
          )
        
      
    
    {\displaystyle {\frac {\partial M}{\partial \alpha }}=h\left({\frac {\partial L_{w}}{\partial \alpha }}+{\frac {\partial L_{t}}{\partial \alpha }}\right)}
  Equating the two expressions for moment increment:

  
    
      
        h
        =
        
          x
          
            g
          
        
        −
        
          l
          
            t
          
        
        
          
            
              
                ∂
                
                  L
                  
                    t
                  
                
              
              
                ∂
                α
              
            
            
              
                
                  
                    ∂
                    
                      L
                      
                        w
                      
                    
                  
                  
                    ∂
                    α
                  
                
              
              +
              
                
                  
                    ∂
                    
                      L
                      
                        t
                      
                    
                  
                  
                    ∂
                    α
                  
                
              
            
          
        
      
    
    {\displaystyle h=x_{g}-l_{t}{\frac {\frac {\partial L_{t}}{\partial \alpha }}{{\frac {\partial L_{w}}{\partial \alpha }}+{\frac {\partial L_{t}}{\partial \alpha }}}}}
  The total lift 
  
    
      
        L
      
    
    {\displaystyle L}
   is the sum of 
  
    
      
        
          L
          
            w
          
        
      
    
    {\displaystyle L_{w}}
   and 
  
    
      
        
          L
          
            t
          
        
      
    
    {\displaystyle L_{t}}
   so the sum in the denominator can be simplified and written as the derivative of the total lift due to angle of attack, yielding:

  
    
      
        h
        =
        
          
            
              x
              
                g
              
            
            c
          
        
        −
        
          (
          
            1
            −
            
              
                
                  ∂
                  ϵ
                
                
                  ∂
                  α
                
              
            
          
          )
        
        
          
            
              
                ∂
                
                  C
                  
                    l
                  
                
              
              
                ∂
                α
              
            
            
              
                ∂
                
                  C
                  
                    L
                  
                
              
              
                ∂
                α
              
            
          
        
        
          
            
              
                l
                
                  t
                
              
              
                S
                
                  t
                
              
            
            
              c
              
                S
                
                  w
                
              
            
          
        
      
    
    {\displaystyle h={\frac {x_{g}}{c}}-\left(1-{\frac {\partial \epsilon }{\partial \alpha }}\right){\frac {\frac {\partial C_{l}}{\partial \alpha }}{\frac {\partial C_{L}}{\partial \alpha }}}{\frac {l_{t}S_{t}}{cS_{w}}}}
  Where c is the mean aerodynamic chord of the main wing.  The term:

  
    
      
        
          V
          
            t
          
        
        =
        
          
            
              
                l
                
                  t
                
              
              
                S
                
                  t
                
              
            
            
              c
              
                S
                
                  w
                
              
            
          
        
      
    
    {\displaystyle V_{t}={\frac {l_{t}S_{t}}{cS_{w}}}}
  is known as the tail volume ratio.  Its rather complicated coefficient, the ratio of the two lift derivatives, has values in the range of 0.50 to 0.65 for typical configurations, according to Piercy.  Hence the expression for h may be written more compactly, though somewhat approximately, as:

  
    
      
        h
        =
        
          x
          
            g
          
        
        −
        0.5
        c
        
          V
          
            t
          
        
        
      
    
    {\displaystyle h=x_{g}-0.5cV_{t}\!}
  h is known as the static margin.  For stability it must be negative. (However, for consistency of language, the static margin is sometimes taken as 
  
    
      
        −
        h
      
    
    {\displaystyle -h}
  , so that positive stability is associated with positive static margin.)


== Neutral point ==
A mathematical analysis of the longitudinal static stability of a complete aircraft (including horizontal stabilizer) yields the position of center of gravity at which stability is neutral.  This position is called the neutral point.  (The larger the area of the horizontal stabilizer, and the greater the moment arm of the horizontal stabilizer about the aerodynamic center, the further aft is the neutral point.)
The static center of gravity margin (c.g. margin) or static margin is the distance between the center of gravity (or mass) and the neutral point.  It is usually quoted as a percentage of the Mean Aerodynamic Chord.  The center of gravity must lie ahead of the neutral point for positive stability (positive static margin). If the center of gravity is behind the neutral point, the aircraft is longitudinally unstable (the static margin is negative), and active inputs to the control surfaces are required to maintain stable flight.  Some combat aircraft that are controlled by fly-by-wire systems are designed to be longitudinally unstable so they will be highly maneuverable.  Ultimately, the position of the center of gravity relative to the neutral point determines the stability, control forces, and controllability of the vehicle.For a tailless aircraft 
  
    
      
        
          V
          
            t
          
        
        =
        0
      
    
    {\displaystyle V_{t}=0}
  , the neutral point coincides with the aerodynamic center, and so for longitudinal static stability the center of gravity must lie ahead of the aerodynamic center.


== Longitudinal dynamic stability ==
An aircraft's static stability is an important, but not sufficient, measure of its handling characteristics, and whether it can be flown with ease and comfort by a human pilot. In particular, the longitudinal dynamic stability of a statically stable aircraft will determine whether or not it is finally able to return to its original position.


== See also ==
Directional stability
Flight dynamics
Handling qualities


== Notes ==


== References ==
Clancy, L.J. (1975), Aerodynamics, Pitman Publishing Limited, London.  ISBN 0-273-01120-0
Hurt, H.H. Jr, (1960), Aerodynamics for Naval Aviators Chapter 4, A National Flightshop Reprint, Florida.
Irving, F.G. (1966), An Introduction to the Longitudinal Static Stability of Low-Speed Aircraft, Pergamon Press, Oxford, UK.
McCormick, B.W., (1979), Aerodynamics, Aeronautics, and Flight Mechanics, Chapter 8, John Wiley and Sons, Inc., New York NY.
Perkins, C.D. and Hage, R.E., (1949), Airplane Performance Stability and Control, Chapter 5, John Wiley and Sons, Inc., New York NY.
Piercy, N.A.V. (1944), Elementary Aerodynamics, The English Universities Press Ltd., London.
Stengel R F: Flight Dynamics. Princeton University Press 2004, ISBN 0-691-11407-2.","pandas(index=84, _1=84, text='in flight dynamics, longitudinal static stability is the stability of an aircraft in the longitudinal, or pitching, plane under steady flight conditions. this characteristic is important in determining whether a human pilot will be able to control the aircraft in the pitching plane without requiring excessive attention or excessive strength.   == static stability ==  as any vehicle moves it will be subjected to minor changes in the forces that act on it, and in its speed.  if such a change causes further changes that tend to restore the vehicle to its original speed and orientation, without human or machine input, the vehicle is said to be statically stable. the aircraft has positive stability. if such a change causes further changes that tend to drive the vehicle away from its original speed and orientation, the vehicle is said to be statically unstable.  the aircraft has negative stability. if such a change causes no tendency for the vehicle to be restored to its original speed and orientation, and no tendency for the vehicle to be driven away from its original speed and orientation, the vehicle is said to be neutrally stable.  the aircraft has zero stability.for a vehicle to possess positive static stability it is not necessary for its speed and orientation to return to exactly the speed and orientation that existed before the minor change that caused the upset.  it is sufficient that the speed and orientation do not continue to diverge but undergo at least a small change back towards the original speed and orientation.   == longitudinal stability == the longitudinal stability of an aircraft, also called pitch stability, refers to the aircraft\'s stability in its plane of symmetry, about the lateral axis (the axis along the wingspan). one important aspect of the handling qualities of the aircraft, it is one of the main factors determining the ease with which the pilot is able to maintain trim.if an aircraft is longitudinally stable, a small increase in angle of attack will create a negative (nose-down) pitching moment on the aircraft so that the angle of attack decreases.  similarly, a small decrease in angle of attack will create a positive (nose-up) pitching moment so that the angle of attack increases.unlike motion about the other two axes and in the other degrees of freedom of the aircraft (sideslip translation, rotation in roll, rotation in yaw), which are usually heavily coupled, motion in the longitudinal degrees of freedom is planar and can be treated as two-dimensional.   == the pilot\'s task == the pilot of an aircraft with positive longitudinal stability, whether it is a human pilot or an autopilot, has an easy task to fly the aircraft and maintain the desired pitch attitude which, in turn, makes it easy to control the speed, angle of attack and fuselage angle relative to the horizon.  the pilot of an aircraft with negative longitudinal stability has a more difficult task to fly the aircraft.  it will be necessary for the pilot devote more effort, make more frequent inputs to the elevator control, and make larger inputs, in an attempt to maintain the desired pitch attitude.most successful aircraft have positive longitudinal stability, providing the aircraft\'s center of gravity lies within the approved range.  some aerobatic and combat aircraft have low-positive or neutral stability to provide high maneuverability.  some advanced aircraft have a form of low-negative stability called relaxed stability to provide extra-high maneuverability.   == center of gravity == the longitudinal static stability of an aircraft is significantly influenced by the distance (moment arm or lever arm) between the centre of gravity (c.g.) and the aerodynamic centre of the airplane.  the c.g. is established by the design of the airplane and influenced by its loading, as by payload, passengers, etc.  the aerodynamic centre (a.c.) of the airplane can be located approximately by taking the algebraic sum of the plan-view areas fore and aft of the c.g. multiplied by their blended moment arms and divided by their areas, in a manner analogous to the method of locating the c.g. itself.  in conventional aircraft, this point is aft of, but close to, the one-quarter-chord point of the wing.  in unconventional aircraft, e.g. the quickie, it is between the two wings because the aft wing is so large.  the pitching moment at the a.c. is typically negative and constant. the a.c. of an airplane typically does not change with loading or other changes; but the c.g. does, as noted above.  if the c.g. moves forward, the airplane becomes more stable (greater moment arm between the a.c. and the c.g.), and if too far forward will cause the airplane to be difficult for the pilot to bring nose-up as for landing.  if the c.g. is too far aft, the moment arm between it and the a.c. diminishes, reducing the inherent stability of the airplane and in the extreme going negative and rendering the airplane longitudinally unstable; see the diagram below. accordingly, the operating handbook for every airplane specifies the range over which the c.g. is permitted to move.  inside this range, the airplane is considered to be inherently stable, which is to say that it will self-correct longitudinal (pitch) disturbances without pilot input.   == analysis == near the cruise condition most of the lift force is generated by the wings, with ideally only a small amount generated by the fuselage and tail. we may analyse the longitudinal static stability by considering the aircraft in equilibrium under wing lift, tail force, and weight.  the moment equilibrium condition is called trim, and we are generally interested in the longitudinal stability of the aircraft about this trim condition.  equating forces in the vertical direction:     w =  l  wl  t      , the neutral point coincides with the aerodynamic center, and so for longitudinal static stability the center of gravity must lie ahead of the aerodynamic center.   == longitudinal dynamic stability == an aircraft\'s static stability is an important, but not sufficient, measure of its handling characteristics, and whether it can be flown with ease and comfort by a human pilot. in particular, the longitudinal dynamic stability of a statically stable aircraft will determine whether or not it is finally able to return to its original position.   == see also == directional stability flight dynamics handling qualities   == notes ==   == references == clancy, l.j. (1975), aerodynamics, pitman publishing limited, london.  isbn 0-273-01120-0 hurt, h.h. jr, (1960), aerodynamics for naval aviators chapter 4, a national flightshop reprint, florida. irving, f.g. (1966), an introduction to the longitudinal static stability of low-speed aircraft, pergamon press, oxford, uk. mccormick, b.w., (1979), aerodynamics, aeronautics, and flight mechanics, chapter 8, john wiley and sons, inc., new york ny. perkins, c.d. and hage, r.e., (1949), airplane performance stability and control, chapter 5, john wiley and sons, inc., new york ny. piercy, n.a.v. (1944), elementary aerodynamics, the english universities press ltd., london. stengel r f: flight dynamics. princeton university press 2004, isbn 0-691-11407-2.')"
85,"The Malewicki equations (or Fehskens–Malewicki equations) for sub-sonic endo-atmospheric rocket flight describe the maximum altitude and coast time of a vehicle such as a model rocket.  Aerospace engineer and inventor Douglas Malewicki first published them as a technical report by the model rocket company Estes Industries in 1967.


== References ==","pandas(index=85, _1=85, text='the malewicki equations (or fehskens–malewicki equations) for sub-sonic endo-atmospheric rocket flight describe the maximum altitude and coast time of a vehicle such as a model rocket.  aerospace engineer and inventor douglas malewicki first published them as a technical report by the model rocket company estes industries in 1967.   == references ==')"
86,"Biomedical engineering (BME) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). BME is also traditionally known as ""bioengineering"", but this term has come to also refer to biological engineering. This field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy. Also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. This involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a Biomedical Equipment Technician (BMET) or as clinical engineering.
Biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. Such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. Much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). Prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as MRIs and EKG/ECGs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.


== Bioinformatics ==

Bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. As an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data.
Bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. Common uses of bioinformatics include the identification of candidate genes and nucleotides (SNPs). Often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. In a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.


== Biomechanics ==

Biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.


== Biomaterial ==

A biomaterial is any matter, surface, or construct that interacts with living systems. As a science, biomaterials is about fifty years old. The study of biomaterials is called biomaterials science or biomaterials engineering. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.


== Biomedical optics ==
Biomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.


== Tissue engineering ==

Tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with BME.
One of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. Biomedical engineers are currently researching methods of creating such organs. Researchers have grown solid jawbones and tracheas from human stem cells towards this end. Several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. Bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.


== Genetic engineering ==

Genetic engineering, recombinant DNA technology, genetic modification/manipulation (GM) and gene splicing are terms that apply to the direct manipulation of an organism's genes. Unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. Genetic engineering techniques have found success in numerous applications. Some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.


== Neural engineering ==
Neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. Neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.


== Pharmaceutical engineering ==
Pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of Chemical Engineering, and Pharmaceutical Analysis. It may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.


== Medical devices ==

This is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism.
A medical device is intended for use in:

the diagnosis of disease or other conditions
in the cure, mitigation, treatment, or prevention of disease.Some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.

Stereolithography is a practical example of medical modeling being used to create physical objects. Beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases.
Medical devices are regulated and classified (in the US) as follows (see also Regulation):

Class I devices present minimal potential for harm to the user and are often simpler in design than Class II or Class III devices. Devices in this category include tongue depressors, bedpans, elastic bandages, examination gloves, and hand-held surgical instruments and other similar types of common equipment.
Class II devices are subject to special controls in addition to the general controls of Class I devices. Special controls may include special labeling requirements, mandatory performance standards, and postmarket surveillance. Devices in this class are typically non-invasive and include X-ray machines, PACS, powered wheelchairs, infusion pumps, and surgical drapes.
Class III devices generally require premarket approval (PMA) or premarket notification (510k), a scientific review to ensure the device's safety and effectiveness, in addition to the general controls of Class I. Examples include replacement heart valves, hip and knee joint implants, silicone gel-filled breast implants, implanted cerebellar stimulators, implantable pacemaker pulse generators and endosseous (intra-bone) implants.


=== Medical imaging ===

Medical/biomedical imaging is a major segment of medical devices. This area deals with enabling clinicians to directly or indirectly ""view"" things not visible in plain sight (such as due to their size, and/or location). This can involve utilizing ultrasound, magnetism, UV, radiology, and other means.

Imaging technologies are often essential to medical diagnosis, and are typically the most complex equipment found in a hospital including: fluoroscopy, magnetic resonance imaging (MRI), nuclear medicine, positron emission tomography (PET), PET-CT scans, projection radiography such as X-rays and CT scans, tomography, ultrasound, optical microscopy, and electron microscopy.


=== Implants ===
An implant is a kind of medical device made to replace and act as a missing biological structure (as compared with a transplant, which indicates transplanted biomedical tissue). The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone or apatite depending on what is the most functional. In some cases, implants contain electronics, e.g. artificial pacemakers and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.


=== Bionics ===

Artificial body part replacements are one of the many applications of bionics. Concerned with the intricate and thorough study of the properties and function of human body systems, bionics may be applied to solve some engineering problems. Careful study of the different functions and processes of the eyes, ears, and other organs paved the way for improved cameras, television, radio transmitters and receivers, and many other tools.


=== Biomedical sensors ===
In recent years biomedical sensors based in microwave technology have gained more attention. Different sensors can be manufactured for specific uses in both diagnosing and monitoring disease conditions, for example microwave sensors can be used as a complementary technique to X-ray to monitor lower extremity trauma. The sensor monitor the dielectric properties and can thus notice change in tissue (bone, muscle, fat etc.) under the skin so when measuring at different times during the healing process the response from the sensor will change as the trauma heals.


== Clinical engineering ==

Clinical engineering is the branch of biomedical engineering dealing with the actual implementation of medical equipment and technologies in hospitals or other clinical settings. Major roles of clinical engineers include training and supervising biomedical equipment technicians (BMETs), selecting technological products/services and logistically managing their implementation, working with governmental regulators on inspections/audits, and serving as technological consultants for other hospital staff (e.g. physicians, administrators, I.T., etc.). Clinical engineers also advise and collaborate with medical device producers regarding prospective design improvements based on clinical experiences, as well as monitor the progression of the state of the art so as to redirect procurement patterns accordingly.
Their inherent focus on practical implementation of technology has tended to keep them oriented more towards incremental-level redesigns and re configurations, as opposed to revolutionary research & development or ideas that would be many years from clinical adoption; however, there is a growing effort to expand this time-horizon over which clinical engineers can influence the trajectory of biomedical innovation. In their various roles, they form a ""bridge"" between the primary designers and the end-users, by combining the perspectives of being both close to the point-of-use, while also trained in product and process engineering. Clinical engineering departments will sometimes hire not just biomedical engineers, but also industrial/systems engineers to help address operations research/optimization, human factors, cost analysis, etc. Also see safety engineering for a discussion of the procedures used to design safe systems. Clinical engineering department is constructed with a manager, supervisor, engineer and technician. One engineer per eighty beds in the hospital is the ratio. Clinical engineers is also authorized audit pharmaceutical and associated stores to monitor FDA recalls of invasive items.


== Rehabilitation engineering ==

Rehabilitation engineering is the systematic application of engineering sciences to design, develop, adapt, test, evaluate, apply, and distribute technological solutions to problems confronted by individuals with disabilities. Functional areas addressed through rehabilitation engineering may include mobility, communications, hearing, vision, and cognition, and activities associated with employment, independent living, education, and integration into the community.While some rehabilitation engineers have master's degrees in rehabilitation engineering, usually a subspecialty of Biomedical engineering, most rehabilitation engineers have an undergraduate or graduate degrees in biomedical engineering, mechanical engineering, or electrical engineering. A Portuguese university provides an undergraduate degree and a master's degree in Rehabilitation Engineering and Accessibility. Qualification to become a Rehab' Engineer in the UK is possible via a University BSc Honours Degree course such as Health Design & Technology Institute, Coventry University.The rehabilitation process for people with disabilities often entails the design of assistive devices such as Walking aids intended to promote the inclusion of their users into the mainstream of society, commerce, and recreation.


== Regulatory issues ==
Regulatory issues have been constantly increased in the last decades to respond to the many incidents caused by devices to patients. For example, from 2008 to 2011, in US, there were 119 FDA recalls of medical devices classified as class I. According to U.S. Food and Drug Administration (FDA), Class I recall is associated to ""a situation in which there is a reasonable probability that the use of, or exposure to, a product will cause serious adverse health consequences or death""Regardless of the country-specific legislation, the main regulatory objectives coincide worldwide. For example, in the medical device regulations, a product must be: 1) safe and 2) effective and 3) for all the manufactured devices
A product is safe if patients, users and third parties do not run unacceptable risks of physical hazards (death, injuries, ...) in its intended use. Protective measures have to be introduced on the devices to reduce residual risks at acceptable level if compared with the benefit derived from the use of it.
A product is effective if it performs as specified by the manufacturer in the intended use. Effectiveness is achieved through clinical evaluation, compliance to performance standards or demonstrations of substantial equivalence with an already marketed device.
The previous features have to be ensured for all the manufactured items of the medical device. This requires that a quality system shall be in place for all the relevant entities and processes that may impact safety and effectiveness over the whole medical device lifecycle.
The medical device engineering area is among the most heavily regulated fields of engineering, and practicing biomedical engineers must routinely consult and cooperate with regulatory law attorneys and other experts. The Food and Drug Administration (FDA) is the principal healthcare regulatory authority in the United States, having jurisdiction over medical devices, drugs, biologics, and combination products. The paramount objectives driving policy decisions by the FDA are safety and effectiveness of healthcare products that have to be assured through a quality system in place as specified under 21 CFR 829 regulation. In addition, because biomedical engineers often develop devices and technologies for ""consumer"" use, such as physical therapy devices (which are also ""medical"" devices), these may also be governed in some respects by the Consumer Product Safety Commission. The greatest hurdles tend to be 510K ""clearance"" (typically for Class 2 devices) or pre-market ""approval"" (typically for drugs and class 3 devices).
In the European context, safety effectiveness and quality is ensured through the ""Conformity Assessment"" that is defined as ""the method by which a manufacturer demonstrates that its device complies with the requirements of the European Medical Device Directive"". The directive specifies different procedures according to the class of the device ranging from the simple Declaration of Conformity (Annex VII) for Class I devices to EC verification (Annex IV), Production quality assurance (Annex V), Product quality assurance (Annex VI) and Full quality assurance (Annex II). The Medical Device Directive specifies detailed procedures for Certification. In general terms, these procedures include tests and verifications that are to be contained in specific deliveries such as the risk management file, the technical file and the quality system deliveries. The risk management file is the first deliverable that conditions the following design and manufacturing steps. Risk management stage shall drive the product so that product risks are reduced at an acceptable level with respect to the benefits expected for the patients for the use of the device. The technical file contains all the documentation data and records supporting medical device certification. FDA technical file has similar content although organized in different structure. The Quality System deliverables usually includes procedures that ensure quality throughout all product life cycle. The same standard (ISO EN 13485) is usually applied for quality management systems in US and worldwide.

In the European Union, there are certifying entities named ""Notified Bodies"", accredited by the European Member States. The Notified Bodies must ensure the effectiveness of the certification process for all medical devices apart from the class I devices where a declaration of conformity produced by the manufacturer is sufficient for marketing. Once a product has passed all the steps required by the Medical Device Directive, the device is entitled to bear a CE marking, indicating that the device is believed to be safe and effective when used as intended, and, therefore, it can be marketed within the European Union area.
The different regulatory arrangements sometimes result in particular technologies being developed first for either the U.S. or in Europe depending on the more favorable form of regulation. While nations often strive for substantive harmony to facilitate cross-national distribution, philosophical differences about the optimal extent of regulation can be a hindrance; more restrictive regulations seem appealing on an intuitive level, but critics decry the tradeoff cost in terms of slowing access to life-saving developments.


=== RoHS II ===
Directive 2011/65/EU, better known as RoHS 2 is a recast of legislation originally introduced in 2002. The original EU legislation ""Restrictions of Certain Hazardous Substances in Electrical and Electronics Devices"" (RoHS Directive 2002/95/EC) was replaced and superseded by 2011/65/EU published in July 2011 and commonly known as RoHS 2.
RoHS seeks to limit the dangerous substances in circulation in electronics products, in particular toxins and heavy metals, which are subsequently released into the environment when such devices are recycled.
The scope of RoHS 2 is widened to include products previously excluded, such as medical devices and industrial equipment. In addition, manufacturers are now obliged to provide conformity risk assessments and test reports – or explain why they are lacking. For the first time, not only manufacturers but also importers and distributors share a responsibility to ensure Electrical and Electronic Equipment within the scope of RoHS comply with the hazardous substances limits and have a CE mark on their products.


=== IEC 60601 ===
The new International Standard IEC 60601 for home healthcare electro-medical devices defining the requirements for devices used in the home healthcare environment. IEC 60601-1-11 (2010) must now be incorporated into the design and verification of a wide range of home use and point of care medical devices along with other applicable standards in the IEC 60601 3rd edition series.
The mandatory date for implementation of the EN European version of the standard is June 1, 2013. The US FDA requires the use of the standard on June 30, 2013, while Health Canada recently extended the required date from June 2012 to April 2013. The North American agencies will only require these standards for new device submissions, while the EU will take the more severe approach of requiring all applicable devices being placed on the market to consider the home healthcare standard.


=== AS/NZS 3551:2012 ===
AS/ANS 3551:2012 is the Australian and New Zealand standards for the management of medical devices. The standard specifies the procedures required to maintain a wide range of medical assets in a clinical setting (e.g. Hospital). The standards are based on the IEC 606101 standards.
The standard covers a wide range of medical equipment management elements including, procurement, acceptance testing, maintenance (electrical safety and preventive maintenance testing) and decommissioning.


== Training and certification ==


=== Education ===
Biomedical engineers require considerable knowledge of both engineering and biology, and typically have a Bachelor's (B.Sc., B.S., B.Eng. or B.S.E.) or Master's (M.S., M.Sc., M.S.E., or M.Eng.) or a doctoral (Ph.D.) degree in BME (Biomedical Engineering) or another branch of engineering with considerable potential for BME overlap. As interest in BME increases, many engineering colleges now have a Biomedical Engineering Department or Program, with offerings ranging from the undergraduate (B.Sc., B.S., B.Eng. or B.S.E.) to doctoral levels. Biomedical engineering has only recently been emerging as its own discipline rather than a cross-disciplinary hybrid specialization of other disciplines; and BME programs at all levels are becoming more widespread, including the Bachelor of Science in Biomedical Engineering which actually includes so much biological science content that many students use it as a ""pre-med"" major in preparation for medical school. The number of biomedical engineers is expected to rise as both a cause and effect of improvements in medical technology.In the U.S., an increasing number of undergraduate programs are also becoming recognized by ABET as accredited bioengineering/biomedical engineering programs. Over 65 programs are currently accredited by ABET.In Canada and Australia, accredited graduate programs in biomedical engineering are common. For example, McMaster University offers an M.A.Sc, an MD/PhD, and a PhD in Biomedical engineering. The first Canadian undergraduate BME program was offered at Ryerson University as a four-year B.Eng. program. The Polytechnique in Montreal is also offering a bachelors's degree in biomedical engineering as is Flinders University.As with many degrees, the reputation and ranking of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees is also linked to the institution's graduate or research programs, which have some tangible factors for rating, such as research funding and volume, publications and citations. With BME specifically, the ranking of a university's hospital and medical school can also be a significant factor in the perceived prestige of its BME department/program.
Graduate education is a particularly important aspect in BME. While many engineering fields (such as mechanical or electrical engineering) do not need graduate-level training to obtain an entry-level job in their field, the majority of BME positions do prefer or even require them. Since most BME-related professions involve scientific research, such as in pharmaceutical and medical device development, graduate education is almost a requirement (as undergraduate degrees typically do not involve sufficient research training and experience). This can be either a Masters or Doctoral level degree; while in certain specialties a Ph.D. is notably more common than in others, it is hardly ever the majority (except in academia). In fact, the perceived need for some kind of graduate credential is so strong that some undergraduate BME programs will actively discourage students from majoring in BME without an expressed intention to also obtain a master's degree or apply to medical school afterwards.
Graduate programs in BME, like in other scientific fields, are highly varied, and particular programs may emphasize certain aspects within the field. They may also feature extensive collaborative efforts with programs in other fields (such as the University's Medical School or other engineering divisions), owing again to the interdisciplinary nature of BME. M.S. and Ph.D. programs will typically require applicants to have an undergraduate degree in BME, or another engineering discipline (plus certain life science coursework), or life science (plus certain engineering coursework).
Education in BME also varies greatly around the world. By virtue of its extensive biotechnology sector, its numerous major universities, and relatively few internal barriers, the U.S. has progressed a great deal in its development of BME education and training opportunities. Europe, which also has a large biotechnology sector and an impressive education system, has encountered trouble in creating uniform standards as the European community attempts to supplant some of the national jurisdictional barriers that still exist. Recently, initiatives such as BIOMEDEA have sprung up to develop BME-related education and professional standards. Other countries, such as Australia, are recognizing and moving to correct deficiencies in their BME education. Also, as high technology endeavors are usually marks of developed nations, some areas of the world are prone to slower development in education, including in BME.


=== Licensure/certification ===

As with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered Professional Engineer (PE), but, in US, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of American engineers). The US model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. This is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine.
Biomedical engineering is regulated in some countries, such as Australia, but registration is typically only recommended and not required.In the UK, mechanical engineers working in the areas of Medical Engineering, Bioengineering or Biomedical engineering can gain Chartered Engineer status through the Institution of Mechanical Engineers. The Institution also runs the Engineering in Medicine and Health Division. The Institute of Physics and Engineering in Medicine (IPEM) has a panel for the accreditation of MSc courses in Biomedical Engineering and Chartered Engineering status can also be sought through IPEM.
The Fundamentals of Engineering exam – the first (and more general) of two licensure examinations for most U.S. jurisdictions—does now cover biology (although technically not BME). For the second exam, called the Principles and Practices, Part 2, or the Professional Engineering exam, candidates may select a particular engineering discipline's content to be tested on; there is currently not an option for BME with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). However, the Biomedical Engineering Society (BMES) is, as of 2009, exploring the possibility of seeking to implement a BME-specific version of this exam to facilitate biomedical engineers pursuing licensure.
Beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. One such example is the Certified Clinical Engineer (CCE) certification for Clinical engineers.


== Career prospects ==
In 2012 there were about 19,400 biomedical engineers employed in the US, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. Biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.


== Notable figures ==
Earl Bakken - Invented the first transistorised pacemaker, co-founder of Medtronic.
Forrest Bird (deceased) – aviator and pioneer in the invention of mechanical ventilators
Y.C. Fung (deceased) – professor emeritus at the University of California, San Diego, considered by many to be the founder of modern biomechanics
Leslie Geddes (deceased) – professor emeritus at Purdue University, electrical engineer, inventor, and educator of over 2000 biomedical engineers, received a National Medal of Technology in 2006 from President George Bush for his more than 50 years of contributions that have spawned innovations ranging from burn treatments to miniature defibrillators, ligament repair to tiny blood pressure monitors for premature infants, as well as a new method for performing cardiopulmonary resuscitation (CPR).
Willem Johan Kolff (deceased) – pioneer of hemodialysis as well as in the field of artificial organs
Robert Langer – Institute Professor at MIT, runs the largest BME laboratory in the world, pioneer in drug delivery and tissue engineering
John Macleod (deceased) – one of the co-discoverers of insulin at Case Western Reserve University.
Alfred E. Mann – Physicist, entrepreneur and philanthropist. A pioneer in the field of Biomedical Engineering.
J. Thomas Mortimer – Emeritus professor of biomedical engineering at Case Western Reserve University. Pioneer in Functional Electrical Stimulation (FES)
Robert M. Nerem – professor emeritus at Georgia Institute of Technology. Pioneer in regenerative tissue, biomechanics, and author of over 300 published works. His works have been cited more than 20,000 times cumulatively.
P. Hunter Peckham – Donnell Professor of Biomedical Engineering and Orthopaedics at Case Western Reserve University. Pioneer in Functional Electrical Stimulation (FES)
Nicholas A. Peppas – Chaired Professor in Engineering, University of Texas at Austin, pioneer in drug delivery, biomaterials, hydrogels and nanobiotechnology.
Robert Plonsey – professor emeritus at Duke University, pioneer of electrophysiology
Otto Schmitt (deceased) – biophysicist with significant contributions to BME, working with biomimetics
Ascher Shapiro (deceased) – Institute Professor at MIT, contributed to the development of the BME field, medical devices (e.g. intra-aortic balloons)
Gordana Vunjak-Novakovic – University Professor at Columbia University, pioneer in tissue engineering and bioreactor design
John G. Webster – professor emeritus at the University of Wisconsin–Madison, a pioneer in the field of instrumentation amplifiers for the recording of electrophysiological signals
Fred Weibell, coauthor of Biomedical Instrumentation and Measurements
U.A. Whitaker (deceased) – provider of the Whitaker Foundation, which supported research and education in BME by providing over $700 million to various universities, helping to create 30 BME programs and helping finance the construction of 13 buildings


== See also ==

Biomedicine – Branch of medical science that applies biological and physiological principles to clinical practice
Cardiophysics
Computational anatomy
Medical physics
Physiome
Biomedical engineering jobs
Biomedical Engineering and Instrumentation Program (BEIP)


== References ==


== Further reading ==
Bronzino, Joseph D. (April 2006). The Biomedical Engineering Handbook (Third ed.). [CRC Press]. ISBN 978-0-8493-2124-5. Archived from the original on 2015-02-24. Retrieved 2009-06-22.
Villafane, Carlos (June 2009). Biomed: From the Student's Perspective (First ed.). [Techniciansfriend.com]. ISBN 978-1-61539-663-4.


== External links ==
 Media related to Biomedical engineering at Wikimedia Commons
Biomedical Engineering at Curlie","pandas(index=86, _1=86, text='biomedical engineering (bme) or medical engineering is the application of engineering principles and design concepts to medicine and biology for healthcare purposes (e.g., diagnostic or therapeutic). bme is also traditionally known as ""bioengineering"", but this term has come to also refer to biological engineering. this field seeks to close the gap between engineering and medicine, combining the design and problem solving skills of engineering with medical biological sciences to advance health care treatment, including diagnosis, monitoring, and therapy. also included under the scope of a biomedical engineer is the management of current medical equipment within hospitals while adhering to relevant industry standards. this involves making equipment recommendations, procurement, routine testing and preventive maintenance, a role also known as a biomedical equipment technician (bmet) or as clinical engineering. biomedical engineering has recently emerged as its own study, as compared to many other engineering fields. such an evolution is common as a new field transitions from being an interdisciplinary specialization among already-established fields, to being considered a field in itself. much of the work in biomedical engineering consists of research and development, spanning a broad array of subfields (see below). prominent biomedical engineering applications include the development of biocompatible prostheses, various diagnostic and therapeutic medical devices ranging from clinical equipment to micro-implants, common imaging equipment such as mris and ekg/ecgs, regenerative tissue growth, pharmaceutical drugs and therapeutic biologicals.   == bioinformatics ==  bioinformatics is an interdisciplinary field that develops methods and software tools for understanding biological data. as an interdisciplinary field of science, bioinformatics combines computer science, statistics, mathematics, and engineering to analyze and interpret biological data. bioinformatics is considered both an umbrella term for the body of biological studies that use computer programming as part of their methodology, as well as a reference to specific analysis ""pipelines"" that are repeatedly used, particularly in the field of genomics. common uses of bioinformatics include the identification of candidate genes and nucleotides (snps). often, such identification is made with the aim of better understanding the genetic basis of disease, unique adaptations, desirable properties (esp. in agricultural species), or differences between populations. in a less formal way, bioinformatics also tries to understand the organisational principles within nucleic acid and protein sequences.   == biomechanics ==  biomechanics is the study of the structure and function of the mechanical aspects of biological systems, at any level from whole organisms to organs, cells and cell organelles, using the methods of mechanics.   == biomaterial ==  a biomaterial is any matter, surface, or construct that interacts with living systems. as a science, biomaterials is about fifty years old. the study of biomaterials is called biomaterials science or biomaterials engineering. it has experienced steady and strong growth over its history, with many companies investing large amounts of money into the development of new products. biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering and materials science.   == biomedical optics == biomedical optics refers to the interaction of biological tissue and light, and how this can be exploited for sensing, imaging, and treatment.   == tissue engineering ==  tissue engineering, like genetic engineering (see below), is a major segment of biotechnology – which overlaps significantly with bme. one of the goals of tissue engineering is to create artificial organs (via biological material) for patients that need organ transplants. biomedical engineers are currently researching methods of creating such organs. researchers have grown solid jawbones and tracheas from human stem cells towards this end. several artificial urinary bladders have been grown in laboratories and transplanted successfully into human patients. bioartificial organs, which use both synthetic and biological component, are also a focus area in research, such as with hepatic assist devices that use liver cells within an artificial bioreactor construct.   == genetic engineering ==  genetic engineering, recombinant dna technology, genetic modification/manipulation (gm) and gene splicing are terms that apply to the direct manipulation of an organism\'s genes. unlike traditional breeding, an indirect method of genetic manipulation, genetic engineering utilizes modern tools such as molecular cloning and transformation to directly alter the structure and characteristics of target genes. genetic engineering techniques have found success in numerous applications. some examples include the improvement of crop technology (not a medical application, but see biological systems engineering), the manufacture of synthetic human insulin through the use of modified bacteria, the manufacture of erythropoietin in hamster ovary cells, and the production of new types of experimental mice such as the oncomouse (cancer mouse) for research.   == neural engineering == neural engineering (also known as neuroengineering) is a discipline that uses engineering techniques to understand, repair, replace, or enhance neural systems. neural engineers are uniquely qualified to solve design problems at the interface of living neural tissue and non-living constructs.   == pharmaceutical engineering == pharmaceutical engineering is an interdisciplinary science that includes drug engineering, novel drug delivery and targeting, pharmaceutical technology, unit operations of chemical engineering, and pharmaceutical analysis. it may be deemed as a part of pharmacy due to its focus on the use of technology on chemical agents in providing better medicinal treatment.   == medical devices ==  this is an extremely broad category—essentially covering all health care products that do not achieve their intended results through predominantly chemical (e.g., pharmaceuticals) or biological (e.g., vaccines) means, and do not involve metabolism. a medical device is intended for use in:  the diagnosis of disease or other conditions in the cure, mitigation, treatment, or prevention of disease.some examples include pacemakers, infusion pumps, the heart-lung machine, dialysis machines, artificial organs, implants, artificial limbs, corrective lenses, cochlear implants, ocular prosthetics, facial prosthetics, somato prosthetics, and dental implants.  stereolithography is a practical example of medical modeling being used to create physical objects. beyond modeling organs and the human body, emerging engineering techniques are also currently used in the research and development of new devices for innovative therapies, treatments, patient monitoring, of complex diseases. medical devices are regulated and classified (in the us) as follows (see also regulation):  class i devices present minimal potential for harm to the user and are often simpler in design than class ii or class iii devices. devices in this category include tongue depressors, bedpans, elastic bandages, examination gloves, and hand-held surgical instruments and other similar types of common equipment. class ii devices are subject to special controls in addition to the general controls of class i devices. special controls may include special labeling requirements, mandatory performance standards, and postmarket surveillance. devices in this class are typically non-invasive and include x-ray machines, pacs, powered wheelchairs, infusion pumps, and surgical drapes. class iii devices generally require premarket approval (pma) or premarket notification (510k), a scientific review to ensure the device\'s safety and effectiveness, in addition to the general controls of class i. examples include replacement heart valves, hip and knee joint implants, silicone gel-filled breast implants, implanted cerebellar stimulators, implantable pacemaker pulse generators and endosseous (intra-bone) implants. as with other learned professions, each state has certain (fairly similar) requirements for becoming licensed as a registered professional engineer (pe), but, in us, in industry such a license is not required to be an employee as an engineer in the majority of situations (due to an exception known as the industrial exemption, which effectively applies to the vast majority of american engineers). the us model has generally been only to require the practicing engineers offering engineering services that impact the public welfare, safety, safeguarding of life, health, or property to be licensed, while engineers working in private industry without a direct offering of engineering services to the public or other businesses, education, and government need not be licensed. this is notably not the case in many other countries, where a license is as legally necessary to practice engineering as it is for law or medicine. biomedical engineering is regulated in some countries, such as australia, but registration is typically only recommended and not required.in the uk, mechanical engineers working in the areas of medical engineering, bioengineering or biomedical engineering can gain chartered engineer status through the institution of mechanical engineers. the institution also runs the engineering in medicine and health division. the institute of physics and engineering in medicine (ipem) has a panel for the accreditation of msc courses in biomedical engineering and chartered engineering status can also be sought through ipem. the fundamentals of engineering exam – the first (and more general) of two licensure examinations for most u.s. jurisdictions—does now cover biology (although technically not bme). for the second exam, called the principles and practices, part 2, or the professional engineering exam, candidates may select a particular engineering discipline\'s content to be tested on; there is currently not an option for bme with this, meaning that any biomedical engineers seeking a license must prepare to take this examination in another category (which does not affect the actual license, since most jurisdictions do not recognize discipline specialties anyway). however, the biomedical engineering society (bmes) is, as of 2009, exploring the possibility of seeking to implement a bme-specific version of this exam to facilitate biomedical engineers pursuing licensure. beyond governmental registration, certain private-sector professional/industrial organizations also offer certifications with varying degrees of prominence. one such example is the certified clinical engineer (cce) certification for clinical engineers.   == career prospects == in 2012 there were about 19,400 biomedical engineers employed in the us, and the field was predicted to grow by 27% (much faster than average) from 2012 to 2022. biomedical engineering has the highest percentage of female engineers compared to other common engineering professions.   == notable figures == earl bakken - invented the first transistorised pacemaker, co-founder of medtronic. forrest bird (deceased) – aviator and pioneer in the invention of mechanical ventilators y.c. fung (deceased) – professor emeritus at the university of california, san diego, considered by many to be the founder of modern biomechanics leslie geddes (deceased) – professor emeritus at purdue university, electrical engineer, inventor, and educator of over 2000 biomedical engineers, received a national medal of technology in 2006 from president george bush for his more than 50 years of contributions that have spawned innovations ranging from burn treatments to miniature defibrillators, ligament repair to tiny blood pressure monitors for premature infants, as well as a new method for performing cardiopulmonary resuscitation (cpr). willem johan kolff (deceased) – pioneer of hemodialysis as well as in the field of artificial organs robert langer – institute professor at mit, runs the largest bme laboratory in the world, pioneer in drug delivery and tissue engineering john macleod (deceased) – one of the co-discoverers of insulin at case western reserve university. alfred e. mann – physicist, entrepreneur and philanthropist. a pioneer in the field of biomedical engineering. j. thomas mortimer – emeritus professor of biomedical engineering at case western reserve university. pioneer in functional electrical stimulation (fes) robert m. nerem – professor emeritus at georgia institute of technology. pioneer in regenerative tissue, biomechanics, and author of over 300 published works. his works have been cited more than 20,000 times cumulatively. p. hunter peckham – donnell professor of biomedical engineering and orthopaedics at case western reserve university. pioneer in functional electrical stimulation (fes) nicholas a. peppas – chaired professor in engineering, university of texas at austin, pioneer in drug delivery, biomaterials, hydrogels and nanobiotechnology. robert plonsey – professor emeritus at duke university, pioneer of electrophysiology otto schmitt (deceased) – biophysicist with significant contributions to bme, working with biomimetics ascher shapiro (deceased) – institute professor at mit, contributed to the development of the bme field, medical devices (e.g. intra-aortic balloons) gordana vunjak-novakovic – university professor at columbia university, pioneer in tissue engineering and bioreactor design john g. webster – professor emeritus at the university of wisconsin–madison, a pioneer in the field of instrumentation amplifiers for the recording of electrophysiological signals fred weibell, coauthor of biomedical instrumentation and measurements u.a. whitaker (deceased) – provider of the whitaker foundation, which supported research and education in bme by providing over $700 million to various universities, helping to create 30 bme programs and helping finance the construction of 13 buildings   == see also ==  biomedicine – branch of medical science that applies biological and physiological principles to clinical practice cardiophysics computational anatomy medical physics physiome biomedical engineering jobs biomedical engineering and instrumentation program (beip)   == references ==   == further reading == bronzino, joseph d. (april 2006). the biomedical engineering handbook (third ed.). [crc press]. isbn 978-0-8493-2124-5. archived from the original on 2015-02-24. retrieved 2009-06-22. villafane, carlos (june 2009). biomed: from the student\'s perspective (first ed.). [techniciansfriend.com]. isbn 978-1-61539-663-4.   == external links == media related to biomedical engineering at wikimedia commons biomedical engineering at curlie')"
87,"In medicine, a prosthesis (plural: prostheses; from Ancient Greek prosthesis, ""addition, application, attachment"") or prosthetic implant is an artificial device that replaces a missing body part, which may be lost through trauma, disease, or a condition present at birth (congenital disorder). Prostheses are intended to restore the normal functions of the missing body part. Amputee rehabilitation is primarily coordinated by a physiatrist as part of an inter-disciplinary team consisting of physiatrists, prosthetists, nurses, physical therapists, and occupational therapists. Prostheses can be created by hand or with computer-aided design (CAD), a software interface that helps creators design and analyze the creation with computer-generated 2-D and 3-D graphics as well as analysis and optimization tools.


== Types ==
A person's prosthesis should be designed and assembled according to the person's appearance and functional needs. For instance, a person may need a transradial prosthesis, but need to choose between an aesthetic functional device, a myoelectric device, a body-powered device, or an activity specific device. The person's future goals and economical capabilities may help them choose between one or more devices.
Craniofacial prostheses include intra-oral and extra-oral prostheses. Extra-oral prostheses are further divided into hemifacial, auricular (ear), nasal, orbital and ocular. Intra-oral prostheses include dental prostheses such as dentures, obturators, and dental implants.
Prostheses of the neck include larynx substitutes, trachea and upper esophageal replacements,
Somato prostheses of the torso include breast prostheses which may be either single or bilateral, full breast devices or nipple prostheses.
Penile prostheses are used to treat erectile dysfunction, correct penile deformity, perform phalloplasty and metoidioplasty procedures in biological men, and to build a new penis in female-to-male gender reassignment surgeries.


=== Limb prostheses ===

Limb prostheses include both upper- and lower-extremity prostheses.
Upper-extremity prostheses are used at varying levels of amputation: forequarter, shoulder disarticulation, transhumeral prosthesis, elbow disarticulation, transradial prosthesis, wrist disarticulation, full hand, partial hand, finger, partial finger. A transradial prosthesis is an artificial limb that replaces an arm missing below the elbow.
Upper limb prostheses can be categorized in three main categories: Passive devices, Body Powered devices, Externally Powered (myoelectric) devices. Passive devices can either be passive hands, mainly used for cosmetic purpose, or passive tools, mainly used for specific activities (e.g. leisure or vocational). An extensive overview and classification of passive devices can be found in a literature review by Maat et.al. A passive device can be static, meaning the device has no movable parts, or it can be adjustable, meaning its configuration can be adjusted (e.g. adjustable hand opening). Despite the absence of active grasping, passive devices are very useful in bimanual tasks that require fixation or support of an object, or for gesticulation in social interaction. According to scientific data a third of the upper limb amputees worldwide use a passive prosthetic hand. Body Powered or cable operated limbs work by attaching a harness and cable around the opposite shoulder of the damaged arm. The third category of prosthetic devices available are myoelectric arms. These work by sensing, via electrodes, when the muscles in the upper arm move, causing an artificial hand to open or close. In the prosthetics industry, a trans-radial prosthetic arm is often referred to as a ""BE"" or below elbow prosthesis.
Lower-extremity prostheses provide replacements at varying levels of amputation. These include hip disarticulation, transfemoral prosthesis, knee disarticulation, transtibial prosthesis, Syme's amputation, foot, partial foot, and toe. The two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency) and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency).
A transfemoral prosthesis is an artificial limb that replaces a leg missing above the knee. Transfemoral amputees can have a very difficult time regaining normal movement. In general, a transfemoral amputee must use approximately 80% more energy to walk than a person with two whole legs. This is due to the complexities in movement associated with the knee. In newer and more improved designs, hydraulics, carbon fiber, mechanical linkages, motors, computer microprocessors, and innovative combinations of these technologies are employed to give more control to the user. In the prosthetics industry a trans-femoral prosthetic leg is often referred to as an ""AK"" or above the knee prosthesis.
A transtibial prosthesis is an artificial limb that replaces a leg missing below the knee. A transtibial amputee is usually able to regain normal movement more readily than someone with a transfemoral amputation, due in large part to retaining the knee, which allows for easier movement.  Lower extremity prosthetics describes artificially replaced limbs located at the hip level or lower. In the prosthetics industry a trans-tibial prosthetic leg is often referred to as a ""BK"" or below the knee prosthesis.
Physical therapists are trained to teach a person to walk with a leg prosthesis. To do so, the physical therapist may provide verbal instructions and may also help guide the person using touch or tactile cues. This may be done in a clinic or home. There is some research suggesting that such training in the home may be more successful if the treatment includes the use of a treadmill. Using a treadmill, along with the physical therapy treatment, helps the person to experience many of the challenges of walking with a prosthesis.
In the United Kingdom, 75% of lower limb amputations are performed due to inadequate circulation (dysvascularity). This condition is often associated with many other medical conditions (co-morbidities) including diabetes and heart disease that may make it a challenge to recover and use a prosthetic limb to regain mobility and independence. For people who have inadequate circulation and have lost a lower limb, there is insufficient evidence due to a lack of research, to inform them regarding their choice of prosthetic rehabilitation approaches.

Lower extremity prostheses are often categorized by the level of amputation or after the name of a surgeon:
Transfemoral (Above-knee)
Transtibial (Below-knee)
Ankle disarticulation (e.g.: Syme amputation)
Knee disarticulation
Hemi-pelvictomy (Hip disarticulation)
Partial foot amputations (Pirogoff, Talo-Navicular and Calcaneo-cuboid (Chopart), Tarso-metatarsal (Lisfranc), Trans-metatarsal, Metatarsal-phalangeal, Ray amputations, toe amputations).
Van Nes rotationplasty


==== Prosthetic raw materials ====
Prosthetic are made lightweight for better convenience for the amputee. Some of these materials include:

Plastics:
Polyethylene
Polypropylene
Acrylics
Polyurethane
Wood (early prosthetics)
Rubber (early prosthetics)
Lightweight metals:
Titanium
Aluminum
Composites:
Carbon fiber reinforced polymersWheeled prostheses have also been used extensively in the rehabilitation of injured domestic animals, including dogs, cats, pigs, rabbits, and turtles.


== History ==

Prosthetics originate from the ancient Egypt Near East circa 3000 BCE, with the earliest evidence of prosthetics appearing in ancient Egypt and Iran. The earliest recorded mention of eye prosthetics is from the Egyptian story of the Eye of Horus dates circa 3000 BC, which involves the left eye of Horus being plucked out and then restored by Thoth. Circa 3000-2800 BC, the earliest archaeological evidence of prosthetics is found in ancient Iran, where an eye prosthetic is found buried with a woman in Shahr-i Shōkhta. It was likely made of bitumen paste that was covered with a thin layer of gold. The Egyptians were also early pioneers of foot prosthetics, as shown by the wooden toe found on a body from the New Kingdom circa 1000 BC. Another early recorded mention is found in South Asia circa 1200 BC, involving the warrior queen Vishpala in the Rigveda. Roman bronze crowns have also been found, but their use could have been more aesthetic than medical.An early mention of a prosthetic comes from the Greek historian Herodotus, who tells the story of Hegesistratus, a Greek diviner who cut off his own foot to escape his Spartan captors and replaced it with a wooden one.


=== Wood and metal prosthetics ===

Pliny the Elder also recorded the tale of a Roman general, Marcus Sergius, whose right hand was cut off while campaigning and had an iron hand made to hold his shield so that he could return to battle. A famous and quite refined historical prosthetic arm was that of Götz von Berlichingen, made at the beginning of the 16th century.  The first confirmed use of a prosthetic device, however, is from 950 to 710 BC. In 2000, research pathologists discovered a mummy from this period buried in the Egyptian necropolis near ancient Thebes that possessed an artificial big toe.  This toe, consisting of wood and leather, exhibited evidence of use.  When reproduced by bio-mechanical engineers in 2011, researchers discovered that this ancient prosthetic enabled its wearer to walk both barefoot and in Egyptian style sandals.  Previously, the earliest discovered prosthetic was an artificial leg from Capua.Around the same time, François de la Noue is also reported to have had an iron hand, as is, in the 17th Century, René-Robert Cavalier de la Salle. Henri de Tonti had a prosthetic hook for a hand. During the Middle Ages, prosthetic remained quite basic in form. Debilitated knights would be fitted with prosthetics so they could hold up a shield, grasp a lance or a sword, or stabilize a mounted warrior. Only the wealthy could afford anything that would assist in daily life.One notable prosthesis was that belonging to an Italian man, who scientists estimate replaced his amputated right hand with a knife. Scientists investigating the skeleton, which was found in a Longobard cemetery in Povegliano Veronese, estimated that the man had lived sometime between the 6th and 8th centuries AD. Materials found near the man's body suggest that the knife prosthesis was attached with a leather strap, which he repeatedly tightened with his teeth.During the Renaissance, prosthetics developed with the use of iron, steel, copper, and wood. Functional prosthetics began to make an appearance in the 1500s.


=== Technology progress before the 20th century ===
An Italian surgeon recorded the existence of an amputee who had an arm that allowed him to remove his hat, open his purse, and sign his name. Improvement in amputation surgery and prosthetic design came at the hands of Ambroise Paré. Among his inventions was an above-knee device that was a kneeling peg leg and foot prosthesis with a fixed position, adjustable harness, and knee lock control. The functionality of his advancements showed how future prosthetics could develop.
Other major improvements before the modern era:

Pieter Verduyn – First non-locking below-knee (BK) prosthesis.
James Potts – Prosthesis made of a wooden shank and socket, a steel knee joint and an articulated foot that was controlled by catgut tendons from the knee to the ankle. Came to be known as ""Anglesey Leg"" or ""Selpho Leg"".
Sir James Syme – A new method of ankle amputation that did not involve amputating at the thigh.
Benjamin Palmer – Improved upon the Selpho leg. Added an anterior spring and concealed tendons to simulate natural-looking movement.
Dubois Parmlee – Created prosthetic with a suction socket, polycentric knee, and multi-articulated foot.
Marcel Desoutter & Charles Desoutter – First aluminium prosthesis
Henry Heather Bigg, and his son Henry Robert Heather Bigg, won the Queen's command to provide ""surgical appliances"" to wounded soldiers after Crimea War. They developed arms that allowed a double arm amputee to crochet, and a hand that felt natural to others based on ivory, felt and leather.At the end of World War II, the NAS (National Academy of Sciences) began to advocate better research and development of prosthetics. Through government funding, a research and development program was developed within the Army, Navy, Air Force, and the Veterans Administration.


=== Lower extremity modern history ===

After the Second World War a team at the University of California, Berkeley including James Foort and C.W. Radcliff helped to develop the quadrilateral socket by developing a jig fitting system for amputations above the knee. Socket technology for lower extremity limbs saw a further revolution during the 1980s when John Sabolich C.P.O., invented the Contoured Adducted Trochanteric-Controlled Alignment Method (CATCAM) socket, later to evolve into the Sabolich Socket. He followed the direction of Ivan Long and Ossur Christensen as they developed alternatives to the quadrilateral socket, which in turn followed the open ended plug socket, created from wood. The advancement was due to the difference in the socket to patient contact model. Prior to this, sockets were made in the shape of a square shape with no specialized containment for muscular tissue. New designs thus help to lock in the bony anatomy, locking it into place and distributing the weight evenly over the existing limb as well as the musculature of the patient. Ischial containment is well known and used today by many prosthetist to help in patient care. Variations of the ischial containment socket thus exists and each socket is tailored to the specific needs of the patient. Others who contributed to socket development and changes over the years include Tim Staats, Chris Hoyt, and Frank Gottschalk. Gottschalk disputed the efficacy of the CAT-CAM socket- insisting the surgical procedure done by the amputation surgeon was most important to prepare the amputee for good use of a prosthesis of any type socket design.The first microprocessor-controlled prosthetic knees became available in the early 1990s. The Intelligent Prosthesis was the first commercially available microprocessor-controlled prosthetic knee. It was released by Chas. A. Blatchford & Sons, Ltd., of Great Britain, in 1993 and made walking with the prosthesis feel and look more natural. An improved version was released in 1995 by the name Intelligent Prosthesis Plus. Blatchford released another prosthesis, the Adaptive Prosthesis, in 1998. The Adaptive Prosthesis utilized hydraulic controls, pneumatic controls, and a microprocessor to provide the amputee with a gait that was more responsive to changes in walking speed. Cost analysis reveals that a sophisticated above-knee prosthesis will be about $1 million in 45 years, given only annual cost of living adjustments.In 2019, a project under AT2030 was launched in which bespoke sockets are made using a thermoplastic, rather than through a plaster cast. This is faster to do and significantly less expensive. The sockets were called Amparo Confidence sockets.


=== Upper extremity modern history ===
In 2005, DARPA started the Revolutionizing Prosthetics program.


== Patient procedure ==
A prosthesis is a functional replacement for an amputated or congenitally malformed or missing limb. Prosthetists are responsible for the prescription, design, and management of a prosthetic device.
In most cases, the prosthetist begins by taking a plaster cast of the patient's affected limb. Lightweight, high-strength thermoplastics are custom-formed to this model of the patient. Cutting-edge materials such as carbon fiber, titanium and Kevlar provide strength and durability while making the new prosthesis lighter. More sophisticated prostheses are equipped with advanced electronics, providing additional stability and control.


== Current technology and manufacturing ==

Over the years, there have been advancements in artificial limbs. New plastics and other materials, such as carbon fiber, have allowed artificial limbs to be stronger and lighter, limiting the amount of extra energy necessary to operate the limb. This is especially important for trans-femoral amputees. Additional materials have allowed artificial limbs to look much more realistic, which is important to trans-radial and transhumeral amputees because they are more likely to have the artificial limb exposed.

In addition to new materials, the use of electronics has become very common in artificial limbs. Myoelectric limbs, which control the limbs by converting muscle movements to electrical signals, have become much more common than cable operated limbs. Myoelectric signals are picked up by electrodes, the signal gets integrated and once it exceeds a certain threshold, the prosthetic limb control signal is triggered which is why inherently, all myoelectric controls lag. Conversely, cable control is immediate and physical, and through that offers a certain degree of direct force feedback that myoelectric control does not. Computers are also used extensively in the manufacturing of limbs. Computer Aided Design and Computer Aided Manufacturing are often used to assist in the design and manufacture of artificial limbs.Most modern artificial limbs are attached to the residual limb (stump) of the amputee by belts and cuffs or by suction. The residual limb either directly fits into a socket on the prosthetic, or—more commonly today—a liner is used that then is fixed to the socket either by vacuum (suction sockets) or a pin lock. Liners are soft and by that, they can create a far better suction fit than hard sockets. Silicone liners can be obtained in standard sizes, mostly with a circular (round) cross section, but for any other residual limb shape, custom liners can be made. The socket is custom made to fit the residual limb and to distribute the forces of the artificial limb across the area of the residual limb (rather than just one small spot), which helps reduce wear on the residual limb.


=== Production of prosthetic socket ===
The production of a prosthetic socket begins with capturing the geometry of the residual limb, this process is called shape capture. The goal of this process is to create an accurate representation of the residual limb, which is critical to achieve good socket fit. The custom socket is created by taking a plaster cast of the residual limb or, more commonly today, of the liner worn over their residual limb, and then making a mold from the plaster cast. The commonly used compound is called Plaster of Paris. In recent years, various digital shape capture systems have been developed which can be input directly to a computer allowing for a more sophisticated design. In general, the shape capturing process begins with the digital acquisition of three-dimensional (3D) geometric data from the amputee's residual limb. Data are acquired with either a probe, laser scanner, structured light scanner, or a photographic-based 3D scanning system.After shape capture, the second phase of the socket production is called rectification, which is the process of modifying the model of the residual limb by adding volume to bony prominence and potential pressure points and remove volume from load bearing area. This can be done manually by adding or removing plaster to the positive model, or virtually by manipulating the computerized model in the software. Lastly, the fabrication of the prosthetic socket begins once the model has been rectified and finalized. The prosthetists would wrap the positive model with a semi-molten plastic sheet or carbon fiber coated with epoxy resin to construct the prosthetic socket. For the computerized model, it can be 3D printed using a various of material with different flexibility and mechanical strength.Optimal socket fit between the residual limb and socket is critical to the function and usage of the entire prosthesis. If the fit between the residual limb and socket attachment is too loose, this will reduce the area of contact between the residual limb and socket or liner, and increase pockets between residual limb skin and socket or liner. Pressure then is higher, which can be painful. Air pockets can allow sweat to accumulate that can soften the skin. Ultimately, this is a frequent cause for itchy skin rashes. Over time, this can lead to breakdown of the skin. On the other hand, a very tight fit may excessively increase the interface pressures that may also lead to skin breakdown after prolonged use.Artificial limbs are typically manufactured using the following steps:
Measurement of the residual limb
Measurement of the body to determine the size required for the artificial limb
Fitting of a silicone liner
Creation of a model of the liner worn over the residual limb
Formation of thermoplastic sheet around the model – This is then used to test the fit of the prosthetic
Formation of permanent socket
Formation of plastic parts of the artificial limb – Different methods are used, including vacuum forming and injection molding
Creation of metal parts of the artificial limb using die casting
Assembly of entire limb


=== Body-powered arms ===
Current technology allows body-powered arms to weigh around one-half to one-third of what a myoelectric arm does.


==== Sockets ====
Current body-powered arms contain sockets that are built from hard epoxy or carbon fiber. These sockets or ""interfaces"" can be made more comfortable by lining them with a softer, compressible foam material that provides padding for the bone prominences. A self-suspending or supra-condylar socket design is useful for those with short to mid-range below elbow absence. Longer limbs may require the use of a locking roll-on type inner liner or more complex harnessing to help augment suspension.


==== Wrists ====
Wrist units are either screw-on connectors featuring the UNF 1/2-20 thread (USA) or quick-release connector, of which there are different models.


==== Voluntary opening and voluntary closing ====
Two types of body-powered systems exist, voluntary opening ""pull to open"" and voluntary closing ""pull to close"". Virtually all ""split hook"" prostheses operate with a voluntary opening type system.
More modern ""prehensors"" called GRIPS utilize voluntary closing systems. The differences are significant. Users of voluntary opening systems rely on elastic bands or springs for gripping force, while users of voluntary closing systems rely on their own body power and energy to create gripping force.
Voluntary closing users can generate prehension forces equivalent to the normal hand, up to or exceeding one hundred pounds. Voluntary closing GRIPS require constant tension to grip, like a human hand, and in that property, they do come closer to matching human hand performance. Voluntary opening split hook users are limited to forces their rubber or springs can generate which usually is below 20 pounds.


==== Feedback ====
An additional difference exists in the biofeedback created that allows the user to ""feel"" what is being held. Voluntary opening systems once engaged provide the holding force so that they operate like a passive vice at the end of the arm. No gripping feedback is provided once the hook has closed around the object being held. Voluntary closing systems provide directly proportional control and biofeedback so that the user can feel how much force that they are applying.
A recent study showed that by stimulating the median and ulnar nerves, according to the information provided by the artificial sensors from a hand prosthesis, physiologically appropriate (near-natural) sensory information could be provided to an amputee. This feedback enabled the participant to effectively modulate the grasping force of the prosthesis with no visual or auditory feedback.In February 2013, researchers from École Polytechnique Fédérale de Lausanne in Switzerland and the Scuola Superiore Sant'Anna in Italy, implanted electrodes into an amputee's arm, which gave the patient sensory feedback and allowed for real time control of the prosthetic. With wires linked to nerves in his upper arm, the Danish patient was able to handle objects and instantly receive a sense of touch through the special artificial hand that was created by Silvestro Micera and researchers both in Switzerland and Italy.In July 2019, this technology was expanded on even further by researchers from the University of Utah, lead by Jacob George. The group of researchers implanted electrodes into the patient's arm to map out several sensory precepts. They would then stimulate each electrode to figure out how each sensory precept was triggered, then proceed to map the sensory information onto the prosthetic. This would allow the researchers to get a good approximation of the same kind of information that the patient would receive from their natural hand. Unfortunately, the arm is too expensive for the average user to acquire, however, Jacob mentioned that insurance companies could cover the costs of the prosthetic.


==== Terminal devices ====
Terminal devices contain a range of hooks, prehensors, hands or other devices.


===== Hooks =====
Voluntary opening split hook systems are simple, convenient, light, robust, versatile and relatively affordable.
A hook does not match a normal human hand for appearance or overall versatility, but its material tolerances can exceed and surpass the normal human hand for mechanical stress (one can even use a hook to slice open boxes or as a hammer whereas the same is not possible with a normal hand), for thermal stability (one can use a hook to grip items from boiling water, to turn meat on a grill, to hold a match until it has burned down completely) and for chemical hazards (as a metal hook withstands acids or lye, and does not react to solvents like a prosthetic glove or human skin).


===== Hands =====

Prosthetic hands are available in both voluntary opening and voluntary closing versions and because of their more complex mechanics and cosmetic glove covering require a relatively large activation force, which, depending on the type of harness used, may be uncomfortable. A recent study by the Delft University of Technology, The Netherlands, showed that the development of mechanical prosthetic hands has been neglected during the past decades. The study showed that the pinch force level of most current mechanical hands is too low for practical use. The best tested hand was a prosthetic hand developed around 1945. In 2017 however, a research has been started with bionic hands by Laura Hruby of the Medical University of Vienna. A few open-hardware 3-d printable bionic hands have also become available. Some companies are also producing robotic hands with integrated forearm, for fitting unto a patient's upper arm and in 2020, at the Italian Institute of Technology (IIT), another robotic hand with integrated forearm (Soft Hand Pro) was developed.


==== Commercial providers and materials ====
Hosmer and Otto Bock are major commercial hook providers. Mechanical hands are sold by Hosmer and Otto Bock as well; the Becker Hand is still manufactured by the Becker family. Prosthetic hands may be fitted with standard stock or custom-made cosmetic looking silicone gloves. But regular work gloves may be worn as well. Other terminal devices include the V2P Prehensor, a versatile robust gripper that allows customers to modify aspects of it, Texas Assist Devices (with a whole assortment of tools) and TRS that offers a range of terminal devices for sports. Cable harnesses can be built using aircraft steel cables, ball hinges, and self-lubricating cable sheaths. Some prosthetics have been designed specifically for use in salt water.


=== Lower-extremity prosthetics ===

Lower-extremity prosthetics describes artificially replaced limbs located at the hip level or lower. Concerning all ages Ephraim et al. (2003) found a worldwide estimate of all-cause lower-extremity amputations of 2.0–5.9 per 10,000 inhabitants. For birth prevalence rates of congenital limb deficiency they found an estimate between 3.5 and 7.1 cases per 10,000 births.The two main subcategories of lower extremity prosthetic devices are trans-tibial (any amputation transecting the tibia bone or a congenital anomaly resulting in a tibial deficiency), and trans-femoral (any amputation transecting the femur bone or a congenital anomaly resulting in a femoral deficiency). In the prosthetic industry, a trans-tibial prosthetic leg is often referred to as a ""BK"" or below the knee prosthesis while the trans-femoral prosthetic leg is often referred to as an ""AK"" or above the knee prosthesis.
Other, less prevalent lower extremity cases include the following:

Hip disarticulations – This usually refers to when an amputee or congenitally challenged patient has either an amputation or anomaly at or in close proximity to the hip joint.
Knee disarticulations – This usually refers to an amputation through the knee disarticulating the femur from the tibia.
Symes – This is an ankle disarticulation while preserving the heel pad.


==== Socket ====
The socket serves as an interface between the residuum and the prosthesis, ideally allowing comfortable weight-bearing, movement control and proprioception. Socket issues, such as discomfort and skin breakdown, are rated among the most important issues faced by lower-limb amputees.


==== Shank and connectors ====
This part creates distance and support between the knee-joint and the foot (in case of an upper-leg prosthesis) or between the socket and the foot. The type of connectors that are used between the shank and the knee/foot determines whether the prosthesis is modular or not. Modular means that the angle and the displacement of the foot in respect to the socket can be changed after fitting. In developing countries prosthesis mostly are non-modular, in order to reduce cost. When considering children modularity of angle and height is important because of their average growth of 1.9 cm annually.


==== Foot ====
Providing contact to the ground, the foot provides shock absorption and stability during stance. Additionally it influences gait biomechanics by its shape and stiffness. This is because the trajectory of the center of pressure (COP) and the angle of the ground reaction forces is determined by the shape and stiffness of the foot and needs to match the subject's build in order to produce a normal gait pattern. Andrysek (2010) found 16 different types of feet, with greatly varying results concerning durability and biomechanics. The main problem found in current feet is durability, endurance ranging from 16 to 32 months These results are for adults and will probably be worse for children due to higher activity levels and scale effects. Evidence comparing different types of feet and ankle prosthetic devices is not strong enough to determine if one mechanism of ankle/foot is superior to another. When deciding on a device, the cost of the device, a person's functional need, and the availability of a particular device should be considered.


==== Knee joint ====
In case of a trans-femoral (above knee) amputation, there also is a need for a complex connector providing articulation, allowing flexion during swing-phase but not during stance. As its purpose is to replace the knee, the prosthetic knee joint is the most critical component of the prosthesis for trans-femoral amputees. The function of the good prosthetic knee joint is to mimic the function of the normal knee, such as providing structural support and stability during stance phase but able to flex in a controllable manner during swing phase. Hence it allows users to have a smooth and energy efficient gait and minimize the impact of amputation. The prosthetic knee is connected to the prosthetic foot by the shank, which is usually made of an aluminum or graphite tube.
One of the most important aspect of a prosthetic knee joint would be its stance-phase control mechanism. The function of stance-phase control is to prevent the leg from buckling when the limb is loaded during weight acceptance. This ensures the stability of the knee in order to support the single limb support task of stance phase and provides a smooth transition to the swing phase. Stance phase control can be achieved in several ways including the mechanical locks, relative alignment of prosthetic components, weight activated friction control, and polycentric mechanisms.


===== Microprocessor control =====
To mimic the knee's functionality during gait, microprocessor-controlled knee joints have been developed that control the flexion of the knee. Some examples are Otto Bock’s C-leg, introduced in 1997, Ossur's Rheo Knee, released in 2005, the Power Knee by Ossur, introduced in 2006, the Plié Knee from Freedom Innovations and DAW Industries’ Self Learning Knee (SLK).The idea was originally developed by Kelly James, a Canadian engineer, at the University of Alberta.A microprocessor is used to interpret and analyze signals from knee-angle sensors and moment sensors. The microprocessor receives signals from its sensors to determine the type of motion being employed by the amputee. Most microprocessor controlled knee-joints are powered by a battery housed inside the prosthesis.
The sensory signals computed by the microprocessor are used to control the resistance generated by hydraulic cylinders in the knee-joint. Small valves control the amount of hydraulic fluid that can pass into and out of the cylinder, thus regulating the extension and compression of a piston connected to the upper section of the knee.The main advantage of a microprocessor-controlled prosthesis is a closer approximation to an amputee's natural gait. Some allow amputees to walk near walking speed or run. Variations in speed are also possible and are taken into account by sensors and communicated to the microprocessor, which adjusts to these changes accordingly. It also enables the amputees to walk downstairs with a step-over-step approach, rather than the one step at a time approach used with mechanical knees. There is some research suggesting that people with microprocessor-controlled prostheses report greater satisfaction and improvement in functionality, residual limb health, and safety. People may be able to perform everyday activities at greater speeds, even while multitasking, and reduce their risk of falls.However, some have some significant drawbacks that impair its use. They can be susceptible to water damage and thus great care must be taken to ensure that the prosthesis remains dry.


=== Myoelectric ===
A myoelectric prosthesis uses the electrical tension generated every time a muscle contracts, as information. This tension can be captured from voluntarily contracted muscles by electrodes applied on the skin to control the movements of the prosthesis, such as elbow flexion/extension, wrist supination/pronation (rotation) or opening/closing of the fingers. A prosthesis of this type utilizes the residual neuromuscular system of the human body to control the functions of an electric powered prosthetic hand, wrist, elbow or foot. This is different from an electric switch prosthesis, which requires straps and/or cables actuated by body movements to actuate or operate switches that control the movements of the prosthesis. There is no clear evidence concluding that myoelectric upper extremity prostheses function better than body-powered prostheses. Advantages to using a myoelectric upper extremity prosthesis include the potential for improvement in cosmetic appeal (this type of prosthesis may have a more natural look), may be better for light everyday activities, and may be beneficial for people experiencing phantom limb pain. When compared to a body-powered prosthesis, a myoelectric prosthesis may not be as durable, may have a longer training time, may require more adjustments, may need more maintenance, and does not provide feedback to the user.The USSR was the first to develop a myoelectric arm in 1958, while the first myoelectric arm became commercial in 1964 by the Central Prosthetic Research Institute of the USSR, and distributed by the Hangar Limb Factory of the UK.


=== Robotic prostheses ===

Robots can be used to generate objective measures of patient's impairment and therapy outcome, assist in diagnosis, customize therapies based on patient's motor abilities, and assure compliance with treatment regimens and maintain patient's records. It is shown in many studies that there is a significant improvement in upper limb motor function after stroke using robotics for upper limb rehabilitation.
In order for a robotic prosthetic limb to work, it must have several components to integrate it into the body's function: Biosensors detect signals from the user's nervous or muscular systems. It then relays this information to a controller located inside the device, and processes feedback from the limb and actuator, e.g., position or force, and sends it to the controller. Examples include surface electrodes that detect electrical activity on the skin, needle electrodes implanted in muscle, or solid-state electrode arrays with nerves growing through them. One type of these biosensors are employed in myoelectric prostheses.
A device known as the controller is connected to the user's nerve and muscular systems and the device itself. It sends intention commands from the user to the actuators of the device and interprets feedback from the mechanical and biosensors to the user. The controller is also responsible for the monitoring and control of the movements of the device.
An actuator mimics the actions of a muscle in producing force and movement. Examples include a motor that aids or replaces original muscle tissue.
Targeted muscle reinnervation (TMR) is a technique in which motor nerves, which previously controlled muscles on an amputated limb, are surgically rerouted such that they reinnervate a small region of a large, intact muscle, such as the pectoralis major. As a result, when a patient thinks about moving the thumb of his missing hand, a small area of muscle on his chest will contract instead. By placing sensors over the reinnervated muscle, these contractions can be made to control the movement of an appropriate part of the robotic prosthesis.A variant of this technique is called targeted sensory reinnervation (TSR). This procedure is similar to TMR, except that sensory nerves are surgically rerouted to skin on the chest, rather than motor nerves rerouted to muscle. Recently, robotic limbs have improved in their ability to take signals from the human brain and translate those signals into motion in the artificial limb. DARPA, the Pentagon's research division, is working to make even more advancements in this area. Their desire is to create an artificial limb that ties directly into the nervous system.


==== Robotic arms ====
Advancements in the processors used in myoelectric arms have allowed developers to make gains in fine-tuned control of the prosthetic. The Boston Digital Arm is a recent artificial limb that has taken advantage of these more advanced processors. The arm allows movement in five axes and allows the arm to be programmed for a more customized feel. Recently the I-LIMB Hand, invented in Edinburgh, Scotland, by David Gow has become the first commercially available hand prosthesis with five individually powered digits. The hand also possesses a manually rotatable thumb which is operated passively by the user and allows the hand to grip in precision, power, and key grip modes.Another neural prosthetic is Johns Hopkins University Applied Physics Laboratory Proto 1. Besides the Proto 1, the university also finished the Proto 2 in 2010. Early in 2013, Max Ortiz Catalan and Rickard Brånemark of the Chalmers University of Technology, and Sahlgrenska University Hospital in Sweden, succeeded in making the first robotic arm which is mind-controlled and can be permanently attached to the body (using osseointegration).An approach that is very useful is called arm rotation which is common for unilateral amputees which is an amputation that affects only one side of the body; and also essential for bilateral amputees, a person who is missing or has had amputated either both arms or legs, to carry out activities of daily living. This involves inserting a small permanent magnet into the distal end of the residual bone of subjects with upper limb amputations. When a subject rotates the residual arm, the magnet will rotate with the residual bone, causing a change in magnetic field distribution. EEG (electroencephalogram) signals, detected using small flat metal discs attached to the scalp, essentially decoding human brain activity used for physical movement, is used to control the robotic limbs. This allows the user to control the part directly.


==== Robotic transtibial prostheses ====
The research of robotic legs has made some advancement over time, allowing exact movement and control.
Researchers at the Rehabilitation Institute of Chicago announced in September 2013 that they have developed a robotic leg that translates neural impulses from the user's thigh muscles into movement, which is the first prosthetic leg to do so. It is currently in testing.Hugh Herr, head of the biomechatronics group at MIT's Media Lab developed a robotic transtibial leg (PowerFoot BiOM).The Icelandic company Össur has also created a robotic transtibial leg with motorized ankle that moves through algorithms and sensors that automatically adjust the angle of the foot during different points in its wearer's stride. Also there are brain-controlled bionic legs that allow an individual to move his limbs with a wireless transmitter.


===== Prosthesis design =====
The main goal of a robotic prosthesis is to provide active actuation during gait to improve the biomechanics of gait, including, among other things, stability, symmetry, or energy expenditure for amputees. There are several powered prosthetic legs currently on the market, including fully powered legs, in which actuators directly drive the joints, and semi-active legs, which use small amounts of energy and a small actuator to change the mechanical properties of the leg but do not inject net positive energy into gait.  Specific examples include The emPOWER from BionX, the Proprio Foot from Ossur, and the Elan Foot from Endolite. Various research groups have also experimented with robotic legs over the last decade. Central issues being researched include designing the behavior of the device during stance and swing phases, recognizing the current ambulation task, and various mechanical design problems such as robustness, weight, battery-life/efficiency, and noise-level. However, scientists from Stanford University and Seoul National University has developed artificial nerves system that will help prosthetic limbs feel. This synthetic nerve system enables prosthetic limbs sense braille, feel the sense of touch and respond to the environment.


=== Use of recycled materials ===
Prosthetics are being made from recycled plastic bottles and lids around the world.


== Attachment to the body ==
Most prostheses can be attached to the exterior of the body, in a non-permanent way. Some others however can be attached in a permanent way. One such example are exoprostheses (see below).


=== Direct bone attachment and osseointegration ===

Osseointegration is a method of attaching the artificial limb to the body. This method is also sometimes referred to as exoprosthesis (attaching an artificial limb to the bone), or endo-exoprosthesis.
The stump and socket method can cause significant pain in the amputee, which is why the direct bone attachment has been explored extensively. The method works by inserting a titanium bolt into the bone at the end of the stump. After several months the bone attaches itself to the titanium bolt and an abutment is attached to the titanium bolt. The abutment extends out of the stump and the (removable) artificial limb is then attached to the abutment. Some of the benefits of this method include the following:

Better muscle control of the prosthetic.
The ability to wear the prosthetic for an extended period of time; with the stump and socket method this is not possible.
The ability for transfemoral amputees to drive a car.The main disadvantage of this method is that amputees with the direct bone attachment cannot have large impacts on the limb, such as those experienced during jogging, because of the potential for the bone to break.


== Cosmesis ==
Cosmetic prosthesis has long been used to disguise injuries and disfigurements. With advances in modern technology, cosmesis, the creation of lifelike limbs made from silicone or PVC, has been made possible. Such prosthetics, including artificial hands, can now be designed to simulate the appearance of real hands, complete with freckles, veins, hair, fingerprints and even tattoos.
Custom-made cosmeses are generally more expensive (costing thousands of U.S. dollars, depending on the level of detail), while standard cosmeses come premade in a variety of sizes, although they are often not as realistic as their custom-made counterparts. Another option is the custom-made silicone cover, which can be made to match a person's skin tone but not details such as freckles or wrinkles. Cosmeses are attached to the body in any number of ways, using an adhesive, suction, form-fitting, stretchable skin, or a skin sleeve.


== Cognition ==

Unlike neuromotor prostheses, neurocognitive prostheses would sense or modulate neural function in order to physically reconstitute or augment cognitive processes such as executive function, attention, language, and memory. No neurocognitive prostheses are currently available but the development of implantable neurocognitive brain-computer interfaces has been proposed to help treat conditions such as stroke, traumatic brain injury, cerebral palsy, autism, and Alzheimer's disease.
The recent field of Assistive Technology for Cognition concerns the development of technologies to augment human cognition.  Scheduling devices such as Neuropage remind users with memory impairments when to perform certain activities, such as visiting the doctor. Micro-prompting devices such as PEAT, AbleLink and Guide have been used to aid users with memory and executive function problems perform activities of daily living.


== Prosthetic enhancement ==

In addition to the standard artificial limb for everyday use, many amputees or congenital patients have special limbs and devices to aid in the participation of sports and recreational activities.
Within science fiction, and, more recently, within the scientific community, there has been consideration given to using advanced prostheses to replace healthy body parts with artificial mechanisms and systems to improve function. The morality and desirability of such technologies are being debated by transhumanists, other ethicists, and others in general. Body parts such as legs, arms, hands, feet, and others can be replaced.
The first experiment with a healthy individual appears to have been that by the British scientist Kevin Warwick. In 2002, an implant was interfaced directly into Warwick's nervous system. The electrode array, which contained around a hundred electrodes, was placed in the median nerve. The signals produced were detailed enough that a robot arm was able to mimic the actions of Warwick's own arm and provide a form of touch feedback again via the implant.The DEKA company of Dean Kamen developed the ""Luke arm"", an advanced nerve-controlled prosthetic. Clinical trials began in 2008, with FDA approval in 2014 and commercial manufacturing by the Universal Instruments Corporation expected in 2017. The price offered at retail by Mobius Bionics is expected to be around $100,000.Further research in April 2019, there have been improvements towards prosthetic function and comfort of 3D-printed personalized wearable systems. Instead of manual integration after printing, integrating electronic sensors at the intersection between a prosthetic and the wearer's tissue can gather information such as pressure across wearer's tissue, that can help improve further iteration of these types of prosthetic.


=== Oscar Pistorius ===
In early 2008, Oscar Pistorius, the ""Blade Runner"" of South Africa, was briefly ruled ineligible to compete in the 2008 Summer Olympics because his transtibial prosthesis limbs were said to give him an unfair advantage over runners who had ankles. One researcher found that his limbs used twenty-five percent less energy than those of an able-bodied runner moving at the same speed.  This ruling was overturned on appeal, with the appellate court stating that the overall set of advantages and disadvantages of Pistorius' limbs had not been considered.
Pistorius did not qualify for the South African team for the Olympics, but went on to sweep the 2008 Summer Paralympics, and has been ruled eligible to qualify for any future Olympics. He qualified for the 2011 World Championship in South Korea and reached the semi-final where he ended last timewise, he was 14th in the first round, his personal best at 400m would have given him 5th place in the finals. At the 2012 Summer Olympics in London, Pistorius became the first amputee runner to compete at an Olympic Games. He ran in the 400 metres race semi-finals, and the 4 × 400 metres relay race finals. He also competed in 5 events in the 2012 Summer Paralympics in London.


== Design considerations ==
There are multiple factors to consider when designing a transtibial prosthesis. Manufacturers must make choices about their priorities regarding these factors.


=== Performance ===
Nonetheless, there are certain elements of socket and foot mechanics that are invaluable for the athlete, and these are the focus of today's high-tech prosthetics companies:

Fit – athletic/active amputees, or those with bony residua, may require a carefully detailed socket fit; less-active patients may be comfortable with a 'total contact' fit and gel liner
Energy storage and return – storage of energy acquired through ground contact and utilization of that stored energy for propulsion
Energy absorption – minimizing the effect of high impact on the musculoskeletal system
Ground compliance – stability independent of terrain type and angle
Rotation – ease of changing direction
Weight – maximizing comfort, balance and speed
Suspension – how the socket will join and fit to the limb


=== Other ===
The buyer is also concerned with numerous other factors:

Cosmetics
Cost
Ease of use
Size availability


== Cost and source freedom ==


=== High-cost ===
In the USA a typical prosthetic limb costs anywhere between $15,000 and $90,000, depending on the type of limb desired by the patient.  With medical insurance, a patient will typically pay 10%–50% of the total cost of a prosthetic limb, while the insurance company will cover the rest of the cost.  The percent that the patient pays varies on the type of insurance plan, as well as the limb requested by the patient. In the United Kingdom, much of Europe, Australia and New Zealand the entire cost of prosthetic limbs is met by state funding or statutory insurance. For example, in Australia prostheses are fully funded by state schemes in the case of amputation due to disease, and by workers compensation or traffic injury insurance in the case of most traumatic amputations. The National Disability Insurance Scheme, which is being rolled out nationally between 2017 and 2020 also pays for prostheses.
Transradial (below the elbow amputation) and transtibial prostheses (below the knee amputation) typically cost between US $6,000 and $8,000, while transfemoral (above the knee amputation) and transhumeral prosthetics (above the elbow amputation) cost approximately twice as much with a range of $10,000 to $15,000 and can sometimes reach costs of $35,000. The cost of an artificial limb often recurs, while a limb typically needs to be replaced every 3–4 years due to wear and tear of everyday use.  In addition, if the socket has fit issues, the socket must be replaced within several months from the onset of pain. If height is an issue, components such as pylons can be changed.Not only does the patient need to pay for their multiple prosthetic limbs, but they also need to pay for physical and occupational therapy that come along with adapting to living with an artificial limb.  Unlike the reoccurring cost of the prosthetic limbs, the patient will typically only pay the $2000 to $5000 for therapy during the first year or two of living as an amputee.  Once the patient is strong and comfortable with their new limb, they will not be required to go to therapy anymore. Throughout one's life, it is projected that a typical amputee will go through $1.4 million worth of treatment, including surgeries, prosthetics, as well as therapies.


=== Low-cost ===

Low-cost above-knee prostheses often provide only basic structural support with limited function. This function is often achieved with crude, non-articulating, unstable, or manually locking knee joints. A limited number of organizations, such as the International Committee of the Red Cross (ICRC), create devices for developing countries. Their device which is manufactured by CR Equipments is a single-axis, manually operated locking polymer prosthetic knee joint.Table. List of knee joint technologies based on the literature review.

A plan for a low-cost artificial leg, designed by Sébastien Dubois, was featured at the 2007 International Design Exhibition and award show in Copenhagen, Denmark, where it won the Index: Award. It would be able to create an energy-return prosthetic leg for US $8.00, composed primarily of fiberglass.Prior to the 1980s, foot prostheses merely restored basic walking capabilities. These early devices can be characterized by a simple artificial attachment connecting one's residual limb to the ground.
The introduction of the Seattle Foot (Seattle Limb Systems) in 1981 revolutionized the field, bringing the concept of an Energy Storing Prosthetic Foot (ESPF) to the fore. Other companies soon followed suit, and before long, there were multiple models of energy storing prostheses on the market. Each model utilized some variation of a compressible heel. The heel is compressed during initial ground contact, storing energy which is then returned during the latter phase of ground contact to help propel the body forward.
Since then, the foot prosthetics industry has been dominated by steady, small improvements in performance, comfort, and marketability.
With 3D printers, it is possible to manufacture a single product without having to have metal molds, so the costs can be drastically reduced.Jaipur Foot, an artificial limb from Jaipur, India, costs about US$40.


=== Open-source robotic prothesis ===

There is currently an open-design Prosthetics forum known as the ""Open Prosthetics Project"".  The group employs collaborators and volunteers to advance Prosthetics technology while attempting to lower the costs of these necessary devices. Open Bionics is a company that is developing open-source robotic prosthetic hands. It uses 3D printing to manufacture the devices and low-cost 3D scanners to fit them, with the aim of lowering the cost of fabricating custom prosthetics. A review study on a wide range of printed prosthetic hands, found that although 3D printing technology holds a promise for individualised prosthesis design, it is not necessarily cheaper when all costs are included. The same study also found that evidence on the functionality, durability and user acceptance of 3D printed hand prostheses is still lacking.


== Low-cost prosthetics for children ==

In the USA an estimate was found of 32,500 children (<21 years) that suffer from major paediatric amputation, with 5,525 new cases each year, of which 3,315 congenital.Carr et al. (1998) investigated amputations caused by landmines for Afghanistan, Bosnia and Herzegovina, Cambodia and Mozambique among children (<14 years), showing estimates of respectively 4.7, 0.19, 1.11 and 0.67 per 1000 children. Mohan (1986) indicated in India a total of 424,000 amputees (23,500 annually), of which 10.3% had an onset of disability below the age of 14, amounting to a total of about 43,700 limb deficient children in India alone.Few low-cost solutions have been created specially for children. Examples of low-cost prosthetic devices include:


=== Pole and crutch ===
This hand-held pole with leather support band or platform for the limb is one of the simplest and cheapest solutions found. It serves well as a short-term solution, but is prone to rapid contracture formation if the limb is not stretched daily through a series of range-of motion (RoM) sets.


=== Bamboo, PVC or plaster limbs ===
This also fairly simple solution comprises a plaster socket with a bamboo or PVC pipe at the bottom, optionally attached to a prosthetic foot. This solution prevents contractures because the knee is moved through its full RoM. The David Werner Collection, an online database for the assistance of disabled village children, displays manuals of production of these solutions.


=== Adjustable bicycle limb ===
This solution is built using a bicycle seat post up side down as foot, generating flexibility and (length) adjustability. It is a very cheap solution, using locally available materials.


=== Sathi Limb ===
It is an endoskeletal modular lower limb from India, which uses thermoplastic parts. Its main advantages are the small weight and adaptability.


=== Monolimb ===
Monolimbs are non-modular prostheses and thus require more experienced prosthetist for correct fitting, because alignment can barely be changed after production. However, their durability on average is better than low-cost modular solutions.


== Cultural and social theory perspectives ==
A number of theorists have explored the meaning and implications of prosthetic extension of the body. Elizabeth Grosz writes, ""Creatures use tools, ornaments, and appliances to augment their bodily capacities. Are their bodies lacking something, which they need to replace with artificial or substitute organs?...Or conversely, should prostheses be understood, in terms of aesthetic reorganization and proliferation, as the consequence of an inventiveness that functions beyond and perhaps in defiance of pragmatic need?"" Elaine Scarry argues that every artifact recreates and extends the body. Chairs supplement the skeleton, tools append the hands, clothing augments the skin. In Scarry's thinking, ""furniture and houses are neither more nor less interior to the human body than the food it absorbs, nor are they fundamentally different from such sophisticated prosthetics as artificial lungs, eyes and kidneys. The consumption of manufactured things turns the body inside out, opening it up to and as the culture of objects."" Mark Wigley, a professor of architecture, continues this line of thinking about how architecture supplements our natural capabilities, and argues that ""a blurring of identity is produced by all prostheses."" Some of this work relies on Freud's earlier characterization of man's relation to objects as one of extension.


== Notable users of prosthetic devices ==
Marie Moentmann (1900–1974), child survivor of industrial accident
Terry Fox (1958–1981), Canadian athlete, humanitarian, and cancer research activist
Oscar Pistorius (1986– ), South African former professional sprinter


== See also ==


== References ==


=== Citations ===


=== Sources ===


== External links ==
Afghan amputees tell their stories at Texas gathering, Fayetteville Observer
Can modern prosthetics actually help reclaim the sense of touch?, PBS Newshour
A hand for Rick, Fayetteville Observer
What is prosthesis , prosthetic limb and its various component","pandas(index=87, _1=87, text='in medicine, a prosthesis (plural: prostheses; from ancient greek prosthesis, ""addition, application, attachment"") or prosthetic implant is an artificial device that replaces a missing body part, which may be lost through trauma, disease, or a condition present at birth (congenital disorder). prostheses are intended to restore the normal functions of the missing body part. amputee rehabilitation is primarily coordinated by a physiatrist as part of an inter-disciplinary team consisting of physiatrists, prosthetists, nurses, physical therapists, and occupational therapists. prostheses can be created by hand or with computer-aided design (cad), a software interface that helps creators design and analyze the creation with computer-generated 2-d and 3-d graphics as well as analysis and optimization tools.   == types == a person\'s prosthesis should be designed and assembled according to the person\'s appearance and functional needs. for instance, a person may need a transradial prosthesis, but need to choose between an aesthetic functional device, a myoelectric device, a body-powered device, or an activity specific device. the person\'s future goals and economical capabilities may help them choose between one or more devices. craniofacial prostheses include intra-oral and extra-oral prostheses. extra-oral prostheses are further divided into hemifacial, auricular (ear), nasal, orbital and ocular. intra-oral prostheses include dental prostheses such as dentures, obturators, and dental implants. prostheses of the neck include larynx substitutes, trachea and upper esophageal replacements, somato prostheses of the torso include breast prostheses which may be either single or bilateral, full breast devices or nipple prostheses. penile prostheses are used to treat erectile dysfunction, correct penile deformity, perform phalloplasty and metoidioplasty procedures in biological men, and to build a new penis in female-to-male gender reassignment surgeries. == external links == afghan amputees tell their stories at texas gathering, fayetteville observer can modern prosthetics actually help reclaim the sense of touch?, pbs newshour a hand for rick, fayetteville observer what is prosthesis , prosthetic limb and its various component')"
88,"A cardiac pacemaker (or artificial pacemaker, so as not to be confused with the natural pacemaker of the heart), is a medical device that generates electrical impulses delivered by electrodes to cause the heart muscle chambers (the upper, or atria and/or the lower, or ventricles) to contract and therefore pump blood; by doing so this device replaces and/or regulates the function of the electrical conduction system of the heart.
The primary purpose of a pacemaker is to maintain an adequate heart rate, either because the heart's natural pacemaker is not fast enough, or because there is a block in the heart's electrical conduction system. Modern pacemakers are externally programmable and allow a cardiologist, particularly a cardiac electrophysiologist to select the optimal pacing modes for individual patients. A specific type of pacemaker called a defibrillator combines pacemaker and defibrillator functions in a single implantable device, which should be called a defibrillator, for clarity. Others, called biventricular pacemakers have multiple electrodes stimulating differing positions within the lower heart chambers to improve synchronization of the ventricles, the lower chambers of the heart.


== Methods of pacing ==


=== Percussive pacing ===
Percussive pacing, also known as transthoracic mechanical pacing, is the use of the closed fist, usually on the left lower edge of the sternum over the right ventricle in the vena cava, striking from a distance of 20 – 30 cm to induce a ventricular beat (the British Journal of Anaesthesia suggests this must be done to raise the ventricular pressure to 10–15 mmHg to induce electrical activity). This is an old procedure used only as a life saving means until an electrical pacemaker is brought to the patient.


=== Transcutaneous pacing ===

Transcutaneous pacing (TCP), also called external pacing, is recommended for the initial stabilization of hemodynamically significant bradycardias of all types. The procedure is performed by placing two pacing pads on the patient's chest, either in the anterior/lateral position or the anterior/posterior position. The rescuer selects the pacing rate, and gradually increases the pacing current (measured in mA) until electrical capture (characterized by a wide QRS complex with a tall, broad T wave on the ECG) is achieved, with a corresponding pulse. Pacing artifact on the ECG and severe muscle twitching may make this determination difficult. External pacing should not be relied upon for an extended period of time. It is an emergency procedure that acts as a bridge until transvenous pacing or other therapies can be applied.


=== Epicardial pacing (temporary) ===

Temporary epicardial pacing is used during open heart surgery should the surgical procedure create atrio-ventricular block. The electrodes are placed in contact with the outer wall of the ventricle (epicardium) to maintain satisfactory cardiac output until a temporary transvenous electrode has been inserted.


=== Transvenous pacing (temporary) ===

Transvenous pacing, when used for temporary pacing, is an alternative to transcutaneous pacing. A pacemaker wire is placed into a vein, under sterile conditions, and then passed into either the right atrium or right ventricle. The pacing wire is then connected to an external pacemaker outside the body. Transvenous pacing is often used as a bridge to permanent pacemaker placement. It can be kept in place until a permanent pacemaker is implanted or until there is no longer a need for a pacemaker and then it is removed.


=== Permanent transvenous pacing ===
Permanent pacing with an implantable pacemaker involves transvenous placement of one or more pacing electrodes within a chamber, or chambers, of the heart, while the pacemaker is implanted inside the skin under the clavicle. The procedure is performed by incision of a suitable vein into which the electrode lead is inserted and passed along the vein, through the valve of the heart, until positioned in the chamber. The procedure is facilitated by fluoroscopy which enables the physician to view the passage of the electrode lead. After satisfactory lodgement of the electrode is confirmed, the opposite end of the electrode lead is connected to the pacemaker generator.
There are three basic types of permanent pacemakers, classified according to the number of chambers involved and their basic operating mechanism:
Single-chamber pacemaker. In this type, only one pacing lead is placed into a chamber of the heart, either the atrium or the ventricle.
Dual-chamber pacemaker. Here, wires are placed in two chambers of the heart. One lead paces the atrium and one paces the ventricle. This type more closely resembles the natural pacing of the heart by assisting the heart in coordinating the function between the atria and ventricles.
Biventricular pacemaker. This pacemaker has three wires placed in three chambers of the heart. One in the atrium and two in either ventricle. It is more complicated to implant.
Rate-responsive pacemaker. This pacemaker has sensors that detect changes in the patient's physical activity and automatically adjust the pacing rate to fulfill the body's metabolic needs.The pacemaker generator is a hermetically sealed device containing a power source, usually a lithium battery, a sensing amplifier which processes the electrical manifestation of naturally occurring heart beats as sensed by the heart electrodes, the computer logic for the pacemaker and the output circuitry which delivers the pacing impulse to the electrodes.
Most commonly, the generator is placed below the subcutaneous fat of the chest wall, above the muscles and bones of the chest. However, the placement may vary on a case by case basis.
The outer casing of pacemakers is so designed that it will rarely be rejected by the body's immune system. It is usually made of titanium, which is inert in the body.


=== Leadless pacing ===
Leadless pacemakers are devices that are small enough to allow the generator to be placed within the heart, therefore avoiding the need for pacing leads.    As pacemaker leads can fail over time, a pacing system that avoids these components offers theoretical advantages.  Leadless pacemakers can be implanted into the heart using a steerable catheter fed into the femoral vein via an incision in the groin.


== Basic function ==

Modern pacemakers usually have multiple functions. The most basic form monitors the heart's native electrical rhythm. When the pacemaker wire or ""lead"" does not detect heart electrical activity in the chamber - atrium or ventricle - within a normal beat-to-beat time period - most commonly one second - it will stimulate either the atrium or the ventricle with a short low voltage pulse. If it does sense electrical activity, it will hold off stimulating. This sensing and stimulating activity continues on a beat by beat basis and is called ""demand pacing"". In the case of a dual chamber device, when the upper chambers have a spontaneous or stimulated activation, the device starts a countdown to ensure that in an acceptable - and programmable - interval, there is an activation of the ventricle, otherwise again an impulse will be delivered.
The more complex forms include the ability to sense and/or stimulate both the atrial and ventricular chambers.

From this the basic ventricular ""on demand"" pacing mode is VVI or with automatic rate adjustment for exercise VVIR – this mode is suitable when no synchronization with the atrial beat is required, as in atrial fibrillation. The equivalent atrial pacing mode is AAI or AAIR which is the mode of choice when atrioventricular conduction is intact but the natural pacemaker the sinoatrial node is unreliable – sinus node disease (SND) or sick sinus syndrome. Where the problem is atrioventricular block (AVB) the pacemaker is required to detect (sense) the atrial beat and after a normal delay (0.1–0.2 seconds) trigger a ventricular beat, unless it has already happened – this is VDD mode and can be achieved with a single pacing lead with electrodes in the right atrium (to sense) and ventricle (to sense and pace). These modes AAIR and VDD are unusual in the US but widely used in Latin America and Europe. The DDDR mode is most commonly used as it covers all the options though the pacemakers require separate atrial and ventricular leads and are more complex, requiring careful programming of their functions for optimal results.


== Biventricular pacing ==

Cardiac resynchronization therapy (CRT) is used for people with heart failure in whom the left and right ventricles do not contract simultaneously (ventricular dyssynchrony), which occurs in approximately 25–50% of heart failure patients. To achieve CRT, a biventricular pacemaker (BVP) is used, which can pace both the septal and lateral walls of the left ventricle. By pacing both sides of the left ventricle, the pacemaker can resynchronize the ventricular contractions.
CRT devices have at least two leads, one passing through the vena cava and the right atrium into the right ventricle to stimulate the septum, and another passing through the vena cava and the right atrium and inserted through the coronary sinus to pace the epicardial wall of the left ventricle. Often, for patients in normal sinus rhythm, there is also a lead in the right atrium to facilitate synchrony with the atrial contraction. Thus, timing between the atrial and ventricular contractions, as well as between the septal and lateral walls of the left ventricle can be adjusted to achieve optimal cardiac function.
CRT devices have been shown to reduce mortality and improve quality of life in patients with heart failure symptoms; a LV ejection fraction less than or equal to 35% and QRS duration on EKG of 120 ms or greater.Biventricular pacing alone is referred to as CRT-P (for pacing). For selected patients at risk of arrhythmias, CRT can be combined with an implantable cardioverter-defibrillator (ICD): such devices, known as CRT-D (for defibrillation), also provide effective protection against life-threatening arrhythmias.


== His bundle pacing ==
Conventional placement of ventricular leads in or around the tip or apex of the right ventricle, or RV apical pacing, can have negative effects on heart function. Indeed, it has been associated with increased risk of atrial fibrillation, heart failure, weakening of the heart muscle and potentially shorter life expectancy. His bundle pacing (HBP) leads to a more natural or perfectly natural ventricular activation and has generated strong research and clinical interest. By stimulating the His–Purkinje fiber network directly with a special lead and placement technique, HBP causes a synchronized and therefore more effective ventricular activation and avoid long term heart muscle disease. HBP in some cases can also correct bundle branch block patterns.


== Advancements in function ==

A major step forward in pacemaker function has been to attempt to mimic nature by utilizing various inputs to produce a rate-responsive pacemaker using parameters such as the QT interval, pO2 – pCO2 (dissolved oxygen or carbon dioxide levels) in the arterial-venous system, physical activity as determined by an accelerometer, body temperature, ATP levels, adrenaline, etc.
Instead of producing a static, predetermined heart rate, or intermittent control, such a pacemaker, a 'Dynamic Pacemaker', could compensate for both actual respiratory loading and potentially anticipated respiratory loading. The first dynamic pacemaker was invented by Anthony Rickards of the National Heart Hospital, London, UK, in 1982.Dynamic pacemaking technology could also be applied to future artificial hearts. Advances in transitional tissue welding would support this and other artificial organ/joint/tissue replacement efforts. Stem cells may be of interest in transitional tissue welding.Many advancements have been made to improve the control of the pacemaker once implanted. Many of these have been made possible by the transition to microprocessor controlled pacemakers. Pacemakers that control not only the ventricles but the atria as well have become common. Pacemakers that control both the atria and ventricles are called dual-chamber pacemakers. Although these dual-chamber models are usually more expensive, timing the contractions of the atria to precede that of the ventricles improves the pumping efficiency of the heart and can be useful in congestive heart failure.
Rate responsive pacing allows the device to sense the physical activity of the patient and respond appropriately by increasing or decreasing the base pacing rate via rate response algorithms.
The DAVID trials have shown that unnecessary pacing of the right ventricle can exacerbate heart failure and increases the incidence of atrial fibrillation. The newer dual chamber devices can keep the amount of right ventricle pacing to a minimum and thus prevent worsening of the heart disease.


== Considerations ==


=== Insertion ===
A pacemaker may be implanted whilst a person is awake using local anesthetic to numb the skin with or without sedation, or asleep using a general anesthetic. An antibiotic is usually given to reduce the risk of infection.  Pacemakers are generally implanted in the front of the chest in the region of the left or right shoulder.  The skin is prepared by clipping or shaving any hair over the implant site before cleaning the skin with a disinfectant such as chlorhexidine.  An incision is made below the collar bone and a space or pocket is created under the skin to house the pacemaker generator.  This pocket is usually created just above the pectoralis major muscle (prepectoral), but in some cases the device may be inserted beneath the muscle (submuscular). The lead or leads are fed into the heart through a large vein guided by X-ray imaging (fluoroscopy). The tips of the leads may be positioned within the right ventricle, the right atrium, or the coronary sinus, depending on the type of pacemaker required. Surgery is typically completed within 30 to 90 minutes.  Following implantation, the surgical wound should be kept clean and dry until it has healed.  Care should be taken to avoid excessive movement of the shoulder within the first few weeks to reduce the risk of dislodging the pacemaker leads.The batteries within a pacemaker generator typically last 5 to 10 years. When the batteries are nearing the end of life, the generator is replaced in a procedure that is usually simpler than a new implant. Replacement involves making an incision to remove the existing device, disconnecting the leads from the old device and reconnecting them to a new generator, reinserting the new device and closing the skin.


==== Periodic pacemaker checkups ====

Once the pacemaker is implanted, it is periodically checked to ensure the device is operational and performing appropriately. Depending on the frequency set by the following physician, the device can be checked as often as is necessary. Routine pacemaker checks are typically done in-office every six (6) months, though will vary depending upon patient/device status and remote monitoring availability. Newer pacemaker models can also be interrogated remotely, with the patient transmitting their pacemaker data using an at-home transmitter connected to their geographical cellular network. This data can then be accessed by the technician through the device manufacturer's web portal.
At the time of in-office follow-up, the device will be interrogated to perform diagnostic testing. These tests include:

Sensing: the ability of the device to ""see"" intrinsic cardiac activity (Atrial and ventricular depolarization).
Impedance: A test to measure lead integrity. Large and/or sudden increases in impedance can be indicative of a lead fracture while large and/or sudden decreases in impedance can signify a breach in lead insulation.
Threshold amplitude: The minimum amount of energy (generally in hundredths of volts) required in order to pace the atrium or ventricle connected to the lead.
Threshold duration: The amount of time that the device requires at the preset amplitude to reliably pace the atrium or ventricle connected to the lead.
Percentage of pacing: Defines how dependent the patient is on the device, the percentage of time that the pacemaker has been actively pacing since the previous device interrogation.
Estimated battery life at current rate: As modern pacemakers are ""on-demand"", meaning that they only pace when necessary, device longevity is affected by how much it is utilized. Other factors affecting device longevity include programmed output and algorithms (features) causing a higher level of current drain from the battery.
Any events that were stored since the last follow-up, in particular arrhythmias such as atrial fibrillation. These are typically stored based on specific criteria set by the physician and specific to the patient. Some devices have the availability to display intracardiac electrograms of the onset of the event as well as the event itself. This is especially helpful in diagnosing the cause or origin of the event and making any necessary programming changes.


=== Magnetic fields, MRIs, and other lifestyle issues ===
A patient's lifestyle is usually not modified to any great degree after insertion of a pacemaker. There are a few activities that are unwise such as full contact sports and activities that involve intense magnetic fields.
The pacemaker patient may find that some types of everyday actions need to be modified. For instance, the shoulder harness of a vehicle seatbelt may be uncomfortable if the harness should fall across the pacemaker insertion site.
If the patient does wish to practice any type of sport or physical activity, special pacemaker protection can be worn to prevent possible physical injuries or damage to the pacemaker leads.
Any kind of an activity that involves intense electro-magnetic fields should be avoided. This includes activities such as arc welding possibly, with certain types of equipment, or maintaining heavy equipment that may generate intense magnetic fields (such as a magnetic resonance imaging (MRI) machine).
However, in February 2011 the FDA approved a new pacemaker device from Medtronic called the Revo MRI SureScan which was the first to be labeled as conditional for MRI use. There are several limitations to its use including certain patients' qualifications and scan settings. An MRI conditional device has to be reprogrammed right before and right after MRI scanning.  All the 5 most common cardiac pacing device manufacturers (covering more than 99% of the US market) now have FDA-approved MR-conditional pacemakers.A 2008 US study has found that the magnetic field created by some headphones included with portable music players or cell phones, when placed within inches of pacemakers, may cause interference.
In addition, according to the American Heart Association, some home devices have a remote potential to cause interference by occasionally inhibiting a single beat. Cellphones available in the United States (less than 3 watts) do not seem to damage pulse generators or affect how the pacemaker works.Having a pacemaker does not imply that a patient requires the use of antibiotics to be administered before procedures such as dental work. The patient should inform all medical personnel that he or she has a pacemaker. The use of MRI may be ruled out by the patient having a pacemaker manufactured before MRI conditional devices became common, or by the patient having old pacing wires abandoned inside the heart, no longer connected to their pacemaker.


=== Turning off the pacemaker ===
A panel of The Heart Rhythm Society, a specialist organization based in Washington, DC found that it was legal and ethical to honor requests by patients, or by those with legal authority to make decisions for patients, to deactivate implanted cardiac devices. Lawyers say that the legal situation is similar to removing a feeding tube, though there is currently no legal precedent involving pacemakers in the United States of America. A patient in the United States is thought to have a right to refuse or discontinue treatment, including a pacemaker that keeps him or her alive. Physicians have a right to refuse to turn it off, but are advised by the HRS panel that they should refer the patient to a physician who will. Some patients believe that hopeless, debilitating conditions, like those brought on by severe strokes or late-stage dementia, can cause so much suffering that they would prefer not to prolong their lives with supportive measures, such as cardiac devices.


=== Privacy and security ===
Security and privacy concerns have been raised with pacemakers that allow wireless communication. Unauthorized third parties may be able to read patient records contained in the pacemaker, or reprogram the devices, as has been demonstrated by a team of researchers. The demonstration worked at short range; they did not attempt to develop a long range antenna. The proof of concept exploit helps demonstrate the need for better security and patient alerting measures in remotely accessible medical implants. In response to this threat, Purdue University and Princeton University researchers have developed a prototype firewall device, called MedMon, which is designed to protect wireless medical devices such as pacemakers and insulin pumps from attackers.


=== Complications ===

Complications from having surgery to implant a pacemaker are uncommon (each 1-3 % approximately), but could include: infection where the pacemaker is implanted or in the bloodstream; allergic reaction to the dye or anesthesia used during the procedure; swelling, bruising or bleeding at the generator site, or around the heart, especially if the patient is taking blood thinners, elderly, of thin frame or otherwise on chronic steroids use. A possible complication of dual-chamber artificial pacemakers is 'pacemaker-mediated tachycardia' (PMT), a form of reentrant tachycardia. In PMT, the artificial pacemaker forms the anterograde (atrium to ventricle) limb of the circuit and the atrioventricular (AV) node forms the retrograde limb (ventricle to atrium) of the circuit. Treatment of PMT typically involves reprogramming the pacemaker.Another possible complication is ""pacemaker-tracked tachycardia,"" where a supraventricular tachycardia such as atrial fibrillation or atrial flutter is tracked by the pacemaker and produces beats from a ventricular lead. This is becoming exceedingly rare as newer devices are often programmed to recognize supraventricular tachycardias and switch to non-tracking modes.
Sometimes the leads, which are small diameter wires, from the pacemaker to the implantation site in the heart muscle will need to be removed. The most common reason for lead removal is infection, however over time leads can degrade due to a number of reasons such as lead flexing. Changes to programming of the pacemaker may overcome lead degradation to some extent. However, a patient who has several pacemaker replacements over a decade or two in which the leads were reused may require a lead replacement surgery.
Lead replacement may be done in one of two ways. Insert a new set of leads without removing the current leads (not recommended as it provides additional obstruction to blood flow and heart valve function) or remove the current leads and then insert replacements. The lead removal technique will vary depending on the surgeon's estimation of the probability that simple traction will suffice to more complex procedures. Leads can normally be disconnected from the pacemaker easily which is why device replacement usually entails simple surgery to access the device and replace it by simply unhooking the leads from the device to replace and hooking the leads to the new device. The possible complications, such as perforation of the heart wall, come from removing the lead{s} from the patient's body.
The other end of a pacemaker lead is actually implanted into the heart muscle with a miniature screw or anchored with small plastic hooks called tines. In addition, the longer the leads have been implanted starting from a year or two, the more likely that they will have attachments to the patient's body at various places in the pathway from device to heart muscle, since the human body tends to incorporate foreign devices into tissue. In some cases, for a lead that has been inserted for a short amount of time, removal may involve simple traction to pull the lead from the body. Removal in other cases is typically done with a laser or cutting device which threads like a cannula with a cutting edge over the lead and is moved down the lead to remove any organic attachments with tiny cutting lasers or similar device.
Pacemaker lead malposition in various locations has been described in the literature. Depending on the location of the pacer lead and symptoms treatment varies.Another possible complication called twiddler's syndrome occurs when a patient manipulates the pacemaker and causes the leads to be removed from their intended location and causes possible stimulation of other nerves.


== Other devices ==
Sometimes devices resembling pacemakers, called implantable cardioverter-defibrillators (ICDs) are implanted. These devices are often used in the treatment of patients at risk from sudden cardiac death. An ICD has the ability to treat many types of heart rhythm disturbances by means of pacing, cardioversion, or defibrillation. Some ICD devices can distinguish between ventricular fibrillation and ventricular tachycardia (VT), and may try to pace the heart faster than its intrinsic rate in the case of VT, to try to break the tachycardia before it progresses to ventricular fibrillation. This is known as fast-pacing, overdrive pacing, or anti-tachycardia pacing (ATP). ATP is only effective if the underlying rhythm is ventricular tachycardia, and is never effective if the rhythm is ventricular fibrillation.


== History ==


=== Origin ===
In 1889, John Alexander MacWilliam reported in the British Medical Journal (BMJ) of his experiments in which application of an electrical impulse to the human heart in asystole caused a ventricular contraction and that a heart rhythm of 60–70 beats per minute could be evoked by impulses applied at spacings equal to 60–70/minute.In 1926, Mark C Lidwill of the Royal Prince Alfred Hospital of Sydney, supported by physicist Edgar H. Booth of the University of Sydney, devised a portable apparatus which ""plugged into a lighting point"" and in which ""One pole was applied to a skin pad soaked in strong salt solution"" while the other pole ""consisted of a needle insulated except at its point, and was plunged into the appropriate cardiac chamber"". ""The pacemaker rate was variable from about 80 to 120 pulses per minute, and likewise the voltage variable from 1.5 to 120 volts"". In 1928, the apparatus was used to revive a stillborn infant at Crown Street Women's Hospital, Sydney whose heart continued ""to beat on its own accord"", ""at the end of 10 minutes"" of stimulation.In 1932, American physiologist Albert Hyman, with the help of his brother, described an electro-mechanical instrument of his own, powered by a spring-wound hand-cranked motor. Hyman himself referred to his invention as an ""artificial pacemaker"", the term continuing in use to this day.An apparent hiatus in publication of research conducted between the early 1930s and World War II may be attributed to the public perception of interfering with nature by ""reviving the dead"". For example, ""Hyman did not publish data on the use of his pacemaker in humans because of adverse publicity, both among his fellow physicians, and due to newspaper reporting at the time. Lidwell may have been aware of this and did not proceed with his experiments in humans"".


=== Transcutaneous ===
In 1950, Canadian electrical engineer John Hopps designed and built the first external pacemaker based upon observations by cardio-thoracic surgeons Wilfred Gordon Bigelow and John Callaghan at Toronto General Hospital, although the device was first tested on a dog at the University of Toronto's Banting Institute. A substantial external device using vacuum tube technology to provide transcutaneous pacing, it was somewhat crude and painful to the patient in use and, being powered from an AC wall socket, carried a potential hazard of electrocution of the patient and inducing ventricular fibrillation.
A number of innovators, including Paul Zoll, made smaller but still bulky transcutaneous pacing devices from 1952 using a large rechargeable battery as the power supply.In 1957, William L. Weirich published the results of research performed at the University of Minnesota. These studies demonstrated the restoration of heart rate, cardiac output and mean aortic pressures in animal subjects with complete heart block through the use of a myocardial electrode.In 1958 Colombian doctor Alberto Vejarano Laverde and Colombian electrical engineer Jorge Reynolds Pombo constructed an external pacemaker, similar to those of Hopps and Zoll, weighing 45 kg and powered by a 12 volt car lead–acid battery, but connected to electrodes attached to the heart. This apparatus was successfully used to sustain a 70-year-old priest, Gerardo Florez.The development of the silicon transistor and its first commercial availability in 1956 was the pivotal event that led to rapid development of practical cardiac pacemaking.


=== Wearable ===
In 1958, engineer Earl Bakken of Minneapolis, Minnesota, produced the first wearable external pacemaker for a patient of C. Walton Lillehei. This transistorized pacemaker, housed in a small plastic box, had controls to permit adjustment of pacing heart rate and output voltage and was connected to electrode leads which passed through the skin of the patient to terminate in electrodes attached to the surface of the myocardium of the heart.
One of the earliest patients to receive this Lucas pacemaker device was a woman in her early 30s in an operation carried out in 1964 at the Radcliffe Infirmary in Oxford by cardiac surgeon Alf Gunning from South Africa and later Professor Gunning who was a student of Christiaan Barnard. This pioneering operation was carried out under the guidance of cardiac consultant Peter Sleight at the Radcliffe Infirmary in Oxford and his cardiac research team at St George's Hospital in London. Sleight later became Professor of Cardiovascular Medicine at Oxford University.


=== Implantable ===

The first clinical implantation into a human of a fully implantable pacemaker was in 1958 at the Karolinska Institute in Solna, Sweden, using a pacemaker designed by inventor Rune Elmqvist and surgeon Åke Senning (in collaboration with Elema-Schönander AB, later Siemens-Elema AB), connected to electrodes attached to the myocardium of the heart by thoracotomy. The device failed after three hours. A second device was then implanted which lasted for two days. The world's first implantable pacemaker patient, Arne Larsson, went on to receive 26 different pacemakers during his lifetime. He died in 2001, at the age of 86, outliving the inventor as well as the surgeon.In 1959, temporary transvenous pacing was first demonstrated by Seymour Furman and John Schwedel, whereby the catheter electrode was inserted via the patient's basilic vein.In February 1960, an improved version of the Swedish Elmqvist design was implanted in Montevideo, Uruguay in the Casmu 1 Hospital by Doctors Orestes Fiandra and Roberto Rubio. That device lasted until the patient died of other ailments, nine months later. The early Swedish-designed devices used rechargeable batteries, which were charged by an induction coil from the outside. It was the first pacemaker implanted in America.
Implantable pacemakers constructed by engineer Wilson Greatbatch entered use in humans from April 1960 following extensive animal testing. The Greatbatch innovation varied from the earlier Swedish devices in using primary cells (mercury battery) as the energy source. The first patient lived for a further 18 months.
The first use of transvenous pacing in conjunction with an implanted pacemaker was by Parsonnet in the United States, Lagergren in Sweden and Jean-Jacques Welti in France in 1962–63.
The transvenous, or pervenous, procedure involved incision of a vein into which was inserted the catheter electrode lead under fluoroscopic guidance, until it was lodged within the trabeculae of the right ventricle. This method was to become the method of choice by the mid-1960s.
Cardiothoracic surgeon Leon Abrams and medical engineer Ray Lightwood developed and implanted the first patient-controlled variable-rate heart pacemaker in 1960 at Birmingham University. The first implant took place in March 1960, with two further implants the following month. These three patients made good recoveries and returned to a high quality of life. By 1966, 56 patients had undergone implantation with one surviving for over ​5 1⁄2 years.


=== Lithium battery ===

The preceding implantable devices all suffered from the unreliability and short lifetime of the available primary cell technology which was mainly that of the mercury battery. In the late 1960s, several companies, including ARCO in the USA, developed isotope-powered pacemakers, but this development was overtaken by the development in 1971 of the lithium iodide cell battery by Wilson Greatbatch. Lithium-iodide or lithium anode cells became the standard for future pacemaker designs.
A further impediment to reliability of the early devices was the diffusion of water vapour from the body fluids through the epoxy resin encapsulation affecting the electronic circuitry. This phenomenon was overcome by encasing the pacemaker generator in a hermetically sealed metal case, initially by Telectronics of Australia in 1969 followed by Cardiac Pacemakers Inc of Minneapolis in 1972. This technology, using titanium as the encasing metal, became the standard by the mid-1970s.
On July 9, 1974, Manuel A. Villafaña and Anthony Adducci founders of Cardiac Pacemakers, Inc. (Guidant) in St. Paul, Minnesota, manufactured the world's first pacemaker with a lithium anode and a lithium-iodide electrolyte solid-state battery.


=== Intra-cardial ===
In 2013, multiple firms announced devices that could be inserted via a leg catheter rather than invasive surgery. The devices are roughly the size and shape of a pill, much smaller than the size of a traditional pacemaker. Once implanted, the device's prongs contact the muscle and stabilize heartbeats. Engineers and scientists are currently working on this type of device. In November 2014 a patient, Bill Pike of Fairbanks, Alaska, received a Medtronic Micra pacemaker in Providence St Vincent Hospital in Portland Oregon. D. Randolph Jones was the EP doctor. In 2014 also St. Jude Medical Inc. announced the first enrollments in the company’s leadless Pacemaker Observational Study evaluating the Nanostim leadless pacing technology. The Nanostim pacemaker received CE marking in 2013. The post-approval implants have occurred in Europe. The European study was recently stopped, after there were reports of six perforations that led to two patient deaths. After investigations St Jude Medical restarted the study. But in the United States this therapy is still not approved by the FDA. While the St Jude Nanostim and the Medtronic Micra are just single-chamber pacemakers it is anticipated that leadless dual-chamber pacing for patients with atrioventricular block will become possible with further development.


=== Reusable pacemakers ===
Thousands of pacemakers are removed by funeral home personnel each year all over the world. They have to be removed postmortem from bodies that are going to be cremated to avoid explosions. It is a fairly simple procedure that can be carried out by a mortician. Pacemakers with significant battery life are potentially life-saving devices for people in low and middle income countries (LMICs). The Institute of Medicine, a United States non-governmental organization, has reported that inadequate access to advanced cardiovascular technologies is one of the major contributors to cardiovascular disease morbidity and mortality in LMICs. Ever since the 1970s, multiple studies all over the world have reported on the safety and efficacy of pacemaker reuse. As of 2016, widely acceptable standards for safe pacemaker and ICD reuse have not been developed, and there continue to be legal and regulatory barriers to widespread adoption of medical device reuse.


== Manufacturers ==
Current and prior manufacturers of implantable pacemakers

Biotronik (Germany)
Boston Scientific (USA)
Guidant (USA) (now owned by Boston Scientific)
Intermedics (USA)
Lepu Medical (China)
Medico (Italy)
Medtronic (USA)
Sorin Group (Italy) (merged with Cyberonics to form LivaNova)
St. Jude Medical (USA) (now owned by Abbott Laboratories)


== See also ==
Biological pacemaker
Button cell
Electrical conduction system of the heart
Implantable cardioverter-defibrillator
Infective endocarditis
Pacemaker syndrome


== References ==


== External links ==
Detecting and Distinguishing Cardiac Pacing Artifacts
Implantable Cardioverter Defibrillator from National Heart, Lung and Blood Institute
Current indications for CRT-P and CRT-D: Webinar from the European Heart Rhythm Association (EHRA)","pandas(index=88, _1=88, text='a cardiac pacemaker (or artificial pacemaker, so as not to be confused with the natural pacemaker of the heart), is a medical device that generates electrical impulses delivered by electrodes to cause the heart muscle chambers (the upper, or atria and/or the lower, or ventricles) to contract and therefore pump blood; by doing so this device replaces and/or regulates the function of the electrical conduction system of the heart. the primary purpose of a pacemaker is to maintain an adequate heart rate, either because the heart\'s natural pacemaker is not fast enough, or because there is a block in the heart\'s electrical conduction system. modern pacemakers are externally programmable and allow a cardiologist, particularly a cardiac electrophysiologist to select the optimal pacing modes for individual patients. a specific type of pacemaker called a defibrillator combines pacemaker and defibrillator functions in a single implantable device, which should be called a defibrillator, for clarity. others, called biventricular pacemakers have multiple electrodes stimulating differing positions within the lower heart chambers to improve synchronization of the ventricles, the lower chambers of the heart.   == methods of pacing == thousands of pacemakers are removed by funeral home personnel each year all over the world. they have to be removed postmortem from bodies that are going to be cremated to avoid explosions. it is a fairly simple procedure that can be carried out by a mortician. pacemakers with significant battery life are potentially life-saving devices for people in low and middle income countries (lmics). the institute of medicine, a united states non-governmental organization, has reported that inadequate access to advanced cardiovascular technologies is one of the major contributors to cardiovascular disease morbidity and mortality in lmics. ever since the 1970s, multiple studies all over the world have reported on the safety and efficacy of pacemaker reuse. as of 2016, widely acceptable standards for safe pacemaker and icd reuse have not been developed, and there continue to be legal and regulatory barriers to widespread adoption of medical device reuse.   == manufacturers == current and prior manufacturers of implantable pacemakers  biotronik (germany) boston scientific (usa) guidant (usa) (now owned by boston scientific) intermedics (usa) lepu medical (china) medico (italy) medtronic (usa) sorin group (italy) (merged with cyberonics to form livanova) st. jude medical (usa) (now owned by abbott laboratories)   == see also == biological pacemaker button cell electrical conduction system of the heart implantable cardioverter-defibrillator infective endocarditis pacemaker syndrome   == references ==   == external links == detecting and distinguishing cardiac pacing artifacts implantable cardioverter defibrillator from national heart, lung and blood institute current indications for crt-p and crt-d: webinar from the european heart rhythm association (ehra)')"
89,"Tissue engineering is a biomedical engineering discipline that uses a combination of cells, engineering, materials methods, and suitable biochemical and physicochemical factors to restore, maintain, improve, or replace different types of biological tissues. Tissue engineering often involves the use of cells placed on tissue scaffolds in the formation of new viable tissue for a medical purpose but is not limited to applications involving cells and tissue scaffolds. While it was once categorized as a sub-field of biomaterials, having grown in scope and importance it can be considered as a field in its own.

While most definitions of tissue engineering cover a broad range of applications, in practice the term is closely associated with applications that repair or replace portions of or whole tissues (i.e., bone, cartilage, blood vessels, bladder, skin, muscle etc.). Often, the tissues involved require certain mechanical and structural properties for proper functioning. The term has also been applied to efforts to perform specific biochemical functions using cells within an artificially-created support system (e.g. an artificial pancreas, or a bio artificial liver). The term regenerative medicine is often used synonymously with tissue engineering, although those involved in regenerative medicine place more emphasis on the use of stem cells or progenitor cells to produce tissues.


== Overview ==

A commonly applied definition of tissue engineering, as stated by Langer and Vacanti, is ""an interdisciplinary field that applies the principles of engineering and life sciences toward the development of biological substitutes that restore, maintain, or improve [Biological tissue] function or a whole organ"". In addition, Langer and Vacanti also state that there are three main types of tissue engineering: cells, tissue-inducing substances, and a cells + matrix approach (often referred to as a scaffold).Tissue engineering has also been defined as ""understanding the principles of tissue growth, and applying this to produce functional replacement tissue for clinical use"". A further description goes on to say that an ""underlying supposition of tissue engineering is that the employment of natural biology of the system will allow for greater success in developing therapeutic strategies aimed at the replacement, repair, maintenance, or enhancement of tissue function"".Developments in the multidisciplinary field of tissue engineering have yielded a novel set of tissue replacement parts and implementation strategies. Scientific advances in biomaterials, stem cells, growth and differentiation factors, and biomimetic environments have created unique opportunities to fabricate or improve existing tissues in the laboratory from combinations of engineered extracellular matrices (""scaffolds""), cells, and biologically active molecules. Among the major challenges now facing tissue engineering is the need for more complex functionality, biomechanical stability, and vascularization in laboratory-grown tissues destined for transplantation. The continued success of tissue engineering and the eventual development of true human replacement parts will grow from the convergence of engineering and basic research advances in tissue, matrix, growth factor, stem cell, and developmental biology, as well as materials science and bioinformatics.
In 2003, the NSF published a report entitled ""The Emergence of Tissue Engineering as a Research Field"", which gives a thorough description of the history of this field.


== Etymology ==
The historic origins of the term are unclear as the definition of the word has changed throughout the past decades. The term first appeared in a 1984 publication that described the organization of an endothelium-like membrane on the surface of a long-implanted, synthetic ophthalmic prosthesis The first modern use of the term as recognized today was in 1985 by the researcher, physiologist and bioengineer Y.C Fung of the Engineering Research Center. He proposed the joining of the terms tissue (in reference to the fundamental relationship between cells and organs) and engineering (in reference to the field of modification of said tissues). The term was officially adopted in 1987.


== History ==


=== Ancient Era (Pre-17th Century) ===
A rudimentary understanding of the inner workings of human tissues may date back further than most would expect. As early as the Neolithic period, sutures were being used to close wounds and aid in healing. Later on, societies such as ancient Egypt developed better materials for sewing up wounds such as linen sutures. Around 2500 BC in ancient India, skin grafts were developed by cutting skin from the buttock and suturing it to wound sites in the ear, nose, or lips. Ancient Egyptians often would graft skin from corpses onto living humans and even attempted to use honey as a type of antibiotic and grease as a protective barrier to prevent infection. In the 1st and 2nd centuries AD, Gallo-Romans developed wrought iron implants and dental implants could be found in ancient Mayans.
Enlightenment (17th Century-19th Century)
While these ancient societies had developed techniques that were way ahead of their time, they still lacked a mechanistic understanding of how the body was reacting to these procedures. This mechanistic approach came along in tandem with the development of the empirical method of science pioneered by Rene Descartes. Sir Isaac Newton began to describe the body as a “physiochemical machine” and postured that disease was a breakdown in the machine. In the 17th century, Robert Hooke discovered the cell and a letter from Benedict de Spinoza brought forward the idea of the homeostasis between the dynamic processes in the body. Hydra experiments performed by Abraham Trembley in the 18th century began to delve into the regenerative capabilities of cells. During the 19th century, a better understanding of how different metals reacted with the body led to the development of better sutures and a shift towards screw and plate implants in bone fixation. Further, it was first hypothesized in the mid-1800’s that cell-environment interactions and cell proliferation were vital for tissue regeneration.


=== Modern Era (20th and  21st Centuries) ===
As time progresses and technology advances, there is a constant need for change in the approach researchers take in their studies. Tissue engineering has continued to evolve over centuries. In the beginning people used to look at and use samples directly from human or animal cadavers. Now, tissue engineers have the ability to remake many of the tissues in the body through the use of modern techniques such as microfabrication and three-dimensional bioprinting in conjunction with native tissue cells/stem cells. These advances have allowed researchers to generate new tissues in a much more efficient manner. For example, these techniques allow for more personalization which allow for better biocompatibility, decreased immune response, cellular integration, and longevity. There is no doubt that these techniques will continue to evolve, as we have continued to see microfabrication and bioprinting evolve over the past decade.
In 1960, Wichterle and Lim were the first to publish experiments on hydrogels for biomedical applications by using them in contact lens construction. Work on the field developed slowly over the next two decades, but later found traction when hydrogels were repurposed for drug delivery. In 1984, Charles Hull developed bioprinting by converting a Hewlett-Packard inkjet printer into a device capable of depositing cells in 2D. 3D printing is a type of additive manufacturing which has since found various applications in Medical engineering, due to its high precision and efficiency. With Biologist James Thompson’s development of first human stem cell lines in 1998 followed by transplantation of first laboratory-grown internal organs in 1999 and creation of the first bioprinter in 2003 by the University of Missouri when they printed spheroids without the need of scaffolds, 3D bioprinting became more conventionally used in medical field than ever before. 
So far, scientists have been able to print mini organoids and organs-on-chips that have rendered practical insights into the functions of a human body. Pharmaceutical companies are using these models to test drugs before moving on to animal studies. However, a fully functional and structurally similar organ hasn’t been printed yet. A team at University of Utah has reportedly printed ears and successfully transplanted those onto children born with defects that left their ears partially developed.
Today hydrogels are considered the preferred choice of bioinks for 3D bioprinting since they mimic cells’ natural ECM while also containing strong mechanical properties capable of sustaining 3D structures. Furthermore, hydrogels in conjunction with 3D bioprinting allow researchers to produce different scaffolds which can be used to form new tissues or organs.
3-D printed tissues still face many challenges such as adding vasculature. Meanwhile, 3-D printing parts of tissues definitely will improve our understanding of the human body, thus accelerating both basic and clinical research.


== Examples ==

As defined by Langer and Vacanti, examples of tissue engineering fall into one or more of three categories: ""just cells,"" ""cells and scaffold,"" or ""tissue-inducing factors."" 

In vitro meat: Edible artificial animal muscle tissue cultured in vitro.
Bioartificial liver device, “Temporary Liver”, Extracorporeal Liver Assist Device (ELAD): The human hepatocyte cell line (C3A line) in a hollow fiber bioreactor can mimic the hepatic function of the liver for acute instances of liver failure. A fully capable ELAD would temporarily function as an individual’s liver, thus avoiding transplantation and allowing regeneration of their own liver.
Artificial pancreas: Research involves using islet cells to regulate the body’s blood sugar, particularly in cases of diabetes . Biochemical factors may be used to cause human pluripotent stem cells to differentiate (turn into) cells that function similarly to beta cells, which are in an islet cell in charge of producing insulin.
Artificial bladders: Anthony Atala (Wake Forest University) has successfully implanted artificial bladders, constructed of cultured cells seeded onto a bladder-shaped scaffold, into seven out of approximately 20 human test subjects as part of a long-term experiment.
Cartilage: lab-grown cartilage, cultured in vitro on a scaffold, was successfully used as an autologous transplant to repair patients' knees.
Scaffold-free cartilage: Cartilage generated without the use of exogenous scaffold material. In this methodology, all material in the construct is cellular produced directly by the cells.
bioartificial heart: Doris Taylor's lab constructed a biocompatible rat heart by re-cellularizing a de-cellularized rat heart. This scaffold and cells were placed in a bioreactor, where it matured to become a partially or fully transplantable organ. the work was called a ""landmark"". The lab first stripped the cells away from a rat heart (a process called ""decellularization"") and then injected rat stem cells into the decellularized rat heart.
Tissue-engineered airway: A donor trachea was successfully decellularized and recellularized with autologous cells and transplanted into the recipient.
Tissue-engineered blood vessels: Blood vessels that have been grown in a lab and can be used to repair damaged blood vessels without eliciting an immune response.
Artificial skin constructed from human skin cells embedded in a hydrogel, such as in the case of bio-printed constructs for battlefield burn repairs.
Artificial bone marrow: Bone marrow cultured in vitro to be transplanted serves as a ""just cells"" approach to tissue engineering.
Tissue engineered bone: A structural matrix can be composed of metals such as titanium, polymers of varying degradation rates, or certain types of ceramics. Materials are often chosen to recruit osteoblasts to aid in reforming the bone and returning biological function. Various types of cells can be added directly into the matrix to expediate the process.
Laboratory-grown penis: Decellularized scaffolds of rabbit penises were recellularized with smooth muscle and endothelial cells. The organ was then transplanted to live rabbits and functioned comparably to the native organ, suggesting potential as treatment for genital trauma.
Oral mucosa tissue engineering uses a cells and scaffold approach to replicate the 3 dimensional structure and function of oral mucosa.


== Cells as building blocks ==

Cells are one of the main components for the success of tissue engineering approaches. Tissue engineering uses cells as strategies for creation/replacement of new tissue. Examples include fibroblasts used for skin repair or renewal, chondrocytes used for cartilage repair (MACI -FDA approved product), and hepatocytes used in liver support systems
Cells can be used alone or with support matrices for tissue engineering applications. An adequate environment for promoting cell growth, differentiation, and integration with the existing tissue is a critical factor for cell-based building blocks. Manipulation of any of these cell processes create alternative avenues for the development of new tissue (e.g., reprogramming of somatic cells, vascularization).


=== Isolation ===
Techniques for cell isolation depend on the cell source. Centrifugation and apheresis are techniques used for extracting cells from biofluids (e.g., blood). Whereas digestion processes, typically using enzymes to remove the extracellular matrix (ECM), are required prior to centrifugation or apheresis techniques to extract cells from tissues/organs. Trypsin and collagenase are the most common enzymes used for tissue digestion. While trypsin is temperature dependent, collagenase is less sensitive to changes in temperature.


=== Cell sources ===

Primary cells are those directly isolated from host tissue. These cells provide an ex-vivo model of cell behavior without any genetic, epigenetic, or developmental changes; making them a closer replication of in-vivo conditions than cells derived from other methods. This constraint however, can also make studying them difficult. These are mature cells, often terminally differentiated, meaning that for many cell types proliferation is difficult or impossible. Additionally, the microenvironments these cells exist in are highly specialized, often making replication of these conditions difficult.Secondary cells A portion of cells from a primary culture is moved to a new repository/vessel to continue being cultured. Medium from the primary culture is removed, the cells that are desired to be transferred are obtained, and then cultured in a new vessel with fresh growth medium. A secondary cell culture is useful in order to ensure that cells have both the room and nutrients that they require to grow. Secondary cultures are most notably used in any scenario in which a larger quantity of cells than can be found in the primary culture is desired. Secondary cells share the constraints of primary cells (see above) but have an added risk of contamination when transferring to a new vessel.


=== Genetic classifications of cells ===
Autologous: The donor and the recipient of the cells are the same individual. Cells are harvested, cultured or stored, and then reintroduced to the host. As a result of the host’s own cells being reintroduced, an antigenic response is not elicited. The body’s immune system recognizes these reimplanted cells as its own, and does not target them for attack. Autologous cell dependence on host cell health and donor site morbidity may be deterrents to their use. Adipose-derived and bone marrow-derived mesenchymal stem cells are commonly autologous in nature, and can be used in a myriad of ways, from helping repair skeletal tissue to replenishing beta cells in diabetic patients.Allogenic: Cells are obtained from the body of a donor of the same species as the recipient. While there are some ethical constraints to the use of human cells for in vitro studies (ie. human brain tissue chimerae development ), the employment of dermal fibroblasts from human foreskin demonstrates an immunologically safe and thus a viable choice for allogenic tissue engineering of the skin.
Xenogenic: These cells are derived isolated cells from alternate species from the recipient. A notable example of xenogenic tissue utilization is cardiovascular implant construction via animal cells. Chimeric human-animal farming raises ethical concerns around the potential for improved consciousness from implanting human organs in animals.Syngenic or isogenic: These cells describe those borne from identical genetic code. This imparts an immunologic benefit similar to autologous cell lines (see above). Autologous cells can be considered syngenic, but the classification also extends to non-autologously derived cells such as those from an identical twin, from genetically identical (cloned) research models, or induced stem cells (iSC)  as related to the donor.


=== Stem cells ===
Stem cells are undifferentiated cells with the ability to divide in culture and give rise to different forms of specialized cells. Stem cells are divided into ""adult"" and ""embryonic"" stem cells according to their source. While there is still a large ethical debate related to the use of embryonic stem cells, it is thought that another alternative source-- induced pluripotent stem cells—may be useful for the repair of diseased or damaged tissues, or may be used to grow new organs.
Totipotent cells are stem cells which can divide into further stem cells or differentiate into any cell type in the body, including extra-embryonic tissue.
Pluripotent cells are stem cells which can differentiate into any cell type in the body except extra-embryonic tissue. induced pluripotent stem cells (iPSCs) are  subclass of pluripotent stem cells resembling embryonic stem cells (ESCs) that have been derived from adult differentiated cells. iPSCs are created by altering the expression of transcriptional factors in adult cells until they become like embryonic stem cells. As of November 2020, a popular method is to use modified retroviruses to introduce specific genes into the genome of adult cells to induce them to an embryonic stem cell-like state.Multipotent stem cells can be differentiated into any cell within the same class, such as blood or bone. A common example of multipotent cells is Mesenchymal stem cells (MSCs).


== Scaffolds ==
Scaffolds are materials that have been engineered to cause desirable cellular interactions to contribute to the formation of new functional tissues for medical purposes. Cells are often 'seeded' into these structures capable of supporting three-dimensional tissue formation. Scaffolds mimic the extracellular matrix of the native tissue, recapitulating the in vivo milieu and allowing cells to influence their own microenvironments. They usually serve at least one of the following purposes: allow cell attachment and migration, deliver and retain cells and biochemical factors, enable diffusion of vital cell nutrients and expressed products, exert certain mechanical and biological influences to modify the behaviour of the cell phase.
In 2009, an interdisciplinary team led by the thoracic surgeon Thorsten Walles implanted the first bioartificial transplant that provides an innate vascular network for post-transplant graft supply successfully into a patient awaiting tracheal reconstruction.

To achieve the goal of tissue reconstruction, scaffolds must meet some specific requirements. High porosity and adequate pore size are necessary to facilitate cell seeding and diffusion throughout the whole structure of both cells and nutrients. Biodegradability is often an essential factor since scaffolds should preferably be absorbed by the surrounding tissues without the necessity of surgical removal. The rate at which degradation occurs has to coincide as much as possible with the rate of tissue formation: this means that while cells are fabricating their own natural matrix structure around themselves, the scaffold is able to provide structural integrity within the body and eventually it will break down leaving the newly formed tissue which will take over the mechanical load. Injectability is also important for clinical uses.
Recent research on organ printing is showing how crucial a good control of the 3D environment is to ensure reproducibility of experiments and offer better results.


=== Materials ===
Material selection is an essential aspect of producing a scaffold.  The materials utilized can be natural or synthetic and can be biodegradable or non-biodegradable. Additionally, they must be biocompatible, meaning that they don’t cause any adverse effects to cells. Silicone, for example, is a synthetic, non-biodegradable material commonly used as a drug delivery material, while gelatin is a biodegradable, natural material commonly used in cell-culture scaffoldsThe material needed for each application is different, and dependent the desired mechanical properties of the material. Tissue engineering of bone, for example, will require a much more rigid scaffold compared to a scaffold for skin regeneration.There are a few versatile synthetic materials used for many different scaffold applications. One of these commonly used materials is polylactic acid (PLA), a synthetic polymer. PLA - polylactic acid. This is a polyester which degrades within the human body to form lactic acid, a naturally occurring chemical which is easily removed from the body. Similar materials are polyglycolic acid (PGA) and polycaprolactone (PCL): their degradation mechanism is similar to that of PLA,  but PCL degrades slower and PGA degrades faster. PLA is commonly combined with PGA to create poly-lactic-co-glycolic acid (PLGA). This is especially useful because the degradation of PLGA can be tailored by altering the weight percentages of PLA and PGA: More PLA – slower degradation, more PLA – faster degradation. This tunability, along with its biocompatibility, makes it an extremely useful material for scaffold creation.Scaffolds may also be constructed from natural materials: in particular different derivatives of the extracellular matrix have been studied to evaluate their ability to support cell growth. Protein based materials - such as collagen, or fibrin, and polysaccharidic materials- like chitosan or glycosaminoglycans (GAGs), have all proved suitable in terms of cell compatibility. Among GAGs, hyaluronic acid, possibly in combination with cross linking agents (e.g. glutaraldehyde, water-soluble carbodiimide, etc.), is one of the possible choices as scaffold material. Another form of scaffold is decellularized tissue. This is a process where chemicals are used to extracts cells from tissues, leaving just the extracellular matrix. This has the benefit of a foully formed matrix specific to the desired tissue type. However, the decellurized scaffold may present immune problems with future introduced cells.


=== Synthesis ===

A number of different methods have been described in the literature for preparing porous structures to be employed as tissue engineering scaffolds. Each of these techniques presents its own advantages, but none are free of drawbacks.


==== Nanofiber self-assembly ====
Molecular self-assembly is one of the few methods for creating biomaterials with properties similar in scale and chemistry to that of the natural in vivo extracellular matrix (ECM), a crucial step toward tissue engineering of complex tissues. Moreover, these hydrogel scaffolds have shown superiority in in vivo toxicology and biocompatibility compared to traditional macroscaffolds and animal-derived materials.


==== Textile technologies ====
These techniques include all the approaches that have been successfully employed for the preparation of non-woven meshes of different polymers. In particular, non-woven polyglycolide structures have been tested for tissue engineering applications: such fibrous structures have been found useful to grow different types of cells. The principal drawbacks are related to the difficulties in obtaining high porosity and regular pore size.


==== Solvent casting and particulate leaching ====
Solvent casting and particulate leaching (SCPL) allows for the preparation of structures with regular porosity, but with limited thickness. First, the polymer is dissolved into a suitable organic solvent (e.g. polylactic acid could be dissolved into dichloromethane), then the solution is cast into a mold filled with porogen particles. Such porogen can be an inorganic salt like sodium chloride, crystals of saccharose, gelatin spheres or paraffin spheres. The size of the porogen particles will affect the size of the scaffold pores, while the polymer to porogen ratio is directly correlated to the amount of porosity of the final structure. After the polymer solution has been cast the solvent is allowed to fully evaporate, then the composite structure in the mold is immersed in a bath of a liquid suitable for dissolving the porogen: water in the case of sodium chloride, saccharose and gelatin or an aliphatic solvent like hexane for use with paraffin. Once the porogen has been fully dissolved, a porous structure is obtained. Other than the small thickness range that can be obtained, another drawback of SCPL lies in its use of organic solvents which must be fully removed to avoid any possible damage to the cells seeded on the scaffold.


==== Gas foaming ====
To overcome the need to use organic solvents and solid porogens, a technique using gas as a porogen has been developed. First, disc-shaped structures made of the desired polymer are prepared by means of compression molding using a heated mold. The discs are then placed in a chamber where they are exposed to high pressure CO2 for several days. The pressure inside the chamber is gradually restored to atmospheric levels. During this procedure the pores are formed by the carbon dioxide molecules that abandon the polymer, resulting in a sponge-like structure. The main problems resulting from such a technique are caused by the excessive heat used during compression molding (which prohibits the incorporation of any temperature labile material into the polymer matrix) and by the fact that the pores do not form an interconnected structure.


==== Emulsification freeze-drying ====
This technique does not require the use of a solid porogen like SCPL. First, a synthetic polymer is dissolved into a suitable solvent (e.g. polylactic acid in dichloromethane) then water is added to the polymeric solution and the two liquids are mixed in order to obtain an emulsion. Before the two phases can separate, the emulsion is cast into a mold and quickly frozen by means of immersion into liquid nitrogen. The frozen emulsion is subsequently freeze-dried to remove the dispersed water and the solvent, thus leaving a solidified, porous polymeric structure. While emulsification and freeze-drying allow for a faster preparation when compared to SCPL (since it does not require a time-consuming leaching step), it still requires the use of solvents. Moreover, pore size is relatively small and porosity is often irregular. Freeze-drying by itself is also a commonly employed technique for the fabrication of scaffolds. In particular, it is used to prepare collagen sponges: collagen is dissolved into acidic solutions of acetic acid or hydrochloric acid that are cast into a mold, frozen with liquid nitrogen and then lyophilized.


==== Thermally induced phase separation ====
Similar to the previous technique, the TIPS phase separation procedure requires the use of a solvent with a low melting point that is easy to sublime. For example, dioxane could be used to dissolve polylactic acid, then phase separation is induced through the addition of a small quantity of water: a polymer-rich and a polymer-poor phase are formed. Following cooling below the solvent melting point and some days of vacuum-drying to sublime the solvent, a porous scaffold is obtained. Liquid-liquid phase separation presents the same drawbacks of emulsification/freeze-drying.


==== Electrospinning ====
Electrospinning is a highly versatile technique that can be used to produce continuous fibers ranging in diameter from a few microns to a few nanometers. In a typical electrospinning set-up, the desired scaffold material is dissolved within a solvent and placed within a syringe. This solution is fed through a needle and a high voltage is applied to the tip and to a conductive collection surface. The buildup of electrostatic forces within the solution causes it to eject a thin fibrous stream towards the oppositely charged or grounded collection surface. During this process the solvent evaporates, leaving solid fibers leaving a highly porous network. This technique is highly tunable, with variation to solvent, voltage, working distance (distance from the needle to collection surface), flow rate of solution, solute concentration, and collection surface. This allows for precise control of fiber morphology.
On a commercial level however, due to scalability reasons, there are 40 or sometimes 96 needles involved operating at once. The bottle-necks in such set-ups are: 1) Maintaining the aforementioned variables uniformly for all of the needles and 2) formation of ""beads"" in single fibers that we as engineers, want to be of a uniform diameter. By modifying variables such as the distance to collector, magnitude of applied voltage, or solution flow rate—researchers can dramatically change the overall scaffold architecture.
Historically, research on electrospun fibrous scaffolds dates back to at least the late 1980s when Simon showed that electrospinning could be used to produced nano- and submicron-scale fibrous scaffolds from polymer solutions specifically intended for use as in vitro cell and tissue substrates. This early use of electrospun lattices for cell culture and tissue engineering showed that various cell types would adhere to and proliferate upon polycarbonate fibers. It was noted that as opposed to the flattened morphology typically seen in 2D culture, cells grown on the electrospun fibers exhibited a more rounded 3-dimensional morphology generally observed of tissues in vivo.


==== CAD/CAM technologies ====
Because most of the above techniques are limited when it comes to the control of porosity and pore size, computer assisted design and manufacturing techniques have been introduced to tissue engineering. First, a three-dimensional structure is designed using CAD software. The porosity can be tailored using algorithms within the software. The scaffold is then realized by using ink-jet printing of polymer powders or through Fused Deposition Modeling of a polymer melt.A 2011 study by El-Ayoubi et al. investigated ""3D-plotting technique to produce (biocompatible and biodegradable) poly-L-Lactide macroporous scaffolds with two different pore sizes"" via solid free-form fabrication (SSF) with computer-aided-design (CAD), to explore therapeutic articular cartilage replacement as an ""alternative to conventional tissue repair"". The study found the smaller the pore size paired with mechanical stress in a bioreactor (to induce in vivo-like conditions), the higher the cell viability in potential therapeutic functionality via decreasing recovery time and increasing transplant effectiveness.


==== Laser-assisted bioprinting ====
In a 2012 study, Koch et al. focused on whether Laser-assisted BioPrinting (LaBP) can be used to build multicellular 3D patterns in natural matrix, and whether the generated constructs are functioning and forming tissue. LaBP arranges small volumes of living cell suspensions in set high-resolution patterns. The investigation was successful, the researchers foresee that ""generated tissue constructs might be used for in vivo testing by implanting them into animal models"" (14). As of this study, only human skin tissue has been synthesized, though researchers project that ""by integrating further cell types (e.g. melanocytes, Schwann cells, hair follicle cells) into the printed cell construct, the behavior of these cells in a 3D in vitro microenvironment similar to their natural one can be analyzed"", which is useful for drug discovery and toxicology studies.


==== Self-assembled recombinant spider silk nanomembranes ====
Gustafsson et al. demonstrated free‐standing, bioactive membranes of cm-sized area, but only 250 nm thin, that were formed by self‐assembly of spider silk at the interface of an aqueous solution. The membranes uniquely combine nanoscale thickness, biodegradability, ultrahigh strain and strength, permeability to proteins and promote rapid cell adherence and proliferation. They demonstrated growing a coherent layer of keratinocytes.


== Assembly methods ==
A persistent problem within tissue engineering is mass transport limitations. Engineered tissues generally lack an initial blood supply, thus making it difficult for any implanted cells to obtain sufficient oxygen and nutrients to survive, or function properly.


=== Self-assembly ===
Self-assembly methods have been shown to be promising methods for tissue engineering. Self-assembly methods have the advantage of allowing tissues to develop their own extracellular matrix, resulting in tissue that better recapitulates biochemical and biomechanical properties of native tissue. Self-assembling engineered articular cartilage was introduced by Jerry Hu and Kyriacos A. Athanasiou in 2006 and applications of the process have resulted in engineered cartilage approaching the strength of native tissue. Self-assembly is a prime technology to get cells grown in a lab to assemble into three-dimensional shapes. To break down tissues into cells, researchers first have to dissolve the extracellular matrix that normally binds them together. Once cells are isolated, they must form the complex structures that make up our natural tissues.


=== Liquid-based template assembly ===
The air-liquid surface established by Faraday waves is explored as a template to assemble biological entities for bottom-up tissue engineering. This liquid-based template can be dynamically reconfigured in a few seconds, and the assembly on the template can be achieved in a scalable and parallel manner. Assembly of microscale hydrogels, cells, neuron-seeded micro-carrier beads, cell spheroids into various symmetrical and periodic structures was demonstrated with good cell viability. Formation of 3D neural network was achieved after 14-day tissue culture.


=== Additive manufacturing ===

It might be possible to print organs, or possibly entire organisms using additive manufacturing techniques. A recent innovative method of construction uses an ink-jet mechanism to print precise layers of cells in a matrix of thermoreversible gel. Endothelial cells, the cells that line blood vessels, have been printed in a set of stacked rings. When incubated, these fused into a tube. This technique has been referred to as “bioprinting” within the field as it involves the printing of biological components in a structure resembling the organ of focus.
The field of three-dimensional and highly accurate models of biological systems is pioneered by multiple projects and technologies including a rapid method for creating tissues and even whole organs involve a 3D printer that can bioprint the scaffolding and cells layer by layer into a working tissue sample or organ. The device is presented in a TED talk by Dr. Anthony Atala, M.D. the Director of the Wake Forest Institute for Regenerative Medicine, and the W.H. Boyce Professor and Chair of the Department of Urology at Wake Forest University, in which a kidney is printed on stage during the seminar and then presented to the crowd. It is anticipated that this technology will enable the production of livers in the future for transplantation and theoretically for toxicology and other biological studies as well.
Recently Multi-Photon Processing (MPP) was employed for in vivo experiments by engineering artificial cartilage constructs. An ex vivo histological examination showed that certain pore geometry and the pre-growing of chondrocytes (Cho) prior to implantation significantly improves the performance of the created 3D scaffolds. The achieved biocompatibility was comparable to the commercially available collagen membranes. The successful outcome of this study supports the idea that hexagonal-pore-shaped hybrid organic-inorganic microstructured scaffolds in combination with Cho seeding may be successfully implemented for cartilage tissue engineering.


=== Scaffolding ===
In 2013, using a 3-d scaffolding of Matrigel in various configurations, substantial pancreatic organoids was produced in vitro. Clusters of small numbers of cells proliferated into 40,000 cells within one week. The clusters transform into cells that make either digestive enzymes or hormones like insulin, self-organizing into branched pancreatic organoids that resemble the pancreas.The cells are sensitive to the environment, such as gel stiffness and contact with other cells. Individual cells do not thrive; a minimum of four proximate cells was required for subsequent organoid development. Modifications to the medium composition produced either hollow spheres mainly composed of pancreatic progenitors, or complex organoids that spontaneously undergo pancreatic morphogenesis and differentiation. Maintenance and expansion of pancreatic progenitors require active Notch and FGF signaling, recapitulating in vivo niche signaling interactions.The organoids were seen as potentially offering mini-organs for drug testing and for spare insulin-producing cells.Aside from Matrigel 3-D scaffolds, other collagen gel systems have been developed. Collagen/hyaluronic acid scaffolds have been used for modeling the mammary gland In Vitro while co-coculturing epithelial and adipocyte cells. The HyStem kit is another 3-D platform containing ECM components and hyaluronic acid that has been used for cancer research. Additionally, hydrogel constituents can be chemically modified to assist in crosslinking and enhance their mechanical properties.


== Tissue culture ==
In many cases, creation of functional tissues and biological structures in vitro requires extensive culturing to promote survival, growth and inducement of functionality. In general, the basic requirements of cells must be maintained in culture, which include oxygen, pH, humidity, temperature, nutrients and osmotic pressure maintenance.
Tissue engineered cultures also present additional problems in maintaining culture conditions. In standard cell culture, diffusion is often the sole means of nutrient and metabolite transport. However, as a culture becomes larger and more complex, such as the case with engineered organs and whole tissues, other mechanisms must be employed to maintain the culture, such as the creation of capillary networks within the tissue.

Another issue with tissue culture is introducing the proper factors or stimuli required to induce functionality. In many cases, simple maintenance culture is not sufficient. Growth factors, hormones, specific metabolites or nutrients, chemical and physical stimuli are sometimes required. For example, certain cells respond to changes in oxygen tension as part of their normal development, such as chondrocytes, which must adapt to low oxygen conditions or hypoxia during skeletal development. Others, such as endothelial cells, respond to shear stress from fluid flow, which is encountered in blood vessels. Mechanical stimuli, such as pressure pulses seem to be beneficial to all kind of cardiovascular tissue such as heart valves, blood vessels or pericardium.


=== Bioreactors ===

In tissue engineering, a bioreactor is a device that attempts to simulate a physiological environment in order to promote cell or tissue growth in vitro. A physiological environment can consist of many different parameters such as temperature, pressure, oxygen or carbon dioxide concentration, or osmolality of fluid environment, and it can extend to all kinds of biological, chemical or mechanical stimuli. Therefore, there are systems that may include the application of forces such as electromagnetic forces, mechanical pressures, or fluid pressures to the tissue. These systems can be two- or three-dimensional setups. Bioreactors can be used in both academic and industry applications. General-use and application-specific bioreactors are also commercially available, which may provide static chemical stimulation or a combination of chemical and mechanical stimulation.
Cell proliferation and differentiation are largely influenced by mechanical and biochemical cues in the surrounding extracellular matrix environment. Bioreactors are typically developed to replicate the specific physiological environment of the tissue being grown (e.g., flex and fluid shearing for heart tissue growth). This can allow specialized cell lines to thrive in cultures replicating their native environments, but it also makes bioreactors attractive tools for culturing stem cells. A successful stem-cell-based bioreactor is effective at expanding stem cells with uniform properties and/or promoting controlled, reproducible differentiation into selected mature cell types.There are a variety of bioreactors designed for 3D cell cultures. There are small plastic cylindrical chambers, as well as glass chambers, with regulated internal humidity and moisture specifically engineered for the purpose of growing cells in three dimensions. The bioreactor uses bioactive synthetic materials such as polyethylene terephthalate membranes to surround the spheroid cells in an environment that maintains high levels of nutrients. They are easy to open and close, so that cell spheroids can be removed for testing, yet the chamber is able to maintain 100% humidity throughout. This humidity is important to achieve maximum cell growth and function. The bioreactor chamber is part of a larger device that rotates to ensure equal cell growth in each direction across three dimensions.QuinXell Technologies now under Quintech Life Sciences from Singapore has developed a bioreactor known as the TisXell Biaxial Bioreactor which is specially designed for the purpose of tissue engineering. It is the first bioreactor in the world to have a spherical glass chamber with biaxial rotation; specifically to mimic the rotation of the fetus in the womb; which provides a conducive environment for the growth of tissues.Multiple forms of mechanical stimulation have also been combined into a single bioreactor. Using gene expression analysis, one academic study found that applying a combination of cyclic strain and ultrasound stimulation to pre-osteoblast cells in a bioreactor accelerated matrix maturation and differentiation. The technology of this combined stimulation bioreactor could be used to grow bone cells more quickly and effectively in future clinical stem cell therapies.MC2 Biotek has also developed a bioreactor known as ProtoTissue that uses gas exchange to maintain high oxygen levels within the cell chamber; improving upon previous bioreactors, since the higher oxygen levels help the cell grow and undergo normal cell respiration.Active areas of research on bioreactors includes increasing production scale and refining the physiological environment, both of which could improve the efficiency and efficacy of bioreactors in research or clinical use. Bioreactors are currently used to study, among other things, cell and tissue level therapies, cell and tissue response to specific physiological environment changes, and development of disease and injury.


=== Long fiber generation ===
In 2013, a group from the University of Tokyo developed cell laden fibers up to a meter in length and on the order of 100 µm in size. These fibers were created using a microfluidic device that forms a double coaxial laminar flow. Each 'layer' of the microfluidic device (cells seeded in ECM, a hydrogel sheath, and finally a calcium chloride solution). The seeded cells culture within the hydrogel sheath for several days, and then the sheath is removed with viable cell fibers. Various cell types were inserted into the ECM core, including myocytes, endothelial cells, nerve cell fibers, and epithelial cell fibers. This group then showed that these fibers can be woven together to fabricate tissues or organs in a mechanism similar to textile weaving. Fibrous morphologies are advantageous in that they provide an alternative to traditional scaffold design, and many organs (such as muscle) are composed of fibrous cells.


=== Bioartificial organs ===

An artificial organ is an engineered device that can be extra corporeal or implanted to support impaired or failing organ systems. Bioartificial organs are typically created with the intent to restore critical biological functions like in the replacement of diseased hearts and lungs, or provide drastic quality of life improvements like in the use of engineered skin on burn victims. While some examples of bioartificial organs are still in the research stage of development due to the limitations involved with creating functional organs, others are currently being used in clinical settings experimentally and commercially.


==== Lung ====
Extracorporeal membrane oxygenation (ECMO) machines, otherwise known as heart and lung machines, are an adaptation of cardiopulmonary bypass techniques that provide heart and lung support. It is used primarily to support the lungs for a prolonged but still temporary timeframe (1–30 days) and allow for recovery from reversible diseases. Robert Bartlett is known as the father of ECMO and performed the first treatment of a newborn using an EMCO machine in 1975.Skin
Tissue-engineered skin is a type of bioartificial organ that is often used to treat burns, diabetic foot ulcers, or other large wounds that cannot heal well on their own. Artificial skin can be made from autografts, allografts, and xenografts. Autografted skin comes from a patient’s own skin, which allows the dermis to have a faster healing rate, and the donor site can be reharvested a few times. Allograft skin often comes from cadaver skin and is mostly used to treat burn victims. Lastly, xenografted skin comes from animals and provides a temporary healing structure for the skin. They assist in dermal regeneration, but cannot become part of the host skin. Tissue-engineered skin is now available in commercial products. Integra, originally used to only treat burns, consists of a collagen matrix and chondroitin sulfate that can be used as a skin replacement. The chondroitin sulfate functions as a component of proteoglycans, which helps to form the extracellular matrix. Integra can be repopulated and revascularized while maintaining its dermal collagen architecture, making it a bioartificial organ  Dermagraft, another commercial-made tissue-engineered skin product, is made out of living fibroblasts. These fibroblasts proliferate and produce growth factors, collagen, and ECM proteins, that help build granulation tissue.


==== Heart ====
Since the number of patients awaiting a heart transplant is continuously increasing over time, and the number of patients on the waiting list surpasses the organ availability, artificial organs used as replacement therapy for terminal heart failure would help alleviate this difficulty.  Artificial hearts are usually used to bridge the heart transplantation or can be applied as replacement therapy for terminal heart malfunction. The total artificial heart (TAH), first introduced by Dr. Vladimir P. Demikhov in 1937, emerged as an ideal alternative. Since then it has been developed and improved as a mechanical pump that provides long-term circulatory support and replaces diseased or damaged heart ventricles that cannot properly pump the blood, restoring thus the pulmonary and systemic flow. Some of the current TAHs include AbioCor, an FDA-approved device that comprises two artificial ventricles and their valves, and does not require subcutaneous connections, and is indicated for patients with biventricular heart failure. In 2010 SynCardia released the portable freedom driver that allows patients to have a portable device without being confined to the hospital.


==== Kidney ====
While kidney transplants are possible, renal failure is more often treated using an artificial kidney. The first artificial kidneys and the majority of those currently in use are extracorporeal, such as with hemodialysis, which filters blood directly, or peritoneal dialysis, which filters via a fluid in the abdomen. In order to contribute to the biological functions of a kidney such as producing metabolic factors or hormones, some artificial kidneys incorporate renal cells. There has been progress in the way of making these devices smaller and more transportable, or even implantable . One challenge still to be faced in these smaller devices is countering the limited volume and therefore limited filtering capabilities.


=== Biomimetics ===

Biomimetics is a field that aims to produce materials and systems that replicate those present in nature. In the context of tissue engineering, this is a common approach used by engineers to create materials for these applications that are comparable to native tissues in terms of their structure, properties, and biocompatibility. Material properties are largely dependent on physical, structural, and chemical characteristics of that material. Subsequently, a biomimetic approach to system design will become significant in material integration, and a sufficient understanding of biological processes and interactions will be necessary. Replication of biological systems and processes may also be used in the synthesis of bio-inspired materials to achieve conditions that produce the desired biological material. Therefore, if a material is synthesized having the same characteristics of biological tissues both structurally and chemically, then ideally the synthesized material will have similar properties. This technique has an extensive history originating from the idea of using natural phenomenon as design inspiration for solutions to human problems. Many modern advancements in technology have been inspired by nature and natural systems, including aircraft, automobiles, architecture, and even industrial systems. Advancements in nanotechnology initiated the application of this technique to micro- and nano-scale problems, including tissue engineering. This technique has been used to develop synthetic bone tissues, vascular technologies, scaffolding materials and integration techniques, and functionalized nanoparticles.


== Constructing neural networks in soft material ==
In 2018, scientists at Brandeis University reported their research on soft material embedded with chemical networks which can mimic the smooth and coordinated behavior of neural tissue. This research was funded by the U.S. Army Research Laboratory. The researchers presented an experimental system of neural networks, theoretically modeled as reaction-diffusion systems. Within the networks was an array of patterned reactors, each performing the Belousov-Zhabotinsky (BZ) reaction. These reactors could function on a nanoliter scale.The researchers state that the inspiration for their project was the movement of the blue ribbon eel. The eel's movements are controlled by electrical impulses determined by a class of neural networks called the central pattern generator.  Central Pattern Generators function within the autonomic nervous system to control bodily functions such as respiration, movement, and peristalsis.Qualities of the reactor that were designed were the network topology, boundary conditions, initial conditions, reactor volume, coupling strength, and the synaptic polarity of the reactor (whether its behavior is inhibitory or excitatory). A BZ emulsion system with a solid elastomer polydimethylsiloxane (PDMS) was designed. Both light and bromine permeable PDMS have been reported as viable methods to create a pacemaker for neural networks.


== Market ==
The history of the tissue engineering market can be divided into three major parts. The time before the crash of the biotech market in the early 2000s, the crash and the time afterward.


=== Beginning ===
Most early progress in tissue engineering research was done in the US. This is due to less strict regulations regarding stem cell research and more available funding than in other countries. This leads to the creation of academic startups many of them coming from Harvard or MIT. Examples are BioHybridTechnologies whose founder, Bill Chick,  went to Harvard Medical School and focused on the creation of artificial pancreas. Another example would be Organogenesis Inc. whose founder went to MIT and worked on skin engineering products. Other companies with links to the MIT are TEI Biosciences, Therics and Guilford Pharmaceuticals. The renewed interest in biotechnologies in the 1980s leads to many private investors investing in these new technologies even though the business models of these early startups were often not very clear and did not present a path to long term profitability. Government sponsors were more restraint in their funding as tissue engineering was considered a high-risk investment.In the UK the market got off to a slower start even though the regulations on stem cell research were not strict as well. This is mainly due to more investors being less willing to invest in these new technologies which were considered to be high-risk investments. Another problem faced by British companies was getting the NHS to pay for their products. This especially because the NHS runs a cost-effectiveness analysis on all supported products. Novel technologies often do not do well in this respect.In Japan, the regulatory situation was quite different. First cell cultivation was only allowed in a hospital setting and second academic scientists employed by state-owned universities were not allowed outside employment until 1998. Moreover, the Japanese authorities took longer to approve new drugs and treatments than there US and European counterparts.For these reasons in the early days of the Japanese market, the focus was mainly on getting products that were already approved elsewhere in Japan and selling them. Contrary to the US market the early actors in Japan were mainly big firms or sub-companies of such big firms, such as J-TEC, Menicon and Terumo,  and not small startups. After regulatory changes in 2014, which allowed cell cultivation outside of a hospital setting, the speed of research in Japan increased and Japanese companies also started to develop their own products.Japanese companies, such as ReproCell and iPS Academia Japan,  are currently working on iPS cell-related products.


=== Crash ===
Soon after the big boom, the first problems started to appear. There were problems getting products approved by the FDA and if they got approved there were often difficulties in getting insurance providers to pay for the products and getting it accepted by health care providers.For example, organogenesis ran into problems marketing its product and integrating its product in the health system. This partially due to the difficulties of handling living cells and the increased difficulties faced by physicians in using these products over conventional methods.Another example would be Advanced Tissue Sciences Dermagraft skin product which could not create a high enough demand without reimbursements from insurance providers. Reasons for this were $4000 price-tag and the circumstance that Additionally Advanced Tissue Sciences struggled to get their product known by physicians.The above examples demonstrate how companies struggled to make profit. This, in turn, lead investors to lose patience and stoping further funding. In consequence, several Tissue Engineering companies such as Organogenesis and Advanced Tissue Sciences filed for bankruptcy in the early 2000s. At this time, these were the only ones having commercial skin products on the market.


=== Reemergence ===
The technologies of the bankrupt or struggling companies were often bought by other companies which continued the development under more conservative business models. Examples of companies who sold their products after folding were Curis and Intercytex.Many of the companies abandoned their long term goals of developing fully functional organs in favour of products and technologies that could turn a profit in the short run. Examples of these kinds of products are products in the cosmetic and testing industry.
In other cases such as in the case of Advanced Tissue Sciences, the founders started new companies.In the 2010s the regulatory framework also started to facilitate faster time to market especially in the USA as new centres and pathways were created by the FDA specifically aimed at products coming from living cells such as the Center for Biologics Evaluation and Research.The first tissue engineering products started to get commercially profitable in the 2010s.


== Regulation ==
In Europe, regulation is currently split into three areas of regulation: medical devices, medicinal products, and biologics. Tissue Engineering products are often of hybrid nature, as they are often composed of cells and a supporting structure. While some products can be approved as medicinal products, others need to gain approval as medical devices. Derksen explains in her thesis that Tissue Engineering researchers are sometimes confronted with regulation that does not fit the characteristics of Tissue Engineering.New regulatory regimes have been observed in Europe that tackle these issues. An explanation for the difficulties in finding regulatory consensus in this matter is given by a survey conducted in the UK. The authors attribute these problems to the close relatedness and overlap with other technologies such as xenotransplantation. It can therefore not be handled separately by regulatory bodies. Regulation is further complicated by the ethical controversies associated with this and related fields of research (e.g. stem cells controversy, ethics of organ transplantation). The same survey as mentioned above  shows on the example of autologous cartilage transplantation that a specific technology can be regarded as ‘pure’ or ‘polluted’ by the same social actor.
Two regulatory movements are most relevant to tissue engineering in the European Union. These are the Directive on standards of quality and safety for the sourcing and processing of human tissues  which was adopted by the European Parliament in 2004 and a proposed cells and the Human Tissue- Engineered Products regulation. The latter was developed under the abscise of the European Commission DG Enterprise and presented in Brussels in 2004.


== See also ==


== Notes ==


== References ==


== External links ==
Clinical Tissue Engineering Center State of Ohio Initiative for Tissue Engineering (National Center for Regenerative Medicine)
Organ Printing Multi-site NSF-funded initiative
LOEX Center Université Laval Initiative for Tissue Engineering","pandas(index=89, _1=89, text='tissue engineering is a biomedical engineering discipline that uses a combination of cells, engineering, materials methods, and suitable biochemical and physicochemical factors to restore, maintain, improve, or replace different types of biological tissues. tissue engineering often involves the use of cells placed on tissue scaffolds in the formation of new viable tissue for a medical purpose but is not limited to applications involving cells and tissue scaffolds. while it was once categorized as a sub-field of biomaterials, having grown in scope and importance it can be considered as a field in its own.  while most definitions of tissue engineering cover a broad range of applications, in practice the term is closely associated with applications that repair or replace portions of or whole tissues (i.e., bone, cartilage, blood vessels, bladder, skin, muscle etc.). often, the tissues involved require certain mechanical and structural properties for proper functioning. the term has also been applied to efforts to perform specific biochemical functions using cells within an artificially-created support system (e.g. an artificial pancreas, or a bio artificial liver). the term regenerative medicine is often used synonymously with tissue engineering, although those involved in regenerative medicine place more emphasis on the use of stem cells or progenitor cells to produce tissues.   == overview ==  a commonly applied definition of tissue engineering, as stated by langer and vacanti, is ""an interdisciplinary field that applies the principles of engineering and life sciences toward the development of biological substitutes that restore, maintain, or improve [biological tissue] function or a whole organ"". in addition, langer and vacanti also state that there are three main types of tissue engineering: cells, tissue-inducing substances, and a cellsmatrix approach (often referred to as a scaffold).tissue engineering has also been defined as ""understanding the principles of tissue growth, and applying this to produce functional replacement tissue for clinical use"". a further description goes on to say that an ""underlying supposition of tissue engineering is that the employment of natural biology of the system will allow for greater success in developing therapeutic strategies aimed at the replacement, repair, maintenance, or enhancement of tissue function"".developments in the multidisciplinary field of tissue engineering have yielded a novel set of tissue replacement parts and implementation strategies. scientific advances in biomaterials, stem cells, growth and differentiation factors, and biomimetic environments have created unique opportunities to fabricate or improve existing tissues in the laboratory from combinations of engineered extracellular matrices (""scaffolds""), cells, and biologically active molecules. among the major challenges now facing tissue engineering is the need for more complex functionality, biomechanical stability, and vascularization in laboratory-grown tissues destined for transplantation. the continued success of tissue engineering and the eventual development of true human replacement parts will grow from the convergence of engineering and basic research advances in tissue, matrix, growth factor, stem cell, and developmental biology, as well as materials science and bioinformatics. in 2003, the nsf published a report entitled ""the emergence of tissue engineering as a research field"", which gives a thorough description of the history of this field.   == etymology == the historic origins of the term are unclear as the definition of the word has changed throughout the past decades. the term first appeared in a 1984 publication that described the organization of an endothelium-like membrane on the surface of a long-implanted, synthetic ophthalmic prosthesis the first modern use of the term as recognized today was in 1985 by the researcher, physiologist and bioengineer y.c fung of the engineering research center. he proposed the joining of the terms tissue (in reference to the fundamental relationship between cells and organs) and engineering (in reference to the field of modification of said tissues). the term was officially adopted in 1987.   == history == the technologies of the bankrupt or struggling companies were often bought by other companies which continued the development under more conservative business models. examples of companies who sold their products after folding were curis and intercytex.many of the companies abandoned their long term goals of developing fully functional organs in favour of products and technologies that could turn a profit in the short run. examples of these kinds of products are products in the cosmetic and testing industry. in other cases such as in the case of advanced tissue sciences, the founders started new companies.in the 2010s the regulatory framework also started to facilitate faster time to market especially in the usa as new centres and pathways were created by the fda specifically aimed at products coming from living cells such as the center for biologics evaluation and research.the first tissue engineering products started to get commercially profitable in the 2010s.   == regulation == in europe, regulation is currently split into three areas of regulation: medical devices, medicinal products, and biologics. tissue engineering products are often of hybrid nature, as they are often composed of cells and a supporting structure. while some products can be approved as medicinal products, others need to gain approval as medical devices. derksen explains in her thesis that tissue engineering researchers are sometimes confronted with regulation that does not fit the characteristics of tissue engineering.new regulatory regimes have been observed in europe that tackle these issues. an explanation for the difficulties in finding regulatory consensus in this matter is given by a survey conducted in the uk. the authors attribute these problems to the close relatedness and overlap with other technologies such as xenotransplantation. it can therefore not be handled separately by regulatory bodies. regulation is further complicated by the ethical controversies associated with this and related fields of research (e.g. stem cells controversy, ethics of organ transplantation). the same survey as mentioned above  shows on the example of autologous cartilage transplantation that a specific technology can be regarded as ‘pure’ or ‘polluted’ by the same social actor. two regulatory movements are most relevant to tissue engineering in the european union. these are the directive on standards of quality and safety for the sourcing and processing of human tissues  which was adopted by the european parliament in 2004 and a proposed cells and the human tissue- engineered products regulation. the latter was developed under the abscise of the european commission dg enterprise and presented in brussels in 2004.   == see also ==   == notes ==   == references ==   == external links == clinical tissue engineering center state of ohio initiative for tissue engineering (national center for regenerative medicine) organ printing multi-site nsf-funded initiative loex center université laval initiative for tissue engineering')"
90,"A heart rate monitor (HRM) is a personal monitoring device that allows one to measure/display heart rate in real time or record the heart rate for later study. It is largely used to gather heart rate data while performing various types of physical exercise. Measuring electrical heart information is referred to as Electrocardiography (ECG or EKG).
Medical heart rate monitoring used in hospitals is usually wired and usually multiple sensors are used. Portable medical units are referred to as a Holter monitor. Consumer heart rate monitors are designed for everyday use and do not use wires to connect.


== History ==
Early models consisted of a monitoring box with a set of electrode leads which attached to the chest. The first wireless EKG heart rate monitor was invented in 1977 by Polar Electro as a training aid for the Finnish National Cross Country Ski team. As ""intensity training"" became a popular concept in athletic circles in the mid-80s, retail sales of wireless personal heart monitors started in 1983.


== Technologies ==

Modern heart rate monitors commonly use one of two different methods to record heart signals (electrical and optical). Both types of signals can provide the same basic heart rate data, using fully automated algorithms to measure heart rate, such as the Pan-Tompkins algorithm.ECG (Electrocardiography) sensors measure the bio-potential generated by electrical signals that control the expansion and contraction of heart chambers, typically implemented in medical devices.
PPG (Photoplethysmography) sensors use a light-based technology to measure the blood volume controlled by the heart's pumping action.


=== Electrical ===
The electrical monitors consist of two elements: a monitor/transmitter, which is worn on a chest strap, and a receiver. When a heartbeat is detected a radio signal is transmitted, which the receiver uses to display/determine the current heart rate. This signal can be a simple radio pulse or a unique coded signal from the chest strap (such as Bluetooth, ANT, or other low-power radio links). Newer technology prevents one user's receiver from using signals from other nearby transmitters (known as cross-talk interference) or eavesdropping. Note the older Polar 5.1 kHz radio transmission technology is usable underwater. Both Bluetooth and Ant+ use the 2.4  GHz radio band, which cannot send signals underwater.


=== Optical ===

More recent devices use optics to measure heart rate by shining light from an LED through the skin and measuring how it scatters off blood vessels. In addition to measuring the heart rate, some devices using this technology are able to measure blood oxygen saturation (SpO2). Some recent optical sensors can also transmit data as mentioned above.
Newer devices such as cell phones or watches can be used to display and/or collect the information. Some devices can simultaneously monitor heart rate, oxygen saturation, and other parameters. These may include sensors such as accelerometers, gyroscopes, and GPS to detect speed, location and distance.
In recent years, it has been common for smartwatches to include heart rate monitors, which has greatly increased popularity.
Some smart watches, smart bands and cell phones often use PPG sensors.


=== Fitness Metrics ===
Garmin, Polar Electro, Suunto and Fitbit are vendors selling consumer heart rate products. Most companies use their own proprietary heart rate algorithms.


== Accuracy ==
The newer, wrist based heart rate monitors have achieved almost identical levels of accuracy as their chest strap counterparts with independent tests showing up to 95% accuracy, but sometimes more than 30% error can persist for several minutes. Optical devices can be less accurate when used during vigorous activity or when used underwater.
Currently heart rate variability is less available on optical devices. Apple introduced HRV data collection to the Apple Watch devices in 2018.


== See also ==
Apple Watch
GPS watch
Activity tracker
Pedometer
eHealth
E-textiles


== References ==


== External links ==
 Media related to Heart rate monitors at Wikimedia Commons","pandas(index=90, _1=90, text='a heart rate monitor (hrm) is a personal monitoring device that allows one to measure/display heart rate in real time or record the heart rate for later study. it is largely used to gather heart rate data while performing various types of physical exercise. measuring electrical heart information is referred to as electrocardiography (ecg or ekg). medical heart rate monitoring used in hospitals is usually wired and usually multiple sensors are used. portable medical units are referred to as a holter monitor. consumer heart rate monitors are designed for everyday use and do not use wires to connect.   == history == early models consisted of a monitoring box with a set of electrode leads which attached to the chest. the first wireless ekg heart rate monitor was invented in 1977 by polar electro as a training aid for the finnish national cross country ski team. as ""intensity training"" became a popular concept in athletic circles in the mid-80s, retail sales of wireless personal heart monitors started in 1983.   == technologies ==  modern heart rate monitors commonly use one of two different methods to record heart signals (electrical and optical). both types of signals can provide the same basic heart rate data, using fully automated algorithms to measure heart rate, such as the pan-tompkins algorithm.ecg (electrocardiography) sensors measure the bio-potential generated by electrical signals that control the expansion and contraction of heart chambers, typically implemented in medical devices. ppg (photoplethysmography) sensors use a light-based technology to measure the blood volume controlled by the heart\'s pumping action. garmin, polar electro, suunto and fitbit are vendors selling consumer heart rate products. most companies use their own proprietary heart rate algorithms.   == accuracy == the newer, wrist based heart rate monitors have achieved almost identical levels of accuracy as their chest strap counterparts with independent tests showing up to 95% accuracy, but sometimes more than 30% error can persist for several minutes. optical devices can be less accurate when used during vigorous activity or when used underwater. currently heart rate variability is less available on optical devices. apple introduced hrv data collection to the apple watch devices in 2018.   == see also == apple watch gps watch activity tracker pedometer ehealth e-textiles   == references ==   == external links == media related to heart rate monitors at wikimedia commons')"
91,"Strategies for Engineered Negligible Senescence (SENS) is the term coined by British biogerontologist Aubrey de Grey for the diverse range of regenerative medical therapies, either planned or currently in development, for the periodical repair of all age-related damage to human tissue with the ultimate purpose of maintaining a state of negligible senescence in the patient, thereby postponing age-associated disease for as long as the therapies are reapplied.The term ""negligible senescence"" was first used in the early 1990s by professor Caleb Finch to describe organisms such as lobsters and hydras, which do not show symptoms of aging. The term ""engineered negligible senescence"" first appeared in print in Aubrey de Grey's 1999 book The Mitochondrial Free Radical Theory of Aging. De Grey called SENS a ""goal-directed rather than curiosity-driven"" approach to the science of aging, and ""an effort to expand regenerative medicine into the territory of aging"".While many biogerontologists find it ""worthy of discussion"", some contend that the ultimate goals of de Grey's programme are too speculative given the current state of technology, referring to it as ""fantasy rather than science"".


== Framework ==

The ultimate objective of SENS is the eventual elimination of age-related diseases and infirmity by repeatedly reducing the state of senescence in the organism. The SENS project consists in implementing a series of periodic medical interventions designed to repair, prevent or render irrelevant all the types of molecular and cellular damage that cause age-related pathology and degeneration, in order to avoid debilitation and death from age-related causes.


=== The Strategies ===
The following table as transcribed from the SENS's website details the following major ailments and preventative strategies:


== Scientific controversy ==
While some fields mentioned as branches of SENS are broadly supported by the medical research community, e.g., stem cell research (RepleniSENS), anti-Alzheimers research (AmyloSENS) and oncogenomics (OncoSENS), the SENS programme as a whole has been a highly controversial proposal, with many critics arguing that the SENS agenda is fanciful and the highly complicated biomedical phenomena involved in the aging process contain too many unknowns for SENS to be fully implementable in the foreseeable future. Cancer may well deserve special attention as an aging-associated disease (OncoSENS), but the SENS claim that nuclear DNA damage only matters for aging because of cancer has been challenged in the literature as well as by material in the article DNA damage theory of aging.
In November 2005, 28 biogerontologists published a statement of criticism in EMBO Reports, ""Science fact and the SENS agenda: what can we reasonably expect from ageing research?,"" arguing ""each one of the specific proposals that comprise the SENS agenda is, at our present stage of ignorance, exceptionally optimistic,"" and that some of the specific proposals ""will take decades of hard work [to be medically integrated], if [they] ever prove to be useful."" The researchers argue that while there is ""a rationale for thinking that we might eventually learn how to postpone human illnesses to an important degree,"" increased basic research, rather than the goal-directed approach of SENS, is presently the scientifically appropriate goal.
More recently, biogerontologist Marios Kyriazis has sharply criticised the clinical applicability of SENS claiming that such therapies, even if developed in the laboratory, would be practically unusable by the general public. De Grey responded to one such criticism.


=== Technology Review controversy ===
In February 2005, Technology Review, which is owned by the Massachusetts Institute of Technology, published an article by Sherwin Nuland, a Clinical Professor of Surgery at Yale University and the author of ""How We Die"", that drew a skeptical portrait of SENS, at the time de Grey was a computer associate in the Flybase Facility of the Department of Genetics at the University of Cambridge.
During June 2005, David Gobel, CEO and Co-founder of Methuselah Foundation offered Technology Review $20,000 to fund a prize competition to publicly clarify the viability of the SENS approach. In July 2005, Pontin announced a $20,000 prize, funded 50/50 by Methuselah Foundation and MIT Technology Review, open to any molecular biologist, with a record of publication in biogerontology, who could prove that the alleged benefits of SENS were ""so wrong that it is unworthy of learned debate."" Technology Review received five submissions to its Challenge. In March 2006, Technology Review announced that it had chosen a panel of judges for the Challenge: Rodney Brooks, Anita Goel, Nathan Myhrvold, Vikram Sheel Kumar, and Craig Venter. Three of the five submissions met the terms of the prize competition. They were published by Technology Review on June 9, 2006. On July 11, 2006, Technology Review published the results of the SENS Challenge.In the end, no one won the $20,000 prize. The judges felt that no submission met the criterion of the challenge and discredited SENS, although they unanimously agreed that one submission, by Preston Estep and his colleagues, was the most eloquent. Craig Venter succinctly expressed the prevailing opinion: ""Estep et al. ... have not demonstrated that SENS is unworthy of discussion, but the proponents of SENS have not made a compelling case for it."" Summarizing the judges' deliberations, Pontin wrote that SENS is ""highly speculative"" and that many of its proposals could not be reproduced with the scientific technology of that period. Myhrvold described SENS as belonging to a kind of ""antechamber of science"" where they wait until technology and scientific knowledge advance to the point where it can be tested.


== SENS Research Foundation ==

The SENS Research Foundation is a non-profit organization co-founded by Michael Kope, Aubrey de Grey, Jeff Hall, Sarah Marr and Kevin Perrott, which is based in California, United States. Its activities include SENS-based research programs and public relations work for the acceptance of and interest in related research.


== See also ==


== References ==","pandas(index=91, _1=91, text='strategies for engineered negligible senescence (sens) is the term coined by british biogerontologist aubrey de grey for the diverse range of regenerative medical therapies, either planned or currently in development, for the periodical repair of all age-related damage to human tissue with the ultimate purpose of maintaining a state of negligible senescence in the patient, thereby postponing age-associated disease for as long as the therapies are reapplied.the term ""negligible senescence"" was first used in the early 1990s by professor caleb finch to describe organisms such as lobsters and hydras, which do not show symptoms of aging. the term ""engineered negligible senescence"" first appeared in print in aubrey de grey\'s 1999 book the mitochondrial free radical theory of aging. de grey called sens a ""goal-directed rather than curiosity-driven"" approach to the science of aging, and ""an effort to expand regenerative medicine into the territory of aging"".while many biogerontologists find it ""worthy of discussion"", some contend that the ultimate goals of de grey\'s programme are too speculative given the current state of technology, referring to it as ""fantasy rather than science"".   == framework ==  the ultimate objective of sens is the eventual elimination of age-related diseases and infirmity by repeatedly reducing the state of senescence in the organism. the sens project consists in implementing a series of periodic medical interventions designed to repair, prevent or render irrelevant all the types of molecular and cellular damage that cause age-related pathology and degeneration, in order to avoid debilitation and death from age-related causes. in february 2005, technology review, which is owned by the massachusetts institute of technology, published an article by sherwin nuland, a clinical professor of surgery at yale university and the author of ""how we die"", that drew a skeptical portrait of sens, at the time de grey was a computer associate in the flybase facility of the department of genetics at the university of cambridge. during june 2005, david gobel, ceo and co-founder of methuselah foundation offered technology review $20,000 to fund a prize competition to publicly clarify the viability of the sens approach. in july 2005, pontin announced a $20,000 prize, funded 50/50 by methuselah foundation and mit technology review, open to any molecular biologist, with a record of publication in biogerontology, who could prove that the alleged benefits of sens were ""so wrong that it is unworthy of learned debate."" technology review received five submissions to its challenge. in march 2006, technology review announced that it had chosen a panel of judges for the challenge: rodney brooks, anita goel, nathan myhrvold, vikram sheel kumar, and craig venter. three of the five submissions met the terms of the prize competition. they were published by technology review on june 9, 2006. on july 11, 2006, technology review published the results of the sens challenge.in the end, no one won the $20,000 prize. the judges felt that no submission met the criterion of the challenge and discredited sens, although they unanimously agreed that one submission, by preston estep and his colleagues, was the most eloquent. craig venter succinctly expressed the prevailing opinion: ""estep et al. ... have not demonstrated that sens is unworthy of discussion, but the proponents of sens have not made a compelling case for it."" summarizing the judges\' deliberations, pontin wrote that sens is ""highly speculative"" and that many of its proposals could not be reproduced with the scientific technology of that period. myhrvold described sens as belonging to a kind of ""antechamber of science"" where they wait until technology and scientific knowledge advance to the point where it can be tested.   == sens research foundation ==  the sens research foundation is a non-profit organization co-founded by michael kope, aubrey de grey, jeff hall, sarah marr and kevin perrott, which is based in california, united states. its activities include sens-based research programs and public relations work for the acceptance of and interest in related research.   == see also ==   == references ==')"
92,"Fantastic Voyage: Live Long Enough to Live Forever (Rodale Books, ISBN 1-57954-954-3) is a book authored by Ray Kurzweil and Terry Grossman published in 2004. The basic premise of the book is that if middle aged people can live long enough, until approximately 120 years, they will be able to live forever—as humanity overcomes all diseases and old age itself. This might also be considered a break-even scenario where developments made during a year increase life expectancy by more than one year. Biogerontologist Aubrey de Grey called this the ""Longevity escape velocity"" in a 2005 TED talk.The book focuses primarily on health topics such as heart disease, cancer, and type 2 diabetes. It promotes lifestyle changes such as a low glycemic index diet, calorie restriction, exercise, drinking green tea and alkalinized water, and other changes to daily living. They also promote aggressive supplementation to make up for nutrient deficiencies they believe are common in Western society.  In contrast to his previous book The 10% Solution for a Healthy Life, in which he recommended a diet with 10% of calories from fat, in this book, Kurzweil recommends consuming less than one third of calories from carbohydrates (and less than one sixth of calories in his low-carbohydrate diet) and consuming 25% of calories from fat.The book states that the purpose of these changes is to obtain and maintain idyllic health so that an individual can extend his or her life as long as possible. The authors believe that within the next 20 to 50 years technology will advance to the point where much of the aging process will be conquered, and degenerative diseases eliminated. The book is peppered with side notes on these futuristic topics, showing how current research is leading us toward life extension, and explaining how future technologies such as nanotechnology and bioengineering might change the way humans live their lives. Ray Kurzweil discusses these topics at further length in his 2005 book The Singularity Is Near.
A follow-up on Fantastic Voyage, Transcend: Nine Steps to Living Well Forever, was released on April 28, 2009.


== Organization ==
Chapter 1: You can live long enough to live forever
Chapter 2: The bridges to come
Chapter 3: Our personal journeys
Chapter 4: Food and water
Chapter 5: Carbohydrates and the glycemic load
Chapter 6: Fat and protein
Chapter 7: You are what you digest
Chapter 8: Change your weight for life in one day
Chapter 9: The problem with sugar (and insulin)
Chapter 10: Ray's personal program
Chapter 11: The promise of genomics
Chapter 12: Inflammation—the latest ""smoking gun""
Chapter 13: Methylation—critically important to your health
Chapter 14: Cleaning up the mess: Toxins and detoxification
Chapter 15: The real cause of heart disease and how to prevent it
Chapter 16: The prevention and early detection of cancer
Chapter 17: Terry's personal program
Chapter 18: Your brain: The power of thinking...and of ideas
Chapter 19: Hormones and aging, hormones of youth
Chapter 20: Other hormones of youth: Sex hormones
Chapter 21: Aggressive supplementation
Chapter 22: Keep moving: The power of exercise
Chapter 23: Stress and balance
Epilogue


== Criticisms ==
One claim in the book has been called pseudoscience. Dr. Stephen Lower, retired Professor of Chemistry at Simon Fraser University, disputes some of the book's statements about alkaline water on his web site. Kurzweil and Grossman counter this specific criticism directly in their Reader Q&A.


== See also ==
Nutrition
Simulated reality
Technological singularity


== References ==


== External links ==
Fantastic-voyage.net
Short Guide - lifestyle changes from the book in bullet point format
Reader Q&A - response to criticism of alkaline water claim","pandas(index=92, _1=92, text='fantastic voyage: live long enough to live forever (rodale books, isbn 1-57954-954-3) is a book authored by ray kurzweil and terry grossman published in 2004. the basic premise of the book is that if middle aged people can live long enough, until approximately 120 years, they will be able to live forever—as humanity overcomes all diseases and old age itself. this might also be considered a break-even scenario where developments made during a year increase life expectancy by more than one year. biogerontologist aubrey de grey called this the ""longevity escape velocity"" in a 2005 ted talk.the book focuses primarily on health topics such as heart disease, cancer, and type 2 diabetes. it promotes lifestyle changes such as a low glycemic index diet, calorie restriction, exercise, drinking green tea and alkalinized water, and other changes to daily living. they also promote aggressive supplementation to make up for nutrient deficiencies they believe are common in western society.  in contrast to his previous book the 10% solution for a healthy life, in which he recommended a diet with 10% of calories from fat, in this book, kurzweil recommends consuming less than one third of calories from carbohydrates (and less than one sixth of calories in his low-carbohydrate diet) and consuming 25% of calories from fat.the book states that the purpose of these changes is to obtain and maintain idyllic health so that an individual can extend his or her life as long as possible. the authors believe that within the next 20 to 50 years technology will advance to the point where much of the aging process will be conquered, and degenerative diseases eliminated. the book is peppered with side notes on these futuristic topics, showing how current research is leading us toward life extension, and explaining how future technologies such as nanotechnology and bioengineering might change the way humans live their lives. ray kurzweil discusses these topics at further length in his 2005 book the singularity is near. a follow-up on fantastic voyage, transcend: nine steps to living well forever, was released on april 28, 2009.   == organization == chapter 1: you can live long enough to live forever chapter 2: the bridges to come chapter 3: our personal journeys chapter 4: food and water chapter 5: carbohydrates and the glycemic load chapter 6: fat and protein chapter 7: you are what you digest chapter 8: change your weight for life in one day chapter 9: the problem with sugar (and insulin) chapter 10: ray\'s personal program chapter 11: the promise of genomics chapter 12: inflammation—the latest ""smoking gun"" chapter 13: methylation—critically important to your health chapter 14: cleaning up the mess: toxins and detoxification chapter 15: the real cause of heart disease and how to prevent it chapter 16: the prevention and early detection of cancer chapter 17: terry\'s personal program chapter 18: your brain: the power of thinking...and of ideas chapter 19: hormones and aging, hormones of youth chapter 20: other hormones of youth: sex hormones chapter 21: aggressive supplementation chapter 22: keep moving: the power of exercise chapter 23: stress and balance epilogue   == criticisms == one claim in the book has been called pseudoscience. dr. stephen lower, retired professor of chemistry at simon fraser university, disputes some of the book\'s statements about alkaline water on his web site. kurzweil and grossman counter this specific criticism directly in their reader q&a.   == see also == nutrition simulated reality technological singularity   == references ==   == external links == fantastic-voyage.net short guide - lifestyle changes from the book in bullet point format reader q&a - response to criticism of alkaline water claim')"
93,"Sensory substitution is a change of the characteristics of one sensory modality into stimuli of another sensory modality.
A sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. The sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. In case the sensor obtains signals of a kind not originally available to the bearer it is a case of sensory augmentation. Sensory substitution concerns human perception and the plasticity of the human brain; and therefore, allows us to study these aspects of neuroscience more through neuroimaging.
Sensory substitution systems may help people by restoring their ability to perceive  certain defective sensory modality by using sensory information from a functioning sensory modality.


== History ==
The idea of sensory substitution was introduced in the '80s by Paul Bach-y-Rita as a means of using one sensory modality, mainly taction, to gain environmental information to be used by another sensory modality, mainly vision. Thereafter, the entire field was discussed by Chaim-Meyer Scheff in ""Experimental model for the study of changes in the organization of human sensory information processing through the design and testing of non-invasive prosthetic devices for sensory impaired people"". The first sensory substitution system was developed by Bach-y-Rita et al. as a means of brain plasticity in congenitally blind individuals. After this historic invention, sensory substitution has been the basis of many studies investigating perceptive and cognitive neuroscience. Since then, sensory substitution has contributed to the study of brain function, human cognition and rehabilitation.


== Physiology ==
When a person becomes blind or deaf they generally do not lose the ability to hear or see; they simply lose their ability to transmit the sensory signals from the periphery (retina for visions and cochlea for hearing) to brain. Since the vision processing pathways are still intact, a person who has lost the ability to retrieve data from the retina can still see subjective images by using data gathered from other sensory modalities such as touch or audition.In a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. Because it is the brain that is responsible for the final perception, sensory substitution is possible. During sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive sight. With sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. Touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. For example, through fMRI, one can determine which parts of the brain are activated during sensory perception. In blind persons, it is seen that while they are only receiving tactile information, their visual cortex is also activated as they perceive sight objects. Touch-to-touch sensory substitution is also possible, wherein information from touch receptors of one region of the body can be used to perceive touch in another region. For example, in one experiment by Bach-y-Rita,touch perception was able to be restored in a patient who lost peripheral sensation due to leprosy.


=== Technological support ===
In order to achieve sensory substitution and stimulate the brain without intact sensory organs to relay the information, machines can be used to do the signal transduction, rather than the sensory organs. This brain–machine interface collects external signals and transforms them into electrical signals for the brain to interpret. Generally, a camera or a microphone is used to collect visual or auditory stimuli that are used to replace lost sight and hearing, respectively. The visual or auditory data collected from the sensors is transformed into tactile stimuli that are then relayed to the brain for visual and auditory perception. This and all types of sensory substitution are only possible due to neuroplasticity.


=== Brain plasticity ===
Brain plasticity refers to the brain's ability to adapt to a changing environment, for instance to the absence or deterioration of a sense. It is conceivable that cortical remapping or reorganization in response to the loss of one sense may be an evolutionary mechanism that allows people to adapt and compensate by using other senses better. Functional imaging of congenitally blind patients showed a cross-modal recruitment of the occipital cortex during perceptual tasks such as Braille reading, tactile perception, tactual object recognition, sound localization, and sound discrimination. This may suggest that blind people can use their occipital lobe, generally used for vision, to perceive objects through the use of other sensory modalities. This cross modal plasticity may explain the often described tendency of blind people to show enhanced ability in the other senses.


=== Perception versus sensing ===
While considering the physiological aspects of sensory substitution, it is essential to distinguish between sensing and perceiving. The general question posed by this differentiation is: Are blind people seeing or perceiving to see by putting together different sensory data? While sensation comes in one modality – visual, auditory, tactile etc. – perception due to sensory substitution is not one modality but a result of cross-modal interactions. It is therefore concluded that while sensory substitution for vision induces visual-like perception in sighted individuals, it induces auditory or tactile perception in blind individuals. In short, blind people perceive to see through touch and audition with sensory substitution.


== Different applications ==
Applications are not restricted to handicapped persons, but also include artistic presentations, games, and augmented reality. Some examples are substitution of visual stimuli to audio or tactile, and of audio stimuli to tactile. Some of the most popular are probably Paul Bach-y-Rita's Tactile Vision Sensory Substitution (TVSS), developed with Carter Collins at Smith-Kettlewell Institute and Peter Meijer's Seeing with Sound approach (The vOICe). Technical developments, such as miniaturization and electrical stimulation help the advance of sensory substitution devices.
In sensory substitution systems, we generally have sensors that collect the data from the external environment. This data is then relayed to a coupling system that interprets and transduces the information and then replays it to a stimulator. This stimulator ultimately stimulates a functioning sensory modality. After training, people learn to use the information gained from this stimulation to experience a perception of the sensation they lack instead of the actually stimulated sensation. For example, a leprosy patient, whose perception of peripheral touch was restored, was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead.


=== Tactile systems ===
To understand tactile sensory substitution it is essential to understand some basic physiology of the tactile receptors of the skin. There are five basic types of tactile receptors: Pacinian corpuscle, Meissner's corpuscle, Ruffini endings, Merkel nerve endings, and free nerve endings. These receptors are mainly characterized by which type of stimuli best activates them, and by their rate of adaptation to sustained stimuli. Because of the rapid adaptation of some of these receptors to sustained stimuli, those receptors require rapidly changing tactile stimulation systems in order to be optimally activated. Among all these mechanoreceptors Pacinian corpuscle offers the highest sensitivity to high frequency vibration starting from few 10s of Hz to a few kHz with the help of its specialized mechanotransduction mechanism.There have been two different types of stimulators: electrotactile or vibrotactile. Electrotactile stimulators use direct electrical stimulation of the nerve ending in the skin to initiate the action potentials; the sensation triggered, burn, itch, pain, pressure etc. depends on the stimulating voltage. Vibrotactile stimulators use pressure and the properties of the mechanoreceptors of the skin to initiate action potentials. There are advantages and disadvantages for both these stimulation systems. With the electrotactile stimulating systems a lot of factors affect the sensation triggered: stimulating voltage, current, waveform, electrode size, material, contact force, skin location, thickness and hydration. Electrotactile stimulation may involve the direct stimulation of the nerves (percutaneous), or through the skin (transcutaneous). Percutaneous application causes additional distress to the patient, and is a major disadvantage of this approach. Furthermore, stimulation of the skin without insertion leads to the need for high voltage stimulation because of the high impedance of the dry skin, unless the tongue is used as a receptor, which requires only about 3% as much voltage. This latter technique is undergoing clinical trials for various applications, and been approved for assistance to the blind in the UK. Alternatively, the roof of the mouth has been proposed as another area where low currents can be felt.Electrostatic arrays are explored as human-computer interaction devices for touch screens. These are based on a phenomenon called electrovibration, which allows microamperre-level currents to be felt as roughness on a surface.Vibrotactile systems use the properties of mechanoreceptors in the skin so they have fewer parameters that need to be monitored as compared to electrotactile stimulation. However, vibrotactile stimulation systems need to account for the rapid adaptation of the tactile sense.
Another important aspect of tactile sensory substitution systems is the location of the tactile stimulation. Tactile receptors are abundant on the fingertips, face, and tongue while sparse on the back, legs and arms. It is essential to take into account the spatial resolution of the receptor as it has a major effect on the resolution of the sensory substitution. A high resolution pin-arrayed display is able to present spatial information via tactile symbols, such as city maps and obstacle maps.Below you can find some descriptions of current tactile substitution systems.


==== Tactile–visual ====
One of the earliest and most well known form of sensory substitution devices was Paul Bach-y-Rita's TVSS that converted the image from a video camera into a tactile image and coupled it to the tactile receptors on the back of his blind subject. Recently, several new systems have been developed that interface the tactile image to tactile receptors on different areas of the body such as the on the chest, brow, fingertip, abdomen, and forehead. The tactile image is produced by hundreds of activators placed on the person. The activators are solenoids of one millimeter diameter. In experiments, blind (or blindfolded) subjects equipped with the TVSS can learn to detect shapes and to orient themselves. In the case of simple geometric shapes, it took around 50 trials to achieve 100 percent correct recognition. To identify objects in different orientations requires several hours of learning.
A system using the tongue as the human-machine interface is most practical.  The tongue-machine interface is both protected by the closed mouth and the saliva in the mouth provides a good electrolytic environment that ensures good electrode contact. Results from a study by Bach-y-Rita et al. show that electrotactile stimulation of the tongue required 3% of the voltage required to stimulate the finger. Also, since it is more practical to wear an orthodontic retainer holding the stimulation system than an apparatus strapped to other parts of the body, the tongue-machine interface is more popular among TVSS systems.
This tongue TVSS system works by delivering electrotactile stimuli to the dorsum of the tongue via a flexible electrode array placed in the mouth. This electrode array is connected to a Tongue Display Unit [TDU] via a ribbon cable passing out of the mouth. A video camera records a picture, transfers it to the TDU for conversion into a tactile image. The tactile image is then projected onto the tongue via the ribbon cable where the tongue's receptors pick up the signal. After training, subjects are able to associate certain types of stimuli to certain types of visual images. In this way, tactile sensation can be used for visual perception.
Sensory substitutions have also been successful with the emergence of wearable haptic actuators like vibrotactile motors, solenoids, peltier diodes, etc. At the Center for Cognitive Ubiquitous Computing at Arizona State University, researchers have developed technologies that enable people who are blind to perceive social situational information using wearable vibrotactile belts (Haptic Belt) and gloves (VibroGlove). Both technologies use miniature cameras that are mounted on a pair of glasses worn by the user who is blind. The Haptic Belt provides vibrations that convey the direction and distance at which a person is standing in front of a user, while the VibroGlove uses spatio-temporal mapping of vibration patterns to convey facial expressions of the interaction partner. Alternatively, it has been shown that even very simple cues indicating the presence or absence of obstacles (through small vibration modules located at strategic places in the body) can be useful for navigation, gait stabilization and reduced anxiety when evolving in an unknown space. This approach, called the ""Haptic Radar"" has been studied since 2005 by researchers at the University of Tokyo in collaboration with the University of Rio de Janeiro. Similar products include the Eyeronman vest and belt, and the forehead retina system.


==== Tactile–auditory ====
Neuroscientist David Eagleman presented a new device for sound-to-touch hearing at TED in 2015; his laboratory research then expanded into a company based in Palo Alto, California, called Neosensory.  Neosensory devices capture sound and turn them into high-dimensional patterns of touch on the skin.Experiments by Schurmann et al. show that tactile senses can activate the human auditory cortex. Currently vibrotactile stimuli can be used to facilitate hearing in normal and hearing-impaired people. To test for the auditory areas activated by touch, Schurmann et al. tested subjects while stimulating their fingers and palms with vibration bursts and their fingertips with tactile pressure. They found that tactile stimulation of the fingers lead to activation of the auditory belt area, which suggests that there is a relationship between audition and tactition. Therefore, future research can be done to investigate the likelihood of a tactile–auditory sensory substitution system. One promising invention is the 'Sense organs synthesizer' which aims at delivering a normal hearing range of nine octaves via 216 electrodes to sequential touch nerve zones, next to the spine.


==== Tactile–vestibular ====
Some people with balance disorders or adverse reactions to antibiotics suffer from bilateral vestibular damage (BVD). They experience difficulty maintaining posture, unstable gait, and oscillopsia. Tyler et al. studied the restitution of postural control through a tactile for vestibular sensory substitution. Because BVD patients cannot integrate visual and tactile cues, they have a lot of difficulty standing. Using a head-mounted accelerometer and a brain-machine interface that employs electrotactile stimulation on the tongue, information about head-body orientation was relayed to the patient so that a new source of data is available to orient themselves and maintain good posture.


==== Tactile–tactile to restore peripheral sensation ====
Touch to touch sensory substitution is where information from touch receptors of one region can be used to perceive touch in another. For example, in one experiment by Bach-y-Rita, the touch perception was restored in a patient who lost peripheral sensation from leprosy. For example, this leprosy patient was equipped with a glove containing artificial contact sensors coupled to skin sensory receptors on the forehead (which was stimulated). After training and acclimation, the patient was able to experience data from the glove as if it was originating in the fingertips while ignoring the sensations in the forehead. After two days of training one of the leprosy subjects reported ""the wonderful sensation of touching his wife, which he had been unable to experience for 20 years.""


==== Tactile feedback system for prosthetic limbs ====
The development of new technologies has now made it plausible to provide patients with prosthetic arms with tactile and kinesthetic sensibilities. While this is not purely a sensory substitution system, it uses the same principles to restore perception of senses. Some tactile feedback methods of restoring a perception of touch to amputees would be direct or micro stimulation of the tactile nerve afferents.Other applications of sensory substitution systems can be seen in function robotic prostheses for patients with high level quadriplegia. These robotic arms have several mechanisms of slip detection, vibration and texture detection that they relay to the patient through feedback.  After more research and development, the information from these arms can be used by patients to perceive that they are holding and manipulating objects while their robotic arm actually accomplishes the task.


=== Auditory systems ===
Auditory sensory substitution systems like the tactile sensory substitution systems aim to use one sensory modality to compensate for the lack of another in order to gain a perception of one that is lacking. With auditory sensory substitution, visual or tactile sensors detect and store information about the external environment. This information is then transformed by interfaces into sound. Most systems are auditory-vision substitutions aimed at using the sense of hearing to convey visual information to the blind.


==== The vOICe Auditory Display ====
""The vOICe"" converts live camera views from a video camera into soundscapes, patterns of scores of different tones at different volumes and pitches emitted simultaneously. The technology of the vOICe was invented in the 1990s by Peter Meijer and uses general video to audio mapping by associating height to pitch and brightness with loudness in a left-to-right scan of any video frame.


==== EyeMusic ====
The EyeMusic user wears a miniature camera connected to a small computer (or smartphone) and stereo headphones. The images are converted into ""soundscapes"". The high locations on the image are projected as high-pitched musical notes on a pentatonic scale, and low vertical locations as low-pitched musical notes.
The EyeMusic conveys color information by using different musical instruments for each of the following five colors: white, blue, red, green, yellow. The EyeMusic employs an intermediate resolution of 30×50 pixels.


==== LibreAudioView ====
This project, presented in 2015, proposes a new versatile mobile device and a sonification method specifically designed to the pedestrian locomotion of the visually impaired. It sonifies in real-time spatial information from a video stream acquired at a standard frame rate. The device is composed of a miniature camera integrated into a glasses frame which is connected to a battery-powered minicomputer worn around the neck with a strap. The audio signal is transmitted to the user via running headphones. This system has two operating modes. With the first mode, when the user is static, only the edges of the moving objects are sonified. With the second mode, when the user is moving, the edges of both static and moving objects are sonified. Thus, the video stream is simplified by extracting only the edges of objects that can become dangerous obstacles. The system enables the localization of moving objects, the estimation of trajectories, and the detection of approaching objects.


==== PSVA ====
Another successful visual-to-auditory sensory substitution device is the Prosthesis Substituting Vision for Audition (PSVA). This system utilizes a head-mounted TV camera that allows real-time, online translation of visual patterns into sound. While the patient moves around, the device captures visual frames at a high frequency and generates the corresponding complex sounds that allow recognition. Visual stimuli are transduced into auditory stimuli with the use of a system that uses pixel to frequency relationship and couples a rough model of the human retina with an inverse model of the cochlea.


==== The Vibe ====
The sound produced by this software is a mixture of sinusoidal sounds produced by virtual ""sources"", corresponding each to a ""receptive field"" in the image. Each receptive field is a set of localized pixels. The sound's amplitude is determined by the mean luminosity of the pixels of the corresponding receptive field. The frequency and the inter-aural disparity are determined by the center of gravity of the co-ordinates of the receptive field's pixels in the image (see ""There is something out there: distal attribution in sensory substitution, twenty years later""; Auvray M., Hanneton S., Lenay C., O'Regan K. Journal of Integrative Neuroscience 4 (2005) 505-21). The Vibe is an Open Source project hosted by Sourceforge.


==== Other systems ====
Other approaches to the substitution of hearing for vision use binaural directional cues, much as natural human echolocation does.  An example of the latter approach is the ""SeeHear"" chip from Caltech.Other visual-auditory substitution devices deviate from the vOICe's greyscale mapping of images. Zach Capalbo's Kromophone uses a basic color spectrum correlating to different sounds and timbres to give users perceptual information beyond the vOICe's capabilities.


=== Nervous system implants ===
By means of stimulating electrodes implanted into the human nervous system, it is possible to apply current pulses to be learned and reliably recognized by the recipient. It has been shown successfully in experimentation, by Kevin Warwick, that signals can be employed from force/touch indicators on a robot hand as a means of communication.


== Criticism ==
It has been argued that the term ""substitution"" is misleading, as it is merely an ""addition"" or ""supplementation"" not a substitution of a sensory modality.


== Sensory augmentation ==

Building upon the research conducted on sensory substitution, investigations into the possibility of augmenting the body's sensory apparatus are now beginning. The intention is to extend the body's ability to sense aspects of the environment that are not normally perceivable by the body in its natural state.
Active work in this direction is being conducted by, among others, the e-sense project of the Open University and Edinburgh University, the feelSpace project of the University of Osnabrück, and the hearSpace project at University of Paris.
The findings of research into sensory augmentation (as well as sensory substitution in general) that investigate the emergence of perceptual experience (qualia) from the activity of neurons have implications for the understanding of consciousness.


== See also ==
Biological neural network
Brain implant
Human echolocation, blind people navigating by listening to the echo of sounds


== References ==


== External links ==
Tongue display for sensory substitution
The vOICe auditory display for sensory substitution.
Artificial Retinas
Sensory Substitution:limits and perspectives C. Lenay et al.
The Vibe
feelSpace - The Magnetic Perception Group of the University of Osnabrück
The Kromophone
Sensory Substitution For Blind (Nihat Erim İnceoğlu)
Sensory augmentation: integration of an auditory compass signal into human perception of space","pandas(index=93, _1=93, text='sensory substitution is a change of the characteristics of one sensory modality into stimuli of another sensory modality. a sensory substitution system consists of three parts: a sensor, a coupling system, and a stimulator. the sensor records stimuli and gives them to a coupling system which interprets these signals and transmits them to a stimulator. in case the sensor obtains signals of a kind not originally available to the bearer it is a case of sensory augmentation. sensory substitution concerns human perception and the plasticity of the human brain; and therefore, allows us to study these aspects of neuroscience more through neuroimaging. sensory substitution systems may help people by restoring their ability to perceive  certain defective sensory modality by using sensory information from a functioning sensory modality.   == history == the idea of sensory substitution was introduced in the \'80s by paul bach-y-rita as a means of using one sensory modality, mainly taction, to gain environmental information to be used by another sensory modality, mainly vision. thereafter, the entire field was discussed by chaim-meyer scheff in ""experimental model for the study of changes in the organization of human sensory information processing through the design and testing of non-invasive prosthetic devices for sensory impaired people"". the first sensory substitution system was developed by bach-y-rita et al. as a means of brain plasticity in congenitally blind individuals. after this historic invention, sensory substitution has been the basis of many studies investigating perceptive and cognitive neuroscience. since then, sensory substitution has contributed to the study of brain function, human cognition and rehabilitation.   == physiology == when a person becomes blind or deaf they generally do not lose the ability to hear or see; they simply lose their ability to transmit the sensory signals from the periphery (retina for visions and cochlea for hearing) to brain. since the vision processing pathways are still intact, a person who has lost the ability to retrieve data from the retina can still see subjective images by using data gathered from other sensory modalities such as touch or audition.in a regular visual system, the data collected by the retina is converted into an electrical stimulus in the optic nerve and relayed to the brain, which re-creates the image and perceives it. because it is the brain that is responsible for the final perception, sensory substitution is possible. during sensory substitution an intact sensory modality relays information to the visual perception areas of the brain so that the person can perceive sight. with sensory substitution, information gained from one sensory modality can reach brain structures physiologically related to other sensory modalities. touch-to-visual sensory substitution transfers information from touch receptors to the visual cortex for interpretation and perception. for example, through fmri, one can determine which parts of the brain are activated during sensory perception. in blind persons, it is seen that while they are only receiving tactile information, their visual cortex is also activated as they perceive sight objects. touch-to-touch sensory substitution is also possible, wherein information from touch receptors of one region of the body can be used to perceive touch in another region. for example, in one experiment by bach-y-rita,touch perception was able to be restored in a patient who lost peripheral sensation due to leprosy. by means of stimulating electrodes implanted into the human nervous system, it is possible to apply current pulses to be learned and reliably recognized by the recipient. it has been shown successfully in experimentation, by kevin warwick, that signals can be employed from force/touch indicators on a robot hand as a means of communication.   == criticism == it has been argued that the term ""substitution"" is misleading, as it is merely an ""addition"" or ""supplementation"" not a substitution of a sensory modality.   == sensory augmentation ==  building upon the research conducted on sensory substitution, investigations into the possibility of augmenting the body\'s sensory apparatus are now beginning. the intention is to extend the body\'s ability to sense aspects of the environment that are not normally perceivable by the body in its natural state. active work in this direction is being conducted by, among others, the e-sense project of the open university and edinburgh university, the feelspace project of the university of osnabrück, and the hearspace project at university of paris. the findings of research into sensory augmentation (as well as sensory substitution in general) that investigate the emergence of perceptual experience (qualia) from the activity of neurons have implications for the understanding of consciousness.   == see also == biological neural network brain implant human echolocation, blind people navigating by listening to the echo of sounds   == references ==   == external links == tongue display for sensory substitution the voice auditory display for sensory substitution. artificial retinas sensory substitution:limits and perspectives c. lenay et al. the vibe feelspace - the magnetic perception group of the university of osnabrück the kromophone sensory substitution for blind (nihat erim i̇nceoğlu) sensory augmentation: integration of an auditory compass signal into human perception of space')"
94,"An implant is a medical device manufactured to replace a missing biological structure, support a damaged biological structure, or enhance an existing biological structure. Medical implants are man-made devices, in contrast to a transplant, which is a transplanted biomedical tissue. The surface of implants that contact the body might be made of a biomedical material such as titanium, silicone, or apatite depending on what is the most functional. In some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. Some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.


== Applications ==
Implants can roughly be categorized into groups by application:


=== Sensory and neurological ===
Sensory and neurological implants are used for disorders affecting the major senses and the brain, as well as other neurological disorders. They are predominately used in the treatment of conditions such as cataract, glaucoma, keratoconus, and other visual impairments; otosclerosis and other hearing loss issues, as well as middle ear diseases such as otitis media; and neurological diseases such as epilepsy, Parkinson's disease, and treatment-resistant depression. Examples include the intraocular lens, intrastromal corneal ring segment, cochlear implant, tympanostomy tube, and neurostimulator.


=== Cardiovascular ===
Cardiovascular medical devices are implanted in cases where the heart, its valves, and the rest of the circulatory system is in disorder. They are used to treat conditions such as heart failure, cardiac arrhythmia, ventricular tachycardia, valvular heart disease, angina pectoris, and atherosclerosis. Examples include the artificial heart, artificial heart valve, implantable cardioverter-defibrillator, cardiac pacemaker, and coronary stent.


=== Orthopedic ===
Orthopaedic implants help alleviate issues with the bones and joints of the body. They're used to treat bone fractures, osteoarthritis, scoliosis, spinal stenosis, and chronic pain. Examples include a wide variety of pins, rods, screws, and plates used to anchor fractured bones while they heal.Metallic glasses based on magnesium with zinc and calcium addition are tested as the potential metallic biomaterials for biodegradable medical implants.Patient with orthopaedic implants sometimes need to be put under magnetic resonance imaging (MRI) machine for detailed musculoskeletal study. Therefore, concerns have been raised regarding the loosening and migration of implant, heating of the implant metal which could cause thermal damage to surrounding tissues, and distortion of the MRI scan that affects the imaging results. A study of orthopaedic implants in 2005 has shown that majority of the orthopaedic implants does not react with magnetic fields under the 1.0 Tesla MRI scanning machine with the exception of external fixator clamps. However, at 7.0 Tesla, several orthopaedic implants would show significant interaction with the MRI magnetic fields, such as heel and fibular implant.


=== Electric ===
Electrical implants are being used to relieve pain and suffering from rheumatoid arthritis. The electric implant is embedded in the neck of patients with rheumatoid arthritics, the implant sends electrical signals to electrodes in the vagus nerve. The application of this device is being tested an alternative to medicating sufferers of rheumatoid arthritis for their lifetime.


=== Contraception ===
Contraceptive implants are primarily used to prevent unintended pregnancy and treat conditions such as non-pathological forms of menorrhagia. Examples include copper- and hormone-based intrauterine devices.


=== Cosmetic ===
Cosmetic implants — often prosthetics — attempt to bring some portion of the body back to an acceptable aesthetic norm. They are used as a follow-up to mastectomy due to breast cancer, for correcting some forms of disfigurement, and modifying aspects of the body (as in buttock augmentation and chin augmentation). Examples include the breast implant, nose prosthesis, ocular prosthesis, and injectable filler.


=== Other organs and systems ===
Other types of organ dysfunction can occur in the systems of the body, including the gastrointestinal, respiratory, and urological systems. Implants are used in those and other locations to treat conditions such as gastroesophageal reflux disease, gastroparesis, respiratory failure, sleep apnea, urinary and fecal incontinence, and erectile dysfunction. Examples include the LINX, implantable gastric stimulator, diaphragmatic/phrenic nerve stimulator, neurostimulator, surgical mesh, artificial urinary sphincter and penile implant.


== Classification ==


=== United States classification ===
Medical devices are classified by the US Food and Drug Administration (FDA) under three different classes depending on the risks the medical device may impose on the user. According to 21CFR 860.3, Class I devices are considered to pose the least amount of risk to the user and require the least amount of control. Class I devices include simple devices such as arm slings and hand-held surgical instruments. Class II devices are considered to need more regulation than Class I devices and are required to undergo specific requirements before FDA approval. Class II devices include X-ray systems and physiological monitors. Class III devices require the most regulatory controls since the device supports or sustains human life or may not be well tested. Class III devices include replacement heart valves and implanted cerebellar stimulators. Many implants typically fall under Class II and Class III devices.


== Materials ==


=== Commonly implanted metals ===
A variety of minimally bioreactive metals are routinely implanted.  The most commonly implanted form of stainless steel is 316L. Cobalt-chromium and titanium-based implant alloys are also permanently implanted.  All of these are made passive by a thin layer of oxide on their surface. A consideration, however, is that metal ions diffuse outward through the oxide and end up in the surrounding tissue.  Bioreaction to metal implants includes the formation of a small envelope of fibrous tissue.  The thickness of this layer is determined by the products being dissolved, and the extent to which the implant moves around within the enclosing tissue.  Pure titanium may have only a minimal fibrous encapsulation.  Stainless steel, on the other hand, may elicit encapsulation of as much as 2 mm.


=== List of implantable metal alloys ===


==== Stainless Steel ====
ASTM F138/F139 316L
ASTM F1314 22Cr-13Ni–5Mn


==== Titanium Alloy ====
ASTM F67 Unalloyed (Commercially Pure) Titanium
ASTM F136 Ti-6Al-4V-ELI
ASTM F1295 Ti-6Al-7Nb
ASTM F1472 Ti-6Al-4V


==== Cobalt Chrome Alloy ====
ASTM F90 Co-20Cr-15W-10Ni
ASTM F562 Co-35Ni-20Cr-10Mo
ASTM F1537 Co-28Cr-6Mo


==== Tantalum ====
ASTM F560 Unalloyed Tantalum


== Complications ==

Under ideal conditions, implants should initiate the desired host response. Ideally, the implant should not cause any undesired reaction from neighboring or distant tissues. However, the interaction between the implant and the tissue surrounding the implant can lead to complications. The process of implantation of medical devices is subjected to the same complications that other invasive medical procedures can have during or after surgery. Common complications include infection, inflammation, and pain. Other complications that can occur include risk of rejection from implant-induced coagulation and allergic foreign body response. Depending on the type of implant, the complications may vary.When the site of an implant becomes infected during or after surgery, the surrounding tissue becomes infected by microorganisms. Three main categories of infection can occur after operation. Superficial immediate infections are caused by organisms that commonly grow near or on skin. The infection usually occurs at the surgical opening. Deep immediate infection, the second type, occurs immediately after surgery at the site of the implant. Skin-dwelling and airborne bacteria cause deep immediate infection. These bacteria enter the body by attaching to the implant's surface prior to implantation. Though not common, deep immediate infections can also occur from dormant bacteria from previous infections of the tissue at the implantation site that have been activated from being disturbed during the surgery. The last type, late infection, occurs months to years after the implantation of the implant. Late infections are caused by dormant blood-borne bacteria attached to the implant prior to implantation. The blood-borne bacteria colonize on the implant and eventually get released from it. Depending on the type of material used to make the implant, it may be infused with antibiotics to lower the risk of infections during surgery. However, only certain types of materials can be infused with antibiotics, the use of antibiotic-infused implants runs the risk of rejection by the patient since the patient may develop a sensitivity to the antibiotic, and the antibiotic may not work on the bacteria.Inflammation, a common occurrence after any surgical procedure, is the body's response to tissue damage as a result of trauma, infection, intrusion of foreign materials, or local cell death, or as a part of an immune response. Inflammation starts with the rapid dilation of local capillaries to supply the local tissue with blood. The inflow of blood causes the tissue to become swollen and may cause cell death. The excess blood, or edema, can activate pain receptors at the tissue. The site of the inflammation becomes warm from local disturbances of fluid flow and the increased cellular activity to repair the tissue or remove debris from the site.Implant-induced coagulation is similar to the coagulation process done within the body to prevent blood loss from damaged blood vessels. However, the coagulation process is triggered from proteins that become attached to the implant surface and lose their shapes. When this occurs, the protein changes conformation and different activation sites become exposed, which may trigger an immune system response where the body attempts to attack the implant to remove the foreign material. The trigger of the immune system response can be accompanied by inflammation. The immune system response may lead to chronic inflammation where the implant is rejected and has to be removed from the body. The immune system may encapsulate the implant as an attempt to remove the foreign material from the site of the tissue by encapsulating the implant in fibrinogen and platelets. The encapsulation of the implant can lead to further complications, since the thick layers of fibrous encapsulation may prevent the implant from performing the desired functions. Bacteria may attack the fibrous encapsulation and become embedded into the fibers. Since the layers of fibers are thick, antibiotics may not be able to reach the bacteria and the bacteria may grow and infect the surrounding tissue. In order to remove the bacteria, the implant would have to be removed. Lastly, the immune system may accept the presence of the implant and repair and remodel the surrounding tissue. Similar responses occur when the body initiates an allergic foreign body response. In the case of an allergic foreign body response, the implant would have to be removed.


== Failures ==

The many examples of implant failure include rupture of silicone breast implants, hip replacement joints, and artificial heart valves, such as the Bjork–Shiley valve, all of which have caused FDA intervention. The consequences of implant failure depend on the nature of the implant and its position in the body. Thus, heart valve failure is likely to threaten the life of the individual, while breast implant or hip joint failure is less likely to be life-threatening.Devices implanted directly in the grey matter of the brain produce the highest quality signals, but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.In 2018, Implant files, an investigation made by ICIJ revealed that medical devices that are unsafe and have not been adequately tested were implanted in patients' bodies. In United Kingdom, Prof Derek Alderson, president of the Royal College of Surgeons, concludes: ""All implantable devices should be registered and tracked to monitor efficacy and patient safety in the long-term.""


== See also ==
Biofunctionalisation
Implantable devices
List of orthopedic implants
Medical device
Prosthesis
(in French)Implant Files scandal by ICIJ, November 2018.


== References ==


== External links ==
AAOMS - Dental Implant Surgery
ACOG - IUDs and Birth Control Implants: Resource Overview
FDA - Implants and Prosthetics
International Medical Devices Database – Recalls, Safety Alerts and Field Safety Notices of medical devices – International Consortium of Investigative Journalists
Implant-Register","pandas(index=94, _1=94, text='an implant is a medical device manufactured to replace a missing biological structure, support a damaged biological structure, or enhance an existing biological structure. medical implants are man-made devices, in contrast to a transplant, which is a transplanted biomedical tissue. the surface of implants that contact the body might be made of a biomedical material such as titanium, silicone, or apatite depending on what is the most functional. in some cases implants contain electronics e.g. artificial pacemaker and cochlear implants. some implants are bioactive, such as subcutaneous drug delivery devices in the form of implantable pills or drug-eluting stents.   == applications == implants can roughly be categorized into groups by application: astm f560 unalloyed tantalum   == complications ==  under ideal conditions, implants should initiate the desired host response. ideally, the implant should not cause any undesired reaction from neighboring or distant tissues. however, the interaction between the implant and the tissue surrounding the implant can lead to complications. the process of implantation of medical devices is subjected to the same complications that other invasive medical procedures can have during or after surgery. common complications include infection, inflammation, and pain. other complications that can occur include risk of rejection from implant-induced coagulation and allergic foreign body response. depending on the type of implant, the complications may vary.when the site of an implant becomes infected during or after surgery, the surrounding tissue becomes infected by microorganisms. three main categories of infection can occur after operation. superficial immediate infections are caused by organisms that commonly grow near or on skin. the infection usually occurs at the surgical opening. deep immediate infection, the second type, occurs immediately after surgery at the site of the implant. skin-dwelling and airborne bacteria cause deep immediate infection. these bacteria enter the body by attaching to the implant\'s surface prior to implantation. though not common, deep immediate infections can also occur from dormant bacteria from previous infections of the tissue at the implantation site that have been activated from being disturbed during the surgery. the last type, late infection, occurs months to years after the implantation of the implant. late infections are caused by dormant blood-borne bacteria attached to the implant prior to implantation. the blood-borne bacteria colonize on the implant and eventually get released from it. depending on the type of material used to make the implant, it may be infused with antibiotics to lower the risk of infections during surgery. however, only certain types of materials can be infused with antibiotics, the use of antibiotic-infused implants runs the risk of rejection by the patient since the patient may develop a sensitivity to the antibiotic, and the antibiotic may not work on the bacteria.inflammation, a common occurrence after any surgical procedure, is the body\'s response to tissue damage as a result of trauma, infection, intrusion of foreign materials, or local cell death, or as a part of an immune response. inflammation starts with the rapid dilation of local capillaries to supply the local tissue with blood. the inflow of blood causes the tissue to become swollen and may cause cell death. the excess blood, or edema, can activate pain receptors at the tissue. the site of the inflammation becomes warm from local disturbances of fluid flow and the increased cellular activity to repair the tissue or remove debris from the site.implant-induced coagulation is similar to the coagulation process done within the body to prevent blood loss from damaged blood vessels. however, the coagulation process is triggered from proteins that become attached to the implant surface and lose their shapes. when this occurs, the protein changes conformation and different activation sites become exposed, which may trigger an immune system response where the body attempts to attack the implant to remove the foreign material. the trigger of the immune system response can be accompanied by inflammation. the immune system response may lead to chronic inflammation where the implant is rejected and has to be removed from the body. the immune system may encapsulate the implant as an attempt to remove the foreign material from the site of the tissue by encapsulating the implant in fibrinogen and platelets. the encapsulation of the implant can lead to further complications, since the thick layers of fibrous encapsulation may prevent the implant from performing the desired functions. bacteria may attack the fibrous encapsulation and become embedded into the fibers. since the layers of fibers are thick, antibiotics may not be able to reach the bacteria and the bacteria may grow and infect the surrounding tissue. in order to remove the bacteria, the implant would have to be removed. lastly, the immune system may accept the presence of the implant and repair and remodel the surrounding tissue. similar responses occur when the body initiates an allergic foreign body response. in the case of an allergic foreign body response, the implant would have to be removed.   == failures ==  the many examples of implant failure include rupture of silicone breast implants, hip replacement joints, and artificial heart valves, such as the bjork–shiley valve, all of which have caused fda intervention. the consequences of implant failure depend on the nature of the implant and its position in the body. thus, heart valve failure is likely to threaten the life of the individual, while breast implant or hip joint failure is less likely to be life-threatening.devices implanted directly in the grey matter of the brain produce the highest quality signals, but are prone to scar-tissue build-up, causing the signal to become weaker, or even non-existent, as the body reacts to a foreign object in the brain.in 2018, implant files, an investigation made by icij revealed that medical devices that are unsafe and have not been adequately tested were implanted in patients\' bodies. in united kingdom, prof derek alderson, president of the royal college of surgeons, concludes: ""all implantable devices should be registered and tracked to monitor efficacy and patient safety in the long-term.""   == see also == biofunctionalisation implantable devices list of orthopedic implants medical device prosthesis (in french)implant files scandal by icij, november 2018.   == references ==   == external links == aaoms - dental implant surgery acog - iuds and birth control implants: resource overview fda - implants and prosthetics international medical devices database – recalls, safety alerts and field safety notices of medical devices – international consortium of investigative journalists implant-register')"
95,"Porous silicon (abbreviated as ""PS"" or ""pSi"") is a form of the chemical element silicon that has introduced nanopores in its microstructure, rendering a large surface to volume ratio in the order of 500 m2/cm3.


== History ==
Porous silicon was discovered by accident in 1956 by Arthur Uhlir Jr. and Ingeborg Uhlir at the Bell Labs in the U.S. At the time, the Ulhirs were in the process of developing a technique for polishing and shaping the surfaces of silicon and germanium. However, it was found that under several conditions a crude product in the form of thick black, red or brown film were formed on the surface of the material. At the time, the findings were not taken further and were only mentioned in Bell Lab's technical notes.Despite the discovery of porous silicon in the 1950s, the scientific community was not interested in porous silicon until the late 1980s. At the time, Leigh Canham – while working at the Defence Research Agency in England – reasoned that the porous silicon may display quantum confinement effects. The intuition was followed by successful experimental results published in 1990. In the published experiment, it was revealed that silicon wafers can emit light if subjected to electrochemical and chemical dissolution.
The published result stimulated the interest of the scientific community in its non-linear optical and electrical properties. The growing interest was evidenced in the number of published work concerning the properties and potential applications of porous silicon. In an article published in 2000, it was found that the number of published work grew exponentially in between 1991 and 1995.In 2001, a team of scientists at the Technical University of Munich inadvertently discovered that hydrogenated porous silicon reacts explosively with oxygen at cryogenic temperatures, releasing several times as much energy as an equivalent amount of TNT, at a much greater speed. (An abstract of the study can be found below.) Explosion occurs because the oxygen, which is in a liquid state at the necessary temperatures, is able to oxidize through the porous molecular structure of the silicon extremely rapidly, causing a very quick and efficient detonation. Although hydrogenated porous silicon would probably not be effective as a weapon, due to its functioning only at low temperatures, other uses are being explored for its explosive properties, such as providing thrust for satellites.


== Fabrication of porous silicon ==
Anodization and stain-etching are the two most common methods used for fabrication of porous silicon; however, there are almost twenty other methods to fabricate this material.  Drying and surface modification might be needed afterwards.  If anodization in an aqueous solution is used to form microporous silicon, the material is commonly treated in ethanol immediately after fabrication, to avoid damage to the structure that results due to the stresses of the capillary effect of the aqueous solution.


=== Anodization ===

One method of introducing pores in silicon is through the use of an anodization cell. A possible anodization cell employs platinum cathode and silicon wafer anode immersed in hydrogen fluoride (HF) electrolyte. Recently, inert diamond cathodes are used to avoid metallic impurities in the electrolyte and inert diamond anodes form an improved electrical back plate contact to the silicon wafers. Corrosion of the anode is produced by running electric current through the cell. It is noted that the running of constant DC is usually implemented to ensure steady tip-concentration of HF resulting in a more homogeneous porosity layer although pulsed current is more appropriate for the formation of thick silicon wafers bigger than 50 µm.It was noted by Halimaoui that hydrogen evolution occurs during the formation of porous silicon. 

When purely aqueous HF solutions are used for the PS formation, the hydrogen bubbles stick to the surface and induce lateral and in-depth inhomogeneity
The hydrogen evolution is normally treated with absolute ethanol in concentration exceeding 15%. It was found that the introduction of ethanol eliminates hydrogen and ensures complete infiltration of HF solution within the pores. Subsequently, uniform distribution of porosity and thickness is improved.


=== Stain etching ===
It is possible to obtain porous silicon through stain-etching with hydrofluoric acid, nitric acid and water. A publication in 1957 revealed that stain films can be grown in dilute solutions of nitric acid in concentrated hydrofluoric acid. Porous silicon formation by stain-etching is particularly attractive because of its simplicity and the presence of readily available corrosive reagents; namely nitric acid (HNO3) and hydrogen fluoride (HF). Furthermore, stain-etching is useful if one needs to produce a very thin porous Si films. A publication in 1960 by R. J. Archer revealed that it is possible to create stain films as thin as 25 Å through stain-etching with HF-HNO3 solution.


=== Bottom-Up Synthesis ===
Porous silicon can be synthesized chemically from silicon tetrachloride, using self-forming salt byproducts as templates for pore formation. The salt templates are later removed with water.


== Drying of porous silicon ==
Porous silicon is systematically prone to presence of cracks when the water is evaporated. The cracks are particularly evident in thick or highly porous silicon layers. The origin of the cracks has been attributed to the large capillary stress due to the minute size of the pores. In particular, it has been known that cracks will appear for porous silicon samples with thickness larger than a certain critical value. Bellet concluded that it was impossible to avoid cracking in thick porous silicon layers under normal evaporating conditions. Hence, several appropriate techniques have been developed to minimize the risk of cracks formed during drying.

Supercritical dryingSupercritical drying is reputed to be the most efficient drying technique but is rather expensive and difficult to implement. It was first implemented by Canham in 1994 and involves superheating the liquid pore above the critical point to avoid interfacial tension.
Freeze dryingFreeze drying procedure was first documented around 1996. After the formation of porous silicon, the sample is frozen at a temperature of about 200 K and sublimed under vacuum.
Pentane dryingThe technique uses pentane as the drying liquid instead of water. In doing so the capillary stress is reduced because pentane has a lower surface tension than water.
Slow evaporationSlow evaporating technique can be implemented following the water or ethanol rinsing. It was found that slow evaporation decreased the trap density


== Surface modification of porous silicon ==
The surface of porous silicon may be modified to exhibit different properties. Often, freshly etched porous silicon may be unstable due to the rate of its oxidation by the atmosphere or unsuitable for cell attachment purposes. Therefore, it can be surface modified to improve stability and cell attachment


=== Surface modification improving stability ===
Following the formation of porous silicon, its surface is covered with covalently bonded hydrogen. Although the hydrogen coated surface is sufficiently stable when exposed to inert atmosphere for a short period of time, prolonged exposure render the surface prone to oxidation by atmospheric oxygen. The oxidation promotes instability in the surface and is undesirable for many applications. Thus, several methods were developed to promote the surface stability of porous silicon.
An approach that can be taken is through thermal oxidation. The process involves heating the silicon to a temperature above 1000 C to promote full oxidation of silicon. The method reportedly produced samples with good stability to aging and electronic surface passivation.Porous silicon exhibits a high degree of biocompatibility. The large surface area enables organic molecules to adhere well. It degrades to Orthosillicic acid (H4SiO4), which causes no harm to the body. This has opened potential applications in medicine such as a framework of the growth of bone.


=== Surface modification improving cell adhesion ===
Surface modification can also affect properties that promote cell adhesion. One particular research in 2005 studied the mammalian cell adhesion on the modified surfaces of porous silicon. The research used rat PC12 cells and Human Lens Epithelial (HLE) cells cultured for four hours on the surface modified porous silicon. Cells were then stained with vital dye FDA and observed under fluorescence microscopy. The research concluded that ""amino silanisation and coating the pSi surface with collagen enhanced cell attachment and spreading"".


== Classification of porous silicon ==


=== Porosity ===
Porosity is defined as the fraction of void within the pSi layer and can be determined easily by weight measurement. During formation of porous silicon layer through anodization, the porosity of a wafer can be increased through increasing current density, decreasing HF concentration and thicker silicon layer. The porosity of porous silicon may range from 4% for macroporous layers to 95% for mesoporous layers. A study by Canham in 1995 found that ""a 1 µm thick layer of high porosity silicon completely dissolved within a day of in-vitro exposure to a simulated body fluid"". It was also found that a silicon wafer with medium to low porosity displayed more stability. Hence, the porosity of porous silicon is varied depending on its potential application areas.


=== Pore size ===
The porosity value of silicon is a macroscopic parameter and doesn’t yield any information regarding the microstructure of the layer. It is proposed that the properties of a sample are more accurately predicted if the pore size and its distribution within the sample can be obtained. Therefore, porous silicon has been divided into three categories based on the size of its pores; macroporous, mesoporous, and microporous.


== Key characteristic of porous silicon ==


=== Highly controllable properties ===
Porous silicon studies conducted in 1995 showed that the behavior of porous silicon can be altered in between ""bio-inert"", ""bioactive"" and ""resorbable"" by varying the porosity of the silicon sample. The in-vitro study used simulated body fluid containing ion concentration similar to the human blood and tested the activities of porous silicon sample when exposed to the fluids for prolonged period of time. It was found that high porosity mesoporous layers were completely removed by the simulated body fluids within a day. In contrast, low to medium porosity microporous layers displayed more stable configurations and induced hydroxyapatite growth.


=== Bioactive ===
The first sign of porous silicon as a bioactive material was found in 1995. In the conducted study, it was found that hydroxyapatite growth was occurring on porous silicon areas. It was then suggested that ""hydrated microporous Si could be a bioactive form of the semiconductor and suggest that Si itself should be seriously considered for development as a material for widespread in vivo applications."" Another paper published the finding that porous silicon may be used a substrate for hydroxyapatite growth either by simple soaking process or laser-liquid-solid interaction process.Since then, in-vitro studies have been conducted to evaluate the interaction of cells with porous silicon. A 1995 study of the interaction of B50 rat hippocampal cells with porous silicon found that B50 cells have clear preference for adhesion to porous silicon over untreated surface. The study indicated that porous silicon can be suitable for cell culturing purposes and can be used to control cell growth pattern.


=== Non-toxic waste product ===
Another positive attribute of porous silicon is the degradation of porous silicon into monomeric silicic acid (SiOH4). Silicic acid is reputed to be the most natural form of element in the environment and is readily removed by kidneys.
The human blood plasma contains monomeric silicic acid at levels of less than 1 mg Si/l, corresponding to the average dietary intake of 20–50 mg/day. It was proposed that the small thickness of silicon coatings presents minimal risk to a toxic concentration being reached. The proposal was supported by an experiment involving volunteers and silicic-acid drinks. It was found that concentration of the acid rose only briefly above the normal 1 mg Si/l level and was efficiently expelled by urine excretion.


=== Superhydrophobicity ===
The simple adjustment of pore morphology and geometry of porous silicon also offers a convenient way to control its wetting behavior. Stable ultra- and superhydrophobic states on porous silicon can be fabricated and used in lab-on-a-chip, microfluidic devices for the improved surface-based bioanalysis.


=== Optical properties ===
pSi demonstrates optical properties based on porosity and the medium inside the pores. The effective refractive index of pSi is determined by the porosity and refractive index of the medium inside the pores. If the refractive index of the medium inside pores is high, the effective refractive index of pSi will be high as well. This phenomenon causes the spectrum to shift towards longer wavelength.


== See also ==
Nanocrystalline silicon
Silicon
Porosity
Quantum wire
Etching (microfabrication)


== References ==


== Further reading ==
Feng Z.C.; Tsu R., eds. (1994). Porous Silicon. Singapore: World Scientific. ISBN 978-981-02-1634-4.
Kovalev D.; Timoshenko V. Y.; Künzner N.; Gross E.; Koch F. (August 2001). ""Strong explosive interaction of hydrogenated porous silicon with oxygen at cryogenic temperatures"". Phys. Rev. Lett. 87 (6): 068301. Bibcode:2001PhRvL..87f8301K. doi:10.1103/PhysRevLett.87.068301. PMID 11497868.","pandas(index=95, _1=95, text='porous silicon (abbreviated as ""ps"" or ""psi"") is a form of the chemical element silicon that has introduced nanopores in its microstructure, rendering a large surface to volume ratio in the order of 500 m2/cm3.   == history == porous silicon was discovered by accident in 1956 by arthur uhlir jr. and ingeborg uhlir at the bell labs in the u.s. at the time, the ulhirs were in the process of developing a technique for polishing and shaping the surfaces of silicon and germanium. however, it was found that under several conditions a crude product in the form of thick black, red or brown film were formed on the surface of the material. at the time, the findings were not taken further and were only mentioned in bell lab\'s technical notes.despite the discovery of porous silicon in the 1950s, the scientific community was not interested in porous silicon until the late 1980s. at the time, leigh canham – while working at the defence research agency in england – reasoned that the porous silicon may display quantum confinement effects. the intuition was followed by successful experimental results published in 1990. in the published experiment, it was revealed that silicon wafers can emit light if subjected to electrochemical and chemical dissolution. the published result stimulated the interest of the scientific community in its non-linear optical and electrical properties. the growing interest was evidenced in the number of published work concerning the properties and potential applications of porous silicon. in an article published in 2000, it was found that the number of published work grew exponentially in between 1991 and 1995.in 2001, a team of scientists at the technical university of munich inadvertently discovered that hydrogenated porous silicon reacts explosively with oxygen at cryogenic temperatures, releasing several times as much energy as an equivalent amount of tnt, at a much greater speed. (an abstract of the study can be found below.) explosion occurs because the oxygen, which is in a liquid state at the necessary temperatures, is able to oxidize through the porous molecular structure of the silicon extremely rapidly, causing a very quick and efficient detonation. although hydrogenated porous silicon would probably not be effective as a weapon, due to its functioning only at low temperatures, other uses are being explored for its explosive properties, such as providing thrust for satellites.   == fabrication of porous silicon == anodization and stain-etching are the two most common methods used for fabrication of porous silicon; however, there are almost twenty other methods to fabricate this material.  drying and surface modification might be needed afterwards.  if anodization in an aqueous solution is used to form microporous silicon, the material is commonly treated in ethanol immediately after fabrication, to avoid damage to the structure that results due to the stresses of the capillary effect of the aqueous solution. psi demonstrates optical properties based on porosity and the medium inside the pores. the effective refractive index of psi is determined by the porosity and refractive index of the medium inside the pores. if the refractive index of the medium inside pores is high, the effective refractive index of psi will be high as well. this phenomenon causes the spectrum to shift towards longer wavelength.   == see also == nanocrystalline silicon silicon porosity quantum wire etching (microfabrication)   == references ==   == further reading == feng z.c.; tsu r., eds. (1994). porous silicon. singapore: world scientific. isbn 978-981-02-1634-4. kovalev d.; timoshenko v. y.; künzner n.; gross e.; koch f. (august 2001). ""strong explosive interaction of hydrogenated porous silicon with oxygen at cryogenic temperatures"". phys. rev. lett. 87 (6): 068301. bibcode:2001phrvl..87f8301k. doi:10.1103/physrevlett.87.068301. pmid 11497868.')"
96,"Retinal prostheses for restoration of sight to patients blinded by retinal degeneration are being developed by a number of private companies and research institutions worldwide.  The system is meant to partially restore useful vision to people who have lost their photoreceptors due to retinal diseases such as retinitis pigmentosa (RP) or age-related macular degeneration (AMD).  Three types of retinal implants are currently in clinical trials: epiretinal (on the retina), subretinal (behind the retina), and suprachoroidal (between the choroid and the sclera).  Retinal implants introduce visual information into the retina by electrically stimulating the surviving retinal neurons.  So far, elicited percepts had rather low resolution, and may be suitable for light perception and recognition of simple objects.


== History ==
Foerster was the first to discover that electrical stimulation of the occipital cortex could be used to create visual percepts, phosphenes.  The first application of an implantable stimulator for vision restoration was developed by Drs.  Brindley and Lewin in 1968.    This experiment demonstrated the viability of creating visual percepts using direct electrical stimulation, and it motivated the development of several other implantable devices for stimulation of the visual pathway, including retinal implants.  Retinal stimulation devices, in particular, have become a focus of research as approximately half of all cases of blindness are caused by retinal damage.  The development of retinal implants has also been motivated in part by the advancement and success of cochlear implants, which has demonstrated that humans can regain significant sensory function with limited input.The Argus II retinal implant, manufactured by Second Sight Medical Products received market approval in the US in Feb 2013 and in Europe in Feb 2011, becoming the first approved implant. The device may help adults with RP who have lost the ability to perceive shapes and movement to be more mobile and to perform day-to-day activities. The epiretinal device is known as the Retina Implant and was originally developed in Germany by Retina Implant AG. It completed a multi-centre clinical trial in Europe and was awarded a CE Mark in 2013, making it the first wireless epiretinal electronic device to gain approval.


== Candidates ==
Optimal candidates for retinal implants have retinal diseases, such as retinitis pigmentosa or age-related macular degeneration.  These diseases cause blindness by affecting the photoreceptor cells in the outer layer of the retina, while leaving the inner and middle retinal layers intact.  Minimally, a patient must have an intact ganglion cell layer in order to be a candidate for a retinal implant. This can be assessed non-invasively using optical coherence tomography (OCT) imaging.  Other factors, including the amount of residual vision, overall health, and family commitment to rehabilitation, are also considered when determining candidates for retinal implants.  In subjects with age-related macular degeneration, who may have intact peripheral vision, retinal implants could result in a hybrid form of vision.  In this case the implant would supplement the remaining peripheral vision with central vision information.


== Types ==
There are two main types of retinal implants by placement. Epiretinal implants are placed in the internal surface of the retina, while subretinal implants are placed between the outer retinal layer and the retinal pigment epithelium.


=== Epiretinal implants ===


==== Design principles ====
Epiretinal implants are placed on top of the retinal surface, above the nerve fiber layer, directly stimulating ganglion cells and bypassing all other retinal layers. Array of electrodes is stabilized on the retina using micro tacks which penetrate into the sclera.  Typically, external video camera onto eyeglasses acquires images and transmits processed video information to the stimulating electrodes via wireless telemetry.  An external transmitter is also required to provide power to the implant via radio-frequency induction coils or infrared lasers.  The real-time image processing involves reducing the resolution, enhancing contrast, detecting the edges in the image and converting it into a spatio-temporal pattern of stimulation delivered to the electrode array on the retina. The majority of electronics can be incorporated into the associated external components, allowing for a smaller implant and simpler upgrades without additional surgery.  The external electronics provides full control over the image processing for each patient.


==== Advantages ====
Epiretinal implants directly stimulate the retinal ganglion cells, thereby bypassing all other retinal layers. Therefore, in principle, epiretinal implants could provide visual perception to individuals even if all other retinal layers have been damaged.


==== Disadvantages ====
Since the nerve fiber layer has similar stimulation threshold to that of the retinal ganglion cells, axons passing under the epiretinal electrodes are stimulated, creating arcuate percepts, and thereby distorting the retinotopic map. So far, none of the epiretinal implants had light-sensitive pixels, and hence they rely on external camera for capturing the visual information. Therefore, unlike natural vision, eye movements do not shift the transmitted image on the retina, which creates a perception of the moving object  when person with such an implant changes the direction of gaze. Therefore, patients with such implants are asked to not move their eyes, but rather scan the visual field with their head. Additionally, encoding visual information at the ganglion cell layer requires very sophisticated image processing techniques in order to account for various types of the retinal ganglion cells encoding different features of the image.


==== Clinical study ====
The first epiretinal implant, the ARGUS device, included a silicon platinum array with 16 electrodes.  The Phase I clinical trial of ARGUS began in 2002 by implanting six participants with the device.  All patients reported gaining a perception of light and discrete phosphenes, with the visual function of some patients improving significantly over time.  Future versions of the ARGUS device are being developed with increasingly dense electrode arrays, allowing for improved spatial resolution.  The most recent ARGUS II device contains 60 electrodes, and a 200 electrode device is under development by ophthalmologists and engineers at the USC Eye Institute.  The ARGUS II device received marketing approval in February 2011 (CE Mark demonstrating safety and performance), and it is available in Germany, France, Italy, and UK. Interim results on 30 patients long term trials were published in Ophthalmology in 2012. Argus II received approval from the US FDA on April 14, 2013 FDA Approval.
Another epiretinal device, the Learning Retinal Implant, has been developed by IIP technologies GmbH, and has begun to be evaluated in clinical trials.  A third epiretinal device, EPI-RET, has been developed and progressed to clinical testing in six patients. The EPI-RET device contains 25 electrodes and requires the crystalline lens to be replaced with a receiver chip.  All subjects have demonstrated the ability to discriminate between different spatial and temporal patterns of stimulation.


=== Subretinal implants ===


==== Design principles ====
Subretinal implants sit on the outer surface of the retina, between the photoreceptor layer and the retinal pigment epithelium, directly stimulating retinal cells and relying on the normal processing of the inner and middle retinal layers.   Adhering a subretinal implant in place is relatively simple, as the implant is mechanically constrained by the minimal distance between the outer retina and the retinal pigment epithelium.  A subretinal implant consists of a silicon wafer containing light sensitive microphotodiodes, which generate signals directly from the incoming light.  Incident light passing through the retina generates currents within the microphotodiodes, which directly inject the resultant current into the underlying retinal cells via arrays of microelectrodes.  The pattern of microphotodiodes activated by incident light therefore stimulates a pattern of bipolar, horizontal, amacrine, and ganglion cells, leading to a visual perception representative of the original incident image.  In principle, subretinal implants do not require any external hardware beyond the implanted microphotodiodes array.  However, some subretinal implants require power from external circuitry to enhance the image signal.


==== Advantages ====
A subretinal implant is advantageous over an epiretinal implant in part because of its simpler design.  The light acquisition, processing, and stimulation are all carried out by microphotodiodes mounted onto a single chip, as opposed to the external camera, processing chip, and implanted electrode array associated with an epiretinal implant.  The subretinal placement is also more straightforward, as it places the stimulating array directly adjacent to the damaged photoreceptors.  By relying on the function of the remaining retinal layers, subretinal implants allow for normal inner retinal processing, including amplification, thus resulting in an overall lower threshold for a visual response.  Additionally, subretinal implants enable subjects to use normal eye movements to shift their gaze.  The retinotopic stimulation from subretinal implants is inherently more accurate, as the pattern of incident light on the microphotodiodes is a direct reflection of the desired image.  Subretinal implants require minimal fixation, as the subretinal space is mechanically constrained and the retinal pigment epithelium creates negative pressure within the subretinal space.


==== Disadvantages ====
The main disadvantage of subretinal implants is the lack of sufficient incident light to enable the microphotodiodes to generate adequate current.  Thus, subretinal implants often incorporate an external power source to amplify the effect of incident light.  The compact nature of the subretinal space imposes significant size constraints on the implant.  The close proximity between the implant and the retina also increases the possibility of thermal damage to the retina from heat generated by the implant.  Subretinal implants require intact inner and middle retinal layers, and therefore are not beneficial for retinal diseases extending beyond the outer photoreceptor layer.  Additionally, photoreceptor loss can result in the formation of a membrane at the boundary of the damaged photoreceptors, which can impede stimulation and increase the stimulation threshold.


==== Clinical studies ====
Optobionics was the first company to develop a subretinal implant and evaluate the design in a clinical trial.  Initial reports indicated that the implantation procedure was safe, and all subjects reported some perception of light and mild improvement in visual function.  The current version of this device has been implanted in 10 patients, who have each reported improvements in the perception of visual details, including contrast, shape, and movement.  Retina Implant AG in Germany has also developed a subretinal implant, which has undergone clinical testing in nine patients. Trial was put on hold due to repeated failures.  The Retina Implant AG device contains 1500 microphotodiodes, allowing for increased spatial resolution, but requires an external power source. Retina implant AG reported 12 months results on the Alpha IMS study in Feb 2013 showing that six out of nine patients had a device failure in the nine months post implant Proceedings of the royal society B, and that five of the eight subjects reported various implant-mediated visual perceptions in daily life. One had optic nerve damage and did not perceive stimulation.  The Boston Subretinal Implant Project has also developed several iterations of a functional subretinal implant, and focused on short term analysis of implant function.  Results from all clinical trials to date indicate that patients receiving subretinal implants report perception of phosphenes, with some gaining the ability to perform basic visual tasks, such as shape recognition and motion detection.


== Spatial resolution ==
The quality of vision expected from a retinal implant is largely based on the maximum spatial resolution of the implant.  Current prototypes of retinal implants are capable of providing low resolution, pixelated images.
""State-of-the-art"" retinal implants incorporate 60-100 channels, sufficient for basic object discrimination and recognition tasks.  However, simulations of the resultant pixelated images assume that all electrodes on the implant are in contact with the desired retinal cell; in reality the expected spatial resolution is lower, as a few of the electrodes may not function optimally.  Tests of reading performance indicated that a 60-channel implant is sufficient to restore some reading ability, but only with significantly enlarged text.  Similar experiments evaluating room navigation ability with pixelated images demonstrated that 60 channels were sufficient for experienced subjects, while naïve subjects required 256 channels.  This experiment, therefore, not only demonstrated the functionality provided by low resolution visual feedback, but also the ability for subjects to adapt and improve over time.  However, these experiments are based merely on simulations of low resolution vision in normal subjects, rather than clinical testing of implanted subjects.  The number of electrodes necessary for reading or room navigation may differ in implanted subjects, and further testing needs to be conducted within this clinical population to determine the required spatial resolution for specific visual tasks.
Simulation results indicate that 600-1000 electrodes would be required to enable subjects to perform a wide variety of tasks, including reading, face recognition, and navigating around rooms. Thus, the available spatial resolution of retinal implants needs to increase by a factor of 10, while remaining small enough to implant, to restore sufficient visual function for those tasks. It is worth to note high-density stimulation is not equal to high visual acuity (resolution), which requires a lot of factors in both hardware (electrodes and coatings) and software (stimulation strategies based on surgical results).


== Current status and future developments ==
Clinical reports to date have demonstrated mixed success, with all patients report at least some sensation of light from the electrodes, and a smaller proportion gaining more detailed visual function, such as identifying patterns of light and dark areas.  The clinical reports indicate that, even with low resolution, retinal implants are potentially useful in providing crude vision to individuals who otherwise would not have any visual sensation.  However, clinical testing in implanted subjects is somewhat limited and the majority of spatial resolution simulation experiments have been conducted in normal controls.  It remains unclear whether the low level vision provided by current retinal implants is sufficient to balance the risks associated with the surgical procedure, especially for subjects with intact peripheral vision.  Several other aspects of retinal implants need to be addressed in future research, including the long term stability of the implants and the possibility of retinal neuron plasticity in response to prolonged stimulation.The Manchester Royal Infirmary and Prof Paulo E Stanga announced on July 22, 2015 the first successful implantation of Second Sight's Argus II in patients suffering from severe Age Related Macular Degeneration. These results are very impressive as it appears that the patients integrate the residual vision and the artificial vision. It potentially opens the use of retinal implants to millions of patients suffering from AMD.


== See also ==
Retinal regeneration


== References ==


== External links ==
Japan Retinal Implant Project
- The Retinal Implant Project - rle.mit.edu
National Eye Institute of the National Institutes of Heath (NIH)","pandas(index=96, _1=96, text='retinal prostheses for restoration of sight to patients blinded by retinal degeneration are being developed by a number of private companies and research institutions worldwide.  the system is meant to partially restore useful vision to people who have lost their photoreceptors due to retinal diseases such as retinitis pigmentosa (rp) or age-related macular degeneration (amd).  three types of retinal implants are currently in clinical trials: epiretinal (on the retina), subretinal (behind the retina), and suprachoroidal (between the choroid and the sclera).  retinal implants introduce visual information into the retina by electrically stimulating the surviving retinal neurons.  so far, elicited percepts had rather low resolution, and may be suitable for light perception and recognition of simple objects.   == history == foerster was the first to discover that electrical stimulation of the occipital cortex could be used to create visual percepts, phosphenes.  the first application of an implantable stimulator for vision restoration was developed by drs.  brindley and lewin in 1968.    this experiment demonstrated the viability of creating visual percepts using direct electrical stimulation, and it motivated the development of several other implantable devices for stimulation of the visual pathway, including retinal implants.  retinal stimulation devices, in particular, have become a focus of research as approximately half of all cases of blindness are caused by retinal damage.  the development of retinal implants has also been motivated in part by the advancement and success of cochlear implants, which has demonstrated that humans can regain significant sensory function with limited input.the argus ii retinal implant, manufactured by second sight medical products received market approval in the us in feb 2013 and in europe in feb 2011, becoming the first approved implant. the device may help adults with rp who have lost the ability to perceive shapes and movement to be more mobile and to perform day-to-day activities. the epiretinal device is known as the retina implant and was originally developed in germany by retina implant ag. it completed a multi-centre clinical trial in europe and was awarded a ce mark in 2013, making it the first wireless epiretinal electronic device to gain approval.   == candidates == optimal candidates for retinal implants have retinal diseases, such as retinitis pigmentosa or age-related macular degeneration.  these diseases cause blindness by affecting the photoreceptor cells in the outer layer of the retina, while leaving the inner and middle retinal layers intact.  minimally, a patient must have an intact ganglion cell layer in order to be a candidate for a retinal implant. this can be assessed non-invasively using optical coherence tomography (oct) imaging.  other factors, including the amount of residual vision, overall health, and family commitment to rehabilitation, are also considered when determining candidates for retinal implants.  in subjects with age-related macular degeneration, who may have intact peripheral vision, retinal implants could result in a hybrid form of vision.  in this case the implant would supplement the remaining peripheral vision with central vision information.   == types == there are two main types of retinal implants by placement. epiretinal implants are placed in the internal surface of the retina, while subretinal implants are placed between the outer retinal layer and the retinal pigment epithelium. optobionics was the first company to develop a subretinal implant and evaluate the design in a clinical trial.  initial reports indicated that the implantation procedure was safe, and all subjects reported some perception of light and mild improvement in visual function.  the current version of this device has been implanted in 10 patients, who have each reported improvements in the perception of visual details, including contrast, shape, and movement.  retina implant ag in germany has also developed a subretinal implant, which has undergone clinical testing in nine patients. trial was put on hold due to repeated failures.  the retina implant ag device contains 1500 microphotodiodes, allowing for increased spatial resolution, but requires an external power source. retina implant ag reported 12 months results on the alpha ims study in feb 2013 showing that six out of nine patients had a device failure in the nine months post implant proceedings of the royal society b, and that five of the eight subjects reported various implant-mediated visual perceptions in daily life. one had optic nerve damage and did not perceive stimulation.  the boston subretinal implant project has also developed several iterations of a functional subretinal implant, and focused on short term analysis of implant function.  results from all clinical trials to date indicate that patients receiving subretinal implants report perception of phosphenes, with some gaining the ability to perform basic visual tasks, such as shape recognition and motion detection.   == spatial resolution == the quality of vision expected from a retinal implant is largely based on the maximum spatial resolution of the implant.  current prototypes of retinal implants are capable of providing low resolution, pixelated images. ""state-of-the-art"" retinal implants incorporate 60-100 channels, sufficient for basic object discrimination and recognition tasks.  however, simulations of the resultant pixelated images assume that all electrodes on the implant are in contact with the desired retinal cell; in reality the expected spatial resolution is lower, as a few of the electrodes may not function optimally.  tests of reading performance indicated that a 60-channel implant is sufficient to restore some reading ability, but only with significantly enlarged text.  similar experiments evaluating room navigation ability with pixelated images demonstrated that 60 channels were sufficient for experienced subjects, while naïve subjects required 256 channels.  this experiment, therefore, not only demonstrated the functionality provided by low resolution visual feedback, but also the ability for subjects to adapt and improve over time.  however, these experiments are based merely on simulations of low resolution vision in normal subjects, rather than clinical testing of implanted subjects.  the number of electrodes necessary for reading or room navigation may differ in implanted subjects, and further testing needs to be conducted within this clinical population to determine the required spatial resolution for specific visual tasks. simulation results indicate that 600-1000 electrodes would be required to enable subjects to perform a wide variety of tasks, including reading, face recognition, and navigating around rooms. thus, the available spatial resolution of retinal implants needs to increase by a factor of 10, while remaining small enough to implant, to restore sufficient visual function for those tasks. it is worth to note high-density stimulation is not equal to high visual acuity (resolution), which requires a lot of factors in both hardware (electrodes and coatings) and software (stimulation strategies based on surgical results).   == current status and future developments == clinical reports to date have demonstrated mixed success, with all patients report at least some sensation of light from the electrodes, and a smaller proportion gaining more detailed visual function, such as identifying patterns of light and dark areas.  the clinical reports indicate that, even with low resolution, retinal implants are potentially useful in providing crude vision to individuals who otherwise would not have any visual sensation.  however, clinical testing in implanted subjects is somewhat limited and the majority of spatial resolution simulation experiments have been conducted in normal controls.  it remains unclear whether the low level vision provided by current retinal implants is sufficient to balance the risks associated with the surgical procedure, especially for subjects with intact peripheral vision.  several other aspects of retinal implants need to be addressed in future research, including the long term stability of the implants and the possibility of retinal neuron plasticity in response to prolonged stimulation.the manchester royal infirmary and prof paulo e stanga announced on july 22, 2015 the first successful implantation of second sight\'s argus ii in patients suffering from severe age related macular degeneration. these results are very impressive as it appears that the patients integrate the residual vision and the artificial vision. it potentially opens the use of retinal implants to millions of patients suffering from amd.   == see also == retinal regeneration   == references ==   == external links == japan retinal implant project - the retinal implant project - rle.mit.edu national eye institute of the national institutes of heath (nih)')"
97,"Health technology is defined by the World Health Organization as the ""application of organized knowledge and skills in the form of devices, medicines, vaccines, procedures, and systems developed to solve a health problem and improve quality of lives"". This includes pharmaceuticals, devices, procedures, and organizational systems used in the healthcare industry, as well as computer-supported information systems. In the United States, these technologies involve standardized physical objects, as well as traditional and designed social means and methods to treat or care for patients.


== Development ==


=== Pre-digital Era ===
During a pre-digital era, patients suffered from inefficient and faulty clinical systems, processes, and conditions. Many medical errors happened in the past due to undeveloped health technologies. Some examples of these medical errors included adverse drug events and alarm fatigue. Alarm fatigue is caused when an alarm is repeatedly triggered or activated and one becomes desensitized to them. As the alarms were sometimes triggered by unimportant events in the past, nurses thought the alarm was not significant. Alarm fatigue is dangerous because it could lead to death and dangerous situations. With technological development, an intelligent program of integration and physiologic sense-making was developed and helped reduce the number of false alarms.Also, with greater investment in health technologies, fewer medical errors happened. Outdated paper records were replaced in many healthcare organizations by electronic health records (EHR). According to studies, this change has brought a lot of changes to healthcare. Drug administration has improved, healthcare providers can now access medical information easier, provide better treatments and faster results, and save more costs.


=== Improvement ===
To help promote and expand the adoption of health information technology, Congress passed the HITECH act as part of the American Recovery and Reinvestment Act of 2009. HITECH stands for Health Information Technology for Economic and Clinical Health Act. It gave the department of health and human services the authority to improve healthcare quality and efficiency through the promotion of health IT. The act provided financial incentives or penalties to organizations to motivate healthcare providers to improve healthcare. The purpose of the act was to improve quality, safety, efficiency, and ultimately to reduce health disparities.One of the main parts of the HITECH act was setting the meaningful use requirement, which required EHRs to allow for the electronic exchange of health information and to submit clinical information. The purpose of HITECH is to ensure the sharing of electronic information with patients and other clinicians are secure. HITECH also aimed to help healthcare providers have more efficient operations and reduce medical errors. The program consisted of three phases. Phase one aimed to improve healthcare quality, safety and efficiency. Phase two expanded on phase one and focused on clinical processes and ensuring the meaningful use of EHRs. Lastly, phase three focused on using Certified Electronic Health Record Technology (CEHRT) to improve health outcomes.In 2014, the implementation of electronic records in US hospitals rose from a low percentage of 10% to a high percentage of 70%.At the beginning of 2018, healthcare providers who participated in the Medicare Promoting Interoperability Program needed to report on Quality Payment Program requirements. The program focused more on interoperability and aimed to improve patient access to health information.


=== Privacy of Health Data ===
Phones that can track one's whereabouts, steps and more can serve as medical devices, and medical devices have much the same effect as these phones. In the research article, Privacy Attitudes among Early Adopters of Emerging Health Technologies by Cynthia Cheung, Matthew Bietz, Kevin Patrick and Cinnamon Bloss discovered people were willing to share personal data for scientific advancements, although they still expressed uncertainty about who would have access to their data. People are naturally cautious about giving out sensitive personal information. Phones add an extra level of threat according to the research article Security in Cloud-Computing-Based Mobile Health. Mobile devices continue to increase in popularity each year. The addition of mobile devices serving as medical devices increases the chances for an attacker to gain unauthorized information.In 2015 the Medical Access and CHIP Reauthorization Act (MACRA) was passed which will be put into play in 2018 pushing towards electronic health records. Health Information Technology: Integration, Patient Empowerment, and Security by K. Marvin provided multiple different polls based on people's views on different types of technology entering the medical field most answers were responded with somewhat likely and very few completely disagreed on the technology being used in medicine. Marvin discusses the maintenance required to protect medical data and technology against cyber attacks as well as providing a proper data backup system for the information.Patient Protection and Affordable Care Act (ACA) also known as Obamacare and health information technology health care is entering the digital era. Although with this development it needs to be protected. Both health information and financial information now made digital within the health industry might become a larger target for cyber-crime. Even with multiple different types of safeguards hackers somehow still find their way in so the security that is in place needs to constantly be updated to prevent these breaches.


==== Policy ====
With the increased use of IT systems, privacy violations were increasing rapidly due to the easier access and poor management. As such, the concern of privacy has become an important topic in healthcare. Privacy breaches happen when organizations do not protect the privacy of people's data. There are four types of privacy breaches, which include unintended disclosure by authorized personnel, intended disclosure by authorized personnel, privacy data loss or theft, and virtual hacking. It became more important to protect the privacy and security of patients’ data because of the high negative impact on both individuals and organizations. Stolen personal information can be used to open credit cards or other unethical behaviors. Also, individuals have to spend a large amount of money to rectify the issue. The exposure of sensitive health information also can cause negative impacts on individuals’ relationships, jobs, or other personal areas. For the organization, the privacy breach can cause loss of trust, customers, legal actions, and monetary fines.

HIPAA stands for the Health Insurance Portability and Accountability Act of 1996. It is a U.S. healthcare legislation to direct how patient data is used and includes two major rules which are privacy and security of data. The privacy rule protects people's rights to privacy and security rule determines how to protect people's privacy.According to the HIPAA Security Rule, it ensures that protected health information has three characteristics. They are confidentiality, availability, and integrity. Confidentiality indicates keeping the data confidential to prevent data loss or individuals who are unauthorized to access that protected health information. Availability allows people who are authorized to access the systems and networks when and where that information is in fact needed, such as natural disasters. In cases like this, protected health information is mostly backed up on to a separate server or printed out in paper copies, so people can access it. Lastly, Integrity ensures not using inaccurate information and improperly modified data due to a bad design system or process to protect the permanence of the patient data. The consequences of using inaccurate or improperly modified data could become useless or even dangerous.Health Organizations of HIPAA also created administrative safeguards, physical safeguards, technical safeguards, to help protect the privacy of patients. Administrative safeguards typically include security management process, security personnel, information access management, workforce training and management, and evaluation of security policies and procedures. Security management processes are one of the important administrative safeguards’ examples. It is essential to reduce the risks and vulnerabilities of the system. The processes are mostly the standard operating procedures written out as training manuals. The purpose is to educate people on how to handle protected health information in proper behavior.Physical safeguards include lock and key, card swipe, positioning of screens, confidential envelopes, and shredding of paper copies. Lock and key are common examples of physical safeguards. They can limit physical access to facilities. Lock and key are simple, but they can prevent individuals from stealing medical records. Individuals must have an actual key to access to the lock.Lastly, technical safeguards include access control, audit controls, integrity controls, and transmission security. The access control mechanism is a common example of technical safeguards. It allows the access of authorized personnel. The technology includes authentication and authorization. Authentication is the proof of identity that handles confidential information like username and password, while authorization is the act of determining whether a particular user is allowed to access certain data and perform activities in a system like add and delete.


=== Assessment ===
The concept of health technology assessment (HTA) was first coined in 1967 by the U.S. Congress in response to the increasing need to address the unintended and potential consequences of health technology, along with its prominent role in society. It was further institutionalized with the establishment of the congressional Office of Technology Assessment (OTA) in 1972–1973. HTA is defined as a comprehensive form of policy research that examines short- and long-term consequences of the application of technology, including benefits, costs, and risks. Due to the broad scope of technology assessment, it requires the participation of individuals besides scientists and health care practitioners such as managers and even the consumers.Several American organizations provide health technology assessments and these include the Centers for Medicare and Medicaid Services (CMS) and the Veterans Administration through its VA Technology Assessment Program (VATAP). The models adopted by these institutions vary, although they focus on whether a medical technology being offered is therapeutically relevant. A study conducted in 2007 noted that the assessments still did not use formal economic analyses.Aside from its development, however, assessment in the health technology industry has been viewed as sporadic and fragmented Issues such as the determination of products that needed to be developed, cost, and access, among others, also emerged. These - some argue - need to be included in the assessment since health technology is never purely a matter of science but also of beliefs, values, and ideologies. One of the mechanisms being suggested – either as an element of- or an alternative to the current TAs is bioethics, which is also referred to as the ""fourth-generation"" evaluation framework. There are at least two dimensions to an ethical HTA. The first involves the incorporation of ethics in the methodological standards employed to assess technologies while the second is concerned with the use of ethical framework in research and judgment on the part of the researchers who produce information used in the industry.


=== Health technology in the future ===

The practice of medicine in the United States is currently in a major transition. This transition is due to many factors, but primarily because of the implementation and integration of health technologies into healthcare. In recent years, the widespread adoption of electronic health records (EHR) has caused a big impact on healthcare. ""The Digital Doctor: Hope, Hype, and Harm at the Dawn of Medicine's Computer Age,"" by Robert Wachter, aims to inform readers about this transition. Dr. Wachter has reviewed and made points about the future of health technologies in the book. He states that there will be fewer hospitals in the future. Due to the advancement of technologies, people will be more likely to go to hospitals for major surgeries or critical illness. In the future, nurse call buttons will not be needed in hospitals. Instead, robots will deliver medication, take care of patients, and administer the system. In the future, the electronic health record will look different. Healthcare providers will be able to enter the notes via speech-to-text transcriptions in real-time.Dr. Wachter stated that information will be edited collaboratively across the patient-care team to improve the quality. Also, natural language processing will be more developed to help parse out keywords. In the future, patient data will reside in the cloud. Patients will be able to access their data from any device or location. The data is also accessible for authorized providers and individuals. In the future, big data analysis will constantly be improving. Artificial Intelligence and machine learning will be constantly improving and developing as it receives new data. Alerts will also be more intelligent and efficient than the current systems.


== Medical technology ==
Medical technology, or ""Medtech"", encompasses a wide range of healthcare products and is used to treat diseases and medical conditions affecting humans. Such technologies are intended to improve the quality of healthcare delivered through earlier diagnosis, less invasive treatment options and reduction in hospital stays and rehabilitation times. Recent advances in medical technology have also focused on cost reduction. Medical technology may broadly include medical devices, information technology, biotech, and healthcare services.
The impacts of medical technology involve social and ethical issues. For example, physicians can seek objective information from technology rather than read subjective patient reports.A major driver of the sector's growth is the consumerization of Medtech. Supported by the widespread availability of smartphones and tablets, providers can reach a large audience at low cost, a trend that stands to be consolidated as wearable technologies spread throughout the market.In the years 2010–2015, venture funding has grown 200%, allowing US$11.7 billion to flow into health tech businesses from over 30,000 investors in the space.


=== Types of Technology ===
Medical technology has evolved into smaller portable devices, for instance, smartphones, touchscreens, tablets, laptops, digital ink, voice and face recognition and more. With this technology, innovations like electronic health records (EHR), health information exchange (HIE), Nationwide Health Information Network (NwHIN), personal health records (PHRs), patient portals, nanomedicine, genome-based personalized medicine, Geographical Positioning System (GPS), radio frequency identification (RFID), telemedicine, clinical decision support (CDS), mobile home health care and cloud computing came to exist.Medical imaging and Magnetic resonance imaging (MRI) have been long used and proven Medical Technologies for medical research, patient reviewing, and treatment analyzing. With the advancement of imagining technologies, including the use of faster and more data, higher resolution images, and specialist automation software, the capabilities of medical imaging technology are growing and yielding better results. As the imaging hardware and software evolve this means that patients will need to use less contrasting agents, and also spend less time and money.3D printing is another major development in healthcare. It can be used to produce specialized splints, prostheses, parts for medical devices and inert implants. The end goal of 3D printing is being able to print out customized replaceable body parts. In the following section, it will explain more about 3D printing in healthcare. New types of technologies also include artificial intelligence and robots.


==== 3D printing ====

3D printing is the use of specialized machines, software programs and materials to automate the process of building certain objects. It is having a rapid growth in the prosthesis, medical implants, novel drug formulations and the bioprinting of human tissues and organs.Companies such as Surgical Theater, provide new technology that is capable of capturing 3D virtual images of patients' brains to use as practice for operations. 3D printing allows medical companies to produce prototypes to practice before an operation created with artificial tissue.3D printing technologies are great for bio-medicine because the materials that are used to make allow the fabrication with control over many design features. 3D printing also has the benefits of affordable customization, more efficient designs, and saving more time. 3D printing is precise to design pills to house several drugs due to different release times. The technology allows the pills to transport to the targeted area and degrade safely in the body. As such, pills can be designed more efficiently and conveniently. In the future, doctors might be giving a digital file of printing instructions instead of a prescription.Besides, 3D printing will be more useful in medical implants. An example includes a surgical team that has designed a tracheal splint made by 3D printing to improve the respiration of a patient. This example shows the potential of 3D printing, which allows physicians to develop new implant and instrument designs easily.Overall, in the future of medicine, 3D printing will be crucial as it can be used in surgical planning, artificial and prosthetic devices, drugs, and medical implants.


==== Artificial Intelligence ====

Artificial Intelligence (AI) is a program that enables computers to sense, reason, act and adapt. AI is not new, but it is growing rapidly and tremendously. AI can now deal with large data sets, solve problems, and provide more efficient operation. AI will be more potential in healthcare because it provides easier accessibility of information, improves healthcare, and reduce cost. There are different factors that drive AI in healthcare, but the two most important are economics and the advent of big data analytics. Costs, new payment options, and people's desire to improve health outcomes are the primary economic drivers of the AI. Based on the reading, AI can save $150 million annually in the US by 2026. Also, AI growth is expected to reach $6.6 million by 2021. Big data analytics is another big driver because we are in the age of big data. The data is extremely helpful to assist the integration of AI in healthcare because it ensures the execution of complex tasks, quality, and efficiency.


===== Applications of Artificial Intelligence =====
AI brings many benefits to the healthcare industry. AI helps to detect diseases, administer chronic conditions, deliver health services, and discover the drug. Also, AI has the potential to address important health challenges. In healthcare organizations, AI is able to plan and relocate resources. AI is able to match patients with healthcare providers that meet their needs. AI also helps improve the healthcare experience by using an app to identify patients' anxieties. In medical research, AI helps to analyze and evaluate the patterns and complex data. For instance, AI is important in drug discovery because it can search relevant studies and analyze different kinds of data. In clinical care, AI helps to detect diseases, analyze clinical data, publications, and guidelines. As such, AI aids to find the best treatments for the patients. Other uses of AI in clinical care include medical imaging, echocardiography, screening, and surgery.


===== Education =====
Medical virtual reality provides doctors multiple surgical scenarios that could happen and allows them to practice and prepare themselves for these situations. It also permits medical students a hands-on experience of different procedures without the consequences of making potential mistakes. ORamaVR is one of the leading companies that employ such medical virtual reality technologies to transform medical education (knowledge) and training (skills) to improve patient outcomes, reduce surgical errors and training time and democratize medical education and training.


==== Robots ====
Modern robotics have made huge progress and contribution to healthcare. Robots can help doctors in performing  variety tasks. Robotics adoption is increasing tremendously in hospitals . The following are different ways to improve healthcare by using robots:

Surgical robots are one of the robotic systems, which allows a surgeon to bend and rotate tissues in a  more flexible and efficient way. The system is equipped with a 3D magnification vision system that can translate the hand movements of the surgeon to be precise in-order to perform a surgery with minimal incisions. Other robotics systems include the ability to diagnose and treat cancers. Many scientists began working on creating a next-generation robot system to assist the surgeon in performing knee and hip replacement surgeries.Assistant robots will also be important to help reduce the workload for regular medical staff. They can help nurses with simple and time-consuming tasks like carrying multiple racks of medicines, lab specimen or other sensitive materials.Shortly, robotic pills are expected to reduce the number of surgeries. They can be moved inside a patient and delivered to the desired area. In addition, they can conduct biopsies, film the area and clear clogged arteries.
Overall, medical robots are extremely useful in assisting physicians; however, it might take time to be professionally trained working with medical robots and for the robots to respond to a clinician's instructions. As such, many researchers and startups were working constantly to provide solutions to these challenges.


=== Assistive Technologies ===
Assistive technologies are products designed to provide accessibility to individuals who have physical or cognitive problems or disabilities. They aim to improve the quality of life with assistive technologies. The range of assistive technologies is broad, ranging from low-tech solutions to physical hardware, to technical devices. There are four areas of assistive technologies, which include visual impairment, hearing impairment, physical limitations, cognitive limitations. There are many benefits of assistive technologies. They enable individuals to care for themselves, work, study, access information easily, improve independence and communication, and lastly participate fully in community life.


=== Consumer-driven healthcare software ===
As part of an ongoing trend towards consumer-driven healthcare, websites or apps which provide more information on health care quality and price to help patients choose their providers have grown. As of 2017, the sites with the most number of reviews in descending order included Healthgrades, Vitals.com, and RateMDs.com. Yelp, Google, and Facebook also host reviews with a large amount of traffic, although as of 2017 they had fewer medical reviews per doctor. Disputes around online reviews can lead to websites by health professionals alleging defamation.Patient safety organizations and government programs which have historically assessed quality have made their data more accessible over the internet; notable examples include the HospitalCompare by CMS and the LeapFrog Group's hospitalsafetygrade.org.Patient-oriented software may also help in other ways, including general education and appointments.Disclosure of legal disputes including medical license complaints or malpractice lawsuits has also been made easier. Every state discloses license status and at least some disciplinary action to the public, but as of 2018, this was not accessible via the internet for a few states. Consumers can look up medical licenses in a national database, DocInfo.org, maintained by the medical licensing organizations which contains limited details. Other tools include DocFinder at docfinder.docboard.org and certificationmatters.org from the American Board of Medical Specialties. In some cases more information is available from a mailed or walk-in request than the internet; for example, the Medical Board of California removes dismissed accusations from website profiles, but these are still available from a written or walk-in request, or a lookup in a separate database. The trend to disclosure is controversial and generate significant public debate, particularly about opening up the National Practitioner Data Bank. In 1996, Massachusetts became the first state to require detailed disclosure of malpractice claims.


=== Self-Monitoring ===
Smartphones, tablets, and wearable computers have allowed people to monitor their health. These devices run numerous applications that are designed to provide simple health services and the monitoring of one's health.  An example of this is Fitbit, a fitness tracker that is worn on the user's wrist. This wearable technology allows people to track their steps, heart rate, floors climbed, miles walked, active minutes, and even sleep patterns. The data collected and analyzed allow users not just to keep track of their health but also help manage it, particularly through its capability to identify health risk factors.There is also the case of the Internet, which serves as a repository of information and expert content that can be used to ""self-diagnose""  instead of going to their doctor. For instance, one need only enumerate symptoms as search parameters at Google and the search engine could identify the illness from the list of contents uploaded to the World Wide Web, particularly those provided by expert/medical sources. These advances may eventually have some effect on doctor visits from patients and change the role of the health professionals from ""gatekeeper to secondary care to facilitator of information interpretation and decision-making."" Apart from basic services provided by Google in Search, there are also companies such as WebMD that already offer dedicated symptom-checking apps.


=== Technology testing ===
All medical equipment introduced commercially must meet both United States and international regulations. The devices are tested on their material, effects on the human body, all components including devices that have other devices included with them, and the mechanical aspects.The Medical Device User Fee and Modernization Act of 2002 was created to speed up the FDA's approval process of medical technology by introducing sponsor user fees for a faster review time with predetermined performance targets for review time. In addition, 36 devices and apps were approved by the FDA in 2016.


== Careers ==
There are numerous careers to choose from in health technology in the USA. Listed below are some job titles and average salaries.

Athletic Trainer,Mean Salary: $41,340. Athletic trainers treat athletes and other individuals who have sustained injuries. They also teach people how to prevent injuries. They perform their job under the supervision of physicians.
Dental Hygienist,  Mean Salary: $67,340. Dental hygienists provide preventive dental care and teach patients how to maintain good oral health. They usually work under dentists' supervision.
Clinical Laboratory Scientists, Technicians, and Technologists, Mean Salary: $51,770. Lab technicians and technologists perform laboratory tests and procedures. Technicians work under the supervision of a laboratory technologist or laboratory manager.
Nuclear Medicine Technologist, Mean Salary: $67,910. Nuclear medicine technologists prepare and administer radiopharmaceuticals, radioactive drugs, to patients to treat or diagnose diseases.
Pharmacy Technician, Mean Salary: $28,070. Pharmacy technicians assist pharmacists with the preparation of prescription medications for customers.


=== Allied Professions ===
The term medical technology may also refer to the duties performed by clinical laboratory professionals or medical technologists in various settings within the public and private sectors. The work of these professionals encompasses clinical applications of chemistry, genetics, hematology, immunohematology (blood banking), immunology, microbiology, serology, urinalysis, and miscellaneous body fluid analysis. Depending on location, educational level, and certifying body, these professionals may be referred to as biomedical scientists, medical laboratory scientists (MLS), medical technologists (MT), medical laboratory technologists and medical laboratory technicians.


== References ==","pandas(index=97, _1=97, text='health technology is defined by the world health organization as the ""application of organized knowledge and skills in the form of devices, medicines, vaccines, procedures, and systems developed to solve a health problem and improve quality of lives"". this includes pharmaceuticals, devices, procedures, and organizational systems used in the healthcare industry, as well as computer-supported information systems. in the united states, these technologies involve standardized physical objects, as well as traditional and designed social means and methods to treat or care for patients.   == development == the term medical technology may also refer to the duties performed by clinical laboratory professionals or medical technologists in various settings within the public and private sectors. the work of these professionals encompasses clinical applications of chemistry, genetics, hematology, immunohematology (blood banking), immunology, microbiology, serology, urinalysis, and miscellaneous body fluid analysis. depending on location, educational level, and certifying body, these professionals may be referred to as biomedical scientists, medical laboratory scientists (mls), medical technologists (mt), medical laboratory technologists and medical laboratory technicians.   == references ==')"
98,"A needle remover is a device used to physically remove a needle from a syringe. In developing countries, there is still a need for improvements in needle safety in hospital settings as most of the needle removal processes are done manually and under severe risk of hazard from needles puncturing skin risking infection. These countries cannot afford needles with individual safety devices attached, so needle-removers must be used to remove the needle from the syringe. This lowers possible pathogen spread by preventing the reuse of the syringes, reducing incidents of accidental needle-sticks, and facilitating syringe disposal.


== Background ==
In regions surveyed by the World Health Organization (WHO), the reported number of needle-stick injuries in developing world countries ranged from .93 to 4.68 injuries per person and per year, which is five times higher than in industrialized nations.  Needle-stick injuries are further complicated by disease transmission, such as Hepatitis B, Hepatitis C and HIV. In Ghana, a study of 803 schoolchildren revealed that 61.2% had at least one marker of hepatitis B virus.  As a result, health care workers, patients, and the community in developing nations are at an increased risk of contracting blood-borne pathogens via the reuse and improper disposal of needles, and accidental needle-sticks.In the U.S., the Needlestick Safety Act signed in 2000 and the 2001 Bloodborne Pathogens Standard both mandated the use of safety devices and needle-removers with any sharps or needles.  As a result, there was a large increase in research, development, and marketing of needle safety devices and needle-remover.  In most hospital and medical settings in the U.S., needle safety regulations are maintained through individual needle safety devices and needle disposal boxes.


== Existing solutions ==
One of the most common causes of needle-stick injuries, which the Needlestick Act and Bloodborne Pathogens Standard were attempting to decrease, was two-handed recapping.  As a result, a one-handed capping mechanism was added to insulin and tuberculin syringes.  The cap is attached to the syringe via a hinge, which allows the cap to be snapped onto the needle using one hand. The disadvantage to the hinge system is that the cap can get caught by jewelry and clothing, can get bumped when used, and the fixed position can be a hindrance during low angle injection.  So Becton Dickinson (BD) has recently come out with a variation on this safety: instead of a hinge, the device slides over the needle and fully covers the tip of the needle, so accidental needle-sticks do not occur.However, the rest of the world does not have similar needle and syringe regulations.  For instance, the WHO is only able to regulate vaccinations in developing countries by ensuring that all vaccination syringes sent to these countries have autodisable features, since the major concern is the reuse of contaminated needles and syringes.  These autodisable features allow the syringes to only be used once, so they cannot be reused.  These mechanisms could be teeth that interlock to prevent the plunger from being pulled back for another use or a bag prefilled with the vaccine to stop reuse.  For example, the SoloShot has a metal clip that locks the plunger down after one use.  The BD Uniject is a prefilled vaccine syringe that uses a plastic bulb instead of a plunger and has a disc valve to prevent reuse.Still, over 90% of syringes worldwide do not have autodisable features.  Individual protection devices are expensive, and regular needles are much more prevalent.  Consequently, many developing world countries use needle-removers to reduce the risk of disease transmission via these exposed.


== Benefits of needle-removers ==
Needle-removers minimize the occurrence of accidental needle-sticks because they allow immediate removal and containment of the needles, especially if the device is near the area of use.  Reuse of syringes is prevented because the needle-remover physically separates the needle from the syringe, making the syringe useless.  They also improve waste disposal by decreasing both the amount of infectious waste and the amount of safety boxes needed for the waste, since safety boxes can pack syringes 20-60% more compactly without the needles.  Additionally, these devices are cost-efficient since one device can handle several hundred needles.  Many developing world countries do not have the resources to afford auto-disable syringes, so with needle-removers, the hospitals can continue to use cheap syringes, while only paying a one-time fee to buy a needle-remover that has a life-span of about 200-500 needles.


== Social and ethical implications ==
A significant ethical issue for the project is whether or not the needle-remover will cause more harm than its potential benefits.  Engineers are obliged to use their skills and knowledge to improve the safety, health, and welfare of the public.  The main concern is for the operator of the device; no engineer should create a device that could injure the operator.  Another concern is that children may gain access to the device and accidentally hurt themselves.  If a device design could potentially cause either of these problems, the team would be ethically obligated to reexamine that design, and it would either have to be improved or abandoned. When the device functions effectively and safely, it will serve to protect the welfare of the community.  In developing countries, the risk of disease transmission is elevated due to the high percentage of needle-stick injuries, which is a result of inadequate needle collection devices.  Increased pathogen transmission also occurs from the reuse of contaminated needles when supplies are low.  The device will prevent reuse of needles and facilitate needle collection and disposal, and thus will improve the health and safety of hospital workers and the community.
The social and economic effects of the device also need to be recognized.  In developing countries, the lack of proper needle collection devices leads to an increase in the number of occupational needle-sticks by health care workers via contaminated needles.  Occupational needle-sticks account for 40%-65% of Hepatitis B and C infections in health care workers.  As a result, more health care workers have to undergo post-exposure testing and treatment, both of which cost money for the hospitals and the countries.  There is also the manpower cost associated with losing trained health care workers to infections acquired on the job.  With fewer than 10 doctors for every 100,000 individuals in sub-Saharan nations, any loss of hospital staff puts a strain of hospital resources.  In addition, developing countries have made significant investments in training their health care workers, which is lost when occupational needle-sticks cause health care workers to leave the medical field.The economic considerations are not just limited to costs associated with health care workers.  Due to the high cost of needle-disposal containers and the fact that the containers usually have to be shipped overseas, unsafe and dangerous substitutes are used instead.  This practice can potentially lead to needle-sticks by health care workers and individuals in the community, as well as needle reuse by members of the community, which can increase the potential spread of diseases.


== Possible designs ==
The easiest needle-removers to operate are electrically powered, and either melt the needle or cut the needles at multiple sections.  One patented design involves a syringe falling down into a chamber where powered movable blades advance the syringe onto fixed blades on the opposite side, at which point the syringe is cut with a shearing motion at multiple points.  There are other patents that use electricity between electrodes or between rotating gears to short-circuit the needle and melt it off the syringe.  A more complex design involves a hammer mill and grinder to break up and grind up the plastic and metal parts of the syringes, after which, the pieces are heated and cooled.  The end result is metal particles encapsulated in a piece of plastic.However, electricity in developing countries is not a dependable source, so hand-powered needle-cutters would be preferred.  Some designs use the squeezing force from a hand to force one or two blades to shear across each other and hence cut the needle between the blades.  There are other designs in which a twisting motion brings a shearing blade in contact with the needle and thus cuts it.  Another design has a stationary outer surface that the syringe body rests against and a cylindrical inner cutting body with a bore for the needle to pass through.  A lever rotates the inner body, which shears the needle from the syringe and dumps the needle into a container.  A crank system can be used to power a similar design, which also uses a cylindrical inner body.  However instead of cutting the needle, the device pulls the needle completely out of the syringe, which deforms the needle, and dumps it into a container.  A more complicated design actually pulls the needle and collar from the barrel of the syringe without a rotational motion: the downward motion of putting the syringe into the device powers two arms to pull the needle off the syringe.  The interesting aspect of this device is that it appears to be one-handed.  Another one-handed device uses a downward motion to cause rotating gears to unscrew the needle and collar from the syringe.  This design is very complex to implement, so an improvement of this design involves pegs that grip and rotate the needle collar instead of gears.  The downward force is transferred into moving the pegs in helical slots, which causes the collar to rotate and the needle to be removed from the syringe.In 2006 a cheap and simple solution utilizing old cola or beer cans to dispose needles and specially developed lid to safely seal them was designed by Yellowone and given the name Antivirus. The lid snaps onto the top of the can permanently sealing it without using any glue or tools. The ’collar’ of the cap is protecting the user during the needle separation process. The insertion hole is designed to separate needle and syringe at the point of use. No finger can pass through the opening. Each can securely contains 150-200 used needles (Yellowone).


== Commercial models ==
There are several electrically powered needle-removers on the market now.  The Disintegrator Needle Destruction Device, offered by American Scientific Resources (ASFX), uses plasma arch technology to destroy the needle, kill pathogens and blunt the syringe. Designed to be used with only one hand, this device completely eliminates the sharp. One model from Techno Fab uses a regular electrical short-circuit to melt the needle, while another needle-remover, seen at CarePathways.com, uses a plasma arc to melt the needle.  A unique needle-remover design is the Needle Remover Device, designed by the Program for Appropriate Technology in Health (PATH).  It uses two handles that are squeezed together to slide two circular blades across each other, which cuts the hub from the syringe.  It is also reusable, and its target cost is about $15.  Another needle-removers currently on the market is Advanced Care Products's Clip&Stor, which uses a hand-powered clipper action to remove the needle.  The cost of the Clip&Stor is about seven dollars.  There is also the BD Hub Cutter, which uses a squeezing hand motion to cut the syringe.  The edges of the squeezable parts have blades that do the actual cutting.  However, unlike a regular needle-remover, the BD Hub Cutter cuts the syringe at the hub so the needle is completely separated from the syringe.  As a result, the risk of a contaminated puncture is completely eliminated because no needle shards remain on the syringe.  The Hub Cutter is not reusable though, and disposal of the whole unit must occur.  The cost of the Hub Cutter is about four dollars.


== Limitations ==
Most of these current needle-removers require the use of two hands; one to hold the needle in place and the other to activate the mechanism.  This form of operation can cause problems because if hospital personnel are busy, especially in a developing world country, they may not have the time or hands needed to operate the device.  As a result, the needle will remain exposed on the syringe, posing a risk to both health care workers and patients.
Furthermore, many of these existing needle-removers do not make use of cheap and readily available materials, like used motor oil jugs, for containers, which raises the price of the device and requires that the hospital continuously buys more containers from the company.  A typical 3-gallon Bemis sharps container with a rotating lid costs about $8 without including shipping costs.  If these containers must be shipped overseas, the price of the device can far exceed the available resources of many hospitals in developing countries, which causes them not to buy needle-remover


== See also ==
Occupational Safety and Health Administration
Hypodermic needle
Injection (medicine)
International Council of Nurses
Biomedical technology
Biomedical engineering
Needle-exchange programme


== References ==


== External links ==
Becton Dickinson Corporate Website
International Health Care Worker Safety Center","pandas(index=98, _1=98, text=""a needle remover is a device used to physically remove a needle from a syringe. in developing countries, there is still a need for improvements in needle safety in hospital settings as most of the needle removal processes are done manually and under severe risk of hazard from needles puncturing skin risking infection. these countries cannot afford needles with individual safety devices attached, so needle-removers must be used to remove the needle from the syringe. this lowers possible pathogen spread by preventing the reuse of the syringes, reducing incidents of accidental needle-sticks, and facilitating syringe disposal.   == background == in regions surveyed by the world health organization (who), the reported number of needle-stick injuries in developing world countries ranged from .93 to 4.68 injuries per person and per year, which is five times higher than in industrialized nations.  needle-stick injuries are further complicated by disease transmission, such as hepatitis b, hepatitis c and hiv. in ghana, a study of 803 schoolchildren revealed that 61.2% had at least one marker of hepatitis b virus.  as a result, health care workers, patients, and the community in developing nations are at an increased risk of contracting blood-borne pathogens via the reuse and improper disposal of needles, and accidental needle-sticks.in the u.s., the needlestick safety act signed in 2000 and the 2001 bloodborne pathogens standard both mandated the use of safety devices and needle-removers with any sharps or needles.  as a result, there was a large increase in research, development, and marketing of needle safety devices and needle-remover.  in most hospital and medical settings in the u.s., needle safety regulations are maintained through individual needle safety devices and needle disposal boxes.   == existing solutions == one of the most common causes of needle-stick injuries, which the needlestick act and bloodborne pathogens standard were attempting to decrease, was two-handed recapping.  as a result, a one-handed capping mechanism was added to insulin and tuberculin syringes.  the cap is attached to the syringe via a hinge, which allows the cap to be snapped onto the needle using one hand. the disadvantage to the hinge system is that the cap can get caught by jewelry and clothing, can get bumped when used, and the fixed position can be a hindrance during low angle injection.  so becton dickinson (bd) has recently come out with a variation on this safety: instead of a hinge, the device slides over the needle and fully covers the tip of the needle, so accidental needle-sticks do not occur.however, the rest of the world does not have similar needle and syringe regulations.  for instance, the who is only able to regulate vaccinations in developing countries by ensuring that all vaccination syringes sent to these countries have autodisable features, since the major concern is the reuse of contaminated needles and syringes.  these autodisable features allow the syringes to only be used once, so they cannot be reused.  these mechanisms could be teeth that interlock to prevent the plunger from being pulled back for another use or a bag prefilled with the vaccine to stop reuse.  for example, the soloshot has a metal clip that locks the plunger down after one use.  the bd uniject is a prefilled vaccine syringe that uses a plastic bulb instead of a plunger and has a disc valve to prevent reuse.still, over 90% of syringes worldwide do not have autodisable features.  individual protection devices are expensive, and regular needles are much more prevalent.  consequently, many developing world countries use needle-removers to reduce the risk of disease transmission via these exposed.   == benefits of needle-removers == needle-removers minimize the occurrence of accidental needle-sticks because they allow immediate removal and containment of the needles, especially if the device is near the area of use.  reuse of syringes is prevented because the needle-remover physically separates the needle from the syringe, making the syringe useless.  they also improve waste disposal by decreasing both the amount of infectious waste and the amount of safety boxes needed for the waste, since safety boxes can pack syringes 20-60% more compactly without the needles.  additionally, these devices are cost-efficient since one device can handle several hundred needles.  many developing world countries do not have the resources to afford auto-disable syringes, so with needle-removers, the hospitals can continue to use cheap syringes, while only paying a one-time fee to buy a needle-remover that has a life-span of about 200-500 needles.   == social and ethical implications == a significant ethical issue for the project is whether or not the needle-remover will cause more harm than its potential benefits.  engineers are obliged to use their skills and knowledge to improve the safety, health, and welfare of the public.  the main concern is for the operator of the device; no engineer should create a device that could injure the operator.  another concern is that children may gain access to the device and accidentally hurt themselves.  if a device design could potentially cause either of these problems, the team would be ethically obligated to reexamine that design, and it would either have to be improved or abandoned. when the device functions effectively and safely, it will serve to protect the welfare of the community.  in developing countries, the risk of disease transmission is elevated due to the high percentage of needle-stick injuries, which is a result of inadequate needle collection devices.  increased pathogen transmission also occurs from the reuse of contaminated needles when supplies are low.  the device will prevent reuse of needles and facilitate needle collection and disposal, and thus will improve the health and safety of hospital workers and the community. the social and economic effects of the device also need to be recognized.  in developing countries, the lack of proper needle collection devices leads to an increase in the number of occupational needle-sticks by health care workers via contaminated needles.  occupational needle-sticks account for 40%-65% of hepatitis b and c infections in health care workers.  as a result, more health care workers have to undergo post-exposure testing and treatment, both of which cost money for the hospitals and the countries.  there is also the manpower cost associated with losing trained health care workers to infections acquired on the job.  with fewer than 10 doctors for every 100,000 individuals in sub-saharan nations, any loss of hospital staff puts a strain of hospital resources.  in addition, developing countries have made significant investments in training their health care workers, which is lost when occupational needle-sticks cause health care workers to leave the medical field.the economic considerations are not just limited to costs associated with health care workers.  due to the high cost of needle-disposal containers and the fact that the containers usually have to be shipped overseas, unsafe and dangerous substitutes are used instead.  this practice can potentially lead to needle-sticks by health care workers and individuals in the community, as well as needle reuse by members of the community, which can increase the potential spread of diseases.   == possible designs == the easiest needle-removers to operate are electrically powered, and either melt the needle or cut the needles at multiple sections.  one patented design involves a syringe falling down into a chamber where powered movable blades advance the syringe onto fixed blades on the opposite side, at which point the syringe is cut with a shearing motion at multiple points.  there are other patents that use electricity between electrodes or between rotating gears to short-circuit the needle and melt it off the syringe.  a more complex design involves a hammer mill and grinder to break up and grind up the plastic and metal parts of the syringes, after which, the pieces are heated and cooled.  the end result is metal particles encapsulated in a piece of plastic.however, electricity in developing countries is not a dependable source, so hand-powered needle-cutters would be preferred.  some designs use the squeezing force from a hand to force one or two blades to shear across each other and hence cut the needle between the blades.  there are other designs in which a twisting motion brings a shearing blade in contact with the needle and thus cuts it.  another design has a stationary outer surface that the syringe body rests against and a cylindrical inner cutting body with a bore for the needle to pass through.  a lever rotates the inner body, which shears the needle from the syringe and dumps the needle into a container.  a crank system can be used to power a similar design, which also uses a cylindrical inner body.  however instead of cutting the needle, the device pulls the needle completely out of the syringe, which deforms the needle, and dumps it into a container.  a more complicated design actually pulls the needle and collar from the barrel of the syringe without a rotational motion: the downward motion of putting the syringe into the device powers two arms to pull the needle off the syringe.  the interesting aspect of this device is that it appears to be one-handed.  another one-handed device uses a downward motion to cause rotating gears to unscrew the needle and collar from the syringe.  this design is very complex to implement, so an improvement of this design involves pegs that grip and rotate the needle collar instead of gears.  the downward force is transferred into moving the pegs in helical slots, which causes the collar to rotate and the needle to be removed from the syringe.in 2006 a cheap and simple solution utilizing old cola or beer cans to dispose needles and specially developed lid to safely seal them was designed by yellowone and given the name antivirus. the lid snaps onto the top of the can permanently sealing it without using any glue or tools. the ’collar’ of the cap is protecting the user during the needle separation process. the insertion hole is designed to separate needle and syringe at the point of use. no finger can pass through the opening. each can securely contains 150-200 used needles (yellowone).   == commercial models == there are several electrically powered needle-removers on the market now.  the disintegrator needle destruction device, offered by american scientific resources (asfx), uses plasma arch technology to destroy the needle, kill pathogens and blunt the syringe. designed to be used with only one hand, this device completely eliminates the sharp. one model from techno fab uses a regular electrical short-circuit to melt the needle, while another needle-remover, seen at carepathways.com, uses a plasma arc to melt the needle.  a unique needle-remover design is the needle remover device, designed by the program for appropriate technology in health (path).  it uses two handles that are squeezed together to slide two circular blades across each other, which cuts the hub from the syringe.  it is also reusable, and its target cost is about $15.  another needle-removers currently on the market is advanced care products's clip&stor, which uses a hand-powered clipper action to remove the needle.  the cost of the clip&stor is about seven dollars.  there is also the bd hub cutter, which uses a squeezing hand motion to cut the syringe.  the edges of the squeezable parts have blades that do the actual cutting.  however, unlike a regular needle-remover, the bd hub cutter cuts the syringe at the hub so the needle is completely separated from the syringe.  as a result, the risk of a contaminated puncture is completely eliminated because no needle shards remain on the syringe.  the hub cutter is not reusable though, and disposal of the whole unit must occur.  the cost of the hub cutter is about four dollars.   == limitations == most of these current needle-removers require the use of two hands; one to hold the needle in place and the other to activate the mechanism.  this form of operation can cause problems because if hospital personnel are busy, especially in a developing world country, they may not have the time or hands needed to operate the device.  as a result, the needle will remain exposed on the syringe, posing a risk to both health care workers and patients. furthermore, many of these existing needle-removers do not make use of cheap and readily available materials, like used motor oil jugs, for containers, which raises the price of the device and requires that the hospital continuously buys more containers from the company.  a typical 3-gallon bemis sharps container with a rotating lid costs about $8 without including shipping costs.  if these containers must be shipped overseas, the price of the device can far exceed the available resources of many hospitals in developing countries, which causes them not to buy needle-remover   == see also == occupational safety and health administration hypodermic needle injection (medicine) international council of nurses biomedical technology biomedical engineering needle-exchange programme   == references ==   == external links == becton dickinson corporate website international health care worker safety center"")"
99,"Metabolic network reconstruction and simulation allows for an in-depth insight into the molecular mechanisms of a particular organism. In particular, these models correlate the genome with molecular physiology. A reconstruction breaks down metabolic pathways (such as glycolysis and the citric acid cycle) into their respective reactions and enzymes, and analyzes them within the perspective of the entire network. In simplified terms, a reconstruction collects all of the relevant metabolic information of an organism and compiles it in a mathematical model.  Validation and analysis of reconstructions can allow identification of key features of metabolism such as growth yield, resource distribution, network robustness, and gene essentiality. This knowledge can then be applied to create novel biotechnology.
In general, the process to build a reconstruction is as follows:

Draft a reconstruction
Refine the model
Convert model into a mathematical/computational representation
Evaluate and debug model through experimentation


== Genome-scale metabolic reconstruction ==
A metabolic reconstruction provides a highly mathematical, structured platform on which to understand the systems biology of metabolic pathways within an organism.  The integration of biochemical metabolic pathways with rapidly available, annotated genome sequences has developed what are called genome-scale metabolic models. Simply put, these models correlate metabolic genes with metabolic pathways. In general, the more information about physiology, biochemistry and genetics is available for the target organism, the better the predictive capacity of the reconstructed models. Mechanically speaking, the process of reconstructing prokaryotic and eukaryotic metabolic networks is essentially the same. Having said this, eukaryote reconstructions are typically more challenging because of the size of genomes, coverage of knowledge, and the multitude of cellular compartments. The first genome-scale metabolic model was generated in 1995 for Haemophilus influenzae.  The first multicellular organism, C. elegans, was reconstructed in 1998.  Since then, many reconstructions have been formed. For a list of reconstructions that have been converted into a model and experimentally validated, see http://sbrg.ucsd.edu/InSilicoOrganisms/OtherOrganisms.


== Drafting a reconstruction ==


=== Resources ===
Because the timescale for the development of reconstructions is so recent, most reconstructions have been built manually. However, now, there are quite a few resources that allow for the semi-automatic assembly of these reconstructions that are utilized due to the time and effort necessary for a reconstruction. An initial fast reconstruction can be developed automatically using resources like PathoLogic or ERGO in combination with encyclopedias like MetaCyc, and then manually updated by using resources like PathwayTools. These semi-automatic methods allow for a fast draft to be created while allowing the fine tune adjustments required once new experimental data is found. It is only in this manner that the field of metabolic reconstructions will keep up with the ever-increasing numbers of annotated genomes.


==== Databases ====
Kyoto Encyclopedia of Genes and Genomes (KEGG): a bioinformatics database containing information on genes, proteins, reactions, and pathways. The ‘KEGG Organisms’ section, which is divided into eukaryotes and prokaryotes, encompasses many organisms for which gene and DNA information can be searched by typing in the enzyme of choice.
BioCyc, EcoCyc, and MetaCyc: BioCyc Is a collection of 3,000 pathway/genome databases (as of Oct 2013), with each database dedicated to one organism. For example, EcoCyc is a highly detailed bioinformatics database on the genome and metabolic reconstruction of Escherichia coli, including thorough descriptions of E. coli signaling pathways and regulatory network. The EcoCyc database can serve as a paradigm and model for any reconstruction. Additionally, MetaCyc, an encyclopedia of experimentally defined metabolic pathways and enzymes, contains 2,100 metabolic pathways and 11,400 metabolic reactions (Oct 2013).
ENZYME: An enzyme nomenclature database (part of the ExPASy proteonomics server of the Swiss Institute of Bioinformatics). After searching for a particular enzyme on the database, this resource gives you the reaction that is catalyzed. ENZYME has direct links to other gene/enzyme/literature databases such as KEGG, BRENDA, and PUBMED.
BRENDA: A comprehensive enzyme database that allows for an enzyme to be searched by name, EC number, or organism.
BiGG: A knowledge base of biochemically, genetically, and genomically structured genome-scale metabolic network reconstructions.
metaTIGER: Is a collection of metabolic profiles and phylogenomic information on a taxonomically diverse range of eukaryotes which provides novel facilities for viewing and comparing the metabolic profiles between organisms.


==== Tools for metabolic modeling ====
Pathway Tools: A bioinformatics software package that assists in the construction of pathway/genome databases such as EcoCyc. Developed by Peter Karp and associates at the SRI International Bioinformatics Research Group, Pathway Tools has several components. Its PathoLogic module takes an annotated genome for an organism and infers probable metabolic reactions and pathways to produce a new pathway/genome database.  Its MetaFlux component can generate a quantitative metabolic model from that pathway/genome database using flux-balance analysis.  Its Navigator component provides extensive query and visualization tools, such as visualization of metabolites, pathways, and the complete metabolic network.
ERGO: A subscription-based service developed by Integrated Genomics. It integrates data from every level including genomic, biochemical data, literature, and high-throughput analysis into a comprehensive user friendly network of metabolic and nonmetabolic pathways.
KEGGtranslator: an easy-to-use stand-alone application that can visualize and convert KEGG files (KGML formatted XML-files) into multiple output formats. Unlike other translators, KEGGtranslator supports a plethora of output formats, is able to augment the information in translated documents (e.g., MIRIAM annotations) beyond the scope of the KGML document, and amends missing components to fragmentary reactions within the pathway to allow simulations on those. KEGGtranslator converts these files to SBML, BioPAX, SIF, SBGN, SBML with qualitative modeling extension, GML, GraphML, JPG, GIF, LaTeX, etc.
ModelSEED: An online resource for the analysis, comparison, reconstruction, and curation of genome-scale metabolic models. Users can submit genome sequences to the RAST annotation system, and the resulting annotation can be automatically piped into the ModelSEED to produce a draft metabolic model. The ModelSEED automatically constructs a network of metabolic reactions, gene-protein-reaction associations for each reaction, and a biomass composition reaction for each genome to produce a model of microbial metabolism that can be simulated using Flux Balance Analysis.
MetaMerge: algorithm for semi-automatically reconciling a pair of existing metabolic network reconstructions into a single metabolic network model.
CoReCo:  algorithm for automatic reconstruction of metabolic models of related species. The first version of the software used KEGG as reaction database to link with the EC number predictions from CoReCo. Its automatic gap filling using atom map of all the reactions produce functional models ready for simulation.


==== Tools for literature ====
PUBMED: This is an online library developed by the National Center for Biotechnology Information, which contains a massive collection of medical journals. Using the link provided by ENZYME, the search can be directed towards the organism of interest, thus recovering literature on the enzyme and its use inside of the organism.


=== Methodology to draft a reconstruction ===

A reconstruction is built by compiling data from the resources above.  Database tools such as KEGG and BioCyc can be used in conjunction with each other to find all the metabolic genes in the organism of interest.  These genes will be compared to closely related organisms that have already developed reconstructions to find homologous genes and reactions. These homologous genes and reactions are carried over from the known reconstructions to form the draft reconstruction of the organism of interest. Tools such as ERGO, Pathway Tools and Model SEED can compile data into pathways to form a network of metabolic and non-metabolic pathways. These networks are then verified and refined before being made into a mathematical simulation.The predictive aspect of a metabolic reconstruction hinges on the ability to predict the biochemical reaction catalyzed by a protein using that protein's amino acid sequence as an input, and to infer the structure of a metabolic network based on the predicted set of reactions. A network of enzymes and metabolites is drafted to relate sequences and function.  When an uncharacterized protein is found in the genome, its amino acid sequence is first compared to those of previously characterized proteins to search for homology.  When a homologous protein is found, the proteins are considered to have a common ancestor and their functions are inferred as being similar.  However, the quality of a reconstruction model is dependent on its ability to accurately infer phenotype directly from sequence, so this rough estimation of protein function will not be sufficient.  A number of algorithms and bioinformatics resources have been developed for refinement of sequence homology-based assignments of protein functions:

InParanoid:  Identifies eukaryotic orthologs by looking only at in-paralogs.
CDD:  Resource for the annotation of functional units in proteins. Its collection of domain models utilizes 3D structure to provide insights into sequence/structure/function relationships.
InterPro:  Provides functional analysis of proteins by classifying them into families and predicting domains and important sites.
STRING:  Database of known and predicted protein interactions.Once proteins have been established, more information about the enzyme structure, reactions catalyzed, substrates and products, mechanisms, and more can be acquired from databases such as KEGG, MetaCyc and NC-IUBMB.  Accurate metabolic reconstructions require additional information about the reversibility and preferred physiological direction of an enzyme-catalyzed reaction which can come from databases such as BRENDA or MetaCyc database.


== Model refinement ==
An initial metabolic reconstruction of a genome is typically far from perfect due to the high variability and diversity of microorganisms.  Often, metabolic pathway databases such as KEGG and MetaCyc will have ""holes"", meaning that there is a conversion from a substrate to a product (i.e., an enzymatic activity) for which there is no known protein in the genome that encodes the enzyme that facilitates the catalysis.  What can also happen in semi-automatically drafted reconstructions is that some pathways are falsely predicted and don't actually occur in the predicted manner.  Because of this, a systematic verification is made in order to make sure no inconsistencies are present and that all the entries listed are correct and accurate.  Furthermore, previous literature can be researched in order to support any information obtained from one of the many metabolic reaction and genome databases. This provides an added level of assurance for the reconstruction that the enzyme and the reaction it catalyzes do actually occur in the organism.
Enzyme promiscuity and spontaneous chemical reactions can damage metabolites. This metabolite damage, and its repair or pre-emption, create energy costs that need to be incorporated into models. It is likely that many genes of unknown function encode proteins that repair or pre-empt metabolite damage, but most genome-scale metabolic reconstructions only include a fraction of all genes.Any new reaction not present in the databases needs to be added to the reconstruction. This is an iterative process that cycles between the experimental phase and the coding phase. As new information is found about the target organism, the model will be adjusted to predict the metabolic and phenotypical output of the cell. The presence or absence of certain reactions of the metabolism will affect the amount of reactants/products that are present for other reactions within the particular pathway. This is because products in one reaction go on to become the reactants for another reaction, i.e. products of one reaction can combine with other proteins or compounds to form new proteins/compounds in the presence of different enzymes or catalysts.Francke et al.  provide an excellent example as to why the verification step of the project needs to be performed in significant detail. During a metabolic network reconstruction of Lactobacillus plantarum, the model showed that succinyl-CoA was one of the reactants for a reaction that was a part of the biosynthesis of methionine. However, an understanding of the physiology of the organism would have revealed that due to an incomplete tricarboxylic acid pathway, Lactobacillus plantarum does not actually produce succinyl-CoA, and the correct reactant for that part of the reaction was acetyl-CoA.
Therefore, systematic verification of the initial reconstruction will bring to light several inconsistencies that can adversely affect the final interpretation of the reconstruction, which is to accurately comprehend the molecular mechanisms of the organism. Furthermore, the simulation step also ensures that all the reactions present in the reconstruction are properly balanced. To sum up, a reconstruction that is fully accurate can lead to greater insight about understanding the functioning of the organism of interest.


== Metabolic network simulation ==
A metabolic network can be broken down into a stoichiometric matrix where the rows represent the compounds of the reactions, while the columns of the matrix correspond to the reactions themselves. Stoichiometry is a quantitative relationship between substrates of a chemical reaction. In order to deduce what the metabolic network suggests, recent research has centered on a few approaches, such as extreme pathways, elementary mode analysis, flux balance analysis, and a number of other constraint-based modeling methods.


=== Extreme pathways ===
Price, Reed, and Papin, from the Palsson lab, use a method of singular value decomposition (SVD) of extreme pathways in order to understand regulation of a human red blood cell metabolism. Extreme pathways are convex basis vectors that consist of steady state functions of a metabolic network. For any particular metabolic network, there is always a unique set of extreme pathways available. Furthermore, Price, Reed, and Papin, define a constraint-based approach, where through the help of constraints like mass balance and maximum reaction rates, it is possible to develop a ‘solution space’ where all the feasible options fall within. Then, using a kinetic model approach, a single solution that falls within the extreme pathway solution space can be determined. Therefore, in their study, Price, Reed, and Papin, use both constraint and kinetic approaches to understand the human red blood cell metabolism. In conclusion, using extreme pathways, the regulatory mechanisms of a metabolic network can be studied in further detail.


=== Elementary mode analysis ===
Elementary mode analysis closely matches the approach used by extreme pathways. Similar to extreme pathways, there is always a unique set of elementary modes available for a particular metabolic network. These are the smallest sub-networks that allow a metabolic reconstruction network to function in steady state. According to Stelling (2002), elementary modes can be used to understand cellular objectives for the overall metabolic network. Furthermore, elementary mode analysis takes into account stoichiometrics and thermodynamics when evaluating whether a particular metabolic route or network is feasible and likely for a set of proteins/enzymes.


=== Minimal metabolic behaviors (MMBs) ===
In 2009, Larhlimi and Bockmayr presented a new approach called ""minimal metabolic behaviors"" for the analysis of metabolic networks. Like elementary modes or extreme pathways, these are uniquely determined by the network, and yield a complete description of the flux cone. However, the new description is much more compact. In contrast with elementary modes and extreme pathways, which use an inner description based on generating vectors of the flux cone, MMBs are using an outer description of the flux cone. This approach is based on sets of non-negativity constraints. These can be identified with irreversible reactions, and thus have a direct biochemical interpretation. One can characterize a metabolic network by MMBs and the reversible metabolic space.


=== Flux balance analysis ===

A different technique to simulate the metabolic network is to perform flux balance analysis. This method uses linear programming, but in contrast to elementary mode analysis and extreme pathways, only a single solution results in the end. Linear programming is usually used to obtain the maximum potential of the objective function that you are looking at, and therefore, when using flux balance analysis, a single solution is found to the optimization problem. In a flux balance analysis approach, exchange fluxes are assigned to those metabolites that enter or leave the particular network only. Those metabolites that are consumed within the network are not assigned any exchange flux value. Also, the exchange fluxes along with the enzymes can have constraints ranging from a negative to positive value (ex: -10 to 10).
Furthermore, this particular approach can accurately define if the reaction stoichiometry is in line with predictions by providing fluxes for the balanced reactions. Also, flux balance analysis can highlight the most effective and efficient pathway through the network in order to achieve a particular objective function. In addition, gene knockout studies can be performed using flux balance analysis. The enzyme that correlates to the gene that needs to be removed is given a constraint value of 0. Then, the reaction that the particular enzyme catalyzes is completely removed from the analysis.


=== Dynamic simulation and parameter estimation ===
In order to perform a dynamic simulation with such a network it is necessary to construct an ordinary differential equation
system that describes the rates of change in each metabolite's concentration or amount. To this end, a rate law, i.e., a kinetic equation that determines the rate of reaction based on the concentrations of all reactants is required for each reaction. Software packages that include numerical integrators, such as COPASI or SBMLsimulator, are then able to simulate the system dynamics given an initial condition. Often these rate laws contain kinetic parameters with uncertain values. In many cases it is desired to estimate these parameter values with respect to given time-series data of metabolite concentrations. The system is then supposed to reproduce the given data. For this purpose the distance between the given data set and the result of the simulation, i.e., the numerically or in few cases analytically obtained solution of the differential equation system is computed. The values of the parameters are then estimated to minimize this distance. One step further, it may be desired to estimate the mathematical structure of the differential equation system because the real rate laws are not known for the reactions within the system under study. To this end, the program SBMLsqueezer allows automatic creation of appropriate rate laws for all reactions with the network.


=== Synthetic accessibility ===
Synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. The synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. To simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. An increase in the total number of steps is predicted to cause lethality. Wunderlich and Mirny showed this simple, parameter-free approach predicted knockout lethality in E. coli and S. cerevisiae as well as elementary mode analysis and flux balance analysis in a variety of media.


== Applications of a reconstruction ==
Several inconsistencies exist between gene, enzyme, reaction databases, and published literature sources regarding the metabolic information of an organism. A reconstruction is a systematic verification and compilation of data from various sources that takes into account all of the discrepancies.
The combination of relevant metabolic and genomic information of an organism.
Metabolic comparisons can be performed between various organisms of the same species as well as between different organisms.
Analysis of synthetic lethality
Predict adaptive evolution outcomes
Use in metabolic engineering for high value outputsReconstructions and their corresponding models allow the formulation of hypotheses about the presence of certain enzymatic activities and the production of metabolites that can be experimentally tested, complementing the primarily discovery-based approach of traditional microbial biochemistry with hypothesis-driven research. The results these experiments can uncover novel pathways and metabolic activities and decipher between discrepancies in previous experimental data.  Information about the chemical reactions of metabolism and the genetic background of various metabolic properties (sequence to structure to function) can be utilized by genetic engineers to modify organisms to produce high value outputs whether those products be medically relevant like pharmaceuticals; high value chemical intermediates such as terpenoids and isoprenoids; or biotechnological outputs like biofuels.Metabolic network reconstructions and models are used to understand how an organism or parasite functions inside of the host cell. For example, if the parasite serves to compromise the immune system by lysing macrophages, then the goal of metabolic reconstruction/simulation would be to determine the metabolites that are essential to the organism's proliferation inside of macrophages. If the proliferation cycle is inhibited, then the parasite would not continue to evade the host's immune system. A reconstruction model serves as a first step to deciphering the complicated mechanisms surrounding disease. These models can also look at the minimal genes necessary for a cell to maintain virulence. The next step would be to use the predictions and postulates generated from a reconstruction model and apply it to discover novel biological functions such as drug-engineering and drug delivery techniques.


== See also ==
Computational systems biology
Computer simulation
Metabolic control analysis
Metabolic network
Metabolic pathway
Metagenomics


== References ==


== Further reading ==
Overbeek R, Larsen N, Walunas T, D'Souza M, Pusch G, Selkov Jr, Liolios K, Joukov V, Kaznadzey D, Anderson I, Bhattacharyya A, Burd H, Gardner W, Hanke P, Kapatral V, Mikhailova N, Vasieva O, Osterman A, Vonstein V, Fonstein M, Ivanova N, Kyrpides N. (2003) The ERGO genome analysis and discovery system.   Nucleic Acids Res. 31(1):164-71
Whitaker, J.W., Letunic, I., McConkey, G.A. and Westhead, D.R. metaTIGER: a metabolic evolution resource. Nucleic Acids Res. 2009 37: D531-8.


== External links ==
ERGO
GeneDB
KEGG
PathCase Case Western Reserve University
BRENDA
BioCyc and Cyclone - provides an open source Java API to the pathway tool BioCyc to extract Metabolic graphs.
EcoCyc
MetaCyc
SEED
ModelSEED
ENZYME
SBRI Bioinformatics Tools and Software
TIGR
Pathway Tools
metaTIGER
Stanford Genomic Resources
Pathway Hunter Tool
IMG The Integrated Microbial Genomes system, for genome analysis by the DOE-JGI.
Systems Analysis, Modelling and Prediction Group at the University of Oxford, Biochemical reaction pathway inference techniques.
efmtool provided by Marco Terzer
SBMLsqueezer
Cellnet analyzer from Klamt and von Kamp
Copasi
gEFM A graph-based tool for EFM computation","pandas(index=99, _1=99, text='metabolic network reconstruction and simulation allows for an in-depth insight into the molecular mechanisms of a particular organism. in particular, these models correlate the genome with molecular physiology. a reconstruction breaks down metabolic pathways (such as glycolysis and the citric acid cycle) into their respective reactions and enzymes, and analyzes them within the perspective of the entire network. in simplified terms, a reconstruction collects all of the relevant metabolic information of an organism and compiles it in a mathematical model.  validation and analysis of reconstructions can allow identification of key features of metabolism such as growth yield, resource distribution, network robustness, and gene essentiality. this knowledge can then be applied to create novel biotechnology. in general, the process to build a reconstruction is as follows:  draft a reconstruction refine the model convert model into a mathematical/computational representation evaluate and debug model through experimentation   == genome-scale metabolic reconstruction == a metabolic reconstruction provides a highly mathematical, structured platform on which to understand the systems biology of metabolic pathways within an organism.  the integration of biochemical metabolic pathways with rapidly available, annotated genome sequences has developed what are called genome-scale metabolic models. simply put, these models correlate metabolic genes with metabolic pathways. in general, the more information about physiology, biochemistry and genetics is available for the target organism, the better the predictive capacity of the reconstructed models. mechanically speaking, the process of reconstructing prokaryotic and eukaryotic metabolic networks is essentially the same. having said this, eukaryote reconstructions are typically more challenging because of the size of genomes, coverage of knowledge, and the multitude of cellular compartments. the first genome-scale metabolic model was generated in 1995 for haemophilus influenzae.  the first multicellular organism, c. elegans, was reconstructed in 1998.  since then, many reconstructions have been formed. for a list of reconstructions that have been converted into a model and experimentally validated, see http://sbrg.ucsd.edu/insilicoorganisms/otherorganisms.   == drafting a reconstruction == synthetic accessibility is a simple approach to network simulation whose goal is to predict which metabolic gene knockouts are lethal. the synthetic accessibility approach uses the topology of the metabolic network to calculate the sum of the minimum number of steps needed to traverse the metabolic network graph from the inputs, those metabolites available to the organism from the environment, to the outputs, metabolites needed by the organism to survive. to simulate a gene knockout, the reactions enabled by the gene are removed from the network and the synthetic accessibility metric is recalculated. an increase in the total number of steps is predicted to cause lethality. wunderlich and mirny showed this simple, parameter-free approach predicted knockout lethality in e. coli and s. cerevisiae as well as elementary mode analysis and flux balance analysis in a variety of media.   == applications of a reconstruction == several inconsistencies exist between gene, enzyme, reaction databases, and published literature sources regarding the metabolic information of an organism. a reconstruction is a systematic verification and compilation of data from various sources that takes into account all of the discrepancies. the combination of relevant metabolic and genomic information of an organism. metabolic comparisons can be performed between various organisms of the same species as well as between different organisms. analysis of synthetic lethality predict adaptive evolution outcomes use in metabolic engineering for high value outputsreconstructions and their corresponding models allow the formulation of hypotheses about the presence of certain enzymatic activities and the production of metabolites that can be experimentally tested, complementing the primarily discovery-based approach of traditional microbial biochemistry with hypothesis-driven research. the results these experiments can uncover novel pathways and metabolic activities and decipher between discrepancies in previous experimental data.  information about the chemical reactions of metabolism and the genetic background of various metabolic properties (sequence to structure to function) can be utilized by genetic engineers to modify organisms to produce high value outputs whether those products be medically relevant like pharmaceuticals; high value chemical intermediates such as terpenoids and isoprenoids; or biotechnological outputs like biofuels.metabolic network reconstructions and models are used to understand how an organism or parasite functions inside of the host cell. for example, if the parasite serves to compromise the immune system by lysing macrophages, then the goal of metabolic reconstruction/simulation would be to determine the metabolites that are essential to the organism\'s proliferation inside of macrophages. if the proliferation cycle is inhibited, then the parasite would not continue to evade the host\'s immune system. a reconstruction model serves as a first step to deciphering the complicated mechanisms surrounding disease. these models can also look at the minimal genes necessary for a cell to maintain virulence. the next step would be to use the predictions and postulates generated from a reconstruction model and apply it to discover novel biological functions such as drug-engineering and drug delivery techniques.   == see also == computational systems biology computer simulation metabolic control analysis metabolic network metabolic pathway metagenomics   == references ==   == further reading == overbeek r, larsen n, walunas t, d\'souza m, pusch g, selkov jr, liolios k, joukov v, kaznadzey d, anderson i, bhattacharyya a, burd h, gardner w, hanke p, kapatral v, mikhailova n, vasieva o, osterman a, vonstein v, fonstein m, ivanova n, kyrpides n. (2003) the ergo genome analysis and discovery system.   nucleic acids res. 31(1):164-71 whitaker, j.w., letunic, i., mcconkey, g.a. and westhead, d.r. metatiger: a metabolic evolution resource. nucleic acids res. 2009 37: d531-8.   == external links == ergo genedb kegg pathcase case western reserve university brenda biocyc and cyclone - provides an open source java api to the pathway tool biocyc to extract metabolic graphs. ecocyc metacyc seed modelseed enzyme sbri bioinformatics tools and software tigr pathway tools metatiger stanford genomic resources pathway hunter tool img the integrated microbial genomes system, for genome analysis by the doe-jgi. systems analysis, modelling and prediction group at the university of oxford, biochemical reaction pathway inference techniques. efmtool provided by marco terzer sbmlsqueezer cellnet analyzer from klamt and von kamp copasi gefm a graph-based tool for efm computation')"
100,"Articles related specifically to biomedical engineering include:


== A ==
Artificial heart —
Artificial heart valve —
Artificial intelligence —
Artificial limb —
Artificial pacemaker —
Automated external defibrillator —


== B ==
Bachelor of Science in Biomedical Engineering—
Bedsores—
Biochemistry —
Biochemistry topics list —
Bioelectrochemistry—
Bioelectronics—
Bioimpedance —
Bio-implants —
Bioinformatics —
Biology —
Biology topics list —
Biomechanics —
Biomedical engineering —
Biomedical imaging —
Biomedical Imaging Resource —
Bionics —
Biotechnology —
Biotelemetry —
Biothermia —
BMES —
Brain-computer interface —
Brain implant


== C ==
Cell engineering —
Chemistry —
Chemistry topics list —
Clinical engineering —
Cochlear implant —
Corrective lens —
Crutch —


== D ==
Dental implant —
Dialysis machines —
Diaphragmatic pacemaker —


== E ==
Engineering —


== F ==
Functional electrical stimulation


== G ==
Genetic engineering —
Genetic engineering topics —
Genetics —


== H ==
Health care —
Heart-lung machine —
Heart rate monitor —


== I ==
Implant —
Implantable cardioverter-defibrillator —
Infusion pump —
Instrumentation for medical devices —


== J ==


== K ==


== L ==
Laser applications in medicine —


== M ==
Magnetic resonance imaging —
Maxillo-facial prosthetics —
Medical equipment —
Medical imaging —
Medical research —
Medication —
Medicine —
Microfluidics —
Molecular biology —
Molecular biology topics —


== N ==
Nanoengineering —
Nano-scaffold —
Nanotechnology —
Neural engineering —
Neurally controlled animat —
Neuroengineering —
Neuroprosthetics —
Neurostimulator —
Neurotechnology —


== O ==
Ocular prosthetics —
Optical imaging —
Optical spectroscopy —
Orthosis —


== P ==
Pharmacology —
Physiological system modelling —
Positron emission tomography —
Prosthesis —
Polysomnograph —


== Q ==


== R ==
Radiological imaging —
Radiation therapy —
Reliability engineering —
Remote physiological monitoring —
Replacement joint —
Retinal implant —


== S ==
Safety engineering —
Stem cell —


== T ==
Tissue engineering —
Tissue viability —


== U ==


== V ==


== W ==


== X ==
X-ray —


== Z ==","pandas(index=100, _1=100, text='articles related specifically to biomedical engineering include:   == a == artificial heart — artificial heart valve — artificial intelligence — artificial limb — artificial pacemaker — automated external defibrillator —   == b == bachelor of science in biomedical engineering— bedsores— biochemistry — biochemistry topics list — bioelectrochemistry— bioelectronics— bioimpedance — bio-implants — bioinformatics — biology — biology topics list — biomechanics — biomedical engineering — biomedical imaging — biomedical imaging resource — bionics — biotechnology — biotelemetry — biothermia — bmes — brain-computer interface — brain implant   == c == cell engineering — chemistry — chemistry topics list — clinical engineering — cochlear implant — corrective lens — crutch —   == d == dental implant — dialysis machines — diaphragmatic pacemaker —   == e == engineering —   == f == functional electrical stimulation   == g == genetic engineering — genetic engineering topics — genetics —   == h == health care — heart-lung machine — heart rate monitor —   == i == implant — implantable cardioverter-defibrillator — infusion pump — instrumentation for medical devices —   == j ==   == k ==   == l == laser applications in medicine —   == m == magnetic resonance imaging — maxillo-facial prosthetics — medical equipment — medical imaging — medical research — medication — medicine — microfluidics — molecular biology — molecular biology topics —   == n == nanoengineering — nano-scaffold — nanotechnology — neural engineering — neurally controlled animat — neuroengineering — neuroprosthetics — neurostimulator — neurotechnology —   == o == ocular prosthetics — optical imaging — optical spectroscopy — orthosis —   == p == pharmacology — physiological system modelling — positron emission tomography — prosthesis — polysomnograph —   == q ==   == r == radiological imaging — radiation therapy — reliability engineering — remote physiological monitoring — replacement joint — retinal implant —   == s == safety engineering — stem cell —   == t == tissue engineering — tissue viability —   == u ==   == v ==   == w ==   == x == x-ray —   == z ==')"
101,"Microshock refers to the risk that patients undergoing medical procedures involving externally protruding intracardiac electrical conductors, such as external pacemaker  electrodes, or saline filled catheters, could suffer an electric shock causing ventricular fibrillation (VF) due to currents entering the body via these parts.


== Some definitions related to micro-shock ==
It is important to note that microshock (or micro-shock) are not IEV defined terms and are not used in any international standard.
""Micro-shock"" is an otherwise imperceptible electric current applied directly, or in very close proximity, to the heart muscle of sufficient strength, frequency, and duration to cause disruption of normal cardiac function.
Note: It can be safely assumed (and it usually is) that micro-shock is only possible during certain medical procedures as the electric current needs to be focused directly into the heart by some conductor inserted by invasive means for some desired medical outcome (for example Cardiac Catheterisation).
Micro-shock, if it occurs, is not always lethal. “Micro-electrocution” is the term that should be used whenever a micro-shock causes death.
“Macro-shock” is when a much larger current is passed through the body, usually via a skin to skin pathway, but more generally the current is not applied directly through the heart muscle. The current in macro-shock events can vary widely from being imperceptible to being extremely destructive of tissue. (see Macroshock)
“Electric Shock” is usually referring to macro-shock. (see Electric Shock)
“Electrocution” is usually referring to a macro-shock that has caused prolonged or severe disruption of normal cardiac function - ultimately leading to death. (see Electrocution)


== Theory ==
Microschock requires direct electrical connection to the heart muscle and is normally illustrated using a diagram such as Figure 1 (from TGE).
(an image is to be uploaded here shortly)
In this scenario the patient has inadvertently contacted both a source of current (it does not have to be AC, as shown) as well as a common return pathway during an invasive cardiac medical procedure. If the current flowing is below the threshold of perception, or the patient is sedated, or anaesthetized, there may be no pain or reflex response of either arm. If the current flow continues for sufficient time, at sufficient strength, the patient may die. Because of the low current and lack of patient response, this death may be unexpected, and without any obvious cause. In practice, however, this has never been proven to have happened.To a novice, however, this scenario looks incredibly dangerous, and it is therefore worth examining in some detail.
Firstly, let’s follow the current path. There is a generic source of current. This source can be either large or small, as only a small voltage is required to drive the low current for micro-shock. Such sources might be, a wall socket, a faulty item of equipment, an inappropriate item of equipment, a poorly designed item of equipment, or an item of equipment designed to deliver current into the body. Our patient has unfortunately contacted one such source and current is dispersing through their right arm and upper torso, to eventually converge on a catheter (as labelled – but it could be a lead or wire) that is placed into their heart. This concentration of current flow at the heart muscle is the danger from micro-shock.  If the catheter is conductive and insulated, the current may follow the catheter, emerging through the skin into some other item of equipment. For the circuit as shown to be complete, the conductive part of the catheter needs to also be connected to ground within this equipment. Finally, the hazardous circuit is complete – current can flow and if it continues the patient is in mortal danger. Again, while this is theoretically possible this has never been proven to have actually happened.So, why is this situation not arisen? The details are explored below but the basic presumptions that a patient will simultaneously contact the mains, with a catheter inserted, using equipment that grounds the catheter, is very, very unlikely. Also most electrical connections made in or around a patients heart will be those of medical electrical equipment. Medical electrical equipment, which have such applied parts, are constructed to strict standards that limit the allowable currents flowing via such connections (applied parts). This ensures the safety of the patient.


== History ==
There has never been a documented case of microshock. A U.S. Senate inquiry in the early 1970s, sparked by exaggerated reports of thousands of U.S hospital patients dying of microshock, heard expert testimony about the effect. A review of the evidence in the early 2000s found that not a single case had been reported in the 30 years since the Senate inquiry. Regular checks of the FDA's MAUDE database also show no evidence of this risk being manifest, before or since the review.
Based on studies with dogs by Prof Leslie Geddes in the middle of last century, it is theorised that a current as low as 10 μA (microampere) directly through the heart, may send a human patient directly into ventricular fibrillation. Of course, the exact outcome is dependent on the duration of the current, the exact position of contact, the frequency of current oscillation, and the timing of the shock with the hearts rhythm e.g R on T phenomenon. It is feared that such a small current may be introduced unwittingly, and unobserved, creating a very perilous situation for the patient. To guard against this slim theoretical possibility then, modern medical devices include a range of protective measures to limit current in cardiac-connected circuits to the assumed safe levels of below 10 μA (microampere) . These measures include isolated patient connections, high impedance connections and current limiting circuits. Despite the in-built protections, and lack of observed incidents, microshock continues to be a concern to many practitioners of the fields of Biomedical and Clinical Engineering.
Despite the evidence of decades of absence of reports, in any condition where electrical conductors are run into the body in proximity of the heart (i.e. cardiac catheterizations) precautions are still taken to ensure hazardous current is not introduced through these conductors and it is still regarded as a high risk activity.


== See also ==
Macroshock
Electric Shock


== Notes ==


== References ==
Gross J (2005) Less Jolts from Your Volts: Electrical Safety in the Operating Room. ASA Refresher Courses in Anesthesiology. 33(1):101-114
Ridgway M The Great Debate on Electrical Safety  - in Retrospect in Clinical Engineering Handbook, 1st Edition, Chapter 65, J Dyro (ed) Academic Press 2004
O'Meley P L Who's Afraid of Microshock - presentation to SMBE NSW Conference, Albury NSW, 2011
Hsu J The Hypertextbook http://hypertextbook.com/facts/2000/JackHsu.shtml accessed 23 July 2013
FDA MAUDE Database http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/TextSearch.cfm accessed 25 July 2013
Road Safety Report http://roadsafety.transport.nsw.gov.au/downloads/fatality_rate_1908_to_2009.pdf accessed 25 July 2013
IEV Electropedia produced by the International Electrotechnical Commission (IEC) http://www.electropedia.org
IEC/TS 60479-1 - Effects of current on human beings and livestock https://webstore.iec.ch/publication/25402","pandas(index=101, _1=101, text='microshock refers to the risk that patients undergoing medical procedures involving externally protruding intracardiac electrical conductors, such as external pacemaker  electrodes, or saline filled catheters, could suffer an electric shock causing ventricular fibrillation (vf) due to currents entering the body via these parts.   == some definitions related to micro-shock == it is important to note that microshock (or micro-shock) are not iev defined terms and are not used in any international standard. ""micro-shock"" is an otherwise imperceptible electric current applied directly, or in very close proximity, to the heart muscle of sufficient strength, frequency, and duration to cause disruption of normal cardiac function. note: it can be safely assumed (and it usually is) that micro-shock is only possible during certain medical procedures as the electric current needs to be focused directly into the heart by some conductor inserted by invasive means for some desired medical outcome (for example cardiac catheterisation). micro-shock, if it occurs, is not always lethal. “micro-electrocution” is the term that should be used whenever a micro-shock causes death. “macro-shock” is when a much larger current is passed through the body, usually via a skin to skin pathway, but more generally the current is not applied directly through the heart muscle. the current in macro-shock events can vary widely from being imperceptible to being extremely destructive of tissue. (see macroshock) “electric shock” is usually referring to macro-shock. (see electric shock) “electrocution” is usually referring to a macro-shock that has caused prolonged or severe disruption of normal cardiac function - ultimately leading to death. (see electrocution)   == theory == microschock requires direct electrical connection to the heart muscle and is normally illustrated using a diagram such as figure 1 (from tge). (an image is to be uploaded here shortly) in this scenario the patient has inadvertently contacted both a source of current (it does not have to be ac, as shown) as well as a common return pathway during an invasive cardiac medical procedure. if the current flowing is below the threshold of perception, or the patient is sedated, or anaesthetized, there may be no pain or reflex response of either arm. if the current flow continues for sufficient time, at sufficient strength, the patient may die. because of the low current and lack of patient response, this death may be unexpected, and without any obvious cause. in practice, however, this has never been proven to have happened.to a novice, however, this scenario looks incredibly dangerous, and it is therefore worth examining in some detail. firstly, let’s follow the current path. there is a generic source of current. this source can be either large or small, as only a small voltage is required to drive the low current for micro-shock. such sources might be, a wall socket, a faulty item of equipment, an inappropriate item of equipment, a poorly designed item of equipment, or an item of equipment designed to deliver current into the body. our patient has unfortunately contacted one such source and current is dispersing through their right arm and upper torso, to eventually converge on a catheter (as labelled – but it could be a lead or wire) that is placed into their heart. this concentration of current flow at the heart muscle is the danger from micro-shock.  if the catheter is conductive and insulated, the current may follow the catheter, emerging through the skin into some other item of equipment. for the circuit as shown to be complete, the conductive part of the catheter needs to also be connected to ground within this equipment. finally, the hazardous circuit is complete – current can flow and if it continues the patient is in mortal danger. again, while this is theoretically possible this has never been proven to have actually happened.so, why is this situation not arisen? the details are explored below but the basic presumptions that a patient will simultaneously contact the mains, with a catheter inserted, using equipment that grounds the catheter, is very, very unlikely. also most electrical connections made in or around a patients heart will be those of medical electrical equipment. medical electrical equipment, which have such applied parts, are constructed to strict standards that limit the allowable currents flowing via such connections (applied parts). this ensures the safety of the patient.   == history == there has never been a documented case of microshock. a u.s. senate inquiry in the early 1970s, sparked by exaggerated reports of thousands of u.s hospital patients dying of microshock, heard expert testimony about the effect. a review of the evidence in the early 2000s found that not a single case had been reported in the 30 years since the senate inquiry. regular checks of the fda\'s maude database also show no evidence of this risk being manifest, before or since the review. based on studies with dogs by prof leslie geddes in the middle of last century, it is theorised that a current as low as 10 μa (microampere) directly through the heart, may send a human patient directly into ventricular fibrillation. of course, the exact outcome is dependent on the duration of the current, the exact position of contact, the frequency of current oscillation, and the timing of the shock with the hearts rhythm e.g r on t phenomenon. it is feared that such a small current may be introduced unwittingly, and unobserved, creating a very perilous situation for the patient. to guard against this slim theoretical possibility then, modern medical devices include a range of protective measures to limit current in cardiac-connected circuits to the assumed safe levels of below 10 μa (microampere) . these measures include isolated patient connections, high impedance connections and current limiting circuits. despite the in-built protections, and lack of observed incidents, microshock continues to be a concern to many practitioners of the fields of biomedical and clinical engineering. despite the evidence of decades of absence of reports, in any condition where electrical conductors are run into the body in proximity of the heart (i.e. cardiac catheterizations) precautions are still taken to ensure hazardous current is not introduced through these conductors and it is still regarded as a high risk activity.   == see also == macroshock electric shock   == notes ==   == references == gross j (2005) less jolts from your volts: electrical safety in the operating room. asa refresher courses in anesthesiology. 33(1):101-114 ridgway m the great debate on electrical safety  - in retrospect in clinical engineering handbook, 1st edition, chapter 65, j dyro (ed) academic press 2004 o\'meley p l who\'s afraid of microshock - presentation to smbe nsw conference, albury nsw, 2011 hsu j the hypertextbook http://hypertextbook.com/facts/2000/jackhsu.shtml accessed 23 july 2013 fda maude database http://www.accessdata.fda.gov/scripts/cdrh/cfdocs/cfmaude/textsearch.cfm accessed 25 july 2013 road safety report http://roadsafety.transport.nsw.gov.au/downloads/fatality_rate_1908_to_2009.pdf accessed 25 july 2013 iev electropedia produced by the international electrotechnical commission (iec) http://www.electropedia.org iec/ts 60479-1 - effects of current on human beings and livestock https://webstore.iec.ch/publication/25402')"
102,"The Whitaker Foundation was based in Arlington, Virginia and was an organization that primarily supported biomedical engineering education and research, but also supported other forms of medical research. It was founded and funded by U. A. Whitaker in 1975 upon his death with additional support coming from his wife Helen Whitaker upon her death in 1982. The foundation contributed more than $700 million to various universities and medical schools. The foundation decided to spend its financial resources over a finite period, rather than creating an organization that would be around forever, in order to have the maximum impact. The Whitaker Foundation closed on June 30, 2006.  The foundation helped create 30 biomedical engineering programs at various universities in the United States and helped finance the construction of 13 buildings, many of them subsequently bearing the name ""Whitaker"" in some form. 


== Whitaker International Fellows and Scholars Program ==
The Whitaker International Fellows and Scholars Program funded more than 400 pre-doctoral research fellows and post-doctoral scholars between 2011 and 2018 to perform biomedical research outside of the United States. The program was managed by the Institute for International Education, who also manages the Fulbright Program. In addition to traditional laboratory research, the Whitaker International Program also funded internships in scientific policy and classroom-based educational programs. The last grants were awarded in 2018, however, the program continues to pursue Concluding Initiatives that develop and promote leadership in biomedical engineering, with an international focus.


== References ==


== External links ==
The Whitaker Foundation
Whitaker International Fellows and Scholars Program","pandas(index=102, _1=102, text='the whitaker foundation was based in arlington, virginia and was an organization that primarily supported biomedical engineering education and research, but also supported other forms of medical research. it was founded and funded by u. a. whitaker in 1975 upon his death with additional support coming from his wife helen whitaker upon her death in 1982. the foundation contributed more than $700 million to various universities and medical schools. the foundation decided to spend its financial resources over a finite period, rather than creating an organization that would be around forever, in order to have the maximum impact. the whitaker foundation closed on june 30, 2006.  the foundation helped create 30 biomedical engineering programs at various universities in the united states and helped finance the construction of 13 buildings, many of them subsequently bearing the name ""whitaker"" in some form.   == whitaker international fellows and scholars program == the whitaker international fellows and scholars program funded more than 400 pre-doctoral research fellows and post-doctoral scholars between 2011 and 2018 to perform biomedical research outside of the united states. the program was managed by the institute for international education, who also manages the fulbright program. in addition to traditional laboratory research, the whitaker international program also funded internships in scientific policy and classroom-based educational programs. the last grants were awarded in 2018, however, the program continues to pursue concluding initiatives that develop and promote leadership in biomedical engineering, with an international focus.   == references ==   == external links == the whitaker foundation whitaker international fellows and scholars program')"
103,"Biotelemetry (or medical telemetry) involves the application of telemetry in biology, medicine, and other health care to remotely monitor various vital signs of ambulatory patients.


== Application ==
The most common usage for biotelemetry is in dedicated cardiac care telemetry units or step-down units in hospitals. Although virtually any physiological signal could be transmitted, application is typically limited to cardiac monitoring and SpO2.
Biotelemetry is increasingly being used to understand animals and wildlife by remotely measuring physiology, behaviour and energetic status. It can be used to understand the way that animals migrate, and also the environment that they are experiencing by measuring the abiotic variables, and how it is affecting their physiological status by measuring biotic variables such as heart rate and temperature. Telemetry systems can either be attached externally to animals, or placed internally, with the types of transmission for the devices dependent on the environment that the animal moves in. For example, to study the movement of swimming animals signals using radio transmission or ultrasonic transmission are often used but land based or flying animals can be tracked with GPS and satellite transmissions.


== Components of a biotelemetry system ==
A typical biotelemetry system comprises:

Sensors appropriate for the particular signals to be monitored
Battery-powered, Patient worn transmitters
A Radio Antenna and Receiver
A display unit capable of concurrently presenting information from multiple patients


== History ==
Some of the first uses of biotelemetry systems date to the early space race, where physiological signals obtained from animals or human passengers were transmitted back to Earth for analysis (the name of the medical device manufacturer Spacelabs Healthcare is a reflection of their start in 1958 developing biotelemetry systems for the early U.S. space program).
Animal biotelemetry has been used since at least the 1980s. Animal biotelemetry has now advanced to not only understand the physiology and movement of free ranging animals, but also how different animals interact, for example, between predators and prey.


== Current trends ==
Because of the crowding of the radio spectrum due to the recent introduction of digital television in the United States and many other countries, the Federal Communications Commission (FCC) as well as similar agencies elsewhere have recently begun to allocate dedicated frequency bands for exclusive biotelemetry usage, for example, the Wireless Medical Telemetry Service (WMTS).   The FCC has designated the American Society for Healthcare Engineering of the American Hospital Association (ASHE/AHA) as the frequency coordinator for the WMTS.
In addition, there are many products that utilize commonly available standard radio devices such as Bluetooth and IEEE 802.11.


== See also ==
Battlefield medicine
Heart rate monitor
Remote physiological monitoring
Remote patient monitoring


== References ==


== External links ==
International Society on Biotelemetry
[1]","pandas(index=103, _1=103, text='biotelemetry (or medical telemetry) involves the application of telemetry in biology, medicine, and other health care to remotely monitor various vital signs of ambulatory patients.   == application == the most common usage for biotelemetry is in dedicated cardiac care telemetry units or step-down units in hospitals. although virtually any physiological signal could be transmitted, application is typically limited to cardiac monitoring and spo2. biotelemetry is increasingly being used to understand animals and wildlife by remotely measuring physiology, behaviour and energetic status. it can be used to understand the way that animals migrate, and also the environment that they are experiencing by measuring the abiotic variables, and how it is affecting their physiological status by measuring biotic variables such as heart rate and temperature. telemetry systems can either be attached externally to animals, or placed internally, with the types of transmission for the devices dependent on the environment that the animal moves in. for example, to study the movement of swimming animals signals using radio transmission or ultrasonic transmission are often used but land based or flying animals can be tracked with gps and satellite transmissions.   == components of a biotelemetry system == a typical biotelemetry system comprises:  sensors appropriate for the particular signals to be monitored battery-powered, patient worn transmitters a radio antenna and receiver a display unit capable of concurrently presenting information from multiple patients   == history == some of the first uses of biotelemetry systems date to the early space race, where physiological signals obtained from animals or human passengers were transmitted back to earth for analysis (the name of the medical device manufacturer spacelabs healthcare is a reflection of their start in 1958 developing biotelemetry systems for the early u.s. space program). animal biotelemetry has been used since at least the 1980s. animal biotelemetry has now advanced to not only understand the physiology and movement of free ranging animals, but also how different animals interact, for example, between predators and prey.   == current trends == because of the crowding of the radio spectrum due to the recent introduction of digital television in the united states and many other countries, the federal communications commission (fcc) as well as similar agencies elsewhere have recently begun to allocate dedicated frequency bands for exclusive biotelemetry usage, for example, the wireless medical telemetry service (wmts).   the fcc has designated the american society for healthcare engineering of the american hospital association (ashe/aha) as the frequency coordinator for the wmts. in addition, there are many products that utilize commonly available standard radio devices such as bluetooth and ieee 802.11.   == see also == battlefield medicine heart rate monitor remote physiological monitoring remote patient monitoring   == references ==   == external links == international society on biotelemetry [1]')"
104,"Six degrees of freedom (6DoF) refers to the freedom of movement of a rigid body in three-dimensional space.   Specifically, the body is free to change position as forward/backward (surge), up/down (heave), left/right (sway) translation in three perpendicular axes, combined with changes in orientation through rotation about three perpendicular axes, often termed yaw (normal axis), pitch (transverse axis), and roll (longitudinal axis). Three degrees of freedom (3DOF), a term often used in the context of virtual reality, refers to tracking of rotational motion only: pitch, yaw, and roll.


== Robotics ==
Serial and parallel manipulator systems are generally designed to position an end-effector with six degrees of freedom, consisting of three in translation and three in orientation.  This provides a direct relationship between actuator positions and the configuration of the manipulator defined by its forward and inverse kinematics.
Robot arms are described by their degrees of freedom. This is a practical metric, in contrast to the abstract definition of degrees of freedom which measures the aggregate positioning capability of a system.In 2007, Dean Kamen, inventor of the Segway, unveiled a prototype robotic arm with 14 degrees of freedom for DARPA. Humanoid robots typically have 30 or more degrees of freedom, with six degrees of freedom per arm, five or six in each leg, and several more in torso and neck.


== Engineering ==
The term is important in mechanical systems, especially biomechanical systems for analyzing and measuring properties of these types of systems that need to account for all six degrees of freedom. Measurement of the six degrees of freedom is accomplished today through both AC and DC magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit. The data is made relevant through software that integrates the data based on the needs and programming of the users.

The six degrees of freedom of a mobile unit are divided in two motional classes well as described below ;
Translational envelopes :

Moving forward and backward on the X-axis. (Surge)
Moving left and right on the Y-axis. (Sway)
Moving up and down on the Z-axis. (Heave)Rotational envelopes :

Tilting side to side on the X-axis. (Roll)
Tilting forward and backward on the Y-axis. (Pitch)
Turning left and right on the Z-axis. (Yaw)


== Operational envelope types ==
There are three types of operational envelope in the Six degrees of freedom. These types are Direct, Semi-direct (conditional) and Non-direct, all regardless of the time remaining for the execution of the maneuver, the energy remaining to execute the maneuver and finally, if the motion is commanded via a biological entity (e.g. human), a robotical entity (e.g. computer) or both.

Direct type: Involved a degree can be commanded directly without particularly conditions and described as a normal operation. (An aileron on a basic airplane)
Semi-direct type: Involved a degree can be commanded when some specific conditions are met. (Reverse thrust on an aircraft)
Non-direct type: Involved a degree when is achieved via the interaction with its environment and cannot be commanded. (Pitching motion of a vessel at sea)Transitional type also exists in some vehicles. For example, when the Space Shuttle operates in space, the craft is described as fully-direct-six because its six degrees can be commanded. However, when the Space Shuttle is in the earth's atmosphere for its return, the fully-direct-six degrees are no longer applicable for many technical reasons.


== Game controllers ==
Six degrees of freedom also refers to movement in video game-play.
First-person shooter (FPS) games generally provide five degrees of freedom: forwards/backwards, slide left/right, up/down (jump/crouch/lie), yaw (turn left/right), and pitch (look up/down).  If the game allows leaning control, then some consider it a sixth DoF; however, this may not be completely accurate, as a lean is a limited partial rotation.
The term 6DoF has sometimes been used to describe games which allow freedom of movement, but do not necessarily meet the full 6DoF criteria.  For example, Dead Space 2, and to a lesser extent, Homeworld and Zone Of The Enders allow freedom of movement.
Some examples of true 6DoF games, which allow independent control of all three movement axes and all three rotational axes, include Elite Dangerous, Shattered Horizon, the Descent franchise, Retrovirus, Miner Wars, Space Engineers, Forsaken and Overload (from the same creators of Descent).  The space MMO Vendetta Online also features 6 degrees of freedom.
Motion tracking devices such as TrackIR are used for 6DoF head tracking. This device often finds its places in flight simulators and other vehicle simulators that require looking around the cockpit to locate enemies or simply avoiding accidents in-game.
The acronym 3DoF, meaning movement in the three dimensions but not rotation, is sometimes encountered.
The Razer Hydra, a motion controller for PC, tracks position and rotation of two wired nunchucks, providing six degrees of freedom on each hand.
The SpaceOrb 360 is a 6DOF computer input device released in 1996 originally manufactured and sold by the SpaceTec IMC company (first bought by Labtec, which itself was later bought by Logitech).
The controllers sold with HTC VIVE provide 6DOF information by the lighthouse, which adopts Time of Flight (TOF) technology to determine the position of controllers.


== See also ==
Degrees of freedom (mechanics) – Number of independent parameters that define the configuration or state of a mechanical system.
Degrees of freedom problem – The multiple ways for multi-joint objects to realize a movement
Geometric terms of location – Directions or positions relative to the shape and position of an object
Ship motions – Terms connected to the 6 degrees of freedom of motion
Aircraft principal axes


== References ==","pandas(index=104, _1=104, text=""six degrees of freedom (6dof) refers to the freedom of movement of a rigid body in three-dimensional space.   specifically, the body is free to change position as forward/backward (surge), up/down (heave), left/right (sway) translation in three perpendicular axes, combined with changes in orientation through rotation about three perpendicular axes, often termed yaw (normal axis), pitch (transverse axis), and roll (longitudinal axis). three degrees of freedom (3dof), a term often used in the context of virtual reality, refers to tracking of rotational motion only: pitch, yaw, and roll.   == robotics == serial and parallel manipulator systems are generally designed to position an end-effector with six degrees of freedom, consisting of three in translation and three in orientation.  this provides a direct relationship between actuator positions and the configuration of the manipulator defined by its forward and inverse kinematics. robot arms are described by their degrees of freedom. this is a practical metric, in contrast to the abstract definition of degrees of freedom which measures the aggregate positioning capability of a system.in 2007, dean kamen, inventor of the segway, unveiled a prototype robotic arm with 14 degrees of freedom for darpa. humanoid robots typically have 30 or more degrees of freedom, with six degrees of freedom per arm, five or six in each leg, and several more in torso and neck.   == engineering == the term is important in mechanical systems, especially biomechanical systems for analyzing and measuring properties of these types of systems that need to account for all six degrees of freedom. measurement of the six degrees of freedom is accomplished today through both ac and dc magnetic or electromagnetic fields in sensors that transmit positional and angular data to a processing unit. the data is made relevant through software that integrates the data based on the needs and programming of the users.  the six degrees of freedom of a mobile unit are divided in two motional classes well as described below ; translational envelopes :  moving forward and backward on the x-axis. (surge) moving left and right on the y-axis. (sway) moving up and down on the z-axis. (heave)rotational envelopes :  tilting side to side on the x-axis. (roll) tilting forward and backward on the y-axis. (pitch) turning left and right on the z-axis. (yaw)   == operational envelope types == there are three types of operational envelope in the six degrees of freedom. these types are direct, semi-direct (conditional) and non-direct, all regardless of the time remaining for the execution of the maneuver, the energy remaining to execute the maneuver and finally, if the motion is commanded via a biological entity (e.g. human), a robotical entity (e.g. computer) or both.  direct type: involved a degree can be commanded directly without particularly conditions and described as a normal operation. (an aileron on a basic airplane) semi-direct type: involved a degree can be commanded when some specific conditions are met. (reverse thrust on an aircraft) non-direct type: involved a degree when is achieved via the interaction with its environment and cannot be commanded. (pitching motion of a vessel at sea)transitional type also exists in some vehicles. for example, when the space shuttle operates in space, the craft is described as fully-direct-six because its six degrees can be commanded. however, when the space shuttle is in the earth's atmosphere for its return, the fully-direct-six degrees are no longer applicable for many technical reasons.   == game controllers == six degrees of freedom also refers to movement in video game-play. first-person shooter (fps) games generally provide five degrees of freedom: forwards/backwards, slide left/right, up/down (jump/crouch/lie), yaw (turn left/right), and pitch (look up/down).  if the game allows leaning control, then some consider it a sixth dof; however, this may not be completely accurate, as a lean is a limited partial rotation. the term 6dof has sometimes been used to describe games which allow freedom of movement, but do not necessarily meet the full 6dof criteria.  for example, dead space 2, and to a lesser extent, homeworld and zone of the enders allow freedom of movement. some examples of true 6dof games, which allow independent control of all three movement axes and all three rotational axes, include elite dangerous, shattered horizon, the descent franchise, retrovirus, miner wars, space engineers, forsaken and overload (from the same creators of descent).  the space mmo vendetta online also features 6 degrees of freedom. motion tracking devices such as trackir are used for 6dof head tracking. this device often finds its places in flight simulators and other vehicle simulators that require looking around the cockpit to locate enemies or simply avoiding accidents in-game. the acronym 3dof, meaning movement in the three dimensions but not rotation, is sometimes encountered. the razer hydra, a motion controller for pc, tracks position and rotation of two wired nunchucks, providing six degrees of freedom on each hand. the spaceorb 360 is a 6dof computer input device released in 1996 originally manufactured and sold by the spacetec imc company (first bought by labtec, which itself was later bought by logitech). the controllers sold with htc vive provide 6dof information by the lighthouse, which adopts time of flight (tof) technology to determine the position of controllers.   == see also == degrees of freedom (mechanics) – number of independent parameters that define the configuration or state of a mechanical system. degrees of freedom problem – the multiple ways for multi-joint objects to realize a movement geometric terms of location – directions or positions relative to the shape and position of an object ship motions – terms connected to the 6 degrees of freedom of motion aircraft principal axes   == references =="")"
105,"A Bachelor of Science in Biomedical Engineering is a kind of bachelor's degree typically conferred after a four-year undergraduate course of study in biomedical engineering (BME). The degree itself is largely equivalent to a Bachelor of Science and many institutions conferring degrees in the fields of biomedical engineering and bioengineering do not append the field to the degree itself. Courses of study in BME are also extremely diverse as the field itself is relatively new and developing. In general, an undergraduate course of study in BME is likened to a cross between engineering and biological science with varying degrees of proportionality between the two.


== Professional status ==
Engineers typically require a type of professional certification, such as satisfying certain education requirements and passing an examination to become a professional engineer. These certifications are usually nationally regulated and registered, but there are also cases where a self-governing body, such as the Canadian Association of Professional Engineers. In many cases, carrying the title of ""Professional Engineer"" is legally protected.
As BME is an emerging field, professional certifications are not as standard and uniform as they are for other engineering fields. For example, the Fundamentals of Engineering exam in the U.S. does not include a biomedical engineering section, though it does cover biology. Biomedical engineers often simply possess a university degree as their qualification. However, some countries do regulate biomedical engineers, such as Australia, however registration is typically recommended, but not always a requirement.As with many engineering fields, a bachelor's degree is usually the minimum and often most common degree for a profession in BME, though it is not uncommon for the bachelor's degree to serve as a launching pad into graduate studies. ABET does accredit undergraduate programs in the field. However, even this is not a strict requirement since it is an emerging field and due to the young age of many programs.


== Curriculum ==
The curriculum for BME programs varies significantly from institution to institution and often within a single program. In general, a basic engineering curriculum, including mathematics through differential equations, statistics, and a basic understanding of biology and other basic sciences are hallmarks of a BME program.
Many BME programs have a series of tracks that focus on a particular area of study within BME. Often, the tracks also coincide with a particular engineering or science field. Examples of tracks include:

Biomechanics: Focus includes medical devices, modeling of biological systems and mechanics of organisms. This track interfaces with mechanical engineering and often physiology.
Bioinstrumentation/Bioelectrical Systems: Focus includes medical devices, modeling of biological systems, in particular circuit analogies to the nervous system, bioelectric phenomena and signal processing. This track interfaces with electrical engineering.
Cell, Tissue and Biomolecular Engineering: This track is often quite diverse, with focus ranging from artificial tissues, modeling of biological systems, drug delivery, genetic engineering, biochemical engineering and protein production. This track can interface with chemical engineering, mechanical engineering, molecular biology, physiology, genetics, materials science and other fields.
Medical Optics: Focus on medical diagnostics and medical optical technology. This track interfaces with optics, physics and electrical engineering.Many other tracks may exist within specific programs as well as combinations of multiple tracks.
Another common feature of many BME programs is a capstone design project where students become involved in researching and developing technology in the field. At some schools, this culminates in the creation of medical devices and prototypes. Capstone design projects also often include exposure to issues like funding, regulatory issues and other topics that are related to careers in the field.


== Research and Industry Experience ==
An important feature of many programs is the inclusion of research and/or industry experience into either the curricular or extracurricular work done by students. Since BME careers often focuses on research or industrial applications of the field, many programs have seen fit to either encourage or sometimes require experience outside of the standard curricular requirements. Many research universities offer chances for students to participate in faculty research at the undergraduate level. Other schools have an industry practicum or co-ops to give students relevant work experience before graduation.
Students that participate in either research or industry during the course of study often see advantages when they enter the job market, as many employers prefer experienced candidates or offer higher pay to those with prior experience. Also, research or industry experience is often a factor in graduate school admission.


== Value of the degree ==
Recently, many universities, such as Case Western Reserve University have been implementing new initiatives to either create or expand upon undergraduate programs in BME. This is in part due to rising demand in the biotechnology sector and the increasing interest in biological research. A degree in BME instantly identifies a candidate as having training in both traditional engineering as well as biological science, which has become an increasingly desirable qualification as aspects of biology are permeating into other industries.
Since BME is a diverse field, many programs have a broad curriculum with students usually choosing to specialize in a particular aspect of BME. However, due to the diversity, some degree holders may find their education lacking in deep emphasis, which may prompt continuing studies in graduate school or by learning through experience. 
Numerous rankings of undergraduate BME programs exist with highly varying basis for each ranking. As with many degrees, the reputation of a program may factor into the desirability of a degree holder for either employment or graduate admission. The reputation of many undergraduate degrees are also linked to the institution's graduate or research programs, which have more tangible factors for rating, such as research funding and volume, publications and citations.


== References ==","pandas(index=105, _1=105, text='a bachelor of science in biomedical engineering is a kind of bachelor\'s degree typically conferred after a four-year undergraduate course of study in biomedical engineering (bme). the degree itself is largely equivalent to a bachelor of science and many institutions conferring degrees in the fields of biomedical engineering and bioengineering do not append the field to the degree itself. courses of study in bme are also extremely diverse as the field itself is relatively new and developing. in general, an undergraduate course of study in bme is likened to a cross between engineering and biological science with varying degrees of proportionality between the two.   == professional status == engineers typically require a type of professional certification, such as satisfying certain education requirements and passing an examination to become a professional engineer. these certifications are usually nationally regulated and registered, but there are also cases where a self-governing body, such as the canadian association of professional engineers. in many cases, carrying the title of ""professional engineer"" is legally protected. as bme is an emerging field, professional certifications are not as standard and uniform as they are for other engineering fields. for example, the fundamentals of engineering exam in the u.s. does not include a biomedical engineering section, though it does cover biology. biomedical engineers often simply possess a university degree as their qualification. however, some countries do regulate biomedical engineers, such as australia, however registration is typically recommended, but not always a requirement.as with many engineering fields, a bachelor\'s degree is usually the minimum and often most common degree for a profession in bme, though it is not uncommon for the bachelor\'s degree to serve as a launching pad into graduate studies. abet does accredit undergraduate programs in the field. however, even this is not a strict requirement since it is an emerging field and due to the young age of many programs.   == curriculum == the curriculum for bme programs varies significantly from institution to institution and often within a single program. in general, a basic engineering curriculum, including mathematics through differential equations, statistics, and a basic understanding of biology and other basic sciences are hallmarks of a bme program. many bme programs have a series of tracks that focus on a particular area of study within bme. often, the tracks also coincide with a particular engineering or science field. examples of tracks include:  biomechanics: focus includes medical devices, modeling of biological systems and mechanics of organisms. this track interfaces with mechanical engineering and often physiology. bioinstrumentation/bioelectrical systems: focus includes medical devices, modeling of biological systems, in particular circuit analogies to the nervous system, bioelectric phenomena and signal processing. this track interfaces with electrical engineering. cell, tissue and biomolecular engineering: this track is often quite diverse, with focus ranging from artificial tissues, modeling of biological systems, drug delivery, genetic engineering, biochemical engineering and protein production. this track can interface with chemical engineering, mechanical engineering, molecular biology, physiology, genetics, materials science and other fields. medical optics: focus on medical diagnostics and medical optical technology. this track interfaces with optics, physics and electrical engineering.many other tracks may exist within specific programs as well as combinations of multiple tracks. another common feature of many bme programs is a capstone design project where students become involved in researching and developing technology in the field. at some schools, this culminates in the creation of medical devices and prototypes. capstone design projects also often include exposure to issues like funding, regulatory issues and other topics that are related to careers in the field.   == research and industry experience == an important feature of many programs is the inclusion of research and/or industry experience into either the curricular or extracurricular work done by students. since bme careers often focuses on research or industrial applications of the field, many programs have seen fit to either encourage or sometimes require experience outside of the standard curricular requirements. many research universities offer chances for students to participate in faculty research at the undergraduate level. other schools have an industry practicum or co-ops to give students relevant work experience before graduation. students that participate in either research or industry during the course of study often see advantages when they enter the job market, as many employers prefer experienced candidates or offer higher pay to those with prior experience. also, research or industry experience is often a factor in graduate school admission.   == value of the degree == recently, many universities, such as case western reserve university have been implementing new initiatives to either create or expand upon undergraduate programs in bme. this is in part due to rising demand in the biotechnology sector and the increasing interest in biological research. a degree in bme instantly identifies a candidate as having training in both traditional engineering as well as biological science, which has become an increasingly desirable qualification as aspects of biology are permeating into other industries. since bme is a diverse field, many programs have a broad curriculum with students usually choosing to specialize in a particular aspect of bme. however, due to the diversity, some degree holders may find their education lacking in deep emphasis, which may prompt continuing studies in graduate school or by learning through experience. numerous rankings of undergraduate bme programs exist with highly varying basis for each ranking. as with many degrees, the reputation of a program may factor into the desirability of a degree holder for either employment or graduate admission. the reputation of many undergraduate degrees are also linked to the institution\'s graduate or research programs, which have more tangible factors for rating, such as research funding and volume, publications and citations.   == references ==')"
106,"Materialise Mimics is an image processing software for 3D design and modeling, developed by Materialise NV, a Belgian company specialized in additive manufacturing software and technology for medical, dental and additive manufacturing industries. Materialise Mimics is used to create 3D surface models from stacks of 2D image data. These 3D models can then be used for a variety of engineering applications. Mimics is an acronym for Materialise Interactive Medical Image Control System. It is developed in an ISO environment with CE and FDA 510k premarket clearance. Materialise Mimics is commercially available as part of the Materialise Mimics Innovation Suite, which also contains Materialise 3-matic, a design and meshing software for anatomical data. The current version is 20.0, it supports Windows 10, Windows 7, Vista and XP in x64.


== Process ==
Materialise Mimics calculates surface 3D models from stacked image data such as Computed Tomography (CT), Micro CT, Magnetic Resonance Imaging (MRI), Confocal Microscopy, X-ray and Ultrasound, through image segmentation. The ROI, selected in the segmentation process is converted to a 3D surface model using an adapted marching cubes algorithm that takes the partial volume effect into account, leading to very accurate 3D models. The 3D files are represented in the STL format.


== Uploading Data ==
DICOM data from CT or MRI images can be uploaded into Materialise Mimics in order to begin the segmentation process. From this data, 3 different views are present: the coronal, axial, and sagittal views. Another window is present to display 3D objects. 


== Mask Creation ==
The ""New Mask"" tool can be used to highlight specific anatomy from the DICOM data.


== Printing Models ==
Models can be sent to 3D printers in the form of STLs. 


== Gallery ==

		
		


== See also ==
3D modeling
3D Slicer
Computer representation of surfaces
Computed tomography
Medical imaging


== References ==


== External links ==
Official website
User community","pandas(index=106, _1=106, text='materialise mimics is an image processing software for 3d design and modeling, developed by materialise nv, a belgian company specialized in additive manufacturing software and technology for medical, dental and additive manufacturing industries. materialise mimics is used to create 3d surface models from stacks of 2d image data. these 3d models can then be used for a variety of engineering applications. mimics is an acronym for materialise interactive medical image control system. it is developed in an iso environment with ce and fda 510k premarket clearance. materialise mimics is commercially available as part of the materialise mimics innovation suite, which also contains materialise 3-matic, a design and meshing software for anatomical data. the current version is 20.0, it supports windows 10, windows 7, vista and xp in x64.   == process == materialise mimics calculates surface 3d models from stacked image data such as computed tomography (ct), micro ct, magnetic resonance imaging (mri), confocal microscopy, x-ray and ultrasound, through image segmentation. the roi, selected in the segmentation process is converted to a 3d surface model using an adapted marching cubes algorithm that takes the partial volume effect into account, leading to very accurate 3d models. the 3d files are represented in the stl format.   == uploading data == dicom data from ct or mri images can be uploaded into materialise mimics in order to begin the segmentation process. from this data, 3 different views are present: the coronal, axial, and sagittal views. another window is present to display 3d objects.   == mask creation == the ""new mask"" tool can be used to highlight specific anatomy from the dicom data.   == printing models == models can be sent to 3d printers in the form of stls.   == gallery ==          == see also == 3d modeling 3d slicer computer representation of surfaces computed tomography medical imaging   == references ==   == external links == official website user community')"
107,"Automated insulin delivery systems are automated (or semi-automated) systems designed to assist people with diabetes, primarily type 1, by automatically adjusting insulin delivery to help them control their blood glucose levels. Currently available systems (as of October, 2020) can only deliver (and regulate delivery of) a single hormone- insulin. Other systems currently in development aim to improve on current systems by adding one or more additional hormones that can be delivered as needed, providing something closer to the endocrine functionality of a healthy pancreas.
The endocrine functionality of the pancreas is provided by islet cells which produce the hormones insulin and glucagon. Artificial pancreatic technology mimics the secretion of these hormones into the bloodstream in response to the body's changing blood glucose levels. Maintaining balanced blood sugar levels is crucial to the function of the brain, liver, and kidneys. Therefore, for type 1 patients, it is necessary that the levels be kept balanced when the body cannot produce insulin itself.Automated insulin delivery systems are often referred to using the term artificial pancreas, but the term has no precise, universally accepted definition. For uses other than automated insulin delivery, see Artificial pancreas (disambiguation). 


== General Overview ==


=== History ===
The first automated insulin delivery system was known as the Biostator.


=== Classes of AID systems ===
Currently available AID systems fall into four broad classes based on their capabilities. The first systems released- suspend systems- can only halt insulin delivery. Loop systems can modulate delivery both up and down.


==== Threshold suspend ====
Threshold suspend systems are the simplest form of insulin delivery automation. They halt the constant flow of insulin from a pump (known as basal insulin) when a connected CGM reports a glucose level below a pre-set threshold. Halting basal delivery stops the normal preprogrammed rate of delivery, but it cannot remove insulin that has already been infused, so the overall efficacy of threshold suspend systems is limited due to the relatively slow pharmacokinetics of insulin delivered subcutaneously.


==== Predictive Low Glucose Suspend ====
A step forward from threshold suspend systems, Predictive Low Glucose Suspend (PLGS) systems use a mathematical model to extrapolate predicted future blood sugar levels based on recent past readings from a CGM. This allows the system to halt insulin delivery as much as 30 minutes prior to a predicted hypoglycemic event, allowing addition time for the slow pharmacokinetics of insulin to reflect that delivery has been halted.


==== Hybrid Closed Loop ====
Hybrid Closed Loop (HCL) systems further expand on the capabilities of PGLS systems by adjusting basal insulin delivery rates both up and down in response to values from a continuous glucose monitor. Through this modulation of basal insulin, the system is able to reduce the magnitude and duration both hyperglycemic and hypoglycemic events.


==== Advanced Hybrid Closed Loop ====
In addition to modulating basal insulin, Advanced Hybrid Closed Loop systems have the ability to deliver boluses of insulin to correct for elevated blood sugar.


=== Required Components ===
An automated insulin delivery system consists of three distinct components: a continuous glucose monitor to determine blood sugar levels, a pump to deliver insulin, and an algorithm that uses the data from the CGM to send commands to the pump. In the United States, the Food and Drug Administration (FDA) allows each component to be approved independently, allowing for more rapid approvals and incremental innovation. Each component is discussed in greater detail below.


==== Continuous Glucose Monitor (CGM) ====

Continuous glucose monitors (CGMs) are medical devices which extrapolate an estimate of the glucose concentration in a patient's blood based on the level of glucose present in the subcutaneous interstitial fluid. A thin, biocompatible sensor wire coated with a glucose-reactive enzyme is inserted into the skin, allowing the system to read the voltage generated, and based on it, estimate blood glucose. The biggest advantage of a CGM over a traditional fingerstick blood glucose meter is that the CGM can take a new reading as often as every 60 seconds (although most only take a reading every 5 minutes), allowing for a sampling frequency that is able to provide not just a current blood sugar level, but a record of past measurements; allowing computer systems to project past short-term trends into the future, showing patients where their blood sugar levels are likely headed.
Early CGMs were not particularly accurate, but were still useful for observing and recording overall trends and provide warnings in the event of rapid changes in blood glucose readings. 
Continuous blood glucose monitors are one of the set of devices that make up an artificial pancreas device system, the other being an insulin pump, and a glucose meter to calibrate the device. Continuous glucose monitors are a more recent breakthrough and have begun to hit the markets for patient use after approval from the FDA. Both the traditional and the continuous monitor require manual insulin delivery or carbohydrate intake depending on the readings from the devices. While the traditional blood glucose meters require the user to prick their finger every few hours to obtain data, continuous monitors use sensors placed just under the skin on the arm or abdomen to deliver blood sugar level data to receivers or smartphone apps as often as every few minutes. The sensors can be used for up to fourteen days.  A number of different continuous monitors are currently approved by the FDA.The first continuous glucose monitor (CGM) was approved in December 2016. Developed by Dexcom, the G5 Mobile Continuous Monitoring System requires users to prick their fingers twice a day (as opposed to the typical average 8 times daily with the traditional meters) in order to calibrate the sensors. The sensors last up to seven days. The device uses Bluetooth technology to warn the user either through a handheld receiver or app on a smartphone if blood glucose levels reach below a certain point. The cost for this device excluding any co-insurance is an estimated $4,800 a year.

Abbott Laboratories' FreeStyle Libre CGM was approved in September 2017. Recently, the technology was modified to support smartphone use through the LibreLink app. This device does not require finger pricks at all and the sensor, placed on the upper arm, lasts 14 days. The estimated cost for this monitor is $1,300 a year.Dexcom's next G6 model CGM was approved in March 2018, which can last up to ten days and does not need finger prick calibration. Like Medtronic's monitor, it can predict glucose level trends. It is compatible for integration into insulin pumps.


==== Control Algorithm ====


==== Insulin Pump ====


== Currently Available Systems ==


=== Do-It-Yourself ===


=== Commercial ===


==== MiniMed 670G ====
In September 2016, the FDA approved the Medtronic MiniMed 670G, which was the first approved hybrid closed loop system. The device senses a diabetic person's basal insulin requirement and automatically adjusts its delivery to the body. It is made up of a continuous glucose monitor, an insulin pump, and a glucose meter for calibration. It automatically functions to modify the level of insulin delivery based on the detection of blood glucose levels by continuous monitor. It does this by sending the blood glucose data through an algorithm that analyzes and makes the subsequent adjustments. The system has two modes. Manual mode lets the user choose the rate at which basal insulin is delivered. Auto mode regulates basal insulin levels from the continuous monitor's readings every five minutes.The device was originally available only to those aged 14 or older, and in June 2018 was approved by the FDA for use in children aged 7–14. Families have reported better sleep quality from use of the new system, as they do not have to worry about manually checking blood glucose levels during the night. The full cost of the system is $3700, but patients have the opportunity to get it for less.


== Systems in Development ==


=== Ilet Bionic Pancreas ===
A team at Boston University working in collaboration with Massachusetts General Hospital on a dual hormone artificial pancreas system  began clinical trials on their device called the Bionic Pancreas in 2008. In 2016, the Public Benefit Corporation Beta Bionics was formed. In conjunction with the formation of the company, Beta Bionics changed the preliminary name for their device from the Bionic Pancreas to the iLet. The device uses a closed-loop system to deliver both insulin and glucagon in response to sensed blood glucose levels. While not yet approved for public use, the 4th generation iLet prototype, presented in 2017, is around the size of an iPhone, with a touchscreen interface. It contains two chambers for both insulin and glucagon, and the device is configurable for use with only one hormone, or both. While trials continue to be run, the iLet has a projected final approval for the insulin-only system in 2020.


=== Inreda Diabetic ===
In collaboration with the Academic Medical Centre (AMC) in Amsterdam, Inreda is developing a closed loop system with insulin and glucagon. The initiator, Robin Koops, started to develop the device in 2004 and ran the first tests on himself. After several highly successful trials it received the European EC license in 2016. The product is expected to market in the second half of 2020. A smaller improved version is scheduled for 2023.


== Approaches ==


=== Medical equipment ===
The medical equipment approach involves combining a continuous glucose monitor and an implanted insulin pump that can function together with a computer-controlled algorithm to replace the normal function of the pancreas. The development of continuous glucose monitors has led to the progress in artificial pancreas technology using this integrated system.


==== Closed-loop systems ====
Unlike the continuous sensor alone, the closed-loop system requires no user input in response to reading from the monitor; the monitor and insulin pump system automatically delivers the correct amount of hormone calculated from the readings transmitted. The system is what makes up the artificial pancreas device.


===== Current studies =====
Four studies on different artificial pancreas systems are being conducted starting in 2017 and going into the near future. The projects are funded by the National Institute of Diabetes and Digestive and Kidney Diseases, and are the final part of testing the devices before applying for approval for use. Participants in the studies are able to live their lives at home while using the devices and being monitored remotely for safety, efficacy, and a number of other factors.The International Diabetes Closed-Loop trial, led by researchers from the University of Virginia, is testing a closed-loop system called inControl, which has a smartphone user interface. 240 people of ages 14 and up are participating for 6 months.A full-year trial led by researchers from the University of Cambridge started in May 2017 and has enrolled an estimated 150 participants of ages 6 to 18 years. The artificial pancreas system being studied uses a smartphone and has a low glucose feature to improve glucose level control.The International Diabetes Center in Minneapolis, Minnesota, in collaboration with Schneider Children's Medical Center of Israel, are planning a 6-month study that will begin in early 2019 and will involve 112 adolescents and young adults, ages 14 to 30. The main object of the study is to compare the current Medtronic 670G system to a new Medtronic-developed system. The new system has programming that aims to improve glucose control around mealtime, which is still a big challenge in the field.The current 6-month study lead by the Bionic Pancreas team started in mid-2018 and enrolled 312 participants of ages 18 and above.


=== Physiological ===

The biotechnical company Defymed, based in France, is developing an implantable bio-artificial device called MailPan which features a bio-compatible membrane with selective permeability to encapsulate different cell types, including pancreatic beta cells. The implantation of the device does not require conjunctive immuno-suppressive therapy because the membrane prevents antibodies of the patient from entering the device and damaging the encapsulated cells. After being surgically implanted, the membrane sheet will be viable for years. The cells that the device holds can be produced from stem cells rather than human donors, and may also be replaced over time using input and output connections without surgery. Defymed is partially funded by JDRF, formerly known as the Juvenile Diabetes Research Foundation, but is now defined as an organization for all ages and all stages of type 1 diabetes.In November 2018, it was announced that Defymed would partner with the Israel-based Kadimastem, a bio-pharmaceutical company developing stem-cell based regenerative therapies, to receive a two-year grant worth approximately $1.47 million for the development of a bio-artificial pancreas that would treat type 1 diabetes. Kadimastem's stem cell technology uses differentiation of human embryonic stem cells to obtain pancreatic endocrine cells. These include insulin-producing beta cells, as well as alpha cells, which produce glucagon. Both cells arrange in islet-like clusters, mimicking the structure of the pancreas. The aim of the partnership is to combine both technologies in a bio-artificial pancreas device, which releases insulin in response to blood glucose levels, to bring to clinical trial stages.The San Diego, California based biotech company ViaCyte has also developed a product aiming to provide a solution for type 1 diabetes which uses an encapsulation device made of a semi-permeable immune reaction-protective membrane. The device contains pancreatic progenitor cells that have been differentiated from embryonic stem cells. After surgical implantation in an outpatient procedure, the cells mature into endocrine cells which arrange in islet-like clusters and mimic the function of the pancreas, producing insulin and glucagon. The technology advanced from pre-clinical studies to FDA approval for phase 1 clinical trials in 2014, and presented two-year data from the trial in June 2018. They reported that their product, called PEC-Encap, has so far been safe and well tolerated in patients at a dose below therapeutic levels. The encapsulated cells were able to survive and mature after implantation, and immune system rejection was decreased due to the protective membrane. The second phase of the trial will evaluate the efficacy of the product. ViaCyte has also been receiving financial support from JDRF on this project.


== Initiatives around the globe ==
In the United States in 2006, JDRF (formerly the Juvenile Diabetes Research Foundation) launched a multi-year initiative to help accelerate the development, regulatory approval, and acceptance of continuous glucose monitoring and artificial pancreas technology.Grassroots efforts to create and commercialize a fully automated artificial pancreas system have also arisen directly from patient advocates and the diabetes community. Bigfoot Biomedical, a company founded by parents of children with T1D have created algorithms and are developing a closed loop device that monitor blood sugar and appropriately provide insulin.


== References ==","pandas(index=107, _1=107, text=""automated insulin delivery systems are automated (or semi-automated) systems designed to assist people with diabetes, primarily type 1, by automatically adjusting insulin delivery to help them control their blood glucose levels. currently available systems (as of october, 2020) can only deliver (and regulate delivery of) a single hormone- insulin. other systems currently in development aim to improve on current systems by adding one or more additional hormones that can be delivered as needed, providing something closer to the endocrine functionality of a healthy pancreas. the endocrine functionality of the pancreas is provided by islet cells which produce the hormones insulin and glucagon. artificial pancreatic technology mimics the secretion of these hormones into the bloodstream in response to the body's changing blood glucose levels. maintaining balanced blood sugar levels is crucial to the function of the brain, liver, and kidneys. therefore, for type 1 patients, it is necessary that the levels be kept balanced when the body cannot produce insulin itself.automated insulin delivery systems are often referred to using the term artificial pancreas, but the term has no precise, universally accepted definition. for uses other than automated insulin delivery, see artificial pancreas (disambiguation).   == general overview == the biotechnical company defymed, based in france, is developing an implantable bio-artificial device called mailpan which features a bio-compatible membrane with selective permeability to encapsulate different cell types, including pancreatic beta cells. the implantation of the device does not require conjunctive immuno-suppressive therapy because the membrane prevents antibodies of the patient from entering the device and damaging the encapsulated cells. after being surgically implanted, the membrane sheet will be viable for years. the cells that the device holds can be produced from stem cells rather than human donors, and may also be replaced over time using input and output connections without surgery. defymed is partially funded by jdrf, formerly known as the juvenile diabetes research foundation, but is now defined as an organization for all ages and all stages of type 1 diabetes.in november 2018, it was announced that defymed would partner with the israel-based kadimastem, a bio-pharmaceutical company developing stem-cell based regenerative therapies, to receive a two-year grant worth approximately $1.47 million for the development of a bio-artificial pancreas that would treat type 1 diabetes. kadimastem's stem cell technology uses differentiation of human embryonic stem cells to obtain pancreatic endocrine cells. these include insulin-producing beta cells, as well as alpha cells, which produce glucagon. both cells arrange in islet-like clusters, mimicking the structure of the pancreas. the aim of the partnership is to combine both technologies in a bio-artificial pancreas device, which releases insulin in response to blood glucose levels, to bring to clinical trial stages.the san diego, california based biotech company viacyte has also developed a product aiming to provide a solution for type 1 diabetes which uses an encapsulation device made of a semi-permeable immune reaction-protective membrane. the device contains pancreatic progenitor cells that have been differentiated from embryonic stem cells. after surgical implantation in an outpatient procedure, the cells mature into endocrine cells which arrange in islet-like clusters and mimic the function of the pancreas, producing insulin and glucagon. the technology advanced from pre-clinical studies to fda approval for phase 1 clinical trials in 2014, and presented two-year data from the trial in june 2018. they reported that their product, called pec-encap, has so far been safe and well tolerated in patients at a dose below therapeutic levels. the encapsulated cells were able to survive and mature after implantation, and immune system rejection was decreased due to the protective membrane. the second phase of the trial will evaluate the efficacy of the product. viacyte has also been receiving financial support from jdrf on this project.   == initiatives around the globe == in the united states in 2006, jdrf (formerly the juvenile diabetes research foundation) launched a multi-year initiative to help accelerate the development, regulatory approval, and acceptance of continuous glucose monitoring and artificial pancreas technology.grassroots efforts to create and commercialize a fully automated artificial pancreas system have also arisen directly from patient advocates and the diabetes community. bigfoot biomedical, a company founded by parents of children with t1d have created algorithms and are developing a closed loop device that monitor blood sugar and appropriately provide insulin.   == references =="")"
108,"The Institute of Biomedical Engineering (BME) is an academic unit at the University of Toronto. The main goal of the institute is provide graduate education and advance research in the field of biomedical engineering. Established in 1962 by Dr. Norman Moody under the moniker 'Institute of Biomedical Electronics', the Institute has taken on several other names in the past.
BME is home to research and teaching interests of the Faculties of Applied Science and Engineering, Dentistry, and Medicine at the University of Toronto. Located in the heart of the Toronto Discovery District, BME offers research ties with sister departments at University of Toronto, as well as researchers in the Hospital for Sick Children, University Health Network, Mount Sinai Hospital, Sunnybrook Health Sciences Centre, the new Women's College Hospital, St. Michael's Hospital (Toronto), Toronto Rehabilitation Institute, Bloorview Kids Rehab and MaRS Discovery District.


== History ==
Founded in 1962 by Dr. Norman Moody, the Institute of Biomedical Engineering (BME) is one of the oldest biomedical engineering academic units in Canada. With a heavy emphasis on applying electrical engineering concept to solve biological problems, the Institute formed between a collaboration between the Faculty of Engineering and the Faculty of Medicine at the University of Toronto.
In 1999, the Institute of Biomedical Engineering (IBME) merged with the Centre for Biomaterials and the tissue engineering group in Chemical Engineering, to create the Institute of Biomaterials and Biomedical Engineering (IBBME, 1999 - 2000). In addition to Medicine and Engineering, IBBME is also a part of the Faculty of Dentistry.
In the fall of 2001, with the help of funding from the Whitaker Foundation, the Institute launched the new Graduate Program in Biomedical Engineering. 
In summer of 2020, the Institute of Biomaterials and Biomedical Engineering changed its name to the Institute of Biomedical Engineering (BME).


== Academics ==
The Institute of Biomedical Engineering hosts faculty members and students who conduct research in the topics of clinical engineering, cell & tissue engineering, and molecular engineering. BME offers Doctors of Philosophy (PhD), Masters of Science (MASc), Masters of Health Science (MHSc), and Masters of Engineering (MEng). Students enrolled in the PhD and the MASc programs are required to perform research in one of the three disciplines listed above. As of 2019, there are a total of 332 graduate students, half of whom were PhD students.


== References ==


== External links ==
Official website","pandas(index=108, _1=108, text=""the institute of biomedical engineering (bme) is an academic unit at the university of toronto. the main goal of the institute is provide graduate education and advance research in the field of biomedical engineering. established in 1962 by dr. norman moody under the moniker 'institute of biomedical electronics', the institute has taken on several other names in the past. bme is home to research and teaching interests of the faculties of applied science and engineering, dentistry, and medicine at the university of toronto. located in the heart of the toronto discovery district, bme offers research ties with sister departments at university of toronto, as well as researchers in the hospital for sick children, university health network, mount sinai hospital, sunnybrook health sciences centre, the new women's college hospital, st. michael's hospital (toronto), toronto rehabilitation institute, bloorview kids rehab and mars discovery district.   == history == founded in 1962 by dr. norman moody, the institute of biomedical engineering (bme) is one of the oldest biomedical engineering academic units in canada. with a heavy emphasis on applying electrical engineering concept to solve biological problems, the institute formed between a collaboration between the faculty of engineering and the faculty of medicine at the university of toronto. in 1999, the institute of biomedical engineering (ibme) merged with the centre for biomaterials and the tissue engineering group in chemical engineering, to create the institute of biomaterials and biomedical engineering (ibbme, 1999 - 2000). in addition to medicine and engineering, ibbme is also a part of the faculty of dentistry. in the fall of 2001, with the help of funding from the whitaker foundation, the institute launched the new graduate program in biomedical engineering. in summer of 2020, the institute of biomaterials and biomedical engineering changed its name to the institute of biomedical engineering (bme).   == academics == the institute of biomedical engineering hosts faculty members and students who conduct research in the topics of clinical engineering, cell & tissue engineering, and molecular engineering. bme offers doctors of philosophy (phd), masters of science (masc), masters of health science (mhsc), and masters of engineering (meng). students enrolled in the phd and the masc programs are required to perform research in one of the three disciplines listed above. as of 2019, there are a total of 332 graduate students, half of whom were phd students.   == references ==   == external links == official website"")"
109,"BMES (the Biomedical Engineering Society) is the professional society for students, faculty, researcher and industry working in the broad area of biomedical engineering. BMES is the leading biomedical engineering society in the United States and was founded on February 1, 1968 ""to promote the increase of biomedical engineering knowledge and its utilization.""  There are 7,000 members in 2018.Since 1972, the society has published an academic journal, the Annals of Biomedical Engineering (online archive).


== References ==


== External links ==
Engineering Society","pandas(index=109, _1=109, text='bmes (the biomedical engineering society) is the professional society for students, faculty, researcher and industry working in the broad area of biomedical engineering. bmes is the leading biomedical engineering society in the united states and was founded on february 1, 1968 ""to promote the increase of biomedical engineering knowledge and its utilization.""  there are 7,000 members in 2018.since 1972, the society has published an academic journal, the annals of biomedical engineering (online archive).   == references ==   == external links == engineering society')"
110,"Annual Review of Biomedical Engineering is an academic journal published by Annual Reviews. In publication since 1999, this journal covers the significant developments in the broad field of biomedical engineering with an annual volume of review articles. It is edited by Martin L. Yarmush and Mehmet Toner. According to the Journal Citation Reports, the journal has a 2019 impact factor of 15.541, ranking it second out of 87 journals in the category ""Biomedical Engineering"".


== History ==
The Annual Review of Biomedical Engineering was first published in 1999 by the nonprofit publisher Annual Reviews. The inaugural editor was Martin L. Yarmush; Yarmush remained editor until 2021, at which point he was co-editor along with Mehmet Toner. Though it began with a physical editiion, it is now only published electronically.


== Scope and indexing ==
The Annual Review of Biomedical Engineering defines its scope as covering significant developments relevant to biomedical engineering. Included subfields are biomechanics; biomaterials; computational genomics; proteomics; healthcare, biochemical, and tissue engineering; biomonitoring; and medical imaging. As of 2019, Journal Citation Reports lists the journal's impact factor as 15.541, ranking it second of 87 journal titles in the category ""Biomedical Engineering"". It is abstracted and indexed in Scopus, Science Citation Index Expanded, MEDLINE, EMBASE, Inspec and Academic Search, among others.


== Editorial processes ==
The Annual Review of Biomedical Engineering is helmed by the editor or the co-editors. The editor is assisted by the editorial committee, which includes associate editors, regular members, and occasionally guest editors. Guest members participate at the invitation of the editor, and serve terms of one year. All other members of the editorial committee are appointed by the Annual Reviews board of directors and serve five-year terms. The editorial committee determines which topics should be included in each volume and solicits reviews from qualified authors. Unsolicited manuscripts are not accepted. Peer review of accepted manuscripts is undertaken by the editorial committee.


=== Current editorial board ===
As of 2021, the editorial committee consists of the co-editors and the following members:


== See also ==
List of engineering journals and magazines


== References ==","pandas(index=110, _1=110, text='annual review of biomedical engineering is an academic journal published by annual reviews. in publication since 1999, this journal covers the significant developments in the broad field of biomedical engineering with an annual volume of review articles. it is edited by martin l. yarmush and mehmet toner. according to the journal citation reports, the journal has a 2019 impact factor of 15.541, ranking it second out of 87 journals in the category ""biomedical engineering"".   == history == the annual review of biomedical engineering was first published in 1999 by the nonprofit publisher annual reviews. the inaugural editor was martin l. yarmush; yarmush remained editor until 2021, at which point he was co-editor along with mehmet toner. though it began with a physical editiion, it is now only published electronically.   == scope and indexing == the annual review of biomedical engineering defines its scope as covering significant developments relevant to biomedical engineering. included subfields are biomechanics; biomaterials; computational genomics; proteomics; healthcare, biochemical, and tissue engineering; biomonitoring; and medical imaging. as of 2019, journal citation reports lists the journal\'s impact factor as 15.541, ranking it second of 87 journal titles in the category ""biomedical engineering"". it is abstracted and indexed in scopus, science citation index expanded, medline, embase, inspec and academic search, among others.   == editorial processes == the annual review of biomedical engineering is helmed by the editor or the co-editors. the editor is assisted by the editorial committee, which includes associate editors, regular members, and occasionally guest editors. guest members participate at the invitation of the editor, and serve terms of one year. all other members of the editorial committee are appointed by the annual reviews board of directors and serve five-year terms. the editorial committee determines which topics should be included in each volume and solicits reviews from qualified authors. unsolicited manuscripts are not accepted. peer review of accepted manuscripts is undertaken by the editorial committee. as of 2021, the editorial committee consists of the co-editors and the following members:   == see also == list of engineering journals and magazines   == references ==')"
111,"NeuroArm is an engineering research surgical robot specifically designed for neurosurgery. It is the first image-guided, MR-compatible surgical robot that has the capability to perform both microsurgery and stereotaxy.IMRIS, Inc. acquired NeuroArm assets in 2010, and the company is working to develop a next generation of the technology for worldwide commercialization.  It will be integrated with the VISIUS(TM) Surgical Theatre under the name SYMBIS(TM) Surgical System.


== Design ==
NeuroArm was designed to be image-guided and can perform procedures inside an MRI. NeuroArm includes two remote detachable manipulators on a mobile base, a workstation and a system control cabinet. For biopsy-stereotaxy, either the left or right arm is transferred to a stereotactic platform that attaches to the MR bore. The procedure is performed with image-guidance, as MR images are acquired in near real-time. The end-effectors interface with surgical tools which are based on standard neurosurgical instruments.
End-effectors are equipped with three-dimensional force-sensors, providing the sense of touch. The surgeon seated at the workstation controls the robot using force feedback hand controllers. The workstation recreates the sight and sensation of microsurgery by displaying the surgical site and 3D MRI displays, with superimposed tools. NeuroArm enables remote manipulation of the surgical tools from a control room adjacent to the surgical suite. It was designed to function within the environment of 1.5 and 3.0 tesla intraoperative MRI systems. As neuroArm is MR-compatible, stereotaxy can be performed inside the bore of the magnet with near real-time image guidance. NeuroArm possesses the dexterity to perform microsurgery, outside of the MRI system.
Telerobotic operations both inside and outside the magnet are performed using specialized tool sets based on standard neurosurgical instruments, adapted to the end effectors. Using these, NeuroArm is able to cut and manipulate soft tissue, dissect tissue planes, suture, biopsy, electrocauterize, aspirate and irrigate.


== History ==
The project began in 2002 when Daryl, B.J., and Don Seaman provided $2 million to fund the design efforts.  Dr. Sutherland and his group established a collaboration with the Canadian space engineering company  MacDonald Dettwiler and Associates (MDA).  Close collaboration between MDA's robotic engineers and University of Calgary physicians, nurses, and scientists contributed to the design and development of NeuroArm. Official launch of the project was on April 17, 2007.NeuroArm was designed to take full advantage of the imaging environment provided by intraoperative MRI. The ability to couple near real-time, high resolution images to robotic technologies provides the surgeon with image guidance, precision, accuracy, and dexterity. MDA's engineers were immersed in the operating room to study typical tool and surgeon motions in order to use biomimicry for effective design of the computer-assisted surgical device. The OR environment, personnel, surgical rhythm and instrumentation remain unchanged. The surgeon, sitting at the workstation, is provided a virtual environment that recreates the sight, sound, and touch of surgery. Functions like tremor filtering and motion scaling were applied to increase precision and accuracy while functions like no-go zones and linear lock were applied to enhance safety. Surgical tools near the patient's head are incapable of fully independent movement and are slaved to the surgeon’s movement at all times. Pre-planned automatic motions are used to move the robot arms away from the patient's head for manual tool exchange, and then return them to the original position and orientation.
On May 12, 2008, the first image-guided MR-compatible robotic neurosurgical procedure was performed at University of Calgary by Dr. Garnette Sutherland using the NeuroArm.


== References ==


== External links ==
Project neuroArm
Seaman Family MR Research Centre
SYMBIS Homepage on IMRIS Website


=== Videos ===
Video in press release for NeuroArm unveiling, University of Calgary, April 17, 2007


=== Related patents ===
Canadian Patent 2246369 Surgical procedure with magnetic resonance imaging
US Patent 5,735,278 (at USPTO) Surgical procedure with magnetic resonance imaging
US Patent 5,735,278 (at Google) Surgical procedure with magnetic resonance imaging","pandas(index=111, _1=111, text=""neuroarm is an engineering research surgical robot specifically designed for neurosurgery. it is the first image-guided, mr-compatible surgical robot that has the capability to perform both microsurgery and stereotaxy.imris, inc. acquired neuroarm assets in 2010, and the company is working to develop a next generation of the technology for worldwide commercialization.  it will be integrated with the visius(tm) surgical theatre under the name symbis(tm) surgical system.   == design == neuroarm was designed to be image-guided and can perform procedures inside an mri. neuroarm includes two remote detachable manipulators on a mobile base, a workstation and a system control cabinet. for biopsy-stereotaxy, either the left or right arm is transferred to a stereotactic platform that attaches to the mr bore. the procedure is performed with image-guidance, as mr images are acquired in near real-time. the end-effectors interface with surgical tools which are based on standard neurosurgical instruments. end-effectors are equipped with three-dimensional force-sensors, providing the sense of touch. the surgeon seated at the workstation controls the robot using force feedback hand controllers. the workstation recreates the sight and sensation of microsurgery by displaying the surgical site and 3d mri displays, with superimposed tools. neuroarm enables remote manipulation of the surgical tools from a control room adjacent to the surgical suite. it was designed to function within the environment of 1.5 and 3.0 tesla intraoperative mri systems. as neuroarm is mr-compatible, stereotaxy can be performed inside the bore of the magnet with near real-time image guidance. neuroarm possesses the dexterity to perform microsurgery, outside of the mri system. telerobotic operations both inside and outside the magnet are performed using specialized tool sets based on standard neurosurgical instruments, adapted to the end effectors. using these, neuroarm is able to cut and manipulate soft tissue, dissect tissue planes, suture, biopsy, electrocauterize, aspirate and irrigate.   == history == the project began in 2002 when daryl, b.j., and don seaman provided $2 million to fund the design efforts.  dr. sutherland and his group established a collaboration with the canadian space engineering company  macdonald dettwiler and associates (mda).  close collaboration between mda's robotic engineers and university of calgary physicians, nurses, and scientists contributed to the design and development of neuroarm. official launch of the project was on april 17, 2007.neuroarm was designed to take full advantage of the imaging environment provided by intraoperative mri. the ability to couple near real-time, high resolution images to robotic technologies provides the surgeon with image guidance, precision, accuracy, and dexterity. mda's engineers were immersed in the operating room to study typical tool and surgeon motions in order to use biomimicry for effective design of the computer-assisted surgical device. the or environment, personnel, surgical rhythm and instrumentation remain unchanged. the surgeon, sitting at the workstation, is provided a virtual environment that recreates the sight, sound, and touch of surgery. functions like tremor filtering and motion scaling were applied to increase precision and accuracy while functions like no-go zones and linear lock were applied to enhance safety. surgical tools near the patient's head are incapable of fully independent movement and are slaved to the surgeon’s movement at all times. pre-planned automatic motions are used to move the robot arms away from the patient's head for manual tool exchange, and then return them to the original position and orientation. on may 12, 2008, the first image-guided mr-compatible robotic neurosurgical procedure was performed at university of calgary by dr. garnette sutherland using the neuroarm.   == references ==   == external links == project neuroarm seaman family mr research centre symbis homepage on imris website canadian patent 2246369 surgical procedure with magnetic resonance imaging us patent 5,735,278 (at uspto) surgical procedure with magnetic resonance imaging us patent 5,735,278 (at google) surgical procedure with magnetic resonance imaging"")"
112,"Biomimetic materials are materials developed using inspiration from nature. This may be useful in the design of composite materials. Natural structures have inspired and innovated human creations. Notable examples of these natural structures include: honeycomb structure of the beehive, strength of spider silks, bird flight mechanics, and shark skin water repellency. The etymological roots of the neologism (new term) biomimetic derive from Greek, since bios means ""life"" and mimetikos means ""imitative"",


== Tissue Engineering ==
Biomimetic materials in tissue engineering are materials that have been designed such that they elicit specified cellular responses mediated by interactions with scaffold-tethered peptides from extracellular matrix (ECM) proteins; essentially, the incorporation of cell-binding peptides into biomaterials via chemical or physical modification. Amino acids located within the peptides are used as building blocks by other biological structures. These peptides are often referred to as ""self-assembling peptides"", since they can be modified to contain biologically active motifs. This allows them to replicate information derived from tissue and to reproduce the same information independently. Thus, these peptides act as building blocks capable of conducting multiple biochemical activities, including tissue engineering. Tissue engineering research currently being performed on both short chain and long chain peptides is still in early stages.
Such peptides include both native long chains of ECM proteins as well as short peptide sequences derived from intact ECM proteins. The idea is that the biomimetic material will mimic some of the roles that an ECM plays in neural tissue. In addition to promoting cellular growth and mobilization, the incorporated peptides could also mediate by specific protease enzymes or initiate cellular responses not present in a local native tissue.In the beginning, long chains of ECM proteins including fibronectin (FN), vitronectin (VN), and laminin (LN) were used, but more recently the advantages of using short peptides have been discovered. Short peptides are more advantageous because, unlike the long chains that fold randomly upon adsorption causing the active protein domains to be sterically unavailable, short peptides remain stable and do not hide the receptor binding domains when adsorbed. Another advantage to short peptides is that they can be replicated more economically due to the smaller size. A bi-functional cross-linker with a long spacer arm is used to tether peptides to the substrate surface. If a functional group is not available for attaching the cross-linker, photochemical immobilization may be used.In addition to modifying the surface, biomaterials can be modified in bulk, meaning that the cell signaling peptides and recognition sites are present not just on the surface but also throughout the bulk of the material. The strength of cell attachment, cell migration rate, and extent of cytoskeletal organization formation is determined by the receptor binding to the ligand bound to the material; thus, receptor-ligand affinity, the density of the ligand, and the spatial distribution of the ligand must be carefully considered when designing a biomimetic material.


== Biomimetic mineralization ==
Proteins of the developing enamel extracellular matrix (such as Amelogenin) control initial mineral deposition (nucleation) and subsequent crystal growth, ultimately determining the physico-mechanical properties of the mature mineralized tissue. Nucleators bring together mineral ions from the surrounding fluids (such as saliva) into the form of a crystal lattice structure, by stabilizing small nuclei to permit crystal growth, forming mineral tissue. Mutations in enamel ECM proteins result in enamel defects such as amelogenesis imperfecta. Type-I collagen is thought to have a similar role for the formation of dentin and bone.Dental enamel mineral (as well as dentin and bone) is made of hydroxylapatite with foreign ions incorporated in the structure. Carbonate, fluoride, and magnesium are the most common heteroionic substituents.In a biomimetic mineralization strategy based on normal enamel histogenesis, a three-dimensional scaffold is formed to attract and arrange calcium and/or phosphate ions to induce de novo precipitation of hydroxylapatite.Two general strategies have been applied. One is using fragments known to support natural mineralization proteins, such as Amelogenin, Collagen, or Dentin Phosphophoryn as the basis. Alternatively, de novo macromolecular structures have been designed to support mineralization, not based on natural molecules, but on rational design. One example is oligopeptide P11-4.In dental orthopedics and implants, a more traditional strategy to improve the density of the underlying jaw bone is via the in situ application of calcium phosphate materials. Commonly used materials include hydroxylapatite, tricalcium phosphate, and calcium phosphate cement. Newer bioactive glasses follow this line of strategy, where the added silicone provides an important bonus to the local absorption of calcium.


== Extracellular matrix proteins ==
Many studies utilize laminin-1 when designing a biomimetic material. Laminin is a component of the extracellular matrix that is able to promote neuron attachment and differentiation, in addition to axon growth guidance. Its primary functional site for bioactivity is its core protein domain isoleucine-lysine-valine-alanine-valine (IKVAV), which is located in the α-1 chain of laminin.A recent study by Wu, Zheng et al., synthesized a self-assembled IKVAV peptide nanofiber and tested its effect on the adhesion of neuron-like pc12 cells. Early cell adhesion is very important for preventing cell degeneration; the longer cells are suspended in culture, the more likely they are to degenerate. The purpose was to develop a biomaterial with good cell adherence and bioactivity with IKVAV, which is able to inhibit differentiation and adhesion of glial cells in addition to promoting neuronal cell adhesion and differentiation. The IKVAV peptide domain is on the surface of the nanofibers so that it is exposed and accessible for promoting cell contact interactions. The IKVAV nanofibers promoted stronger cell adherence than the electrostatic attraction induced by poly-L-lysine, and cell adherence increased with increasing density of IKVAV until the saturation point was reached. IKVAV does not exhibit time dependent effects because the adherence was shown to be the same at 1 hour and at 3 hours.Laminin is known to stimulate neurite outgrowth and it plays a role in the developing nervous system. It is known that gradients are critical for the guidance of growth cones to their target tissues in the developing nervous system. There has been much research done on soluble gradients; however, little emphasis has been placed on gradients of substratum bound substances of the extracellular matrix such as laminin. Dodla and Bellamkonda, fabricated an anisotropic 3D agarose gel with gradients of coupled laminin-1 (LN-1). Concentration gradients of LN-1 were shown to promote faster neurite extension than the highest neurite growth rate observed with isotropic LN-1 concentrations. Neurites grew both up and down the gradients, but growth was faster at less steep gradients and was faster up the gradients than down the gradients.


== Biomimetic artificial muscles ==
Electroactive polymers (EAPs) are also known as artificial muscles. EAPs are polymeric materials and they are able to produce large deformation when applied in an electric field. This provides large potential in applications in biotechnology and robotics, sensors, and actuators.


== Biomimetic photonic structures ==

The production of structural colours concerns a large array of organisms. From bacteria (Flavobacterium strain IR1) to multicellular organisms, (Hibiscus trionum, Doryteuthis pealeii (squid), or Chrysochroa fulgidissima (beetle)), manipulation of light is not limited to rare and exotic life forms. Different organisms evolved different mechanisms to produce structural colours: multilayered cuticle in some insects and plants, grating like surface in plants, geometrically organised cells in bacteria... all of theme stand for a source of inspiration towards the development of structurally coloured materials.
Study of the firefly abdomen revealed the presence of a 3-layer system comprising the cuticle, the Photogenic layer and then a reflector layer. Microscopy of the reflector layer revealed a granulate structure. Directly inspired from the fire fly Reflector layer, an artificial granulate film composed of hollow silica beads of about 1.05 μm was correlated with a high reflection index and could be used to improve light emission in chemiluminescent systems.


== Artificial enzyme ==
Artificial enzymes are synthetic materials that can mimic (partial) function of a natural enzyme without necessarily being a protein. Among them, some nanomaterials have been used to mimic natural enzymes. These nanomaterials are termed nanozymes. Nanozymes as well as other artificial enzymes have found wide applications, from biosensing and immunoassays, to stem cell growth and pollutant removal.


== References ==","pandas(index=112, _1=112, text='biomimetic materials are materials developed using inspiration from nature. this may be useful in the design of composite materials. natural structures have inspired and innovated human creations. notable examples of these natural structures include: honeycomb structure of the beehive, strength of spider silks, bird flight mechanics, and shark skin water repellency. the etymological roots of the neologism (new term) biomimetic derive from greek, since bios means ""life"" and mimetikos means ""imitative"",   == tissue engineering == biomimetic materials in tissue engineering are materials that have been designed such that they elicit specified cellular responses mediated by interactions with scaffold-tethered peptides from extracellular matrix (ecm) proteins; essentially, the incorporation of cell-binding peptides into biomaterials via chemical or physical modification. amino acids located within the peptides are used as building blocks by other biological structures. these peptides are often referred to as ""self-assembling peptides"", since they can be modified to contain biologically active motifs. this allows them to replicate information derived from tissue and to reproduce the same information independently. thus, these peptides act as building blocks capable of conducting multiple biochemical activities, including tissue engineering. tissue engineering research currently being performed on both short chain and long chain peptides is still in early stages. such peptides include both native long chains of ecm proteins as well as short peptide sequences derived from intact ecm proteins. the idea is that the biomimetic material will mimic some of the roles that an ecm plays in neural tissue. in addition to promoting cellular growth and mobilization, the incorporated peptides could also mediate by specific protease enzymes or initiate cellular responses not present in a local native tissue.in the beginning, long chains of ecm proteins including fibronectin (fn), vitronectin (vn), and laminin (ln) were used, but more recently the advantages of using short peptides have been discovered. short peptides are more advantageous because, unlike the long chains that fold randomly upon adsorption causing the active protein domains to be sterically unavailable, short peptides remain stable and do not hide the receptor binding domains when adsorbed. another advantage to short peptides is that they can be replicated more economically due to the smaller size. a bi-functional cross-linker with a long spacer arm is used to tether peptides to the substrate surface. if a functional group is not available for attaching the cross-linker, photochemical immobilization may be used.in addition to modifying the surface, biomaterials can be modified in bulk, meaning that the cell signaling peptides and recognition sites are present not just on the surface but also throughout the bulk of the material. the strength of cell attachment, cell migration rate, and extent of cytoskeletal organization formation is determined by the receptor binding to the ligand bound to the material; thus, receptor-ligand affinity, the density of the ligand, and the spatial distribution of the ligand must be carefully considered when designing a biomimetic material.   == biomimetic mineralization == proteins of the developing enamel extracellular matrix (such as amelogenin) control initial mineral deposition (nucleation) and subsequent crystal growth, ultimately determining the physico-mechanical properties of the mature mineralized tissue. nucleators bring together mineral ions from the surrounding fluids (such as saliva) into the form of a crystal lattice structure, by stabilizing small nuclei to permit crystal growth, forming mineral tissue. mutations in enamel ecm proteins result in enamel defects such as amelogenesis imperfecta. type-i collagen is thought to have a similar role for the formation of dentin and bone.dental enamel mineral (as well as dentin and bone) is made of hydroxylapatite with foreign ions incorporated in the structure. carbonate, fluoride, and magnesium are the most common heteroionic substituents.in a biomimetic mineralization strategy based on normal enamel histogenesis, a three-dimensional scaffold is formed to attract and arrange calcium and/or phosphate ions to induce de novo precipitation of hydroxylapatite.two general strategies have been applied. one is using fragments known to support natural mineralization proteins, such as amelogenin, collagen, or dentin phosphophoryn as the basis. alternatively, de novo macromolecular structures have been designed to support mineralization, not based on natural molecules, but on rational design. one example is oligopeptide p11-4.in dental orthopedics and implants, a more traditional strategy to improve the density of the underlying jaw bone is via the in situ application of calcium phosphate materials. commonly used materials include hydroxylapatite, tricalcium phosphate, and calcium phosphate cement. newer bioactive glasses follow this line of strategy, where the added silicone provides an important bonus to the local absorption of calcium.   == extracellular matrix proteins == many studies utilize laminin-1 when designing a biomimetic material. laminin is a component of the extracellular matrix that is able to promote neuron attachment and differentiation, in addition to axon growth guidance. its primary functional site for bioactivity is its core protein domain isoleucine-lysine-valine-alanine-valine (ikvav), which is located in the α-1 chain of laminin.a recent study by wu, zheng et al., synthesized a self-assembled ikvav peptide nanofiber and tested its effect on the adhesion of neuron-like pc12 cells. early cell adhesion is very important for preventing cell degeneration; the longer cells are suspended in culture, the more likely they are to degenerate. the purpose was to develop a biomaterial with good cell adherence and bioactivity with ikvav, which is able to inhibit differentiation and adhesion of glial cells in addition to promoting neuronal cell adhesion and differentiation. the ikvav peptide domain is on the surface of the nanofibers so that it is exposed and accessible for promoting cell contact interactions. the ikvav nanofibers promoted stronger cell adherence than the electrostatic attraction induced by poly-l-lysine, and cell adherence increased with increasing density of ikvav until the saturation point was reached. ikvav does not exhibit time dependent effects because the adherence was shown to be the same at 1 hour and at 3 hours.laminin is known to stimulate neurite outgrowth and it plays a role in the developing nervous system. it is known that gradients are critical for the guidance of growth cones to their target tissues in the developing nervous system. there has been much research done on soluble gradients; however, little emphasis has been placed on gradients of substratum bound substances of the extracellular matrix such as laminin. dodla and bellamkonda, fabricated an anisotropic 3d agarose gel with gradients of coupled laminin-1 (ln-1). concentration gradients of ln-1 were shown to promote faster neurite extension than the highest neurite growth rate observed with isotropic ln-1 concentrations. neurites grew both up and down the gradients, but growth was faster at less steep gradients and was faster up the gradients than down the gradients.   == biomimetic artificial muscles == electroactive polymers (eaps) are also known as artificial muscles. eaps are polymeric materials and they are able to produce large deformation when applied in an electric field. this provides large potential in applications in biotechnology and robotics, sensors, and actuators.   == biomimetic photonic structures ==  the production of structural colours concerns a large array of organisms. from bacteria (flavobacterium strain ir1) to multicellular organisms, (hibiscus trionum, doryteuthis pealeii (squid), or chrysochroa fulgidissima (beetle)), manipulation of light is not limited to rare and exotic life forms. different organisms evolved different mechanisms to produce structural colours: multilayered cuticle in some insects and plants, grating like surface in plants, geometrically organised cells in bacteria... all of theme stand for a source of inspiration towards the development of structurally coloured materials. study of the firefly abdomen revealed the presence of a 3-layer system comprising the cuticle, the photogenic layer and then a reflector layer. microscopy of the reflector layer revealed a granulate structure. directly inspired from the fire fly reflector layer, an artificial granulate film composed of hollow silica beads of about 1.05 μm was correlated with a high reflection index and could be used to improve light emission in chemiluminescent systems.   == artificial enzyme == artificial enzymes are synthetic materials that can mimic (partial) function of a natural enzyme without necessarily being a protein. among them, some nanomaterials have been used to mimic natural enzymes. these nanomaterials are termed nanozymes. nanozymes as well as other artificial enzymes have found wide applications, from biosensing and immunoassays, to stem cell growth and pollutant removal.   == references ==')"
113,"The Luer taper is a standardized system of small-scale fluid fittings used for making leak-free connections between a male-taper fitting and its mating female part on medical and laboratory instruments, including hypodermic syringe tips and needles or stopcocks and needles. Currently ISO 80369 governs the Luer standards and testing methods.Invented by Karl Schneider and named after the 19th-century German medical instrument maker Hermann Wülfing Lüer, it originated as a 6% taper fitting for glass bottle stoppers (so one side is at 1.72 degrees to the centerline). Key features of Luer taper connectors are defined in the ISO 594 standards. It is also defined in the DIN and EN standard 1707:1996 and 20594-1:1993.


== Variants ==
There are two varieties of Luer taper connections: locking and slipping. Their trade names are confusingly similar to the nonproprietary names. ""Luer-Lock"" and ""Luer-slip"" are registered trademarks of Becton Dickinson. ""Luer-Lock"" style connectors are often generically referred to as ""Luer lock"", and ""Luer-slip"" style connectors may be generically referred to as ""slip tip"". Luer lock fittings are securely joined by means of a tabbed hub on the female fitting which screws into threads in a sleeve on the male fitting.  The Luer lock fitting was developed in the United States by Fairleigh S. Dickinson. 'Luer lock' style connectors are divided into two types ""one piece luer lock"" and ""two piece luer lock"" or ""rotating collar luer lock"". One piece Luer lock comes as a single mold, and locking is achieved by rotating the entire luer connector or system. In two piece luer lock, a free rotating collar with threads is assembled to the luer and the locking is achieved by rotating the collar.
Slip tip (Luer-slip) fittings simply conform to Luer taper dimensions and are pressed together and held by friction (they have no threads). Luer components are manufactured from either metal or plastic and are available from many companies worldwide.


== References ==","pandas(index=113, _1=113, text='the luer taper is a standardized system of small-scale fluid fittings used for making leak-free connections between a male-taper fitting and its mating female part on medical and laboratory instruments, including hypodermic syringe tips and needles or stopcocks and needles. currently iso 80369 governs the luer standards and testing methods.invented by karl schneider and named after the 19th-century german medical instrument maker hermann wülfing lüer, it originated as a 6% taper fitting for glass bottle stoppers (so one side is at 1.72 degrees to the centerline). key features of luer taper connectors are defined in the iso 594 standards. it is also defined in the din and en standard 1707:1996 and 20594-1:1993.   == variants == there are two varieties of luer taper connections: locking and slipping. their trade names are confusingly similar to the nonproprietary names. ""luer-lock"" and ""luer-slip"" are registered trademarks of becton dickinson. ""luer-lock"" style connectors are often generically referred to as ""luer lock"", and ""luer-slip"" style connectors may be generically referred to as ""slip tip"". luer lock fittings are securely joined by means of a tabbed hub on the female fitting which screws into threads in a sleeve on the male fitting.  the luer lock fitting was developed in the united states by fairleigh s. dickinson. \'luer lock\' style connectors are divided into two types ""one piece luer lock"" and ""two piece luer lock"" or ""rotating collar luer lock"". one piece luer lock comes as a single mold, and locking is achieved by rotating the entire luer connector or system. in two piece luer lock, a free rotating collar with threads is assembled to the luer and the locking is achieved by rotating the collar. slip tip (luer-slip) fittings simply conform to luer taper dimensions and are pressed together and held by friction (they have no threads). luer components are manufactured from either metal or plastic and are available from many companies worldwide.   == references ==')"
114,"Critical Reviews in Biomedical Engineering is a bimonthly peer-reviewed scientific journal published by Begell House covering biomedical engineering, bioengineering, clinical engineering, and related subjects. The editor-in-chief is Chenzhong Li.


== External links ==
Official website","pandas(index=114, _1=114, text='critical reviews in biomedical engineering is a bimonthly peer-reviewed scientific journal published by begell house covering biomedical engineering, bioengineering, clinical engineering, and related subjects. the editor-in-chief is chenzhong li.   == external links == official website')"
115,"The IEEE Engineering in Medicine and Biology Society (EMBS) is an IEEE group dedicated to the study of Biomedical Engineering.


== History ==
The IRE Professional Group on Medical Electronics was formed in 1952 to consider ""problems in biology and medicine which might be aided in solution by use of electronic engineering principles and devices."" After IRE's merge with AIEE in 1963, the Professional Group on Medical Electronics merged with AIEE's Committee on Electrical Techniques in Medicine and Biology to form the IEEE Engineering in Medicine and Biology Society.The group also has student chapters.


== Publications ==


=== Magazines ===
IEEE Pulse
IEEE Engineering in Medicine and Biology Magazine (ceased publication in 2010)


=== Journals ===
IEEE Transactions on Biomedical Engineering
IEEE Transactions on Neural Systems and Rehabilitation Engineering
IEEE Journal of Biomedical and Health Informatics
IEEE Journal of Translational Engineering in Health and Medicine
IEEE Open Journal of Engineering in Medicine and Biology
IEEE Reviews in Biomedical Engineering
IEEE Transactions on Medical Imaging
IEEE Transactions on NanoBioscience
IEEE/ACM Transactions on Computational Biology and Bioinformatics
IEEE Transactions on Biomedical Circuits and Systems


== Conferences ==
The Annual International Conference of the IEEE Engineering in Medicine and Biology Society is held in various locations in the United States and around the world. 
The 42th Annual International Conference was to be held in Montreal, Canada on July 20–24, 2020, but due to the global pandemic was held virtually.


== References ==


== External links ==
EMBS Official Website
EMBC 2020","pandas(index=115, _1=115, text='the ieee engineering in medicine and biology society (embs) is an ieee group dedicated to the study of biomedical engineering.   == history == the ire professional group on medical electronics was formed in 1952 to consider ""problems in biology and medicine which might be aided in solution by use of electronic engineering principles and devices."" after ire\'s merge with aiee in 1963, the professional group on medical electronics merged with aiee\'s committee on electrical techniques in medicine and biology to form the ieee engineering in medicine and biology society.the group also has student chapters.   == publications == ieee transactions on biomedical engineering ieee transactions on neural systems and rehabilitation engineering ieee journal of biomedical and health informatics ieee journal of translational engineering in health and medicine ieee open journal of engineering in medicine and biology ieee reviews in biomedical engineering ieee transactions on medical imaging ieee transactions on nanobioscience ieee/acm transactions on computational biology and bioinformatics ieee transactions on biomedical circuits and systems   == conferences == the annual international conference of the ieee engineering in medicine and biology society is held in various locations in the united states and around the world. the 42th annual international conference was to be held in montreal, canada on july 20–24, 2020, but due to the global pandemic was held virtually.   == references ==   == external links == embs official website embc 2020')"
116,"An autonomous building is a building designed to be operated independently from infrastructural support services such as the electric power grid, gas grid, municipal water systems, sewage treatment systems, storm drains, communication services, and in some cases, public roads.
Advocates of autonomous building describe advantages that include reduced environmental impacts, increased security, and lower costs of ownership. Some cited advantages satisfy tenets of green building, not independence per se (see below). Off-grid buildings often rely very little on civil services and are therefore safer and more comfortable during civil disaster or military attacks. For example, Off-grid buildings would not lose power or water if public supplies were compromised.
As of 2018, most research and published articles concerning autonomous building focus on residential homes.
In 2002, British architects Brenda and Robert Vale said that

It is quite possible in all parts of Australia to construct a 'house with no bills', which would be comfortable without heating and cooling, which would make its own electricity, collect its own water and deal with its own waste...These houses can be built now, using off-the-shelf techniques. It is possible to build a ""house with no bills"" for the same price as a conventional house, but it would be (25%) smaller.


== History ==
In the 1970s, groups of activists and engineers were inspired by the warnings of imminent resource depletion and starvation. In the US a group calling themselves the New Alchemists were famous for the depth of research effort placed in their projects. Using conventional construction techniques, they designed a series of ""bioshelter"" projects, the most famous of which was the Ark Bioshelter community for Prince Edward Island. They published the plans for all of these, with detailed design calculations and blueprints. The Ark used wind based water pumping and electricity, and was self-contained in food production. It had living quarters for people, fish tanks raising tilapia for protein, a greenhouse watered with fish water and a closed loop sewage reclamation system that recycled human waste into sanitized fertilizer for the fish tanks. As of January 2010, the successor organization to the New Alchemists has a web page up as the ""New Alchemy Institute"". The PEI Ark has been abandoned and partially renovated several times.

The 1990s saw the development of Earthships, similar in intent to the Ark project, but organized as a for-profit venture, with construction details published in a series of 3 books by Mike Reynolds. The building material is tires filled with earth. This makes a wall that has large amounts of thermal mass (see earth sheltering). Berms are placed on exposed surfaces to further increase the house's temperature stability. The water system starts with rain water, processed for drinking, then washing, then plant watering, then toilet flushing, and finally black water is recycled again for more plant watering. The cisterns are placed and used as thermal masses. Power, including electricity, heat and water heating, is from solar power.
1990s architects such as William McDonough and Ken Yeang applied environmentally responsible building design to large commercial buildings, such as office buildings, making them largely self-sufficient in energy production. One major bank building (ING's Amsterdam headquarters) in the Netherlands was constructed to be autonomous and artistic as well.


== Advantages ==
As an architect or engineer becomes more concerned with the disadvantages of transportation networks, and dependence on distant resources, their designs tend to include more autonomous elements. The historic path to autonomy was a concern for secure sources of heat, power, water and food. A nearly parallel path toward autonomy has been to start with a concern for environmental impacts, which cause disadvantages.
Autonomous buildings can increase security and reduce environmental impacts by using on-site resources (such as sunlight and rain) that would otherwise be wasted. Autonomy often dramatically reduces the costs and impacts of networks that serve the building, because autonomy short-circuits the multiplying inefficiencies of collecting and transporting resources. Other impacted resources, such as oil reserves and the retention of the local watershed, can often be cheaply conserved by thoughtful designs.
Autonomous buildings are usually energy-efficient in operation, and therefore cost-efficient, for the obvious reason that smaller energy needs are easier to satisfy off-grid. But they may substitute energy production or other techniques to avoid diminishing returns in extreme conservation.
An autonomous structure is not always environmentally friendly. The goal of independence from support systems is associated with, but not identical to, other goals of environmentally responsible green building. However, autonomous buildings also usually include some degree of sustainability through the use of renewable energy and other renewable resources, producing no more greenhouse gases than they consume, and other measures.


== Disadvantages ==
First and fundamentally, independence is a matter of degree, with many choices. For example, eliminating dependence on the electrical grid is relatively easy. In contrast, running an efficient, reliable food source can be a chore.
Living within an autonomous shelter may also require sacrifices in lifestyle or social opportunities. Even the most comfortable and technologically advanced autonomous homes could require alterations of residents' behavior. Some may not welcome the extra chores. The Vails described some clients' experiences as inconvenient, irritating, isolating, or even as an unwanted full-time job. A well-designed building can reduce this issue, but usually at the expense of reduced autonomy.
An autonomous house must be custom-built (or extensively retrofitted) to suit the climate and location. Passive solar techniques, alternative toilet and sewage systems, thermal massing designs, basement battery systems, efficient windowing, and the array of other design tactics require some degree of non-standard construction, added expense, ongoing experimentation and maintenance, and also have an effect on the psychology of the space.


== Systems ==
This section includes some minimal descriptions of methods, to give some feel for such a building's practicality, provide indexes to further information, and give a sense of modern trends.


=== Water ===

There are many methods of collecting and conserving water. Use reduction is cost-effective.
Greywater systems reuse drained wash water to flush toilets or to water lawns and gardens. Greywater systems can halve the water use of most residential buildings; however, they require the purchase of a sump, greywater pressurization pump, and secondary plumbing. Some builders are installing waterless urinals and even composting toilets that completely eliminate water usage in sewage disposal.
The classic solution with minimal life-style changes is using a well. Once drilled, a well-foot requires substantial power. However, advanced well-foots can reduce power usage by twofold or more from older models. Well water can be contaminated in some areas. The Sono arsenic filter eliminates unhealthy arsenic in well water.
However drilling a well is an uncertain activity, with aquifers depleted in some areas.  It can also be expensive.
In regions with sufficient rainfall, it is often more economical to design a building to use rainwater harvesting, with supplementary water deliveries in a drought. Rain water makes excellent soft washwater, but needs antibacterial treatment.  If used for drinking, mineral supplements or mineralization is necessary.Most desert and temperate climates get at least 250 millimetres (9.8 in) of rain per year. This means that a typical one-story house with a greywater system can supply its year-round water needs from its roof alone. In the driest areas, it might require a cistern of 30 cubic metres (7,900 US gal). Many areas average 13 millimetres (0.51 in) of rain per week, and these can use a cistern as small as 10 cubic metres (2,600 US gal).
In many areas, it is difficult to keep a roof clean enough for drinking. To reduce dirt and bad tastes, systems use a metal collecting-roof and a ""roof cleaner"" tank that diverts the first 40 liters.  Cistern water is usually chlorinated, though reverse osmosis systems provide even better quality drinking water.
In the classic Roman house (""Domus""), household water was provided from a cistern (the ""impluvium""), which was a decorative feature of the atrium, the house's main public space. It was fed by downspout tiles from the inward-facing roof-opening (the ""compluvium""). Often water lilies were grown in it to purify the water. Wealthy households often supplemented the rain with a small fountain fed from a city's cistern. The impluvium always had an overflow drain so it could not flood the house.Modern cisterns are usually large plastic tanks.  Gravity tanks on short towers are reliable, so pump repairs are less urgent.  The least expensive bulk cistern is a fenced pond or pool at ground level.
Reducing autonomy reduces the size and expense of cisterns. Many autonomous homes can reduce water use below 10 US gallons (38 L) per person per day, so that in a drought a month of water can be delivered inexpensively via truck. Self-delivery is often possible by installing fabric water tanks that fit the bed of a pick-up truck.
It can be convenient to use the cistern as a heat sink or trap for a heat pump or air conditioning system; however this can make cold drinking water warm, and in drier years may decrease the efficiency of the HVAC system.
Solar stills can efficiently produce drinking water from ditch water or cistern water, especially high-efficiency multiple effect humidification designs, which separate the evaporator(s) and condenser(s).
New technologies, like reverse osmosis can create unlimited amounts of pure water from polluted water, ocean water, and even from humid air. Watermakers are available for yachts that convert seawater and electricity into potable water and brine. Atmospheric water generators extract moisture from dry desert air and filter it to pure water.


=== Sewage ===


==== Resource ====

Composting toilets use bacteria to decompose human feces into useful, odourless, sanitary compost. The process is sanitary because soil bacteria eat the human pathogens as well as most of the mass of the waste.  Nevertheless, most health authorities forbid direct use of ""humanure"" for growing food. The risk is microbial and viral contamination.  In a dry composting toilet, the waste is evaporated or digested to gas (mostly carbon dioxide) and vented, so a toilet produces only a few pounds of compost every six months.  To control the odor, modern toilets use a small fan to keep the toilet under negative pressure, and exhaust the gasses to a vent pipe.Some home sewage treatment systems use biological treatment, usually beds of plants and aquaria, that absorb nutrients and bacteria and convert greywater and sewage to clear water. This odor- and color-free reclaimed water can be used to flush toilets and water outside plants. When tested, it approaches standards for potable water. In climates that freeze, the plants and aquaria need to be kept in a small greenhouse space. Good systems need about as much care as a large aquarium.
Electric incinerating toilets turn excrement into a small amount of ash. They are cool to the touch, have no water and no pipes, and require an air vent in a wall. They are used in remote areas where use of septic tanks is limited, usually to reduce nutrient loads in lakes.
NASA's bioreactor is an extremely advanced biological sewage system. It can turn sewage into air and water through microbial action. NASA plans to use it in the manned Mars mission. Another method is NASA's urine-to-water distillation system.
A big disadvantage of complex biological sewage treatment systems is that if the house is empty, the sewage system biota may starve to death.


==== Waste ====
Sewage handling is essential for public health. Many diseases are transmitted by poorly functioning sewage systems.
The standard system is a tiled leach field combined with a septic tank. The basic idea is to provide a small system with primary sewage treatment. Sludge settles to the bottom of the septic tank, is partially reduced by anaerobic digestion, and fluid is dispersed in the leach field. The leach field is usually under a yard growing grass. Septic tanks can operate entirely by gravity, and if well managed, are reasonably safe.
Septic tanks have to be pumped periodically by a vacuum truck to eliminate non reducing solids. Failure to pump a septic tank can cause overflow that damages the leach field, and contaminates ground water. Septic tanks may also require some lifestyle changes, such as not using garbage disposals, minimizing fluids flushed into the tank, and minimizing nondigestible solids flushed into the tank. For example, septic safe toilet paper is recommended.
However, septic tanks remain popular because they permit standard plumbing fixtures, and require few or no lifestyle sacrifices.
Composting or packaging toilets make it economical and sanitary to throw away sewage as part of the normal garbage collection service. They also reduce water use by half, and eliminate the difficulty and expense of septic tanks. However, they require the local landfill to use sanitary practices.
Incinerator systems are quite practical. The ashes are biologically safe, and less than 1/10 the volume of the original waste, but like all incinerator waste, are usually classified as hazardous waste.
Traditional methods of sewage handling include pit toilets, latrines, and outhouses. These can be safe, inexpensive and practical. They are still used in many regions.


=== Storm drains ===
Drainage systems are a crucial compromise between human habitability and a secure, sustainable watershed. Paved areas and lawns or turf do not allow much precipitation to filter through the ground to recharge aquifers. They can cause flooding and damage in neighbourhoods, as the water flows over the surface towards a low point.
Typically, elaborate, capital-intensive storm sewer networks are engineered to deal with stormwater. In some cities, such as the Victorian era London sewers or much of the old City of Toronto, the storm water system is combined with the sanitary sewer system. In the event of heavy precipitation, the load on the sewage treatment plant at the end of the pipe becomes too great to handle and raw sewage is dumped into holding tanks, and sometimes into surface water.
Autonomous buildings can address precipitation in a number of ways:
If a water-absorbing swale for each yard is combined with permeable concrete streets, storm drains can be omitted from the neighbourhood. This can save more than $800 per house (1970s) by eliminating storm drains. One way to use the savings is to purchase larger lots, which permits more amenities at the same cost. Permeable concrete is an established product in warm climates, and in development for freezing climates. In freezing climates, the elimination of storm drains can often still pay for enough land to construct swales (shallow water collecting ditches) or water impeding berms instead. This plan provides more land for homeowners and can offer more interesting topography for landscaping.
A green roof captures precipitation and uses the water to grow plants. It can be built into a new building or used to replace an existing roof.


=== Electricity ===

Since electricity is an expensive utility, the first step towards autonomy is to design a house and lifestyle to reduce demand. LED lights, laptop computers and gas-powered refrigerators save electricity, although gas-powered refrigerators are not very efficient. There are also superefficient electric refrigerators, such as those produced by the Sun Frost company, some of which use only about half as much electricity as a mass-market energy star-rated refrigerator.
Using a solar roof, solar cells can provide electric power. Solar roofs can be more cost-effective than retrofitted solar power, because buildings need roofs anyway. Modern solar cells last about 40 years, which makes them a reasonable investment in some areas. At a sufficient angle, solar cells are cleaned by run-off rain water and therefore have almost no life-style impact.
However, many areas have long winter nights or dark cloudy days. In these climates, a solar installation might not pay for itself or large battery storage systems are necessary to achieve electric self-sufficiency. In stormy or windy climates, wind generators can replace or significantly supplement solar power. The average autonomous house needs only one small wind turbine, 5 metres or less in diameter. On a 30-metre (100-foot) tower, this turbine can provide enough power to supplement solar power on cloudy days. Commercially available wind turbines use sealed, one-moving-part AC generators and passive, self-feathering blades for years of operation without service.
The main advantage of wind power is that larger wind turbines have a lower per-watt cost than solar cells, provided there is wind. However, location is critical. Just as some locations lack sun for solar cells, many areas lack enough wind to make a turbine pay for itself. In the Great Plains of the United States, a 10-metre (33-foot) turbine can supply enough energy to heat and cool a well-built all-electric house. Economic use in other areas requires research, and possibly a site-survey.Some sites have access to a stream with a change in elevation. These sites can use small hydropower systems to generate electricity. If the difference in elevation is above 30 metres (100 feet), and the stream runs in all seasons, this can provide continuous power with a small, inexpensive installation. Lower changes of elevation require larger installations or dams, and can be less efficient. Clogging at the turbine intake can be a practical problem. The usual solution is a small pool and waterfall (a penstock) to carry away floating debris. Another solution is to utilize a turbine that resists debris, such as a Gorlov helical turbine or Ossberger turbine.
During times of low demand, excess power can be stored in batteries for future use. However, batteries need to be replaced every few years. In many areas, battery expenses can be eliminated by attaching the building to the electric power grid and operating the power system with net metering. Utility permission is required, but such cooperative generation is legally mandated in some areas (for example, California).A grid-based building is less autonomous, but more economical and sustainable with fewer lifestyle sacrifices. In rural areas the grid's cost and impacts can be reduced by using single-wire earth return systems (for example, the MALT-system).
In areas that lack access to the grid, battery size can be reduced with a generator to recharge the batteries during energy droughts such as extended fogs. Auxiliary generators are usually run from propane, natural gas, or sometimes diesel. An hour of charging usually provides a day of operation. Modern residential chargers permit the user to set the charging times, so the generator is quiet at night. Some generators automatically test themselves once per week.Recent advances in passively stable magnetic bearings may someday permit inexpensive storage of power in a flywheel in a vacuum. Research groups like Canada's Ballard Power Systems are also working to develop a ""regenerative fuel cell"", a device that can generate hydrogen and oxygen when power is available, and combine these efficiently when power is needed.
Earth batteries tap electric currents in the earth called telluric current. They can be installed anywhere in the ground. They provide only low voltages and current. They were used to power telegraphs in the 19th century. As appliance efficiencies increase, they may become practical.
Microbial fuel cells and thermoelectric generators allow electricity to be generated from biomass. The plant can be dried, chopped and converted or burned as a whole, or it can be left alive so that waste saps from the plant can be converted by bacteria.


=== Heating ===

Most autonomous buildings are designed to use insulation, thermal mass and passive solar heating and cooling. Examples of these are trombe walls and other technologies as skylights.
Passive solar heating can heat most buildings in even the mild and chilly climates. In colder climates, extra construction costs can be as little as 15% more than new, conventional buildings. In warm climates, those having less than two weeks of frosty nights per year, there is no cost impact.
The basic requirement for passive solar heating is that the solar collectors must face the prevailing sunlight (south in the Northern Hemisphere, north in the Southern Hemisphere), and the building must incorporate thermal mass to keep it warm in the night.
A recent, somewhat experimental solar heating system ""Annualized geo solar heating"" is practical even in regions that get little or no sunlight in winter. It uses the ground beneath a building for thermal mass. Precipitation can carry away the heat, so the ground is shielded with 6 m skirts of plastic insulation. The thermal mass of this system is sufficiently inexpensive and large that it can store enough summer heat to warm a building for the whole winter, and enough winter cold to cool the building in summer.
In annualized geo solar systems, the solar collector is often separate from (and hotter or colder than) the living space. The building may actually be constructed from insulation, for example, straw-bale construction. Some buildings have been aerodynamically designed so that convection via ducts and interior spaces eliminates any need for electric fans.
A more modest ""daily solar"" design is very practical.  For example, for about a 15% premium in building costs, the Passivhaus building codes in Europe use high performance insulating windows, R-30 insulation, HRV ventilation, and a small thermal mass. With modest changes in the building's position, modern krypton- or argon-insulated windows permit normal-looking windows to provide passive solar heat without compromising insulation or structural strength. If a small heater is available for the coldest nights, a slab or basement cistern can inexpensively provide the required thermal mass. Passivhaus building codes, in particular, bring unusually good interior air quality, because the buildings change the air several times per hour, passing it through a heat exchanger to keep heat inside.
In all systems, a small supplementary heater increases personal security and reduces lifestyle impacts for a small reduction of autonomy. The two most popular heaters for ultra-high-efficiency houses are a small heat pump, which also provides air conditioning, or a central hydronic (radiator) air heater with water recirculating from the water heater.  Passivhaus designs usually integrate the heater with the ventilation system.
Earth sheltering and windbreaks can also reduce the absolute amount of heat needed by a building. Several feet below the earth, temperature ranges from 4 °C (39 °F) in North Dakota to 26 °C (79 °F), in Southern Florida. Wind breaks reduce the amount of heat carried away from a building.
Rounded, aerodynamic buildings also lose less heat.
An increasing number of commercial buildings use a combined cycle with cogeneration to provide heating, often water heating, from the output of a natural gas reciprocating engine, gas turbine or stirling electric generator.Houses designed to cope with interruptions in civil services generally incorporate a wood stove, or heat and power from diesel fuel or bottled gas, regardless of their other heating mechanisms.
Electric heaters and electric stoves may provide pollution-free heat (depending on the power source), but use large amounts of electricity. If enough electricity is provided by solar panels, wind turbines, or other means, then electric heaters and stoves become a practical autonomous design.


=== Water heating ===

Hot water heat recycling units recover heat from water drain lines. They increase a building's autonomy by decreasing the heat or fuel used to heat water.  They are attractive because they have no lifestyle changes.
Current practical, comfortable domestic water-heating systems combine a solar preheating system with a thermostatic gas-powered flow-through heater, so that the temperature of the water is consistent, and the amount is unlimited. This reduces life-style impacts at some cost in autonomy.
Solar water heaters can save large amounts of fuel. Also, small changes in lifestyle, such as doing laundry, dishes and bathing on sunny days, can greatly increase their efficiency. Pure solar heaters are especially useful for laundries, swimming pools and external baths, because these can be scheduled for use on sunny days.
The basic trick in a solar water heating system is to use a well-insulated holding tank. Some systems are vacuum- insulated, acting something like large thermos bottles. The tank is filled with hot water on sunny days, and made available at all times. Unlike a conventional tank water heater, the tank is filled only when there is sunlight. Good storage makes a smaller, higher-technology collector feasible. Such collectors can use relatively exotic technologies, such as vacuum insulation, and reflective concentration of sunlight.
Cogeneration systems produce hot water from waste heat.  They usually get the heat from the exhaust of a generator or fuel cell.
Heat recycling, cogeneration and solar pre-heating can save 50–75% of the gas otherwise used.  Also, some combinations provide redundant reliability by having several sources of heat.
Some authorities advocate replacing bottled gas or natural gas with biogas. However, this is usually impractical unless live-stock are on-site.  The wastes of a single family are usually insufficient to produce enough methane for anything more than small amounts of cooking.


=== Cooling ===
Annualized geo solar buildings often have buried, sloped water-tight skirts of insulation that extend 6 metres (20 ft) from the foundations, to prevent heat leakage between the earth used as thermal mass, and the surface.
Less dramatic improvements are possible. Windows can be shaded in summer. Eaves can be overhung to provide the necessary shade. These also shade the walls of the house, reducing cooling costs.
Another trick is to cool the building's thermal mass at night, perhaps with a whole-house fan and then cool the building from the thermal mass during the day. It helps to be able to route cold air from a sky-facing radiator (perhaps an air heating solar collector with an alternate purpose) or evaporative cooler directly through the thermal mass. On clear nights, even in tropical areas, sky-facing radiators can cool below freezing.
If a circular building is aerodynamically smooth, and cooler than the ground, it can be passively cooled by the ""dome effect.""  Many installations have reported that a reflective or light-colored dome induces a local vertical heat-driven vortex that sucks cooler overhead air downward into a dome if the dome is vented properly (a single overhead vent, and peripheral vents). Some people have reported a temperature differential as high as 8 °C (15 °F) between the inside of the dome and the outside. Buckminster Fuller discovered this effect with a simple house design adapted from a grain silo, and adapted his Dymaxion house and geodesic domes to use it.
Refrigerators and air conditioners operating from the waste heat of a diesel engine exhaust, heater flue or solar collector are entering use. These use the same principles as a gas refrigerator. Normally, the heat from a flue powers an ""absorptive chiller"". The cold water or brine from the chiller is used to cool air or a refrigerated space.
Cogeneration is popular in new commercial buildings. In current cogeneration systems small gas turbines or stirling engines powered from natural gas produce electricity and their exhaust drives an absorptive chiller.
A truck trailer refrigerator operating from the waste heat of a tractor's diesel exhaust was demonstrated by NRG Solutions, Inc. NRG developed a hydronic ammonia gas heat exchanger and vaporizer, the two essential new, not commercially available components of a waste heat driven refrigerator.
A similar scheme (multiphase cooling) can be by a multistage evaporative cooler. The air is passed through a spray of salt solution to dehumidify it, then through a spray of water solution to cool it, then another salt solution to dehumidify it again. The brine has to be regenerated, and that can be done economically with a low-temperature solar still. Multiphase evaporative coolers can lower the air's temperature by 50 °F (28 °C), and still control humidity. If the brine regenerator uses high heat, they also partially sterilise the air.
If enough electric power is available, cooling can be provided by conventional air conditioning using a heat pump.


=== Food production ===
Food production has often been included in historic autonomous projects to provide security.
Skilled, intensive gardening can support an adult from as little as 100 square meters of land per person,
possibly requiring the use of organic farming and aeroponics. Some proven intensive, low-effort food-production systems include urban gardening (indoors and outdoors). Indoor cultivation may be set up using hydroponics, while outdoor cultivation may be done using permaculture, forest gardening, no-till farming, and do nothing farming.
Greenhouses are also sometimes included. Sometimes they are also outfitted with irrigation systems or heat sink-systems which can respectively irrigate the plants or help to store energy from the sun and redistribute it at night (when the greenhouses starts to cool down).


== See also ==


== Notes ==


== External links ==
The Buckminster Fuller Institute is still in existence. B. Fuller left thousands of pages of notes to the university where he last taught.
There is a section on Autonomous Houses in the Reality Sculptors wiki, including links to a mailing list which frequently discusses autonomous design considerations.
Designs for a geodesic dome version of an Autonomous House can be found at reality.sculptors.com.
""Wind Power for Home and Business"" by Paul Gipe
An opinion piece by Brenda and Robert Vale
The Cropthorne House - notes on design and comparison with the Vales' Southwell House
Bad End 2 - 21st Century Hobbit Hole - precast concrete in home construction
Off-grid.net
Self Sufficiency Guide
Self Sufficient Living
GreenSpec","pandas(index=116, _1=116, text='an autonomous building is a building designed to be operated independently from infrastructural support services such as the electric power grid, gas grid, municipal water systems, sewage treatment systems, storm drains, communication services, and in some cases, public roads. advocates of autonomous building describe advantages that include reduced environmental impacts, increased security, and lower costs of ownership. some cited advantages satisfy tenets of green building, not independence per se (see below). off-grid buildings often rely very little on civil services and are therefore safer and more comfortable during civil disaster or military attacks. for example, off-grid buildings would not lose power or water if public supplies were compromised. as of 2018, most research and published articles concerning autonomous building focus on residential homes. in 2002, british architects brenda and robert vale said that  it is quite possible in all parts of australia to construct a \'house with no bills\', which would be comfortable without heating and cooling, which would make its own electricity, collect its own water and deal with its own waste...these houses can be built now, using off-the-shelf techniques. it is possible to build a ""house with no bills"" for the same price as a conventional house, but it would be (25%) smaller.   == history == in the 1970s, groups of activists and engineers were inspired by the warnings of imminent resource depletion and starvation. in the us a group calling themselves the new alchemists were famous for the depth of research effort placed in their projects. using conventional construction techniques, they designed a series of ""bioshelter"" projects, the most famous of which was the ark bioshelter community for prince edward island. they published the plans for all of these, with detailed design calculations and blueprints. the ark used wind based water pumping and electricity, and was self-contained in food production. it had living quarters for people, fish tanks raising tilapia for protein, a greenhouse watered with fish water and a closed loop sewage reclamation system that recycled human waste into sanitized fertilizer for the fish tanks. as of january 2010, the successor organization to the new alchemists has a web page up as the ""new alchemy institute"". the pei ark has been abandoned and partially renovated several times.  the 1990s saw the development of earthships, similar in intent to the ark project, but organized as a for-profit venture, with construction details published in a series of 3 books by mike reynolds. the building material is tires filled with earth. this makes a wall that has large amounts of thermal mass (see earth sheltering). berms are placed on exposed surfaces to further increase the house\'s temperature stability. the water system starts with rain water, processed for drinking, then washing, then plant watering, then toilet flushing, and finally black water is recycled again for more plant watering. the cisterns are placed and used as thermal masses. power, including electricity, heat and water heating, is from solar power. 1990s architects such as william mcdonough and ken yeang applied environmentally responsible building design to large commercial buildings, such as office buildings, making them largely self-sufficient in energy production. one major bank building (ing\'s amsterdam headquarters) in the netherlands was constructed to be autonomous and artistic as well.   == advantages == as an architect or engineer becomes more concerned with the disadvantages of transportation networks, and dependence on distant resources, their designs tend to include more autonomous elements. the historic path to autonomy was a concern for secure sources of heat, power, water and food. a nearly parallel path toward autonomy has been to start with a concern for environmental impacts, which cause disadvantages. autonomous buildings can increase security and reduce environmental impacts by using on-site resources (such as sunlight and rain) that would otherwise be wasted. autonomy often dramatically reduces the costs and impacts of networks that serve the building, because autonomy short-circuits the multiplying inefficiencies of collecting and transporting resources. other impacted resources, such as oil reserves and the retention of the local watershed, can often be cheaply conserved by thoughtful designs. autonomous buildings are usually energy-efficient in operation, and therefore cost-efficient, for the obvious reason that smaller energy needs are easier to satisfy off-grid. but they may substitute energy production or other techniques to avoid diminishing returns in extreme conservation. an autonomous structure is not always environmentally friendly. the goal of independence from support systems is associated with, but not identical to, other goals of environmentally responsible green building. however, autonomous buildings also usually include some degree of sustainability through the use of renewable energy and other renewable resources, producing no more greenhouse gases than they consume, and other measures.   == disadvantages == first and fundamentally, independence is a matter of degree, with many choices. for example, eliminating dependence on the electrical grid is relatively easy. in contrast, running an efficient, reliable food source can be a chore. living within an autonomous shelter may also require sacrifices in lifestyle or social opportunities. even the most comfortable and technologically advanced autonomous homes could require alterations of residents\' behavior. some may not welcome the extra chores. the vails described some clients\' experiences as inconvenient, irritating, isolating, or even as an unwanted full-time job. a well-designed building can reduce this issue, but usually at the expense of reduced autonomy. an autonomous house must be custom-built (or extensively retrofitted) to suit the climate and location. passive solar techniques, alternative toilet and sewage systems, thermal massing designs, basement battery systems, efficient windowing, and the array of other design tactics require some degree of non-standard construction, added expense, ongoing experimentation and maintenance, and also have an effect on the psychology of the space.   == systems == this section includes some minimal descriptions of methods, to give some feel for such a building\'s practicality, provide indexes to further information, and give a sense of modern trends. food production has often been included in historic autonomous projects to provide security. skilled, intensive gardening can support an adult from as little as 100 square meters of land per person, possibly requiring the use of organic farming and aeroponics. some proven intensive, low-effort food-production systems include urban gardening (indoors and outdoors). indoor cultivation may be set up using hydroponics, while outdoor cultivation may be done using permaculture, forest gardening, no-till farming, and do nothing farming. greenhouses are also sometimes included. sometimes they are also outfitted with irrigation systems or heat sink-systems which can respectively irrigate the plants or help to store energy from the sun and redistribute it at night (when the greenhouses starts to cool down).   == see also ==   == notes ==   == external links == the buckminster fuller institute is still in existence. b. fuller left thousands of pages of notes to the university where he last taught. there is a section on autonomous houses in the reality sculptors wiki, including links to a mailing list which frequently discusses autonomous design considerations. designs for a geodesic dome version of an autonomous house can be found at reality.sculptors.com. ""wind power for home and business"" by paul gipe an opinion piece by brenda and robert vale the cropthorne house - notes on design and comparison with the vales\' southwell house bad end 2 - 21st century hobbit hole - precast concrete in home construction off-grid.net self sufficiency guide self sufficient living greenspec')"
117,"The Eden Project (Cornish: Edenva) is a visitor attraction in Cornwall, England, UK. The project is located in a reclaimed china clay pit, located 2 km (1.2 mi) from the town of St Blazey and 5 km (3 mi) from the larger town of St Austell.The complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. The biomes consist of hundreds of hexagonal and pentagonal ethylene tetrafluoroethylene (ETFE) inflated cells supported by Geodesic tubular steel domes. The largest of the two biomes simulates a rainforest environment (and is the largest indoor rainforest in the world) and the second, a Mediterranean environment. The attraction also has an outside botanical garden which is home to many plants and wildlife native to Cornwall and the UK in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage.
There are plans to build an Eden Project North in the seaside town of Morecambe, Lancashire, with a focus on the marine environment.


== History ==

The clay pit in which the project is sited was in use for over 160 years. In 1981, the pit was used by the BBC as the planet surface of Magrathea in the TV series the Hitchhiker's Guide to the Galaxy. By the mid-1990s the pit was all but exhausted.The initial idea for the project dates back to 1996, with construction beginning in 1998. The work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits 15 m (49 ft) below the water table.The first part of the Eden Project, the visitor centre, opened to the public in May 2000. The first plants began arriving in September of that year, and the full site opened on 17 March 2001.
To counter criticism from environmental groups, the Eden Project committed to investigate a rail link to the site. The rail link was never built, and car parking on the site is still funded from revenue generated from general admission ticket sales, meaning that those who do not drive to the site also pay for the car park.
The Eden Project was used as a filming location for the 2002 James Bond film, Die Another Day. On 2 July 2005 The Eden Project hosted the ""Africa Calling"" concert of the Live 8 concert series.  It has also provided some plants for the British Museum's Africa garden.
In 2005, the Project launched ""A Time of Gifts"" for the winter months, November to February. This features an ice rink covering the lake, with a small café/bar attached, as well as a Christmas market. Cornish choirs regularly perform in the biomes.
In 2007, the Eden Project campaigned unsuccessfully for £50 million in Big Lottery Fund money for a proposed desert biome. It received just 12.07% of the votes, the lowest for the four projects being considered. As part of the campaign, the Eden Project invited people all over Cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to Cornwall.In December 2009, much of the project, including both greenhouses, became available to navigate through Google Street View.
The Eden Trust revealed a trading loss of £1.3 million for 2012–13, on a turnover of £25.4 million. The Eden Project had posted a surplus of £136,000 for the previous year. In 2014 Eden accounts showed a surplus of £2 million.The World Pasty Championships have been held at the Eden Project since 2012, an international competition to find the best Cornish pasties and other pasty-type savoury snacks.The Eden Project is said to have contributed over £1 billion to the Cornish economy. In 2016, Eden became home to Europe's second largest Redwood forest (after the Giants Grove at Birr Castle, Birr Castle Ireland giantsgrove.ie)when forty saplings of coast redwoods, Sequoia sempervirens, which could live for 4,000 years and reach 115 metres in height, were planted there.The Eden Project received 1,010,095 visitors in 2019.In December 2020 the project was closed after heavy rain caused several landslips at the site. Managers at the site are assessing the damage and will announce when the project will reopen on the company's website. Reopening became irrelevant as Covid lockdown measures in the UK indefinitely closed the venue from early 2021.


== Design and construction ==
The project was conceived by Tim Smit and designed by architecture firm Grimshaw Architects and engineering firm Anthony Hunt and Associates (now part of Sinclair Knight Merz). Davis Langdon carried out the project management, Sir Robert McAlpine and Alfred McAlpine did the construction, MERO designed and built the biomes, and Arup was the services engineer, economic consultant, environmental engineer and transportation engineer. Land Use Consultants led the masterplan and landscape design. The project took 2½ years to construct and opened to the public on 17 March 2001.


== Site ==


=== Layout ===

Once into the attraction, there is a meandering path with views of the two biomes, planted landscapes, including vegetable gardens, and sculptures that include a giant bee and previously The WEEE Man (removed in 2016), a towering figure made from old electrical appliances and was meant to represent the average electrical waste used by one person in a lifetime.


=== Biomes ===
At the bottom of the pit are two covered biomes:
The Tropical Biome, covers 1.56 ha (3.9 acres) and measures 55 m (180 ft) high, 100 m (328 ft) wide, and 200 m (656 ft) long. It is used for tropical plants, such as fruiting banana plants, coffee, rubber and giant bamboo, and is kept at a tropical temperature and moisture level.

The Mediterranean Biome covers 0.654 ha (1.6 acres) and measures 35 m (115 ft) high, 65 m (213 ft) wide, and 135 m (443 ft) long. It houses familiar warm temperate and arid plants such as olives and grape vines and various sculptures.
The Outdoor Gardens represent the temperate regions of the world with plants such as tea, lavender, hops, hemp, and sunflowers, as well as local plant species.
The covered biomes are constructed from a tubular steel (hex-tri-hex) with mostly hexagonal external cladding panels made from the thermoplastic ETFE. Glass was avoided due to its weight and potential dangers. The cladding panels themselves are created from several layers of thin UV-transparent ETFE film, which are sealed around their perimeter and inflated to create a large cushion. The resulting cushion acts as a thermal blanket to the structure. The ETFE material is resistant to most stains, which simply wash off in the rain. If required, cleaning can be performed by abseilers. Although the ETFE is susceptible to punctures, these can be easily fixed with ETFE tape. The structure is completely self-supporting, with no internal supports, and takes the form of a geodesic structure. The panels vary in size up to 9 m (29.5 ft) across, with the largest at the top of the structure.
The ETFE technology was supplied and installed by the firm Vector Foiltec, which is also responsible for ongoing maintenance of the cladding. The steel spaceframe and cladding package (with Vector Foiltec as ETFE subcontractor) was designed, supplied and installed by MERO (UK) PLC, who also jointly developed the overall scheme geometry with the architect, Nicholas Grimshaw & Partners.
The entire build project was managed by McAlpine Joint Venture.

		
		
		


=== The Core ===

The Core is the latest addition to the site and opened in September 2005. It provides the Eden Project with an education facility, incorporating classrooms and exhibition spaces designed to help communicate Eden's central message about the relationship between people and plants. Accordingly, the building has taken its inspiration from plants, most noticeable in the form of the soaring timber roof, which gives the building its distinctive shape.
Grimshaw developed the geometry of the copper-clad roof in collaboration with a sculptor, Peter Randall-Page, and Mike Purvis of structural engineers SKM Anthony Hunts. It is derived from phyllotaxis, which is the mathematical basis for nearly all plant growth; the ""opposing spirals"" found in many plants such as the seeds in a sunflower's head, pine cones and pineapples. The copper was obtained from traceable sources, and the Eden Project is working with Rio Tinto Group to explore the possibility of encouraging further traceable supply routes for metals, which would enable users to avoid metals mined unethically. The services and acoustic, mechanical, and electrical engineering design was carried out by Buro Happold.


==== Art at The Core ====

The Core is also home to art exhibitions throughout the year. A permanent installation entitled Seed, by Peter Randall-Page, occupies the anteroom. Seed is a large, 70 tonne egg-shaped stone installation standing some 13 feet (4.0 m) tall and displaying a complex pattern of protrusions that are based upon the geometric and mathematical principles that underlie plant growth.


== Environmental aspects ==
The biomes provide diverse growing conditions, and many plants are on display.
The Eden Project includes environmental education focusing on the interdependence of plants and people; plants are labelled with their medicinal uses. The massive amounts of water required to create the humid conditions of the Tropical Biome, and to serve the toilet facilities, are all sanitised rain water that would otherwise collect at the bottom of the quarry. The only mains water used is for hand washing and for cooking. The complex also uses Green Tariff Electricity – the energy comes from one of the many wind turbines in Cornwall, which were among the first in Europe.
In December 2010 the Eden Project received permission to build a geothermal electricity plant which will generate approx 4MWe, enough to supply Eden and about 5000 households. The project will involve geothermal heating as well as geothermal electricity. Cornwall Council and the European Union came up with the greater part of £16.8m required to start the project.  First a well will be sunk nearly 3 miles (4.5 km) into the granite crust underneath Eden.  Funding has been secured and drilling is set to begin in summer 2020.  Eden co-founder, Sir Tim Smit said, ""Since we began, Eden has had a dream that the world should be powered by renewable energy. The sun can provide massive solar power and the wind has been harnessed by humankind for thousands of years, but because both are intermittent and battery technology cannot yet store all we need there is a gap.  We believe the answer lies beneath our feet in the heat underground that can be accessed by drilling technology that pumps water towards the centre of the Earth and brings it back up superheated to provide us with heat and electricity"".


== Other projects ==


=== Eden Project North ===
In 2018, the Eden Project revealed its design for a new version of the project, located on the seafront in Morecambe, Lancashire. There will be biomes shaped like mussels and a focus on the marine environment. There will also be reimagined lidos, gardens, performance spaces, immersive experiences and observatories.Grimshaw are the architects for the project, which is expected to cost £80 million. The project is a partnership with the Lancashire Enterprise Partnership, Lancaster University, Lancashire County Council and Lancaster City Council. In December 2018, the four local partners agreed to provide £1 million to develop the idea, which will allow the development of an outline planning application for the project. It is expected that there will be 500 jobs created and 8,000 visitors a day to the site.


=== South Downs ===
In 2020 Eastbourne Borough Council and the Eden Project announced a joint project to explore the viability of a new Eden site in the South Downs National Park.


== Eden Sessions ==
Since 2002, the Project has hosted a series of musical performances, called the Eden Sessions. Artists have included Amy Winehouse, James Morrison, Muse, Lily Allen, Snow Patrol, Pulp, Brian Wilson, and The Magic Numbers. Oasis were also set to play in the summer of 2008, but the concert was postponed because Noel Gallagher was unable to perform after breaking three ribs in a stage invasion incident several weeks before. The concert was instead played in the summer of 2009.


=== Performance headliners ===
2008: The Verve, Kaiser Chiefs, and KT Tunstall.
2010: Mika, Jack Johnson, Mojave 3, Doves, Paolo Nutini, Mumford & Sons, and Martha Wainwright.
2011: The Flaming Lips, Primal Scream, Pendulum, Fleet Foxes, and Brandon Flowers with support from The Horrors, The Go! Team, OK Go, Villagers, and The Bees.
2012: Tim Minchin, Example, Frank Turner, Chase & Status, Plan B, Blink-182, Noah and the Whale, and The Vaccines.
2013: Kaiser Chiefs, Jessie J, Eddie Izzard, Sigur Rós, and The xx.
2014: Dizzee Rascal, Skrillex, Pixar in Concert, Ellie Goulding, and Elbow.
2015: Paolo Nutini, Elton John, Paloma Faith, Motörhead, The Stranglers, Spandau Ballet, and Ben Howard.
2016: Lionel Richie, Tom Jones, PJ Harvey, Manic Street Preachers, and Jess Glynne.
2017: Bastille, Madness, Royal Blood, Blondie, Van Morrison, Bryan Adams, and Foals.
2018: Gary Barlow, Massive Attack, A Beautiful Day Out, Ben Howard, Queens of the Stone Age, Jack Johnson, Björk.
2019: Stereophonics, Nile Rogers & Chic, The 100th session, Liam Gallagher, The Chemical Brothers, Snow Patrol, Kylie Minogue.My Chemical Romance were to play the 2020 Eden Sessions in their first UK shows after a six-year hiatus, as part of their reunion tour. The 2020 Sessions were postponed until 2021 due to the COVID-19 pandemic and the closing of the Eden Project.The 2021 sessions will be headlined by Diana Ross, Lionel Richie, The Script and My Chemical Romance.


== In the media ==
Robin Kewell (Ed.): Eden: The Inside Story. St Austell n.d.: The Eden Project. DVD.
The Eden Radio Project. Every Thursday between 5:30 and 7 p.m. on Radio St Austell Bay.
Die Another Day: Graves' diamond mine
Trees A Crowd Podcast: An Interview between the Project's head of interpretation, Dr Jo Elworthy, and David Oakes was released on 18 November.
Andy Day's CBeebies TV show Andy's Aquatic Adventures has had scenes filmed in the Eden Project.


== See also ==


== References ==


== Further reading ==
Philip McMillan Browse, Louise Frost, Alistair Griffiths: Plants of Eden (Eden Project). Penzance 2001: Alison Hodge.
Richard Mabey: Fencing Paradise: Exploring the Gardens of Eden London 2005: Eden Project Books. ISBN 1-903919-31-2
Hugh Pearman, Andrew Whalley: The Architecture of Eden. With a foreword by Sir Nicholas Grimshaw. London 2003: Eden Project Books. ISBN 1-903919-15-0
Eden Team (Ed.): Eden Project: The Guide 2008/9. London 2008: Eden Project Books.
Tim Smit: Eden. London 2001: Bantam Press.
Paul Spooner: The Revenge of the Green Planet: The Eden Project Book of Amazing Facts About Plants. London 2003: Eden Project Books.
Alan Titchmarsh: The Eden Project. United Kingdom: Acorn Media, 2006. OCLC 225403941.


== External links ==
Official website
Eden Sessions Website—Official site for live gigs","pandas(index=117, _1=117, text='the eden project (cornish: edenva) is a visitor attraction in cornwall, england, uk. the project is located in a reclaimed china clay pit, located 2 km (1.2 mi) from the town of st blazey and 5 km (3 mi) from the larger town of st austell.the complex is dominated by two huge enclosures consisting of adjoining domes that house thousands of plant species, and each enclosure emulates a natural biome. the biomes consist of hundreds of hexagonal and pentagonal ethylene tetrafluoroethylene (etfe) inflated cells supported by geodesic tubular steel domes. the largest of the two biomes simulates a rainforest environment (and is the largest indoor rainforest in the world) and the second, a mediterranean environment. the attraction also has an outside botanical garden which is home to many plants and wildlife native to cornwall and the uk in general; it also has many plants that provide an important and interesting backstory, for example, those with a prehistoric heritage. there are plans to build an eden project north in the seaside town of morecambe, lancashire, with a focus on the marine environment.   == history ==  the clay pit in which the project is sited was in use for over 160 years. in 1981, the pit was used by the bbc as the planet surface of magrathea in the tv series the hitchhiker\'s guide to the galaxy. by the mid-1990s the pit was all but exhausted.the initial idea for the project dates back to 1996, with construction beginning in 1998. the work was hampered by torrential rain in the first few months of the project, and parts of the pit flooded as it sits 15 m (49 ft) below the water table.the first part of the eden project, the visitor centre, opened to the public in may 2000. the first plants began arriving in september of that year, and the full site opened on 17 march 2001. to counter criticism from environmental groups, the eden project committed to investigate a rail link to the site. the rail link was never built, and car parking on the site is still funded from revenue generated from general admission ticket sales, meaning that those who do not drive to the site also pay for the car park. the eden project was used as a filming location for the 2002 james bond film, die another day. on 2 july 2005 the eden project hosted the ""africa calling"" concert of the live 8 concert series.  it has also provided some plants for the british museum\'s africa garden. in 2005, the project launched ""a time of gifts"" for the winter months, november to february. this features an ice rink covering the lake, with a small café/bar attached, as well as a christmas market. cornish choirs regularly perform in the biomes. in 2007, the eden project campaigned unsuccessfully for £50 million in big lottery fund money for a proposed desert biome. it received just 12.07% of the votes, the lowest for the four projects being considered. as part of the campaign, the eden project invited people all over cornwall to try to break the world record for the biggest ever pub quiz as part of its campaign to bring £50 million of lottery funds to cornwall.in december 2009, much of the project, including both greenhouses, became available to navigate through google street view. the eden trust revealed a trading loss of £1.3 million for 2012–13, on a turnover of £25.4 million. the eden project had posted a surplus of £136,000 for the previous year. in 2014 eden accounts showed a surplus of £2 million.the world pasty championships have been held at the eden project since 2012, an international competition to find the best cornish pasties and other pasty-type savoury snacks.the eden project is said to have contributed over £1 billion to the cornish economy. in 2016, eden became home to europe\'s second largest redwood forest (after the giants grove at birr castle, birr castle ireland giantsgrove.ie)when forty saplings of coast redwoods, sequoia sempervirens, which could live for 4,000 years and reach 115 metres in height, were planted there.the eden project received 1,010,095 visitors in 2019.in december 2020 the project was closed after heavy rain caused several landslips at the site. managers at the site are assessing the damage and will announce when the project will reopen on the company\'s website. reopening became irrelevant as covid lockdown measures in the uk indefinitely closed the venue from early 2021.   == design and construction == the project was conceived by tim smit and designed by architecture firm grimshaw architects and engineering firm anthony hunt and associates (now part of sinclair knight merz). davis langdon carried out the project management, sir robert mcalpine and alfred mcalpine did the construction, mero designed and built the biomes, and arup was the services engineer, economic consultant, environmental engineer and transportation engineer. land use consultants led the masterplan and landscape design. the project took 2½ years to construct and opened to the public on 17 march 2001.   == site == 2008: the verve, kaiser chiefs, and kt tunstall. 2010: mika, jack johnson, mojave 3, doves, paolo nutini, mumford & sons, and martha wainwright. 2011: the flaming lips, primal scream, pendulum, fleet foxes, and brandon flowers with support from the horrors, the go! team, ok go, villagers, and the bees. 2012: tim minchin, example, frank turner, chase & status, plan b, blink-182, noah and the whale, and the vaccines. 2013: kaiser chiefs, jessie j, eddie izzard, sigur rós, and the xx. 2014: dizzee rascal, skrillex, pixar in concert, ellie goulding, and elbow. 2015: paolo nutini, elton john, paloma faith, motörhead, the stranglers, spandau ballet, and ben howard. 2016: lionel richie, tom jones, pj harvey, manic street preachers, and jess glynne. 2017: bastille, madness, royal blood, blondie, van morrison, bryan adams, and foals. 2018: gary barlow, massive attack, a beautiful day out, ben howard, queens of the stone age, jack johnson, björk. 2019: stereophonics, nile rogers & chic, the 100th session, liam gallagher, the chemical brothers, snow patrol, kylie minogue.my chemical romance were to play the 2020 eden sessions in their first uk shows after a six-year hiatus, as part of their reunion tour. the 2020 sessions were postponed until 2021 due to the covid-19 pandemic and the closing of the eden project.the 2021 sessions will be headlined by diana ross, lionel richie, the script and my chemical romance.   == in the media == robin kewell (ed.): eden: the inside story. st austell n.d.: the eden project. dvd. the eden radio project. every thursday between 5:30 and 7 p.m. on radio st austell bay. die another day: graves\' diamond mine trees a crowd podcast: an interview between the project\'s head of interpretation, dr jo elworthy, and david oakes was released on 18 november. andy day\'s cbeebies tv show andy\'s aquatic adventures has had scenes filmed in the eden project.   == see also ==   == references ==   == further reading == philip mcmillan browse, louise frost, alistair griffiths: plants of eden (eden project). penzance 2001: alison hodge. richard mabey: fencing paradise: exploring the gardens of eden london 2005: eden project books. isbn 1-903919-31-2 hugh pearman, andrew whalley: the architecture of eden. with a foreword by sir nicholas grimshaw. london 2003: eden project books. isbn 1-903919-15-0 eden team (ed.): eden project: the guide 2008/9. london 2008: eden project books. tim smit: eden. london 2001: bantam press. paul spooner: the revenge of the green planet: the eden project book of amazing facts about plants. london 2003: eden project books. alan titchmarsh: the eden project. united kingdom: acorn media, 2006. oclc 225403941.   == external links == official website eden sessions website—official site for live gigs')"
118,"The interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. The intellectual origins of materials science stem from the Enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. Materials science still incorporates elements of physics, chemistry, and engineering. As such, the field was long considered by academic institutions as a sub-field of these related fields. Beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. 
Many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. Thus, breakthroughs in materials science are likely to affect the future of technology significantly.Materials scientists emphasize understanding, how the history of a material (processing) influences its structure, and thus the material's properties and performance. The understanding of processing-structure-properties relationships is called the materials paradigm. This paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. Materials science is also an important part of forensic engineering and failure analysis –  investigating materials, products, structures or components, which fail or do not function as intended, causing personal injury or damage to property. Such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.


== History ==

The material of choice of a given era is often a defining point. Phrases such as Stone Age, Bronze Age, Iron Age, and Steel Age are historic, if arbitrary examples. Originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. Modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. A major breakthrough in the understanding of materials occurred in the late 19th century, when the American scientist Josiah Willard Gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. Important elements of modern materials science were products of the Space Race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. Materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials.
Before the 1960s (and in some cases decades after), many eventual materials science departments were metallurgy or ceramics engineering departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. The growth of materials science in the United States was catalyzed in part by the Advanced Research Projects Agency, which funded a series of university-hosted laboratories in the early 1960s, ""to expand the national program of basic research and training in the materials sciences.""  The field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. The prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties and understand phenomena.


== Fundamentals ==

A material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications.   There are a myriad of materials around us, they can be found in anything from buildings,cars to spacecraft. Materials can generally be further divided into two classes: crystalline and non-crystalline. The traditional examples of materials are metals, semiconductors, ceramics and polymers. New and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few.
The basis of materials science involves studying the structure of materials, and relating them to their properties. Once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. The major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. These characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material's microstructure, and thus its properties.


=== Structure ===
As mentioned above, structure is one of the most important components of the field of materials science. Materials science examines the structure of materials from the atomic scale, all the way up to the macro scale. Characterization is the way materials scientists examine the structure of a material. This involves methods such as diffraction with X-rays, electronsor neutrons, and various forms of spectroscopy and chemical analysis such as Raman spectroscopy, energy-dispersive spectroscopy, chromatography, thermal analysis, electron microscope analysis, etc.
Structure is studied at various levels, as detailed below.


==== Atomic structure ====
This deals with the atoms of the materials, and how they are arranged to give molecules, crystals, etc. Much of the electrical, magnetic and chemical properties of materials arise from this level of structure. The length scales involved are in angstroms (Å). The chemical bonding and atomic arrangement (crystallography) are fundamental to studying the properties and behavior of any material.


===== Bonding =====

To obtain a full understanding of the material structure and how it relates to its properties, the materials scientist must study how the different atoms, ions and molecules are arranged and bonded to each other. This involves the study and use of quantum chemistry or quantum physics. Solid-state physics, solid-state chemistry and physical chemistry are also involved in the study of bonding and structure.


===== Crystallography =====

Crystallography is the science that examines the arrangement of atoms in crystalline solids. Crystallography is a useful tool for materials scientists. In single crystals, the effects of the crystalline arrangement of atoms is often easy to see macroscopically, because the natural shapes of crystals reflect the atomic structure. Further, physical properties are often controlled by crystalline defects. The understanding of crystal structures is an important prerequisite for understanding crystallographic defects. Mostly, materials do not occur as a single crystal, but in polycrystalline form, as an aggregate of small crystals with different orientations. Because of this, the powder diffraction method, which uses diffraction patterns of polycrystalline samples with a large number of crystals, plays an important role in structural determination. Most materials have a crystalline structure, but some important materials do not exhibit regular crystal structure. Polymers display varying degrees of crystallinity, and many are completely non-crystalline. Glass, some ceramics, and many natural materials are amorphous, not possessing any long-range order in their atomic arrangements. The study of polymers combines elements of chemical and statistical thermodynamics to give thermodynamic and mechanical descriptions of physical properties.


==== Nanostructure ====

Materials, which atoms and molecules form constituents in the nanoscale (i.e., they form nanostructure) are called nanomaterials. Nanomaterials are subject of intense research in the materials science community due to the unique properties that they exhibit.
Nanostructure deals with objects and structures that are in the 1 - 100 nm range.  In many materials, atoms or molecules agglomerate together to form objects at the nanoscale. This causes many interesting electrical, magnetic, optical, and mechanical properties.
In describing nanostructures, it is necessary to differentiate between the number of dimensions on the nanoscale. 
Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. 
Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. 
Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometre range. The term 'nanostructure' is often used, when referring to magnetic technology. Nanoscale structure in biology is often called ultrastructure.


==== Microstructure ====

Microstructure is defined as the structure of a prepared surface or thin foil of material as revealed by a microscope above 25× magnification. It deals with objects from 100 nm to a few cm. The microstructure of a material (which can be broadly classified into metallic, polymeric, ceramic and composite) can strongly influence physical properties such as strength, toughness, ductility, hardness, corrosion resistance, high/low temperature behavior, wear resistance, and so on. Most of the traditional materials (such as metals and ceramics) are microstructured.
The manufacture of a perfect crystal of a material is physically impossible. For example, any crystalline material will contain defects such as precipitates, grain boundaries (Hall–Petch relationship), vacancies, interstitial atoms or substitutional atoms. The microstructure of materials reveals these larger defects, so that they can be studied, with significant advances in simulation resulting in exponentially increasing understanding of how defects can be used to enhance material properties.


==== Macrostructure ====
Macrostructure is the appearance of a material in the scale millimeters to meters, it is the structure of the material as seen with the naked eye.


=== Properties ===

Materials exhibit myriad properties, including the following.

Mechanical properties, see Strength of materials
Chemical properties, see Chemistry
Electrical properties, see Electricity
Thermal properties, see Thermodynamics
Optical properties, see Optics and Photonics
Magnetic properties, see MagnetismThe properties of a material determine its usability and hence its engineering application.


=== Processing ===
Synthesis and processing involves the creation of a material with the desired micro-nanostructure. From an engineering standpoint, a material cannot be used in industry, if no economical production method for it has been developed. Thus, the processing of materials is vital to the field of materials science. Different materials require different processing or synthesis methods. For example, the processing of metals has historically been very important and is studied under the branch of materials science named physical metallurgy. Also, chemical and physical methods are also used to synthesize other materials such as polymers, ceramics, thin films, etc. As of the early 21st century, new methods are being developed to synthesize nanomaterials such as graphene.


=== Thermodynamics ===

Thermodynamics is concerned with heat and temperature and their relation to energy and work. It defines macroscopic variables, such as internal energy, entropy, and pressure, that partly describe a body of matter or radiation. It states that the behavior of those variables is subject to general constraints common to all materials. These general constraints are expressed in the four laws of thermodynamics.  Thermodynamics describes the bulk behavior of the body, not the microscopic behaviors of the very large numbers of its microscopic constituents, such as molecules. The behavior of these microscopic particles is described by, and the laws of thermodynamics are derived from, statistical mechanics. 
The study of thermodynamics is fundamental to materials science. It forms the foundation to treat general phenomena in materials science and engineering, including chemical reactions, magnetism, polarizability, and elasticity. It also helps in the understanding of phase diagrams and phase equilibrium.


=== Kinetics ===

Chemical kinetics is the study of the rates at which systems that are out of equilibrium change under the influence of various forces. When applied to materials science, it deals with how a material changes with time (moves from non-equilibrium to equilibrium state) due to application of a certain field. It details the rate of various processes evolving in materials including shape, size, composition and structure. Diffusion is important in the study of kinetics as this is the most common mechanism by which materials undergo change. Kinetics is essential in processing of materials because, among other things, it details how the microstructure changes with application of heat.


== Research ==
Materials science is a highly active area of research. Together with materials science departments, physics, chemistry, and many engineering departments are involved in materials research. Materials research covers a broad range of topics, following non-exhaustive list highlights a few important research areas.


=== Nanomaterials ===

Nanomaterials describe, in principle, materials of which a single unit is sized (in at least one dimension) between 1 and 1000 nanometers (10−9 meter), but is usually 1 nm - 100 nm. Nanomaterials research takes a materials science based approach to nanotechnology, using advances in materials metrology and synthesis, which have been developed in support of microfabrication research. Materials with structure at the nanoscale often have unique optical, electronic, or mechanical properties. The field of nanomaterials is loosely organized, like the traditional field of chemistry, into organic (carbon-based) nanomaterials, such as fullerenes, and inorganic nanomaterials based on other elements, such as silicon. Examples of nanomaterials include fullerenes, carbon nanotubes, nanocrystals, etc.


=== Biomaterials ===

A biomaterial is any matter, surface, or construct that interacts with biological systems. The study of biomaterials is called bio materials science. It has experienced steady and strong growth over its history, with many companies investing large amounts of money into developing new products. Biomaterials science encompasses elements of medicine, biology, chemistry, tissue engineering, and materials science.
Biomaterials can be derived either from nature or synthesized in a laboratory using a variety of chemical approaches using metallic components, polymers, bioceramics, or composite materials. They are often intended or adapted for medical applications, such as biomedical devices which perform, augment, or replace a natural function. Such functions may be benign, like being used for a heart valve, or may be bioactive with a more interactive functionality such as hydroxylapatite-coated hip implants. Biomaterials are also used every day in dental applications, surgery, and drug delivery. For example, a construct with impregnated pharmaceutical products can be placed into the body, which permits the prolonged release of a drug over an extended period of time. A biomaterial may also be an autograft, allograft or xenograft used as an organ transplant material.


=== Electronic, optical, and magnetic ===

Semiconductors, metals, and ceramics are used today to form highly complex systems, such as integrated electronic circuits, optoelectronic devices, and magnetic and optical mass storage media. These materials form the basis of our modern computing world, and hence research into these materials is of vital importance.
Semiconductors are a traditional example of these types of materials. They are materials that have properties that are intermediate between conductors and insulators. Their electrical conductivities are very sensitive to the concentration of impurities, which allows the use of doping to achieve desirable electronic properties. Hence, semiconductors form the basis of the traditional computer.
This field also includes new areas of research such as superconducting materials, spintronics, metamaterials, etc. The study of these materials involves knowledge of materials science and solid-state physics or condensed matter physics.


=== Computational materials science ===

With continuing increases in computing power, simulating the behavior of materials has become possible. This enables materials scientists to understand behavior and mechanisms, design new materials, and explain properties formerly poorly understood. Efforts surrounding Integrated computational materials engineering are now focusing on combining computational methods with experiments to drastically reduce the time and effort to optimize materials properties for a given application. This involves simulating materials at all length scales, using methods such as density functional theory, molecular dynamics, Monte Carlo, dislocation dynamics, phase field, finite element, and many more.


== Industry ==
Radical materials advances can drive the creation of new products or even new industries, but stable industries also employ materials scientists to make incremental improvements and troubleshoot issues with currently used materials. Industrial applications of materials science include materials design, cost-benefit tradeoffs in industrial production of materials, processing methods (casting, rolling, welding, ion implantation, crystal growth, thin-film deposition, sintering, glassblowing, etc.), and analytic methods (characterization methods such as electron microscopy, X-ray diffraction, calorimetry, nuclear microscopy (HEFIB), Rutherford backscattering, neutron diffraction, small-angle X-ray scattering (SAXS), etc.).
Besides material characterization, the material scientist or engineer also deals with extracting materials and converting them into useful forms. Thus ingot casting, foundry methods, blast furnace extraction, and electrolytic extraction are all part of the required knowledge of a materials engineer. Often the presence, absence, or variation of minute quantities of secondary elements and compounds in a bulk material will greatly affect the final properties of the materials produced. For example, steels are classified based on 1/10 and 1/100 weight percentages of the carbon and other alloying elements they contain. Thus, the extracting and purifying methods used to extract iron in a blast furnace can affect the quality of steel that is produced.


=== Ceramics and glasses ===

Another application of material science is the structures of ceramics and glass typically associated with the most brittle materials. Bonding in ceramics and glasses uses covalent and ionic-covalent types with SiO2 (silica or sand) as a fundamental building block. Ceramics are as soft as clay or as hard as stone and concrete. Usually, they are crystalline in form. Most glasses contain a metal oxide fused with silica. At high temperatures used to prepare glass, the material is a viscous liquid. The structure of glass forms into an amorphous state upon cooling. Windowpanes and eyeglasses are important examples. Fibers of glass are also available. Scratch resistant Corning Gorilla Glass is a well-known example of the application of materials science to drastically improve the properties of common components. Diamond and carbon in its graphite form are considered to be ceramics.
Engineering ceramics are known for their stiffness and stability under high temperatures, compression and electrical stress. Alumina, silicon carbide, and tungsten carbide are made from a fine powder of their constituents in a process of sintering with a binder. Hot pressing provides higher density material. Chemical vapor deposition can place a film of a ceramic on another material. Cermets are ceramic particles containing some metals. The wear resistance of tools is derived from cemented carbides with the metal phase of cobalt and nickel typically added to modify properties.


=== Composites ===

Another application of materials science in industry is making composite materials. These are structured materials composed of two or more macroscopic phases. 
Applications range from structural elements such as steel-reinforced concrete, to the thermal insulating tiles, which play a key and integral role in NASA's Space Shuttle thermal protection system, which is used to protect the surface of the shuttle from the heat of re-entry into the Earth's atmosphere. One example is reinforced Carbon-Carbon (RCC), the light gray material, which withstands re-entry temperatures up to 1,510 °C (2,750 °F) and protects the Space Shuttle's wing leading edges and nose cap. RCC is a laminated composite material made from graphite rayon cloth and impregnated with a phenolic resin. After curing at high temperature in an autoclave, the laminate is pyrolized to convert the resin to carbon, impregnated with furfural alcohol in a vacuum chamber, and cured-pyrolized to convert the furfural alcohol to carbon. To provide oxidation resistance for reuse ability, the outer layers of the RCC are converted to silicon carbide. 
Other examples can be seen in the ""plastic"" casings of television sets, cell-phones and so on. These plastic casings are usually a composite material made up of a thermoplastic matrix such as acrylonitrile butadiene styrene (ABS) in which calcium carbonate chalk, talc, glass fibers or carbon fibers have been added for added strength, bulk, or electrostatic dispersion. These additions may be termed reinforcing fibers, or dispersants, depending on their purpose.


=== Polymers ===

Polymers are chemical compounds made up of a large number of identical components linked together like chains. They are an important part of materials science. Polymers are the raw materials (the resins) used to make what are commonly called plastics and rubber. Plastics and rubber are really the final product, created after one or more polymers or additives have been added to a resin during processing, which is then shaped into a final form. Plastics which have been around, and which are in current widespread use, include polyethylene, polypropylene, polyvinyl chloride (PVC), polystyrene, nylons, polyesters, acrylics, polyurethanes, and polycarbonates and also rubbers, which have been around are natural rubber, styrene-butadiene rubber, chloroprene, and butadiene rubber. Plastics are generally classified as commodity, specialty and engineering plastics.
Polyvinyl chloride (PVC) is widely used, inexpensive, and annual production quantities are large. It lends itself to a vast array of applications, from artificial leather to electrical insulation and cabling, packaging, and containers. Its fabrication and processing are simple and well-established. The versatility of PVC is due to the wide range of plasticisers and other additives that it accepts. The term ""additives"" in polymer science refers to the chemicals and compounds added to the polymer base to modify its material properties.
Polycarbonate would be normally considered an engineering plastic (other examples include PEEK, ABS). Such plastics are valued for their superior strengths and other special material properties. They are usually not used for disposable applications, unlike commodity plastics.
Specialty plastics are materials with unique characteristics, such as ultra-high strength, electrical conductivity, electro-fluorescence, high thermal stability, etc.
The dividing lines between the various types of plastics is not based on material but rather on their properties and applications. For example, polyethylene (PE) is a cheap, low friction polymer commonly used to make disposable bags for shopping and trash, and is considered a commodity plastic, whereas medium-density polyethylene (MDPE) is used for underground gas and water pipes, and another variety called ultra-high-molecular-weight polyethylene (UHMWPE) is an engineering plastic which is used extensively as the glide rails for industrial equipment and the low-friction socket in implanted hip joints.


=== Metal alloys ===

The study of metal alloys is a significant part of materials science. Of all the metallic alloys in use today, the alloys of iron (steel, stainless steel, cast iron, tool steel, alloy steels) make up the largest proportion both by quantity and commercial value. 
Iron alloyed with various proportions of carbon gives low, mid and high carbon steels. An iron-carbon alloy is only considered steel, if the carbon level is between 0.01% and 2.00%. For the steels, the hardness and tensile strength of the steel is related to the amount of carbon present, with increasing carbon levels also leading to lower ductility and toughness. Heat treatment processes such as quenching and tempering can significantly change these properties, however. Cast Iron is defined as an iron–carbon alloy with more than 2.00%, but less than 6.67% carbon. Stainless steel is defined as a regular steel alloy with greater than 10% by weight alloying content of Chromium. Nickel and Molybdenum are typically also found in stainless steels.
Other significant metallic alloys are those of aluminium, titanium, copper and magnesium. Copper alloys have been known for a long time (since the Bronze Age), while the alloys of the other three metals have been relatively recently developed. Due to the chemical reactivity of these metals, the electrolytic extraction processes required were only developed relatively recently. The alloys of aluminium, titanium and magnesium are also known and valued for their high strength to weight ratios and, in the case of magnesium, their ability to provide electromagnetic shielding. These materials are ideal for situations, where high strength to weight ratios are more important than bulk cost, such as in the aerospace industry and certain automotive engineering applications.


=== Semiconductors ===
The study of semiconductors is a significant part of materials science. A semiconductor is a material that has a resistivity between a metal and insulator. Its electronic properties can be greatly altered through intentionally introducing impurities or doping. From these semiconductor materials, things such as diodes, transistors, light-emitting diodes (LEDs), and analog and digital electric circuits can be built, making them materials of interest in industry. Semiconductor devices have replaced thermionic devices (vacuum tubes) in most applications. Semiconductor devices are manufactured both as single discrete devices and as integrated circuits (ICs), which consist of a number—from a few to millions—of devices manufactured and interconnected on a single semiconductor substrate.Of all the semiconductors in use today, silicon makes up the largest portion both by quantity and commercial value. Monocrystalline silicon is used to produce wafers used in the semiconductor and electronics industry. Second to silicon, gallium arsenide (GaAs) is the second most popular semiconductor used. Due to its higher electron mobility and saturation velocity compared to silicon, it is a material of choice for high-speed electronics applications. These superior properties are compelling reasons to use GaAs circuitry in mobile phones, satellite communications, microwave point-to-point links and higher frequency radar systems. Other semiconductor materials include germanium, silicon carbide, and gallium nitride and have various applications.


== Relation with other fields ==
Materials science evolved, starting from the 1950s, because it was recognized that to create, discover and design new materials, one had to approach it in a unified manner. Thus, materials science and engineering emerged in many ways: renaming and/or combining existing metallurgy and ceramics engineering departments; splitting from existing solid state physics research (itself growing into condensed matter physics); pulling in relatively new polymer engineering and polymer science; recombining from the previous, as well as chemistry, chemical engineering, mechanical engineering, and electrical engineering; and more.
The field of materials science and engineering is important both from a scientific perspective, as well as for applications field. Materials are of the utmost importance for engineers (or other applied fields), because usage of the appropriate materials is crucial when designing systems. As a result, materials science is an increasingly important part of an engineer's education.
The field is inherently interdisciplinary, and the materials scientists or engineers must be aware and make use of the methods of the physicist, chemist and engineer. Thus, there remain close relationships with these fields. Conversely, many physicists, chemists and engineers find themselves working in materials science due to the significant overlaps between the fields.


== Emerging technologies ==


== Subdisciplines ==
The main branches of materials science stem from the three main classes of materials: ceramics, metals, and polymers.

Ceramic engineering
Metallurgy
Polymer science and polymer engineeringThere are additionally broadly applicable, materials independent, endeavors.

Materials characterization
Computational materials science
Materials informaticsThere are also relatively broad focuses across materials on specific phenomena and techniques.

Crystallography
Nuclear spectroscopy
Surface science
Tribology


== Related fields ==
Condensed matter physics
Mineralogy
Solid-state chemistry
Solid-state physics
Supramolecular chemistry


== Professional societies ==
American Ceramic Society
ASM International
Association for Iron and Steel Technology
Materials Research Society
The Minerals, Metals & Materials Society


== See also ==


== References ==


=== Citations ===


=== Bibliography ===
Ashby, Michael; Hugh Shercliff; David Cebon (2007). Materials: engineering, science, processing and design (1st ed.). Butterworth-Heinemann. ISBN 978-0-7506-8391-3.
Askeland, Donald R.; Pradeep P. Phulé (2005). The Science & Engineering of Materials (5th ed.). Thomson-Engineering. ISBN 978-0-534-55396-8.
Callister, Jr., William D. (2000). Materials Science and Engineering – An Introduction (5th ed.). John Wiley and Sons. ISBN 978-0-471-32013-5.
Eberhart, Mark (2003). Why Things Break: Understanding the World by the Way It Comes Apart. Harmony. ISBN 978-1-4000-4760-4.
Gaskell, David R. (1995). Introduction to the Thermodynamics of Materials (4th ed.). Taylor and Francis Publishing. ISBN 978-1-56032-992-3.
González-Viñas, W. & Mancini, H.L. (2004). An Introduction to Materials Science. Princeton University Press. ISBN 978-0-691-07097-1.
Gordon, James Edward (1984). The New Science of Strong Materials or Why You Don't Fall Through the Floor (eissue ed.). Princeton University Press. ISBN 978-0-691-02380-9.
Mathews, F.L. & Rawlings, R.D. (1999). Composite Materials: Engineering and Science. Boca Raton: CRC Press. ISBN 978-0-8493-0621-1.
Lewis, P.R.; Reynolds, K. & Gagg, C. (2003). Forensic Materials Engineering: Case Studies. Boca Raton: CRC Press.
Wachtman, John B. (1996). Mechanical Properties of Ceramics. New York: Wiley-Interscience, John Wiley & Son's. ISBN 978-0-471-13316-2.
Walker, P., ed. (1993). Chambers Dictionary of Materials Science and Technology. Chambers Publishing. ISBN 978-0-550-13249-9.


== Further reading ==
Timeline of Materials Science at The Minerals, Metals & Materials Society (TMS) –  accessed March 2007
Burns, G.; Glazer, A.M. (1990). Space Groups for Scientists and Engineers (2nd ed.). Boston: Academic Press, Inc. ISBN 978-0-12-145761-7.
Cullity, B.D. (1978). Elements of X-Ray Diffraction (2nd ed.). Reading, Massachusetts: Addison-Wesley Publishing Company. ISBN 978-0-534-55396-8.
Giacovazzo, C; Monaco HL; Viterbo D; Scordari F; Gilli G; Zanotti G; Catti M (1992). Fundamentals of Crystallography. Oxford: Oxford University Press. ISBN 978-0-19-855578-0.
Green, D.J.; Hannink, R.; Swain, M.V. (1989). Transformation Toughening of Ceramics. Boca Raton: CRC Press. ISBN 978-0-8493-6594-2.
Lovesey, S. W. (1984). Theory of Neutron Scattering from Condensed Matter; Volume 1: Neutron Scattering. Oxford: Clarendon Press. ISBN 978-0-19-852015-3.
Lovesey, S. W. (1984). Theory of Neutron Scattering from Condensed Matter; Volume 2: Condensed Matter. Oxford: Clarendon Press. ISBN 978-0-19-852017-7.
O'Keeffe, M.; Hyde, B.G. (1996). Crystal Structures; I. Patterns and Symmetry. Zeitschrift für Kristallographie. 212. Washington, DC: Mineralogical Society of America, Monograph Series. p. 899. Bibcode:1997ZK....212..899K. doi:10.1524/zkri.1997.212.12.899. ISBN 978-0-939950-40-9.
Squires, G.L. (1996). Introduction to the Theory of Thermal Neutron Scattering (2nd ed.). Mineola, New York: Dover Publications Inc. ISBN 978-0-486-69447-4.
Young, R.A., ed. (1993). The Rietveld Method. Oxford: Oxford University Press & International Union of Crystallography. ISBN 978-0-19-855577-3.


== External links ==
MS&T conference organized by the main materials societies
MIT OpenCourseWare for MSE
Materials science at Curlie","pandas(index=118, _1=118, text='the interdisciplinary field of materials science, also commonly termed materials science and engineering, is the design and discovery of new materials, particularly solids. the intellectual origins of materials science stem from the enlightenment, when researchers began to use analytical thinking from chemistry, physics, and engineering to understand ancient, phenomenological observations in metallurgy and mineralogy. materials science still incorporates elements of physics, chemistry, and engineering. as such, the field was long considered by academic institutions as a sub-field of these related fields. beginning in the 1940s, materials science began to be more widely recognized as a specific and distinct field of science and engineering, and major technical universities around the world created dedicated schools for its study. many of the most pressing scientific problems humans currently face are due to the limits of available materials and how they are used. thus, breakthroughs in materials science are likely to affect the future of technology significantly.materials scientists emphasize understanding, how the history of a material (processing) influences its structure, and thus the material\'s properties and performance. the understanding of processing-structure-properties relationships is called the materials paradigm. this paradigm is used to advance understanding in a variety of research areas, including nanotechnology, biomaterials, and metallurgy. materials science is also an important part of forensic engineering and failure analysis –  investigating materials, products, structures or components, which fail or do not function as intended, causing personal injury or damage to property. such investigations are key to understanding, for example, the causes of various aviation accidents and incidents.   == history ==  the material of choice of a given era is often a defining point. phrases such as stone age, bronze age, iron age, and steel age are historic, if arbitrary examples. originally deriving from the manufacture of ceramics and its putative derivative metallurgy, materials science is one of the oldest forms of engineering and applied science. modern materials science evolved directly from metallurgy, which itself evolved from mining and (likely) ceramics and earlier from the use of fire. a major breakthrough in the understanding of materials occurred in the late 19th century, when the american scientist josiah willard gibbs demonstrated that the thermodynamic properties related to atomic structure in various phases are related to the physical properties of a material. important elements of modern materials science were products of the space race; the understanding and engineering of the metallic alloys, and silica and carbon materials, used in building space vehicles enabling the exploration of space. materials science has driven, and been driven by, the development of revolutionary technologies such as rubbers, plastics, semiconductors, and biomaterials. before the 1960s (and in some cases decades after), many eventual materials science departments were metallurgy or ceramics engineering departments, reflecting the 19th and early 20th century emphasis on metals and ceramics. the growth of materials science in the united states was catalyzed in part by the advanced research projects agency, which funded a series of university-hosted laboratories in the early 1960s, ""to expand the national program of basic research and training in the materials sciences.""  the field has since broadened to include every class of materials, including ceramics, polymers, semiconductors, magnetic materials, biomaterials, and nanomaterials, generally classified into three distinct groups: ceramics, metals, and polymers. the prominent change in materials science during the recent decades is active usage of computer simulations to find new materials, predict properties and understand phenomena.   == fundamentals ==  a material is defined as a substance (most often a solid, but other condensed phases can be included) that is intended to be used for certain applications.   there are a myriad of materials around us, they can be found in anything from buildings,cars to spacecraft. materials can generally be further divided into two classes: crystalline and non-crystalline. the traditional examples of materials are metals, semiconductors, ceramics and polymers. new and advanced materials that are being developed include nanomaterials, biomaterials, and energy materials to name a few. the basis of materials science involves studying the structure of materials, and relating them to their properties. once a materials scientist knows about this structure-property correlation, they can then go on to study the relative performance of a material in a given application. the major determinants of the structure of a material and thus of its properties are its constituent chemical elements and the way in which it has been processed into its final form. these characteristics, taken together and related through the laws of thermodynamics and kinetics, govern a material\'s microstructure, and thus its properties. ashby, michael; hugh shercliff; david cebon (2007). materials: engineering, science, processing and design (1st ed.). butterworth-heinemann. isbn 978-0-7506-8391-3. askeland, donald r.; pradeep p. phulé (2005). the science & engineering of materials (5th ed.). thomson-engineering. isbn 978-0-534-55396-8. callister, jr., william d. (2000). materials science and engineering – an introduction (5th ed.). john wiley and sons. isbn 978-0-471-32013-5. eberhart, mark (2003). why things break: understanding the world by the way it comes apart. harmony. isbn 978-1-4000-4760-4. gaskell, david r. (1995). introduction to the thermodynamics of materials (4th ed.). taylor and francis publishing. isbn 978-1-56032-992-3. gonzález-viñas, w. & mancini, h.l. (2004). an introduction to materials science. princeton university press. isbn 978-0-691-07097-1. gordon, james edward (1984). the new science of strong materials or why you don\'t fall through the floor (eissue ed.). princeton university press. isbn 978-0-691-02380-9. mathews, f.l. & rawlings, r.d. (1999). composite materials: engineering and science. boca raton: crc press. isbn 978-0-8493-0621-1. lewis, p.r.; reynolds, k. & gagg, c. (2003). forensic materials engineering: case studies. boca raton: crc press. wachtman, john b. (1996). mechanical properties of ceramics. new york: wiley-interscience, john wiley & son\'s. isbn 978-0-471-13316-2. walker, p., ed. (1993). chambers dictionary of materials science and technology. chambers publishing. isbn 978-0-550-13249-9.   == further reading == timeline of materials science at the minerals, metals & materials society (tms) –  accessed march 2007 burns, g.; glazer, a.m. (1990). space groups for scientists and engineers (2nd ed.). boston: academic press, inc. isbn 978-0-12-145761-7. cullity, b.d. (1978). elements of x-ray diffraction (2nd ed.). reading, massachusetts: addison-wesley publishing company. isbn 978-0-534-55396-8. giacovazzo, c; monaco hl; viterbo d; scordari f; gilli g; zanotti g; catti m (1992). fundamentals of crystallography. oxford: oxford university press. isbn 978-0-19-855578-0. green, d.j.; hannink, r.; swain, m.v. (1989). transformation toughening of ceramics. boca raton: crc press. isbn 978-0-8493-6594-2. lovesey, s. w. (1984). theory of neutron scattering from condensed matter; volume 1: neutron scattering. oxford: clarendon press. isbn 978-0-19-852015-3. lovesey, s. w. (1984). theory of neutron scattering from condensed matter; volume 2: condensed matter. oxford: clarendon press. isbn 978-0-19-852017-7. o\'keeffe, m.; hyde, b.g. (1996). crystal structures; i. patterns and symmetry. zeitschrift für kristallographie. 212. washington, dc: mineralogical society of america, monograph series. p. 899. bibcode:1997zk....212..899k. doi:10.1524/zkri.1997.212.12.899. isbn 978-0-939950-40-9. squires, g.l. (1996). introduction to the theory of thermal neutron scattering (2nd ed.). mineola, new york: dover publications inc. isbn 978-0-486-69447-4. young, r.a., ed. (1993). the rietveld method. oxford: oxford university press & international union of crystallography. isbn 978-0-19-855577-3.   == external links == ms&t conference organized by the main materials societies mit opencourseware for mse materials science at curlie')"
119,"In architecture, post and lintel (also called prop and lintel or a trabeated system)  is a building system where strong horizontal elements are held up by strong vertical elements with large spaces between them. This is usually used to hold up a roof, creating a largely open space beneath, for whatever use the building is designed.  The horizontal elements are called by a variety of names including lintel, header, architrave or beam, and the supporting vertical elements may be called columns, pillars, or posts.  The use of wider elements at the top of the post, called capitals, to help spread the load, is common to many traditions. 
The trabeated system is a fundamental principle of Neolithic architecture, ancient Indian architecture, ancient Greek architecture and ancient Egyptian architecture. Other trabeated styles are the Persian, Lycian, Japanese, traditional Chinese, and ancient Chinese architecture, especially in northern China, and nearly all the Indian styles. The traditions are represented in North and Central America by Mayan architecture, and in South America by Inca architecture.  In all or most of these traditions, certainly in Greece and India, the earliest versions developed using wood, which were later translated into stone for larger and grander buildings. Timber framing, also using trusses, remains common for smaller buildings such as houses to the modern day.
The biggest disadvantage to a post and lintel construction is the limited weight that can be held up, and the small distances required between the posts. Ancient Roman architecture's development of the arch allowed for much larger structures to be constructed. The arcuated system spreads larger loads more effectively, and replaced the post and lintel trabeated system in most larger buildings and structures, until the introduction of steel girder beams in the industrial era.  As with the Roman temple portico front and its descendants in later classical architecture, trabeated features were often retained in parts of buildings as an aesthetic choice.  The classical orders of Greek origin were in particular retained in buildings designed to impress, even though they usually had little or no structural role.


== Lintel beams ==

In architecture, a post-and-lintel or trabeated system refers to the use of horizontal beams or lintels which are borne up by columns or posts.  The name is from the Latin trabs, beam; influenced by trabeatus, clothed in the trabea, a ritual garment.
A noteworthy example of a trabeated system is in Volubilis, from the Roman era, where one side of the Decumanus Maximus is lined with trabeated elements, while the opposite side of the roadway is designed in arched style.In India the style was used originally for wooden construction, but later the technique was adopted for stone structures for decorated load-bearing and purely ornamented non-structural purposes.


== Engineering ==
Post and lintel construction is one of four ancient structural methods of building, the others being the corbel, arch-and-vault, and truss.There are two main force vectors acting upon the post and lintel system: weight carrying compression at the joint between lintel and post, and tension induced by deformation of self-weight and the load above between the posts. The two posts are under compression from the weight of the lintel (or beam) above. The lintel will deform by sagging in the middle because the underside is under tension and the topside is under compression.


== See also ==
Architrave – structural lintel or beam resting on columns-pillars
Atalburu – Basque decorative lintel
Dolmen – Neolithic megalithic tombs with structural stone lintels
Dougong – traditional Chinese structural element
I-beam – steel lintels and beams
Marriage stone – decorative lintel
Opus caementicium
Structural design
Timber framing – post and beam systems
Stonehenge


== Notes ==


== References ==
Summerson, John, The Classical Language of Architecture, 1980 edition, Thames and Hudson World of Art series, ISBN 0500201773","pandas(index=119, _1=119, text=""in architecture, post and lintel (also called prop and lintel or a trabeated system)  is a building system where strong horizontal elements are held up by strong vertical elements with large spaces between them. this is usually used to hold up a roof, creating a largely open space beneath, for whatever use the building is designed.  the horizontal elements are called by a variety of names including lintel, header, architrave or beam, and the supporting vertical elements may be called columns, pillars, or posts.  the use of wider elements at the top of the post, called capitals, to help spread the load, is common to many traditions. the trabeated system is a fundamental principle of neolithic architecture, ancient indian architecture, ancient greek architecture and ancient egyptian architecture. other trabeated styles are the persian, lycian, japanese, traditional chinese, and ancient chinese architecture, especially in northern china, and nearly all the indian styles. the traditions are represented in north and central america by mayan architecture, and in south america by inca architecture.  in all or most of these traditions, certainly in greece and india, the earliest versions developed using wood, which were later translated into stone for larger and grander buildings. timber framing, also using trusses, remains common for smaller buildings such as houses to the modern day. the biggest disadvantage to a post and lintel construction is the limited weight that can be held up, and the small distances required between the posts. ancient roman architecture's development of the arch allowed for much larger structures to be constructed. the arcuated system spreads larger loads more effectively, and replaced the post and lintel trabeated system in most larger buildings and structures, until the introduction of steel girder beams in the industrial era.  as with the roman temple portico front and its descendants in later classical architecture, trabeated features were often retained in parts of buildings as an aesthetic choice.  the classical orders of greek origin were in particular retained in buildings designed to impress, even though they usually had little or no structural role.   == lintel beams ==  in architecture, a post-and-lintel or trabeated system refers to the use of horizontal beams or lintels which are borne up by columns or posts.  the name is from the latin trabs, beam; influenced by trabeatus, clothed in the trabea, a ritual garment. a noteworthy example of a trabeated system is in volubilis, from the roman era, where one side of the decumanus maximus is lined with trabeated elements, while the opposite side of the roadway is designed in arched style.in india the style was used originally for wooden construction, but later the technique was adopted for stone structures for decorated load-bearing and purely ornamented non-structural purposes.   == engineering == post and lintel construction is one of four ancient structural methods of building, the others being the corbel, arch-and-vault, and truss.there are two main force vectors acting upon the post and lintel system: weight carrying compression at the joint between lintel and post, and tension induced by deformation of self-weight and the load above between the posts. the two posts are under compression from the weight of the lintel (or beam) above. the lintel will deform by sagging in the middle because the underside is under tension and the topside is under compression.   == see also == architrave – structural lintel or beam resting on columns-pillars atalburu – basque decorative lintel dolmen – neolithic megalithic tombs with structural stone lintels dougong – traditional chinese structural element i-beam – steel lintels and beams marriage stone – decorative lintel opus caementicium structural design timber framing – post and beam systems stonehenge   == notes ==   == references == summerson, john, the classical language of architecture, 1980 edition, thames and hudson world of art series, isbn 0500201773"")"
120,"The skull crucible process was developed at the Lebedev Physical Institute in Moscow to manufacture cubic zirconia. It was invented to solve the problem of cubic zirconia's melting-point being too high for even platinum crucibles. 
In essence, by heating only the center of a volume of cubic zirconia, the material forms its own ""crucible"" from its cooler outer layers. The term ""skull"" refers to these outer layers forming a shell enclosing the molten volume. Zirconium oxide powder is heated then gradually allowed to cool. Heating is accomplished by radio frequency induction using a coil wrapped around the apparatus.  The outside of the device is water-cooled in order to keep the radio frequency coil from melting and also to cool the outside of the zirconium oxide and thus maintain the shape of the zirconium powder.
Since zirconium oxide in its solid state does not conduct electricity, a piece of zirconium metal is placed inside the gob of zirconium oxide. As the zirconium melts it oxidizes and blends with the now molten zirconium oxide, a conductor, and is heated by radio frequency induction.
When the zirconium oxide is melted on the inside (but not completely, since the outside needs to remain solid) the amplitude of the RF induction coil is gradually reduced and crystals form as the material cools. Normally this would form a monoclinic crystal system of zirconium oxide. 
In order to maintain a cubic crystal system a stabilizer is added, magnesium oxide, calcium oxide or yttrium oxide as well as any material to color the crystal. After the mixture cools the outer shell is broken off and the interior of the gob is then used to manufacture gemstones.","pandas(index=120, _1=120, text='the skull crucible process was developed at the lebedev physical institute in moscow to manufacture cubic zirconia. it was invented to solve the problem of cubic zirconia\'s melting-point being too high for even platinum crucibles. in essence, by heating only the center of a volume of cubic zirconia, the material forms its own ""crucible"" from its cooler outer layers. the term ""skull"" refers to these outer layers forming a shell enclosing the molten volume. zirconium oxide powder is heated then gradually allowed to cool. heating is accomplished by radio frequency induction using a coil wrapped around the apparatus.  the outside of the device is water-cooled in order to keep the radio frequency coil from melting and also to cool the outside of the zirconium oxide and thus maintain the shape of the zirconium powder. since zirconium oxide in its solid state does not conduct electricity, a piece of zirconium metal is placed inside the gob of zirconium oxide. as the zirconium melts it oxidizes and blends with the now molten zirconium oxide, a conductor, and is heated by radio frequency induction. when the zirconium oxide is melted on the inside (but not completely, since the outside needs to remain solid) the amplitude of the rf induction coil is gradually reduced and crystals form as the material cools. normally this would form a monoclinic crystal system of zirconium oxide. in order to maintain a cubic crystal system a stabilizer is added, magnesium oxide, calcium oxide or yttrium oxide as well as any material to color the crystal. after the mixture cools the outer shell is broken off and the interior of the gob is then used to manufacture gemstones.')"
121,"Powder metallurgy (PM) is a term covering a wide range of ways in which materials or components are made from metal powders. PM processes can avoid, or greatly reduce, the need to use metal removal processes, thereby drastically reducing yield losses in manufacture and often resulting in lower costs.
Powder metallurgy is also used to make unique materials impossible to get from melting or forming in other ways. A very important product of this type is tungsten carbide (WC). WC is used to cut and form other metals and is made from WC particles bonded with cobalt. It is very widely used in industry for tools of many types and globally ~50,000 tonnes/year (t/y) is made by PM. Other products include sintered filters, porous oil-impregnated bearings, electrical contacts and diamond tools.
Since the advent of industrial production–scale metal powder–based additive manufacturing (AM) in the 2010s, selective laser sintering and other metal AM processes are a new category of commercially important powder metallurgy applications.


== Overview ==
The powder metallurgy press and sinter process generally consists of three basic steps: powder blending (pulverisation), die compaction, and sintering. Compaction is generally performed at room temperature, and the elevated-temperature process of sintering is usually conducted at atmospheric pressure and under carefully controlled atmosphere composition. Optional secondary processing such as coining or heat treatment often follows to obtain special properties or enhanced precision.One of the older such methods, and still one used to make around 1 Mt/y of structural components of iron-based alloys, is the process of blending fine (<180 microns) metal (normally iron) powders with additives such as a lubricant wax, carbon, copper, and/or nickel, pressing them into a die of the desired shape, and then heating the compressed material (""green part"") in a controlled atmosphere to bond the material by sintering. This produces precise parts, normally very close to the die dimensions, but with 5–15% porosity, and thus sub-wrought steel properties.  There are several other PM processes which have been developed over the last fifty years. These include:

Powder forging: A ""preform"" made by the conventional ""press and sinter"" method is heated and then hot forged to full density, resulting in practically as-wrought properties.
Hot isostatic pressing (HIP): Here the powder (normally gas atomized, spherical type) is filled into a mould, normally consisting of a metallic ""can"" of suitable shape. The can is vibrated, then evacuated and sealed.  It is then placed in a hot isostatic press, where it is heated to a homologous temperature of around 0.7, and subjected to an external gas pressure of ~100 MPa (1000 bar, 15,000 psi) for several hours. This results in a shaped part of full density with as-wrought or better, properties. HIP was invented in the 1950-60s and entered tonnage production in the 1970-80s. In 2015, it was used to produce ~25,000 t/y of stainless and tool steels, as well as important parts of superalloys for jet engines.
Metal injection moulding (MIM):  Here the powder, normally very fine (<25 microns) and spherical, is mixed with plastic or wax binder to near the maximum solid loading, typically around 65vol%, and injection moulded to form a ""green"" part of complex geometry. This part is then heated or otherwise treated to remove the binder (debinding) to give a ""brown"" part. This part is then sintered, and shrinks by ~18% to give a complex and 95–99% dense finished part (surface roughness ~3 microns). Invented in the 1970s, production has increased since 2000 with an estimated global volume in 2014 of 12,000 t worth €1265 millions.
Electric current assisted sintering (ECAS) technologies rely on electric currents to densify powders, with the advantage of reducing production time dramatically (from 15 minutes of the slowest ECAS to a few microseconds of the fastest), not requiring a long furnace heat and allowing near theoretical densities but with the drawback of simple shapes. Powders employed in ECAS can avoid binders thanks to the possibility of direct sintering, without the need of pre-pressing and a green compact. Molds are designed for the final part shape since the powders densify while filling the cavity under an applied pressure thus avoiding the problem of shape variations caused by non isotropic sintering and distortions caused by gravity at high temperatures. The most common of these technologies is hot pressing, which has been under use for the production of the diamond tools employed in the construction industry. Spark plasma sintering and electro sinter forging are two modern, industrial commercial ECAS technologies.
Additive manufacturing (AM) is a relatively novel family of techniques which use metal powders (among other materials, such as plastics) to make parts by laser sintering or melting. This is a process under rapid development as of 2015, and whether to classify it as a PM process is perhaps uncertain at this stage. Processes include 3D printing, selective laser sintering (SLS), selective laser melting (SLM), and electron beam melting (EBM).


== History and capabilities ==
The history of powder metallurgy and the art of metal and ceramic sintering are intimately related to each other. Sintering involves the production of a hard solid metal or ceramic piece from a starting powder. The ancient Incas made jewelry and other artifacts from precious metal powders, though mass manufacturing of PM products did not begin until the mid or late 19th century. In these early manufacturing operations, iron was extracted by hand from metal sponge following reduction and was then reintroduced as a powder for final melting or sintering.
A much wider range of products can be obtained from powder processes than from direct alloying of fused materials. In melting operations the ""phase rule"" applies to all pure and combined elements and strictly dictates the distribution of liquid and solid phases which can exist for specific compositions. In addition, whole body melting of starting materials is required for alloying, thus imposing unwelcome chemical, thermal, and containment constraints on manufacturing. Unfortunately, the handling of aluminium/iron powders poses major problems. Other substances that are especially reactive with atmospheric oxygen, such as titanium, are sinterable in special atmospheres or with temporary coatings.In powder metallurgy or ceramics it is possible to fabricate components which otherwise would decompose or disintegrate. All considerations of solid-liquid phase changes can be ignored, so powder processes are more flexible than casting, extrusion, or forging techniques. Controllable characteristics of products prepared using various powder technologies include mechanical, magnetic, and other unconventional properties of such materials as porous solids, aggregates, and intermetallic compounds. Competitive characteristics of manufacturing processing (e.g. tool wear, complexity, or vendor options) also may be closely controlled.


== Powder production techniques ==
Any fusible material can be atomized. Several techniques have been developed which permit large production rates of powdered particles, often with considerable control over the size ranges of the final grain population. Powders may be prepared by crushing, grinding, chemical reactions, or electrolytic deposition. The most commonly used powders are copper-base and iron-base materials.Powders of the elements titanium, vanadium, thorium, niobium, tantalum, calcium, and uranium have been produced by high-temperature reduction of the corresponding nitrides and carbides. Iron, nickel, uranium, and beryllium submicrometre powders are obtained by reducing metallic oxalates and formates. Exceedingly fine particles also have been prepared by directing a stream of molten metal through a high-temperature plasma jet or flame,  atomizing the material. Various chemical and flame associated powdering processes are adopted in part to prevent serious degradation of particle surfaces by atmospheric oxygen.
In tonnage terms, the production of iron powders for PM structural part production dwarfs the production of all of the non-ferrous metal powders combined. Virtually all iron powders are produced by one of two processes: the sponge iron process or water atomization.


=== Sponge iron process ===
The longest established of these processes is the sponge iron process, the leading example of a family of processes involving solid state reduction of an oxide. In the process, selected magnetite (Fe3O4) ore is mixed with coke and lime and placed in a silicon carbide retort. The filled retort is then heated in a kiln, where the reduction process leaves an iron “cake” and a slag. In subsequent steps, the retort is emptied, the reduced iron sponge is separated from the slag and is crushed and annealed.
The resultant powder is highly irregular in particle shape, therefore ensuring good “green strength” so that die-pressed compacts can be readily handled prior to sintering, and each particle contains internal pores (hence the term “sponge”) so that the good green strength is available at low compacted density levels.
Sponge iron provides the
feedstock for all iron-based self-lubricating bearings, and still accounts for around 30% of iron powder usage in PM structural parts.


=== Atomization ===
Atomization is accomplished by forcing a molten metal stream through an orifice at moderate pressures. A gas is introduced into the metal stream just before it leaves the nozzle, serving to create turbulence as the entrained gas expands (due to heating) and exits into a large collection volume exterior to the orifice. The collection volume is filled with gas to promote further turbulence of the molten metal jet. Air and powder streams are segregated using gravity or cyclonic separation. Most atomized powders are annealed, which helps reduce the oxide and carbon content. The water atomized particles are smaller, cleaner, and nonporous and have a greater breadth of size, which allows better compacting. The particles produced through this method are normally of spherical or pear shape. Usually, they also carry a layer of oxide over them.
There are three types of atomization:

Liquid atomization
Gas atomization
Centrifugal atomizationSimple atomization techniques are available in which liquid metal is forced through an orifice at a sufficiently high velocity to ensure turbulent flow. The usual performance index used is the Reynolds number R = fvd/n, where f = fluid density, v = velocity of the exit stream, d = diameter of the opening, and n = absolute viscosity. At low R the liquid jet oscillates, but at higher velocities the stream becomes turbulent and breaks into droplets. Pumping energy is applied to droplet formation with very low efficiency (on the order of 1%) and control over the size distribution of the metal particles produced is rather poor. Other techniques such as nozzle vibration, nozzle asymmetry, multiple impinging streams, or molten-metal injection into ambient gas are all available to increase atomization efficiency, produce finer grains, and to narrow the particle size distribution. Unfortunately, it is difficult to eject metals through orifices smaller than a few millimeters in diameter, which in practice limits the minimum size of powder grains to approximately 10 μm. Atomization also produces a wide spectrum of particle sizes, necessitating downstream classification by screening and remelting a significant fraction of the grain boundary.


=== Centrifugal disintegration ===
Centrifugal disintegration of molten particles offers one way around these problems. Extensive experience is available with iron, steel, and aluminium. Metal to be powdered is formed into a rod which is introduced into a chamber through a rapidly rotating spindle. Opposite the spindle tip is an electrode from which an arc is established which heats the metal rod. As the tip material fuses, the rapid rod rotation throws off tiny melt droplets which solidify before hitting the chamber walls. A circulating gas sweeps particles from the chamber.  Similar techniques could be employed in space or on the Moon. The chamber wall could be rotated to force new powders into remote collection vessels, and the electrode could be replaced by a solar mirror focused at the end of the rod.
An alternative approach capable of producing a very narrow distribution of grain sizes but with low throughput consists of a rapidly spinning bowl heated to well above the melting point of the material to be powdered. Liquid metal, introduced onto the surface of the basin near the center at flow rates adjusted to permit a thin metal film to skim evenly up the walls and over the edge, breaks into droplets, each approximately the thickness of the film.


=== Other techniques ===
Another powder-production technique involves a thin jet of liquid metal intersected by high-speed streams of atomized water which break the jet into drops and cool the powder before it reaches the bottom of the bin. In subsequent operations the powder is dried. This is called water atomization. The advantage of water atomization is that metal solidifies faster than by gas atomization since the heat capacity of water is some magnitudes higher than gases. Since the solidification rate is inversely proportional to the particle size, smaller particles can be made using water atomization. The smaller the particles, the more homogeneous the micro structure will be. Notice that particles will have a more irregular shape and the particle size distribution will be wider. In addition, some surface contamination can occur by oxidation skin formation. Powder can be reduced by some kind of pre-consolidation treatment, such as annealing used for the manufacture of ceramic tools.


== Powder compaction ==

Powder compaction is the process of compacting metal powder in a die through the application of high pressures. Typically the tools are held in the vertical orientation with the punch tool forming the bottom of the cavity. The powder is then compacted into a shape and then ejected from the die cavity. In a number of these applications the parts may require very little additional work for their intended use; making for very cost efficient manufacturing.
The density of the compacted powder increases with the amount of pressure applied. Typical pressures range from 80 psi to 1000 psi (0.5 MPa to 7 MPa), pressures from 1000 psi to 1,000,000 psi have been obtained. Pressure of 10 t/in² to 50 t/in² (150 MPa to 700 MPa) are commonly used for metal powder compaction. To attain the same compression ratio across a component with more than one level or height, it is necessary to work with multiple lower punches. A cylindrical workpiece is made by single-level tooling. A more complex shape can be made by the common multiple-level tooling.
Production rates of 15 to 30 parts per minute are common.
There are four major classes of tool styles: single-action compaction, used for thin, flat components; opposed double-action with two punch motions, which accommodates thicker components; double-action with floating die; and double action withdrawal die. Double action classes give much better density distribution than single action. Tooling must be designed so that it will withstand the extreme pressure without deforming or bending. Tools must be made from materials that are polished and wear-resistant.
Better workpiece materials can be obtained by repressing and re-sintering.


=== Die pressing ===
The dominant technology for the forming of products from powder materials, in terms of both tonnage quantities and numbers of parts produced, is die pressing. There are mechanical, servo-electrical and hydraulic presses available in the market, whereby the biggest powder throughput is processed by hydraulic presses.
This forming technology involves a production cycle comprising:

Filling a die cavity with a known volume of the powder feedstock, delivered from a fill shoe.
Compaction of the powder within the die with punches to form the compact. Generally, compaction pressure is applied through punches from both ends of the toolset in order to reduce the level of density gradient within the compact.
Ejection of the compact from the die, using the lower punch(es) withdrawal from the die.
Removal of the compact from the upper face of the die using the fill shoe in the fill stage of the next cycle, or an automation system or robot.This cycle offers a readily automated and high production rate process.


=== Design considerations ===
Probably the most basic consideration is being able to remove the part from the die after it is pressed, along with avoiding sharp corners in the design. Keeping the maximum surface area below 20 square inches (0.013 m2) and the height-to-diameter ratio below 7-to-1 is recommended. Along with having walls thicker than 0.08 inches (2.0 mm) and keeping the adjacent wall thickness ratios below 2.5-to-1.
One of the major advantages of this process is its ability to produce complex geometries. Parts with undercuts and threads require a secondary machining operation. Typical part sizes range from 0.1 square inches (0.65 cm2) to 20 square inches (130 cm2). in area and from 0.1 to 4 inches (0.25 to 10.16 cm) in length. However, it is possible to produce parts that are less than 0.1 square inches (0.65 cm2) and larger than 25 square inches (160 cm2). in area and from a fraction of an inch (2.54 cm) to approximately 8 inches (20 cm) in length.


=== Isostatic pressing ===
In some pressing operations, such as hot isostatic pressing (HIP) compact formation and sintering occur simultaneously. This procedure, together with explosion-driven compressive techniques is used extensively in the production of high-temperature and high-strength parts such as turbine disks for jet engines. In most applications of powder metallurgy the compact is hot-pressed, heated to a temperature above which the materials cannot remain work-hardened. Hot pressing lowers the pressures required to reduce porosity and speeds welding and grain deformation processes. It also permits better dimensional control of the product, lessens sensitivity to physical characteristics of starting materials, and allows powder to be compressed to higher densities than with cold pressing, resulting in higher strength. Negative aspects of hot pressing include shorter die life, slower throughput because of powder heating, and the frequent necessity for protective atmospheres during forming and cooling stages.


== Isostatic powder compacting ==
Isostatic powder compacting is a mass-conserving shaping process. Fine metal particles are placed into a flexible mould and then high fluid pressure is applied to the mold, in contrast to the direct pressure applied by the die faces of a die pressing process.  The resulting article is then sintered in a furnace which increases the strength of the part by bonding the metal particles. This manufacturing process produces very little scrap metal and can be used to make many different shapes. The tolerances that this process can achieve are very precise, ranging from +/- 0.008 inches (0.2 mm) for axial dimensions and +/- 0.020 inches (0.5 mm) for radial dimensions.  This is the most efficient type of powder compacting (the following subcategories are also from this reference).  This operation is generally only applicable on small production quantities, although the cost of a mold much lower than that of pressing dies it is generally not reusable and the production time is much longer.Compacting pressures range from 15,000 psi (100,000 kPa) to 40,000 psi (280,000 kPa) for most metals and approximately 2,000 psi (14,000 kPa) to 10,000 psi (69,000 kPa) for non-metals.  The density of isostatic compacted parts is 5% to 10% higher than with other powder metallurgy processes.


=== Equipment ===
There are many types of equipment used in isostatic powder compacting.  There is the mold containing the part, which is flexible, a flexible outer pressure mold that contains and seals the mold, and the machine delivering the pressure.  There are also devices to control the amount of pressure and how long the pressure is held. The machines need to apply pressures from 15,000 to 40,000 pounds per square inch (100 to 280 MPa) for metals.


=== Geometrical possibilities ===
Typical workpiece sizes range from 0.25 in (6.35 mm) to 0.75 in (19.05 mm) thick and 0.5 in (12.70 mm) to 10 in (254 mm) long. It is possible to compact workpieces that are between 0.0625 in (1.59 mm) and 5 in (127 mm) thick and 0.0625 in (1.59 mm) to 40 in (1,016 mm) long.


=== Tool style ===
Isostatic tools are available in three styles, free mold (wet-bag), coarse mold (damp-bag) and fixed mold (dry-bag).  The free mold style is the traditional style of isostatic compaction and is not generally used for high production work. In free mold tooling the mold is removed and filled outside the canister. Damp bag is where the mold is located in the canister, yet filled outside. In fixed mold tooling, the mold is contained within the canister, which facilitates automation of the process.


=== Hot isostatic pressing ===

Hot isostatic pressing (HIP) compresses and sinters the part simultaneously by applying heat ranging from 900 °F (480 °C) to 2250 °F (1230 °C). Argon gas is the most common gas used in HIP because it is an inert gas, thus prevents chemical reactions during the operation.


=== Cold isostatic pressing ===
Cold isostatic pressing (CIP) uses fluid as a means of applying pressure to the mold at room temperature. After removal the part still needs to be sintered.
It is helpful in distributing pressure uniformly over the compaction material contained in a rubber bag.


=== Design considerations ===
Advantages over standard powder compaction are the possibility of thinner walls and larger workpieces. Height to diameter ratio has no limitation. No specific limitations exist in wall thickness variations, undercuts, reliefs, threads, and cross holes. No lubricants are need for isostatic powder compaction. The minimum wall thickness is 0.05 inches (1.27 mm) and the product can have a weight between 40 and 300 pounds (18 and 136 kg). There is 25 to 45% shrinkage of the powder after compacting.


== Sintering ==
After compaction, powdered materials are heated in a controlled atmosphere in a process known as sintering. During this process, the surfaces of the particles are bonded and desirable properties are achieved.Sintering of powder metals is a process in which particles under pressure chemically bond to themselves in order to form a coherent shape when exposed to a high temperature. The temperature in which the particles are sintered is most commonly below the melting point of the main component in the powder. If the temperature is above the melting point of a component in the powder metal part, the liquid of the melted particles fills the pores. This type of sintering is known as liquid-state sintering. A major challenge with sintering in general is knowing the effect of the process on the dimensions of the compact particles. This is especially difficult for tooling purposes in which specific dimensions may be needed. It is most common for the sintered part to shrink and become denser, but it can also expand or experience no net change.The main driving force for solid state sintering is an excess of surface free energy. The process of solid-state sintering is complex and dependent on the material and furnace (temperature and gas) conditions. There are six main stages that sintering processes can be grouped in which may overlap with one another: 1) initial bonding among particles, 2) neck growth, 3) pore channel closure, 4) pore rounding, 5) densification or pore shrinkage, and 6) pore coarsening. The main mechanisms present in these stages are evaporation, condensation, grain boundaries, volume diffusion, and plastic deformation.Most sintering furnaces contain three zones with three different properties that help to carry out the six steps above. The first zone, commonly coined the burn-off or purge stage, is designed to combust air, burn any contaminants such as lubricant or binders, and slowly raise the temperature of the compact materials. If the temperature of the compact parts is raised too quickly, the air in the pores will be at a very high internal pressure which could lead to expansion or fracture of the part. The second zone, known as the high-temperature stage, is used to produce solid-state diffusion and particle bonding. The material is seeking to lower its surface energy and does so by moving toward the points of contact between particles. The contact points become larger and eventually a solid mass with small pores is created. The third zone, also called the cooling period, is used to cool down the parts while still in a controlled atmosphere. This is an important zone as it prevents oxidation from immediate contact with the air or a phenomenon known as rapid cooling. All of the three stages must be carried out in a controlled atmosphere containing no oxygen. Hydrogen, nitrogen, dissociated ammonia, and cracked hydrocarbons are common gases pumped into the furnace zones providing a reducing atmosphere, preventing oxide formation.During this process, a number of characteristics are increased including the strength, ductility, toughness, and electrical and thermal conductivity of the material. If different elemental powders are compact and sintered, the material would form into alloys and intermetallic phases.As the pore sizes decrease, the density of the material will increase. As stated above, this shrinkage is a huge problem in making parts or tooling in which particular dimensions are required. The shrinkage of test materials is monitored and used to manipulate the furnace conditions or to oversize the compact materials in order to achieve the desired dimensions. Although, sintering does not deplete the compact part of porosity. In general, powder metal parts contain five to twenty-five percent porosity after sintering.To allow efficient stacking of product in the furnace during sintering and prevent parts sticking together, many manufacturers separate ware using ceramic powder separator sheets. These sheets are available in various materials such as alumina, zirconia, and magnesia. They are also available in fine, medium, and coarse particle sizes. By matching the material and particle size to the wares being sintered, surface damage and contamination can be reduced, while maximizing furnace loading per batch.
One recently developed technique for high-speed sintering involves passing high electric current through a powder to preferentially heat the asperities. Most of the energy serves to melt that portion of the compact where migration is desirable for densification; comparatively little energy is absorbed by the bulk materials and forming machinery. Naturally, this technique is not applicable to electrically insulating powders.


== Continuous powder processing ==
The phrase ""continuous process"" should be used only to describe modes of manufacturing which could be extended indefinitely in time. Normally, however, the term refers to processes whose products are much longer in one physical dimension than in the other two. Compression, rolling, and extrusion are the most common examples.
In a simple compression process, powder flows from a bin onto a two-walled channel and is repeatedly compressed vertically by a horizontally stationary punch. After stripping the compress from the conveyor, the compacted mass is introduced into a sintering furnace. An even easier approach is to spray powder onto a moving belt and sinter it without compression. However, good methods for stripping cold-pressed materials from moving belts are hard to find. One alternative that avoids the belt-stripping difficulty altogether is the manufacture of metal sheets using opposed hydraulic rams, although weakness lines across the sheet may arise during successive press operations.Powders can also be rolled to produce sheets.  The powdered metal is fed into a two-high rolling mill, and is compacted into strip form at up to 100 feet per minute (0.5 m/s).  The strip is then sintered and subjected to another rolling and further sintering.  Rolling is commonly used to produce sheet metal for electrical and electronic components, as well as coins. Considerable work also has been done on rolling multiple layers of different materials simultaneously into sheets.Extrusion processes are of two general types. In one type, the powder is mixed with a binder or plasticizer at room temperature; in the other, the powder is extruded at elevated temperatures without fortification. Extrusions with binders are used extensively in the preparation of tungsten-carbide composites. Tubes, complex sections, and spiral drill shapes are manufactured in extended lengths and diameters varying in the range 0.5–300 mm (0.020–11.811 in). Hard metal wires of 0.1 mm (0.0039 in) diameter have been drawn from powder stock. At the opposite extreme, large extrusions on a tonnage basis may be feasible.
For softer, easier to form metals such as aluminium and copper alloys continuous extrusion may also be performed using processes such as conform or continuous rotary extrusion. These processes use a rotating wheel with a groove around its circumference to drive the loose powder through a forming die. Through a combination of high pressure and a complex strain path the powder particles deform, generate a large amount of frictional heat and bond together to form a bulk solid. Theoretically fully continuous operation is possible as long as the powder can be fed into the process.There appears to be no limitation to the variety of metals and alloys that can be extruded, provided the temperatures and pressures involved are within the capabilities of die materials. Extrusion lengths may range from 3 to 30 m and diameters from 0.2 to 1 m. Modern presses are largely automatic and operate at high speeds (on the order of m/s).


== Shock (dynamic) consolidation ==
Shock consolidation, or dynamic consolidation, is an experimental technique of consolidating powders using high pressure shock waves.  These are commonly produced by impacting the workpiece with an explosively accelerated plate.  Despite being researched for a long time, the technique still has some problems in controlability and uniformity. However, it offers some valuable potential advantages.  As an example, consolidation occurs so rapidly that metastable microstructures may be retained.


== Electric current assisted sintering ==
These techniques employ electric currents to drive or enhance sintering. Through a combination of electric currents and mechanical pressure powders sinter more rapidly thereby reducing the sintering time compared to conventional thermal solutions. The techniques can be divided into two main categories: resistance sintering, which incorporates spark plasma sintering and hot pressing; and electric discharge sintering, such as capacitor discharge sintering or its derivative, electro sinter forging. Resistance sintering techniques are consolidation methods based on temperature, where heating of the mold and of the powders is accomplished through electric currents, usually with a characteristic processing time of 15 to 30 minutes.  On the other hand, electric discharge sintering methods rely on high-density currents (from 0.1 to 1 kA/mm^2) to directly sinter electrically conductive powders, with a characteristic time between tens of microseconds to hundreds of milliseconds.


== Special products ==
Many special products are possible with powder metallurgy technology. A nonexhaustive list includes Al2O3 whiskers coated with very thin oxide layers for improved refraction; iron compacts with Al2O3 coatings for improved high-temperature creep strength; light bulb filaments made with powder technology; linings for friction brakes; metal glasses for high-strength films and ribbons; heat shields for spacecraft reentry into Earth's atmosphere; electrical contacts for handling large current flows; magnets; microwave ferrites; filters for gases; and bearings which can be infiltrated with lubricants.
Extremely thin films and tiny spheres exhibit high strength. One application of this observation is to coat brittle materials in whisker form with a submicrometre film of much softer metal (e.g. cobalt-coated tungsten). The surface strain of the thin layer places the harder metal under compression, so that when the entire composite is sintered the rupture strength increases markedly. With this method, strengths on the order of 2.8 GPa versus 550 MPa have been observed for, respectively, coated (25% cobalt) and uncoated tungsten carbides.


== Hazards ==
The special materials and processes used in powder metallurgy can pose hazards to life and property. The high surface-area-to-volume ratio of the powders can increase their chemical reactivity in biological exposures (for example, inhalation or ingestion), and increases the risk of dust explosions. Materials considered relatively benign in bulk can pose special toxicological risks when in a finely divided form.


== See also ==
Electro sinter forging
Mechanical powder press
Selective laser melting
Selective laser sintering
Sintering
Spark plasma sintering
Spray forming


== References ==


== Cited sources ==
DeGarmo, E. P. (2008). Materials and Processes in Manufacturing (PDF) (10th ed.). Wiley. ISBN 9780470055120.


== Further reading ==
An earlier version of this article was copied from Appendix 4C of Advanced Automation for Space Missions, a NASA report in the public domain.
R. M. German, ""Powder Metallurgy and Particulate Materials Processing,"" Metal Powder Industries Federation, Princeton, New Jersey, 2005.
F. Thummler and R.Oberacker ""An Introduction to Powder Metallurgy"" The institute of Materials, London 1993
G. S. Upadhyaya, ""Sintered Metallic and Ceramic Materials"" John Wiley and Sons, West Sussex, England, 2000


== External links ==
Rapid manufacturing technique developed at the KU Leuven, Belgium
Slow motion video images of metal atomization at the Ames Laboratory
APMI International ""The Global Professional Society for Powder Metallurgy""[1], a non-profit organization","pandas(index=121, _1=121, text='powder metallurgy (pm) is a term covering a wide range of ways in which materials or components are made from metal powders. pm processes can avoid, or greatly reduce, the need to use metal removal processes, thereby drastically reducing yield losses in manufacture and often resulting in lower costs. powder metallurgy is also used to make unique materials impossible to get from melting or forming in other ways. a very important product of this type is tungsten carbide (wc). wc is used to cut and form other metals and is made from wc particles bonded with cobalt. it is very widely used in industry for tools of many types and globally ~50,000 tonnes/year (t/y) is made by pm. other products include sintered filters, porous oil-impregnated bearings, electrical contacts and diamond tools. since the advent of industrial production–scale metal powder–based additive manufacturing (am) in the 2010s, selective laser sintering and other metal am processes are a new category of commercially important powder metallurgy applications.   == overview == the powder metallurgy press and sinter process generally consists of three basic steps: powder blending (pulverisation), die compaction, and sintering. compaction is generally performed at room temperature, and the elevated-temperature process of sintering is usually conducted at atmospheric pressure and under carefully controlled atmosphere composition. optional secondary processing such as coining or heat treatment often follows to obtain special properties or enhanced precision.one of the older such methods, and still one used to make around 1 mt/y of structural components of iron-based alloys, is the process of blending fine (<180 microns) metal (normally iron) powders with additives such as a lubricant wax, carbon, copper, and/or nickel, pressing them into a die of the desired shape, and then heating the compressed material (""green part"") in a controlled atmosphere to bond the material by sintering. this produces precise parts, normally very close to the die dimensions, but with 5–15% porosity, and thus sub-wrought steel properties.  there are several other pm processes which have been developed over the last fifty years. these include:  powder forging: a ""preform"" made by the conventional ""press and sinter"" method is heated and then hot forged to full density, resulting in practically as-wrought properties. hot isostatic pressing (hip): here the powder (normally gas atomized, spherical type) is filled into a mould, normally consisting of a metallic ""can"" of suitable shape. the can is vibrated, then evacuated and sealed.  it is then placed in a hot isostatic press, where it is heated to a homologous temperature of around 0.7, and subjected to an external gas pressure of ~100 mpa (1000 bar, 15,000 psi) for several hours. this results in a shaped part of full density with as-wrought or better, properties. hip was invented in the 1950-60s and entered tonnage production in the 1970-80s. in 2015, it was used to produce ~25,000 t/y of stainless and tool steels, as well as important parts of superalloys for jet engines. metal injection moulding (mim):  here the powder, normally very fine (<25 microns) and spherical, is mixed with plastic or wax binder to near the maximum solid loading, typically around 65vol%, and injection moulded to form a ""green"" part of complex geometry. this part is then heated or otherwise treated to remove the binder (debinding) to give a ""brown"" part. this part is then sintered, and shrinks by ~18% to give a complex and 95–99% dense finished part (surface roughness ~3 microns). invented in the 1970s, production has increased since 2000 with an estimated global volume in 2014 of 12,000 t worth €1265 millions. electric current assisted sintering (ecas) technologies rely on electric currents to densify powders, with the advantage of reducing production time dramatically (from 15 minutes of the slowest ecas to a few microseconds of the fastest), not requiring a long furnace heat and allowing near theoretical densities but with the drawback of simple shapes. powders employed in ecas can avoid binders thanks to the possibility of direct sintering, without the need of pre-pressing and a green compact. molds are designed for the final part shape since the powders densify while filling the cavity under an applied pressure thus avoiding the problem of shape variations caused by non isotropic sintering and distortions caused by gravity at high temperatures. the most common of these technologies is hot pressing, which has been under use for the production of the diamond tools employed in the construction industry. spark plasma sintering and electro sinter forging are two modern, industrial commercial ecas technologies. additive manufacturing (am) is a relatively novel family of techniques which use metal powders (among other materials, such as plastics) to make parts by laser sintering or melting. this is a process under rapid development as of 2015, and whether to classify it as a pm process is perhaps uncertain at this stage. processes include 3d printing, selective laser sintering (sls), selective laser melting (slm), and electron beam melting (ebm).   == history and capabilities == the history of powder metallurgy and the art of metal and ceramic sintering are intimately related to each other. sintering involves the production of a hard solid metal or ceramic piece from a starting powder. the ancient incas made jewelry and other artifacts from precious metal powders, though mass manufacturing of pm products did not begin until the mid or late 19th century. in these early manufacturing operations, iron was extracted by hand from metal sponge following reduction and was then reintroduced as a powder for final melting or sintering. a much wider range of products can be obtained from powder processes than from direct alloying of fused materials. in melting operations the ""phase rule"" applies to all pure and combined elements and strictly dictates the distribution of liquid and solid phases which can exist for specific compositions. in addition, whole body melting of starting materials is required for alloying, thus imposing unwelcome chemical, thermal, and containment constraints on manufacturing. unfortunately, the handling of aluminium/iron powders poses major problems. other substances that are especially reactive with atmospheric oxygen, such as titanium, are sinterable in special atmospheres or with temporary coatings.in powder metallurgy or ceramics it is possible to fabricate components which otherwise would decompose or disintegrate. all considerations of solid-liquid phase changes can be ignored, so powder processes are more flexible than casting, extrusion, or forging techniques. controllable characteristics of products prepared using various powder technologies include mechanical, magnetic, and other unconventional properties of such materials as porous solids, aggregates, and intermetallic compounds. competitive characteristics of manufacturing processing (e.g. tool wear, complexity, or vendor options) also may be closely controlled.   == powder production techniques == any fusible material can be atomized. several techniques have been developed which permit large production rates of powdered particles, often with considerable control over the size ranges of the final grain population. powders may be prepared by crushing, grinding, chemical reactions, or electrolytic deposition. the most commonly used powders are copper-base and iron-base materials.powders of the elements titanium, vanadium, thorium, niobium, tantalum, calcium, and uranium have been produced by high-temperature reduction of the corresponding nitrides and carbides. iron, nickel, uranium, and beryllium submicrometre powders are obtained by reducing metallic oxalates and formates. exceedingly fine particles also have been prepared by directing a stream of molten metal through a high-temperature plasma jet or flame,  atomizing the material. various chemical and flame associated powdering processes are adopted in part to prevent serious degradation of particle surfaces by atmospheric oxygen. in tonnage terms, the production of iron powders for pm structural part production dwarfs the production of all of the non-ferrous metal powders combined. virtually all iron powders are produced by one of two processes: the sponge iron process or water atomization. advantages over standard powder compaction are the possibility of thinner walls and larger workpieces. height to diameter ratio has no limitation. no specific limitations exist in wall thickness variations, undercuts, reliefs, threads, and cross holes. no lubricants are need for isostatic powder compaction. the minimum wall thickness is 0.05 inches (1.27 mm) and the product can have a weight between 40 and 300 pounds (18 and 136 kg). there is 25 to 45% shrinkage of the powder after compacting.   == sintering == after compaction, powdered materials are heated in a controlled atmosphere in a process known as sintering. during this process, the surfaces of the particles are bonded and desirable properties are achieved.sintering of powder metals is a process in which particles under pressure chemically bond to themselves in order to form a coherent shape when exposed to a high temperature. the temperature in which the particles are sintered is most commonly below the melting point of the main component in the powder. if the temperature is above the melting point of a component in the powder metal part, the liquid of the melted particles fills the pores. this type of sintering is known as liquid-state sintering. a major challenge with sintering in general is knowing the effect of the process on the dimensions of the compact particles. this is especially difficult for tooling purposes in which specific dimensions may be needed. it is most common for the sintered part to shrink and become denser, but it can also expand or experience no net change.the main driving force for solid state sintering is an excess of surface free energy. the process of solid-state sintering is complex and dependent on the material and furnace (temperature and gas) conditions. there are six main stages that sintering processes can be grouped in which may overlap with one another: 1) initial bonding among particles, 2) neck growth, 3) pore channel closure, 4) pore rounding, 5) densification or pore shrinkage, and 6) pore coarsening. the main mechanisms present in these stages are evaporation, condensation, grain boundaries, volume diffusion, and plastic deformation.most sintering furnaces contain three zones with three different properties that help to carry out the six steps above. the first zone, commonly coined the burn-off or purge stage, is designed to combust air, burn any contaminants such as lubricant or binders, and slowly raise the temperature of the compact materials. if the temperature of the compact parts is raised too quickly, the air in the pores will be at a very high internal pressure which could lead to expansion or fracture of the part. the second zone, known as the high-temperature stage, is used to produce solid-state diffusion and particle bonding. the material is seeking to lower its surface energy and does so by moving toward the points of contact between particles. the contact points become larger and eventually a solid mass with small pores is created. the third zone, also called the cooling period, is used to cool down the parts while still in a controlled atmosphere. this is an important zone as it prevents oxidation from immediate contact with the air or a phenomenon known as rapid cooling. all of the three stages must be carried out in a controlled atmosphere containing no oxygen. hydrogen, nitrogen, dissociated ammonia, and cracked hydrocarbons are common gases pumped into the furnace zones providing a reducing atmosphere, preventing oxide formation.during this process, a number of characteristics are increased including the strength, ductility, toughness, and electrical and thermal conductivity of the material. if different elemental powders are compact and sintered, the material would form into alloys and intermetallic phases.as the pore sizes decrease, the density of the material will increase. as stated above, this shrinkage is a huge problem in making parts or tooling in which particular dimensions are required. the shrinkage of test materials is monitored and used to manipulate the furnace conditions or to oversize the compact materials in order to achieve the desired dimensions. although, sintering does not deplete the compact part of porosity. in general, powder metal parts contain five to twenty-five percent porosity after sintering.to allow efficient stacking of product in the furnace during sintering and prevent parts sticking together, many manufacturers separate ware using ceramic powder separator sheets. these sheets are available in various materials such as alumina, zirconia, and magnesia. they are also available in fine, medium, and coarse particle sizes. by matching the material and particle size to the wares being sintered, surface damage and contamination can be reduced, while maximizing furnace loading per batch. one recently developed technique for high-speed sintering involves passing high electric current through a powder to preferentially heat the asperities. most of the energy serves to melt that portion of the compact where migration is desirable for densification; comparatively little energy is absorbed by the bulk materials and forming machinery. naturally, this technique is not applicable to electrically insulating powders.   == continuous powder processing == the phrase ""continuous process"" should be used only to describe modes of manufacturing which could be extended indefinitely in time. normally, however, the term refers to processes whose products are much longer in one physical dimension than in the other two. compression, rolling, and extrusion are the most common examples. in a simple compression process, powder flows from a bin onto a two-walled channel and is repeatedly compressed vertically by a horizontally stationary punch. after stripping the compress from the conveyor, the compacted mass is introduced into a sintering furnace. an even easier approach is to spray powder onto a moving belt and sinter it without compression. however, good methods for stripping cold-pressed materials from moving belts are hard to find. one alternative that avoids the belt-stripping difficulty altogether is the manufacture of metal sheets using opposed hydraulic rams, although weakness lines across the sheet may arise during successive press operations.powders can also be rolled to produce sheets.  the powdered metal is fed into a two-high rolling mill, and is compacted into strip form at up to 100 feet per minute (0.5 m/s).  the strip is then sintered and subjected to another rolling and further sintering.  rolling is commonly used to produce sheet metal for electrical and electronic components, as well as coins. considerable work also has been done on rolling multiple layers of different materials simultaneously into sheets.extrusion processes are of two general types. in one type, the powder is mixed with a binder or plasticizer at room temperature; in the other, the powder is extruded at elevated temperatures without fortification. extrusions with binders are used extensively in the preparation of tungsten-carbide composites. tubes, complex sections, and spiral drill shapes are manufactured in extended lengths and diameters varying in the range 0.5–300 mm (0.020–11.811 in). hard metal wires of 0.1 mm (0.0039 in) diameter have been drawn from powder stock. at the opposite extreme, large extrusions on a tonnage basis may be feasible. for softer, easier to form metals such as aluminium and copper alloys continuous extrusion may also be performed using processes such as conform or continuous rotary extrusion. these processes use a rotating wheel with a groove around its circumference to drive the loose powder through a forming die. through a combination of high pressure and a complex strain path the powder particles deform, generate a large amount of frictional heat and bond together to form a bulk solid. theoretically fully continuous operation is possible as long as the powder can be fed into the process.there appears to be no limitation to the variety of metals and alloys that can be extruded, provided the temperatures and pressures involved are within the capabilities of die materials. extrusion lengths may range from 3 to 30 m and diameters from 0.2 to 1 m. modern presses are largely automatic and operate at high speeds (on the order of m/s).   == shock (dynamic) consolidation == shock consolidation, or dynamic consolidation, is an experimental technique of consolidating powders using high pressure shock waves.  these are commonly produced by impacting the workpiece with an explosively accelerated plate.  despite being researched for a long time, the technique still has some problems in controlability and uniformity. however, it offers some valuable potential advantages.  as an example, consolidation occurs so rapidly that metastable microstructures may be retained.   == electric current assisted sintering == these techniques employ electric currents to drive or enhance sintering. through a combination of electric currents and mechanical pressure powders sinter more rapidly thereby reducing the sintering time compared to conventional thermal solutions. the techniques can be divided into two main categories: resistance sintering, which incorporates spark plasma sintering and hot pressing; and electric discharge sintering, such as capacitor discharge sintering or its derivative, electro sinter forging. resistance sintering techniques are consolidation methods based on temperature, where heating of the mold and of the powders is accomplished through electric currents, usually with a characteristic processing time of 15 to 30 minutes.  on the other hand, electric discharge sintering methods rely on high-density currents (from 0.1 to 1 ka/mm^2) to directly sinter electrically conductive powders, with a characteristic time between tens of microseconds to hundreds of milliseconds.   == special products == many special products are possible with powder metallurgy technology. a nonexhaustive list includes al2o3 whiskers coated with very thin oxide layers for improved refraction; iron compacts with al2o3 coatings for improved high-temperature creep strength; light bulb filaments made with powder technology; linings for friction brakes; metal glasses for high-strength films and ribbons; heat shields for spacecraft reentry into earth\'s atmosphere; electrical contacts for handling large current flows; magnets; microwave ferrites; filters for gases; and bearings which can be infiltrated with lubricants. extremely thin films and tiny spheres exhibit high strength. one application of this observation is to coat brittle materials in whisker form with a submicrometre film of much softer metal (e.g. cobalt-coated tungsten). the surface strain of the thin layer places the harder metal under compression, so that when the entire composite is sintered the rupture strength increases markedly. with this method, strengths on the order of 2.8 gpa versus 550 mpa have been observed for, respectively, coated (25% cobalt) and uncoated tungsten carbides.   == hazards == the special materials and processes used in powder metallurgy can pose hazards to life and property. the high surface-area-to-volume ratio of the powders can increase their chemical reactivity in biological exposures (for example, inhalation or ingestion), and increases the risk of dust explosions. materials considered relatively benign in bulk can pose special toxicological risks when in a finely divided form.   == see also == electro sinter forging mechanical powder press selective laser melting selective laser sintering sintering spark plasma sintering spray forming   == references ==   == cited sources == degarmo, e. p. (2008). materials and processes in manufacturing (pdf) (10th ed.). wiley. isbn 9780470055120.   == further reading == an earlier version of this article was copied from appendix 4c of advanced automation for space missions, a nasa report in the public domain. r. m. german, ""powder metallurgy and particulate materials processing,"" metal powder industries federation, princeton, new jersey, 2005. f. thummler and r.oberacker ""an introduction to powder metallurgy"" the institute of materials, london 1993 g. s. upadhyaya, ""sintered metallic and ceramic materials"" john wiley and sons, west sussex, england, 2000   == external links == rapid manufacturing technique developed at the ku leuven, belgium slow motion video images of metal atomization at the ames laboratory apmi international ""the global professional society for powder metallurgy""[1], a non-profit organization')"
122,"In materials science, the sol–gel process is a method for producing solid materials from small molecules. The method is used for the fabrication of metal oxides, especially the oxides of silicon (Si) and titanium (Ti). The process involves conversion of monomers into a colloidal solution (sol) that acts as the precursor for an integrated network (or gel) of either discrete particles or network polymers. Typical precursors are metal alkoxides.


== Stages in the process ==

In this chemical procedure, a ""sol"" (a colloidal solution) is formed that then gradually evolves towards the formation of a gel-like diphasic system containing both a liquid phase and solid phase whose morphologies range from discrete particles to continuous polymer networks. In the case of the colloid, the volume fraction of particles (or particle density) may be so low that a significant amount of fluid may need to be removed initially for the gel-like properties to be recognized. This can be accomplished in any number of ways. The simplest method is to allow time for sedimentation to occur, and then pour off the remaining liquid. Centrifugation can also be used to accelerate the process of phase separation.
Removal of the remaining liquid (solvent) phase requires a drying process, which is typically accompanied by a significant amount of shrinkage and densification. The rate at which the solvent can be removed is ultimately determined by the distribution of porosity in the gel. The ultimate microstructure of the final component will clearly be strongly influenced by changes imposed upon the structural template during this phase of processing.
Afterwards, a thermal treatment, or firing process, is often necessary in order to favor further polycondensation and enhance mechanical properties and structural stability via final sintering, densification, and grain growth. One of the distinct advantages of using this methodology as opposed to the more traditional processing techniques is that densification is often achieved at a much lower temperature.
The precursor sol can be either deposited on a substrate to form a film (e.g., by dip-coating or spin coating), cast into a suitable container with the desired shape (e.g., to obtain monolithic ceramics, glasses, fibers, membranes, aerogels), or used to synthesize powders (e.g., microspheres, nanospheres). The sol–gel approach is a cheap and low-temperature technique that allows the fine control of the product's chemical composition. Even small quantities of dopants, such as organic dyes and rare-earth elements, can be introduced in the sol and end up uniformly dispersed in the final product. It can be used in ceramics processing and manufacturing as an investment casting material, or as a means of producing very thin films of metal oxides for various purposes. Sol–gel derived materials have diverse applications in optics, electronics, energy, space, (bio)sensors, medicine (e.g., controlled drug release), reactive material, and separation (e.g., chromatography) technology.
The interest in sol–gel processing can be traced back in the mid-1800s with the observation that the hydrolysis of tetraethyl orthosilicate (TEOS) under acidic conditions led to the formation of SiO2 in the form of fibers and monoliths. Sol–gel research grew to be so important that in the 1990s more than 35,000 papers were published worldwide on the process.


== Particles and polymers ==
The sol–gel process is a wet-chemical technique used for the fabrication of both glassy and ceramic materials. In this process, the sol (or solution) evolves gradually towards the formation of a gel-like network containing both a liquid phase and a solid phase. Typical precursors are metal alkoxides and metal chlorides, which undergo hydrolysis and polycondensation reactions to form a colloid. The basic structure or morphology of the solid phase can range anywhere from discrete colloidal particles to continuous chain-like polymer networks.The term colloid is used primarily to describe a broad range of solid-liquid (and/or liquid-liquid) mixtures, all of which contain distinct solid (and/or liquid) particles which are dispersed to various degrees in a liquid medium. The term is specific to the size of the individual particles, which are larger than atomic dimensions but small enough to exhibit Brownian motion. If the particles are large enough, then their dynamic behavior in any given period of time in suspension would be governed by forces of gravity and sedimentation. But if they are small enough to be colloids, then their irregular motion in suspension can be attributed to the collective bombardment of a myriad of thermally agitated molecules in the liquid suspending medium, as described originally by Albert Einstein in his dissertation. Einstein concluded that this erratic behavior could adequately be described using the theory of Brownian motion, with sedimentation being a possible long-term result. This critical size range (or particle diameter) typically ranges from tens of angstroms (10−10 m) to a few micrometres (10−6 m).
Under certain chemical conditions (typically in base-catalyzed sols), the particles may grow to sufficient size to become colloids, which are affected both by sedimentation and forces of gravity. Stabilized suspensions of such sub-micrometre spherical particles may eventually result in their self-assembly—yielding highly ordered microstructures reminiscent of the prototype colloidal crystal: precious opal.
Under certain chemical conditions (typically in acid-catalyzed sols), the interparticle forces have sufficient strength to cause considerable aggregation and/or flocculation prior to their growth. The formation of a more open continuous network of low density polymers exhibits certain advantages with regard to physical properties in the formation of high performance glass and glass/ceramic components in 2 and 3 dimensions.In either case (discrete particles or continuous polymer network) the sol evolves then towards the formation of an inorganic network containing a liquid phase (gel). Formation of a metal oxide involves connecting the metal centers with oxo (M-O-M) or hydroxo (M-OH-M) bridges, therefore generating metal-oxo or metal-hydroxo polymers in solution.
In both cases (discrete particles or continuous polymer network), the drying process serves to remove the liquid phase from the gel, yielding a micro-porous amorphous glass or micro-crystalline ceramic. Subsequent thermal treatment (firing) may be performed in order to favor further polycondensation and enhance mechanical properties.
With the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. In addition, uniform ceramic powders of a wide range of chemical composition can be formed by precipitation.


== Polymerization ==

The Stöber process is a well-studied example of polymerization of an alkoxide, specifically TEOS. The chemical formula for TEOS is given by Si(OC2H5)4, or Si(OR)4, where the alkyl group R = C2H5. Alkoxides are ideal chemical precursors for sol–gel synthesis because they react readily with water. The reaction is called hydrolysis, because a hydroxyl ion becomes attached to the silicon atom as follows:

Si(OR)4 + H2O → HO−Si(OR)3 + R−OHDepending on the amount of water and catalyst present, hydrolysis may proceed to completion to silica:

Si(OR)4 + 2 H2O → SiO2 + 4 R−OHComplete hydrolysis often requires an excess of water and/or the use of a hydrolysis catalyst such as acetic acid or hydrochloric acid. Intermediate species including [(OR)2−Si−(OH)2] or [(OR)3−Si−(OH)] may result as products of partial hydrolysis reactions. Early intermediates result from two partially hydrolyzed monomers linked with a siloxane [Si−O−Si] bond:

(OR)3−Si−OH + HO−Si−(OR)3 → [(OR)3Si−O−Si(OR)3] + H−O−Hor

(OR)3−Si−OR + HO−Si−(OR)3 → [(OR)3Si−O−Si(OR)3] + R−OHThus, polymerization is associated with the formation of a 1-, 2-, or 3-dimensional network of siloxane [Si−O−Si] bonds accompanied by the production of H−O−H and R−O−H species.
By definition, condensation liberates a small molecule, such as water or alcohol. This type of reaction can continue to build larger and larger silicon-containing molecules by the process of polymerization. Thus, a polymer is a huge molecule (or macromolecule) formed from hundreds or thousands of units called monomers. The number of bonds that a monomer can form is called its functionality. Polymerization of silicon alkoxide, for instance, can lead to complex branching of the polymer, because a fully hydrolyzed monomer Si(OH)4 is tetrafunctional (can branch or bond in 4 different directions). Alternatively, under certain conditions (e.g., low water concentration) fewer than 4 of the OR or OH groups (ligands) will be capable of condensation, so relatively little branching will occur. The mechanisms of hydrolysis and condensation, and the factors that bias the structure toward linear or branched structures are the most critical issues of sol–gel science and technology. This reaction is favored in both basic and acidic conditions.


== Sono-Ormosil ==
Sonication is an efficient tool for the synthesis of polymers. The cavitational shear forces, which stretch out and break the chain in a non-random process, result in a lowering of the molecular weight and poly-dispersity. Furthermore, multi-phase systems are very efficient dispersed and emulsified, so that very fine mixtures are provided. This means that ultrasound increases the rate of polymerisation over conventional stirring and results in higher molecular weights with lower polydispersities. Ormosils (organically modified silicate) are obtained when silane is added to gel-derived silica during sol–gel process. The product is a molecular-scale composite with improved mechanical properties. Sono-Ormosils are characterized by a higher density than classic gels as well as an improved thermal stability. An explanation therefore might be the increased degree of polymerization.


== Pechini process ==
For single cation systems like SiO2 and TiO2, hydrolysis and condensation processes naturally give rise to homogenous compositions. For systems involving multiple cations, such as strontium titanate, SrTiO3 and other perovskite systems, the concept of steric immobilisation becomes relevant. To avoid the formation of multiple phases of binary oxides as the result of differing hydrolysis and condensation rates, the entrapment of cations in a polymer network is an effective approach, generally termed the Pechini Process. In this process, a chelating agent is used, most often citric acid, to surround aqueous cations and sterically entrap them. Subsequently, a polymer network is formed to immobilize the chelated cations in a gel or resin. This is most often achieved by poly-esterification using ethylene glycol. The resulting polymer is then combusted under oxidising conditions to remove organic content and yield a product oxide with homogeneously dispersed cations.


== Nanomaterials ==

In the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. Uncontrolled flocculation of powders due to attractive van der Waals forces can also give rise to microstructural heterogeneities.Differential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. Such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved.
In addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding heterogeneous densification.
Some pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. Differential stresses arising from heterogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.It would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. The containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. Monodisperse colloids provide this potential.Monodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. The degree of order appears to be limited by the time and space allowed for longer-range correlations to be established. Such defective polycrystalline structures would appear to be the basic elements of nanoscale materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as sintered ceramic nanomaterials.


== Applications ==
The applications for sol gel-derived products are numerous. For example, scientists have used it to produce the world's lightest materials and also some of its toughest ceramics.


=== Protective coatings ===
One of the largest application areas is thin films, which can be produced on a piece of substrate by spin coating or dip-coating. Protective and decorative coatings, and electro-optic components can be applied to glass, metal and other types of substrates with these methods. Cast into a mold, and with further drying and heat-treatment, dense ceramic or glass articles with novel properties can be formed that cannot be created by any other method. Other coating methods include spraying, electrophoresis, inkjet printing, or roll coating.


=== Thin films and fibers ===
With the viscosity of a sol adjusted into a proper range, both optical and refractory ceramic fibers can be drawn which are used for fiber optic sensors and thermal insulation, respectively. Thus, many ceramic materials, both glassy and crystalline, have found use in various forms from bulk solid-state components to high surface area forms such as thin films, coatings and fibers.


=== Nanoscale powders ===
Ultra-fine and uniform ceramic powders can be formed by precipitation. These powders of single and multiple component compositions can be produced on a nanoscale particle size for dental and biomedical applications. Composite powders have been patented for use as agrochemicals and herbicides. Powder abrasives, used in a variety of finishing operations, are made using a sol–gel type process. One of the more important applications of sol–gel processing is to carry out zeolite synthesis. Other elements (metals, metal oxides) can be easily incorporated into the final product and the silicate sol formed by this method is very stable.
Another application in research is to entrap biomolecules for sensory (biosensors) or catalytic purposes, by physically or chemically preventing them from leaching out and, in the case of protein or chemically-linked small molecules, by shielding them from the external environment yet allowing small molecules to be monitored. The major disadvantages are that the change in local environment may alter the functionality of the protein or small molecule entrapped and that the synthesis step may damage the protein. To circumvent this, various strategies have been explored, such as monomers with protein friendly leaving groups (e.g. glycerol) and the inclusion of polymers which stabilize protein (e.g. PEG).Other products fabricated with this process include various ceramic membranes for microfiltration, ultrafiltration, nanofiltration, pervaporation, and reverse osmosis. If the liquid in a wet gel is removed under a supercritical condition, a highly porous and extremely low density material called aerogel is obtained. Drying the gel by means of low temperature treatments (25-100 °C), it is possible to obtain porous solid matrices called xerogels. In addition, a sol–gel process was developed in the 1950s for the production of radioactive powders of UO2 and ThO2 for nuclear fuels, without generation of large quantities of dust.


=== Opto-mechanical ===
Macroscopic optical elements and active optical components as well as large area hot mirrors, cold mirrors, lenses, and beam splitters all with optimal geometry can be made quickly and at low cost via the sol–gel route. In the processing of high performance ceramic nanomaterials with superior opto-mechanical properties under adverse conditions, the size of the crystalline grains is determined largely by the size of the crystalline particles present in the raw material during the synthesis or formation of the object. Thus a reduction of the original particle size well below the wavelength of visible light (~500 nm) eliminates much of the light scattering, resulting in a translucent or even transparent material.
Furthermore, results indicate that microscopic pores in sintered ceramic nanomaterials, mainly trapped at the junctions of microcrystalline grains, cause light to scatter and prevented true transparency. it has been observed that the total volume fraction of these nanoscale pores (both intergranular and intragranular porosity) must be less than 1% for high-quality optical transmission. I.E. The density has to be 99.99% of the theoretical crystalline density.


=== Medicine ===
Unique properties of the sol–gel provide the possibility of their use for a variety of medical applications. A sol–gel processed alumina can be used as a carrier for the sustained delivery of drugs and as an established wound healer. A marked decrease in scar size was observed because of the wound healing composite including sol–gel processed alumina. A novel approach to thrombolysis treatment is possible by developing a new family of injectable composites: plasminogen activator entrapped within alumina.


== See also ==
Coacervate, small spheroidal droplet of colloidal particles in suspension
Freeze-casting
Freeze gelation
Mechanics of gelation


== References ==


== Further reading ==
Colloidal Dispersions, Russel, W. B., et al., Eds., Cambridge University Press (1989)
Glasses and the Vitreous State, Zarzycki. J., Cambridge University Press, 1991
The Sol to Gel Transition. Plinio Innocenzi. Springer Briefs in Materials. Springer. 2016.


== External links ==
International Sol–Gel Society
The Sol–Gel Gateway","pandas(index=122, _1=122, text='in materials science, the sol–gel process is a method for producing solid materials from small molecules. the method is used for the fabrication of metal oxides, especially the oxides of silicon (si) and titanium (ti). the process involves conversion of monomers into a colloidal solution (sol) that acts as the precursor for an integrated network (or gel) of either discrete particles or network polymers. typical precursors are metal alkoxides.   == stages in the process ==  in this chemical procedure, a ""sol"" (a colloidal solution) is formed that then gradually evolves towards the formation of a gel-like diphasic system containing both a liquid phase and solid phase whose morphologies range from discrete particles to continuous polymer networks. in the case of the colloid, the volume fraction of particles (or particle density) may be so low that a significant amount of fluid may need to be removed initially for the gel-like properties to be recognized. this can be accomplished in any number of ways. the simplest method is to allow time for sedimentation to occur, and then pour off the remaining liquid. centrifugation can also be used to accelerate the process of phase separation. removal of the remaining liquid (solvent) phase requires a drying process, which is typically accompanied by a significant amount of shrinkage and densification. the rate at which the solvent can be removed is ultimately determined by the distribution of porosity in the gel. the ultimate microstructure of the final component will clearly be strongly influenced by changes imposed upon the structural template during this phase of processing. afterwards, a thermal treatment, or firing process, is often necessary in order to favor further polycondensation and enhance mechanical properties and structural stability via final sintering, densification, and grain growth. one of the distinct advantages of using this methodology as opposed to the more traditional processing techniques is that densification is often achieved at a much lower temperature. the precursor sol can be either deposited on a substrate to form a film (e.g., by dip-coating or spin coating), cast into a suitable container with the desired shape (e.g., to obtain monolithic ceramics, glasses, fibers, membranes, aerogels), or used to synthesize powders (e.g., microspheres, nanospheres). the sol–gel approach is a cheap and low-temperature technique that allows the fine control of the product\'s chemical composition. even small quantities of dopants, such as organic dyes and rare-earth elements, can be introduced in the sol and end up uniformly dispersed in the final product. it can be used in ceramics processing and manufacturing as an investment casting material, or as a means of producing very thin films of metal oxides for various purposes. sol–gel derived materials have diverse applications in optics, electronics, energy, space, (bio)sensors, medicine (e.g., controlled drug release), reactive material, and separation (e.g., chromatography) technology. the interest in sol–gel processing can be traced back in the mid-1800s with the observation that the hydrolysis of tetraethyl orthosilicate (teos) under acidic conditions led to the formation of sio2 in the form of fibers and monoliths. sol–gel research grew to be so important that in the 1990s more than 35,000 papers were published worldwide on the process.   == particles and polymers == the sol–gel process is a wet-chemical technique used for the fabrication of both glassy and ceramic materials. in this process, the sol (or solution) evolves gradually towards the formation of a gel-like network containing both a liquid phase and a solid phase. typical precursors are metal alkoxides and metal chlorides, which undergo hydrolysis and polycondensation reactions to form a colloid. the basic structure or morphology of the solid phase can range anywhere from discrete colloidal particles to continuous chain-like polymer networks.the term colloid is used primarily to describe a broad range of solid-liquid (and/or liquid-liquid) mixtures, all of which contain distinct solid (and/or liquid) particles which are dispersed to various degrees in a liquid medium. the term is specific to the size of the individual particles, which are larger than atomic dimensions but small enough to exhibit brownian motion. if the particles are large enough, then their dynamic behavior in any given period of time in suspension would be governed by forces of gravity and sedimentation. but if they are small enough to be colloids, then their irregular motion in suspension can be attributed to the collective bombardment of a myriad of thermally agitated molecules in the liquid suspending medium, as described originally by albert einstein in his dissertation. einstein concluded that this erratic behavior could adequately be described using the theory of brownian motion, with sedimentation being a possible long-term result. this critical size range (or particle diameter) typically ranges from tens of angstroms (10−10 m) to a few micrometres (10−6 m). under certain chemical conditions (typically in base-catalyzed sols), the particles may grow to sufficient size to become colloids, which are affected both by sedimentation and forces of gravity. stabilized suspensions of such sub-micrometre spherical particles may eventually result in their self-assembly—yielding highly ordered microstructures reminiscent of the prototype colloidal crystal: precious opal. under certain chemical conditions (typically in acid-catalyzed sols), the interparticle forces have sufficient strength to cause considerable aggregation and/or flocculation prior to their growth. the formation of a more open continuous network of low density polymers exhibits certain advantages with regard to physical properties in the formation of high performance glass and glass/ceramic components in 2 and 3 dimensions.in either case (discrete particles or continuous polymer network) the sol evolves then towards the formation of an inorganic network containing a liquid phase (gel). formation of a metal oxide involves connecting the metal centers with oxo (m-o-m) or hydroxo (m-oh-m) bridges, therefore generating metal-oxo or metal-hydroxo polymers in solution. in both cases (discrete particles or continuous polymer network), the drying process serves to remove the liquid phase from the gel, yielding a micro-porous amorphous glass or micro-crystalline ceramic. subsequent thermal treatment (firing) may be performed in order to favor further polycondensation and enhance mechanical properties. with the viscosity of a sol adjusted into a proper range, both optical quality glass fiber and refractory ceramic fiber can be drawn which are used for fiber optic sensors and thermal insulation, respectively. in addition, uniform ceramic powders of a wide range of chemical composition can be formed by precipitation.   == polymerization ==  the stöber process is a well-studied example of polymerization of an alkoxide, specifically teos. the chemical formula for teos is given by si(oc2h5)4, or si(or)4, where the alkyl group r = c2h5. alkoxides are ideal chemical precursors for sol–gel synthesis because they react readily with water. the reaction is called hydrolysis, because a hydroxyl ion becomes attached to the silicon atom as follows:  si(or)4h2o → ho−si(or)3r−ohdepending on the amount of water and catalyst present, hydrolysis may proceed to completion to silica:  si(or)42 h2o → sio24 r−ohcomplete hydrolysis often requires an excess of water and/or the use of a hydrolysis catalyst such as acetic acid or hydrochloric acid. intermediate species including [(or)2−si−(oh)2] or [(or)3−si−(oh)] may result as products of partial hydrolysis reactions. early intermediates result from two partially hydrolyzed monomers linked with a siloxane [si−o−si] bond:  (or)3−si−ohho−si−(or)3 → [(or)3si−o−si(or)3]h−o−hor  (or)3−si−orho−si−(or)3 → [(or)3si−o−si(or)3]r−ohthus, polymerization is associated with the formation of a 1-, 2-, or 3-dimensional network of siloxane [si−o−si] bonds accompanied by the production of h−o−h and r−o−h species. by definition, condensation liberates a small molecule, such as water or alcohol. this type of reaction can continue to build larger and larger silicon-containing molecules by the process of polymerization. thus, a polymer is a huge molecule (or macromolecule) formed from hundreds or thousands of units called monomers. the number of bonds that a monomer can form is called its functionality. polymerization of silicon alkoxide, for instance, can lead to complex branching of the polymer, because a fully hydrolyzed monomer si(oh)4 is tetrafunctional (can branch or bond in 4 different directions). alternatively, under certain conditions (e.g., low water concentration) fewer than 4 of the or or oh groups (ligands) will be capable of condensation, so relatively little branching will occur. the mechanisms of hydrolysis and condensation, and the factors that bias the structure toward linear or branched structures are the most critical issues of sol–gel science and technology. this reaction is favored in both basic and acidic conditions.   == sono-ormosil == sonication is an efficient tool for the synthesis of polymers. the cavitational shear forces, which stretch out and break the chain in a non-random process, result in a lowering of the molecular weight and poly-dispersity. furthermore, multi-phase systems are very efficient dispersed and emulsified, so that very fine mixtures are provided. this means that ultrasound increases the rate of polymerisation over conventional stirring and results in higher molecular weights with lower polydispersities. ormosils (organically modified silicate) are obtained when silane is added to gel-derived silica during sol–gel process. the product is a molecular-scale composite with improved mechanical properties. sono-ormosils are characterized by a higher density than classic gels as well as an improved thermal stability. an explanation therefore might be the increased degree of polymerization.   == pechini process == for single cation systems like sio2 and tio2, hydrolysis and condensation processes naturally give rise to homogenous compositions. for systems involving multiple cations, such as strontium titanate, srtio3 and other perovskite systems, the concept of steric immobilisation becomes relevant. to avoid the formation of multiple phases of binary oxides as the result of differing hydrolysis and condensation rates, the entrapment of cations in a polymer network is an effective approach, generally termed the pechini process. in this process, a chelating agent is used, most often citric acid, to surround aqueous cations and sterically entrap them. subsequently, a polymer network is formed to immobilize the chelated cations in a gel or resin. this is most often achieved by poly-esterification using ethylene glycol. the resulting polymer is then combusted under oxidising conditions to remove organic content and yield a product oxide with homogeneously dispersed cations.   == nanomaterials ==  in the processing of fine ceramics, the irregular particle sizes and shapes in a typical powder often lead to non-uniform packing morphologies that result in packing density variations in the powder compact. uncontrolled flocculation of powders due to attractive van der waals forces can also give rise to microstructural heterogeneities.differential stresses that develop as a result of non-uniform drying shrinkage are directly related to the rate at which the solvent can be removed, and thus highly dependent upon the distribution of porosity. such stresses have been associated with a plastic-to-brittle transition in consolidated bodies, and can yield to crack propagation in the unfired body if not relieved. in addition, any fluctuations in packing density in the compact as it is prepared for the kiln are often amplified during the sintering process, yielding heterogeneous densification. some pores and other structural defects associated with density variations have been shown to play a detrimental role in the sintering process by growing and thus limiting end-point densities. differential stresses arising from heterogeneous densification have also been shown to result in the propagation of internal cracks, thus becoming the strength-controlling flaws.it would therefore appear desirable to process a material in such a way that it is physically uniform with regard to the distribution of components and porosity, rather than using particle size distributions which will maximize the green density. the containment of a uniformly dispersed assembly of strongly interacting particles in suspension requires total control over particle-particle interactions. monodisperse colloids provide this potential.monodisperse powders of colloidal silica, for example, may therefore be stabilized sufficiently to ensure a high degree of order in the colloidal crystal or polycrystalline colloidal solid which results from aggregation. the degree of order appears to be limited by the time and space allowed for longer-range correlations to be established. such defective polycrystalline structures would appear to be the basic elements of nanoscale materials science, and, therefore, provide the first step in developing a more rigorous understanding of the mechanisms involved in microstructural evolution in inorganic systems such as sintered ceramic nanomaterials.   == applications == the applications for sol gel-derived products are numerous. for example, scientists have used it to produce the world\'s lightest materials and also some of its toughest ceramics. unique properties of the sol–gel provide the possibility of their use for a variety of medical applications. a sol–gel processed alumina can be used as a carrier for the sustained delivery of drugs and as an established wound healer. a marked decrease in scar size was observed because of the wound healing composite including sol–gel processed alumina. a novel approach to thrombolysis treatment is possible by developing a new family of injectable composites: plasminogen activator entrapped within alumina.   == see also == coacervate, small spheroidal droplet of colloidal particles in suspension freeze-casting freeze gelation mechanics of gelation   == references ==   == further reading == colloidal dispersions, russel, w. b., et al., eds., cambridge university press (1989) glasses and the vitreous state, zarzycki. j., cambridge university press, 1991 the sol to gel transition. plinio innocenzi. springer briefs in materials. springer. 2016.   == external links == international sol–gel society the sol–gel gateway')"
123,"Chemical engineering is a certain type of engineering which deals with the study of operation and design of chemical plants as well as methods of improving production. Chemical engineers develop economical commercial processes to convert raw material into useful products. Chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. The work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. Chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering,  construction specification, and operating instructions.
Chemical engineers typically hold a degree in Chemical Engineering or Process Engineering. Practicing engineers may have professional certification and be accredited members of a professional body. Such bodies include the Institution of Chemical Engineers (IChemE) or the American Institute of Chemical Engineers (AIChE).
A degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.


== Etymology ==

A 1996 British Journal for the History of Science article cites James F. Donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. In the same paper, however, George E. Davis, an English consultant, was credited with having coined the term. Davis also tried to found a Society of Chemical Engineering, but instead it was named the Society of Chemical Industry (1881), with Davis as its first secretary. The History of Science in United States: An Encyclopedia puts the use of the term around 1890. ""Chemical engineering"", describing the use of mechanical equipment in the chemical industry, became common vocabulary in England after 1850. By 1910, the profession, ""chemical engineer,"" was already in common use in Britain and the United States.


== History ==
 
Chemical engineering emerged upon the development make unit operations considered essential to the discipline.


=== New concepts and innovations ===

In 1940s, it became clear that unit operations alone were insufficient in developing chemical reactors. While the predominance of unit operations in chemical engineering courses in Britain and the United States continued until the 1960s, transport phenomena started to experience greater focus. Along with other novel concepts, such as process systems engineering (PSE), a ""second paradigm"" was defined. Transport phenomena gave an analytical approach to chemical engineering while PSE focused on its synthetic elements, such as control system and process design. Developments in chemical engineering before and after World War II were mainly incited by the petrochemical industry; however, advances in other fields were made as well. Advancements in biochemical engineering in the 1940s, for example, found application in the pharmaceutical industry, and allowed for the mass production of various antibiotics, including penicillin and streptomycin. Meanwhile, progress in polymer science in the 1950s paved way for the ""age of plastics"".


=== Safety and hazard developments ===
Concerns regarding the safety and environmental impact of large-scale chemical manufacturing facilities were also raised during this period. Silent Spring, published in 1962, alerted its readers to the harmful effects of DDT, a potent insecticide. The 1974 Flixborough disaster in the United Kingdom resulted in 28 deaths, as well as damage to a chemical plant and three nearby villages. The 1984 Bhopal disaster in India resulted in almost 4,000 deaths. These incidents, along with other incidents, affected the reputation of the trade as industrial safety and environmental protection were given more focus. In response, the IChemE required safety to be part of every degree course that it accredited after 1982. By the 1970s, legislation and monitoring agencies were instituted in various countries, such as France, Germany, and the United States.


=== Recent progress ===
Advancements in computer science found applications designing and managing plants, simplifying calculations and drawings that previously had to be done manually. The completion of the Human Genome Project is also seen as a major development, not only advancing chemical engineering but genetic engineering and genomics as well. Chemical engineering principles were used to produce DNA sequences in large quantities.


== Concepts ==
Chemical engineering involves the application of several principles. Key concepts are presented below.


=== Plant design and construction ===
Chemical engineering design concerns the creation of plans, specifications, and economic analyses for pilot plants, new plants, or plant modifications. Design engineers often work in a consulting role, designing plants to meet clients' needs. Design is limited by several factors, including funding, government regulations, and safety standards. These constraints dictate a plant's choice of process, materials, and equipment.Plant construction is coordinated by project engineers and project managers, depending on the size of the investment. A chemical engineer may do the job of project engineer full-time or part of the time, which requires additional training and job skills or act as a consultant to the project group. In the USA the education of chemical engineering graduates from the Baccalaureate programs accredited by ABET do not usually stress project engineering education, which can be obtained by specialized training, as electives, or from graduate programs. Project engineering jobs are some of the largest employers for chemical engineers.


=== Process design and analysis ===

A unit operation is a physical step in an individual chemical engineering process. Unit operations (such as crystallization, filtration, drying and evaporation) are used to prepare reactants, purifying and separating its products, recycling unspent reactants, and controlling energy transfer in reactors. On the other hand, a unit process is the chemical equivalent of a unit operation. Along with unit operations, unit processes constitute a process operation. Unit processes (such as nitration and oxidation) involve the conversion of material by biochemical, thermochemical and other means. Chemical engineers responsible for these are called process engineers.Process design requires the definition of equipment types and sizes as well as how they are connected and the materials of construction. Details are often printed on a Process Flow Diagram which is used to control the capacity and reliability of a new or existing chemical factory.
Education for chemical engineers in the first college degree 3 or 4 years of study stresses the principles and practices of process design. The same skills are used in existing chemical plants to evaluate the efficiency and make recommendations for improvements.


=== Transport phenomena ===

Modeling and analysis of transport phenomena is essential for many industrial applications. Transport phenomena involve fluid dynamics, heat transfer and mass transfer, which are governed mainly by momentum transfer, energy transfer and transport of chemical species, respectively. Models often involve separate considerations for macroscopic, microscopic and molecular level phenomena. Modeling of transport phenomena, therefore, requires an understanding of applied mathematics.


== Applications and practice ==

Chemical engineers ""develop economic ways of using materials and energy"". Chemical engineers use chemistry and engineering to turn raw materials into usable products, such as medicine, petrochemicals, and plastics on a large-scale, industrial setting. They are also involved in waste management and research. Both applied and research facets could make extensive use of computers.Chemical engineers may be involved in industry or university research where they are tasked with designing and performing experiments to create better and safer methods for production, pollution control, and resource conservation. They may be involved in designing and constructing plants as a project engineer. Chemical engineers serving as project engineers use their knowledge in selecting optimal production methods and plant equipment to minimize costs and maximize safety and profitability. After plant construction, chemical engineering project managers may be involved in equipment upgrades, troubleshooting, and daily operations in either full-time or consulting roles. 


== See also ==


=== Related topics ===


=== Related fields and concepts ===


=== Associations ===


== References ==


== Bibliography ==","pandas(index=123, _1=123, text='chemical engineering is a certain type of engineering which deals with the study of operation and design of chemical plants as well as methods of improving production. chemical engineers develop economical commercial processes to convert raw material into useful products. chemical engineering uses principles of chemistry, physics, mathematics, biology, and economics to efficiently use, produce, design, transport and transform energy and materials. the work of chemical engineers can range from the utilization of nanotechnology and nanomaterials in the laboratory to large-scale industrial processes that convert chemicals, raw materials, living cells, microorganisms, and energy into useful forms and products. chemical engineers are involved in many aspects of plant design and operation, including safety and hazard assessments, process design and analysis, modeling, control engineering, chemical reaction engineering, nuclear engineering, biological engineering,  construction specification, and operating instructions. chemical engineers typically hold a degree in chemical engineering or process engineering. practicing engineers may have professional certification and be accredited members of a professional body. such bodies include the institution of chemical engineers (icheme) or the american institute of chemical engineers (aiche). a degree in chemical engineering is directly linked with all of the other engineering disciplines, to various extents.   == etymology ==  a 1996 british journal for the history of science article cites james f. donnelly for mentioning an 1839 reference to chemical engineering in relation to the production of sulfuric acid. in the same paper, however, george e. davis, an english consultant, was credited with having coined the term. davis also tried to found a society of chemical engineering, but instead it was named the society of chemical industry (1881), with davis as its first secretary. the history of science in united states: an encyclopedia puts the use of the term around 1890. ""chemical engineering"", describing the use of mechanical equipment in the chemical industry, became common vocabulary in england after 1850. by 1910, the profession, ""chemical engineer,"" was already in common use in britain and the united states.   == history ==  chemical engineering emerged upon the development make unit operations considered essential to the discipline. == references ==   == bibliography ==')"
124,"In engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. The conversion of units from one dimensional unit to another is often easier within the metric or SI system than in others, due to the regular 10-base in all units. Dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.Commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. Incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. For example, asking whether a kilogram is larger than an hour is meaningless.
Any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. Checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. It also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation.
The concept of physical dimension, and of dimensional analysis, was introduced by Joseph Fourier in 1822.


== Concrete numbers and base units ==
Many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. Often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. Compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  Other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof.
A set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. For example, units for length and time are normally chosen as base units. Units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units.
Sometimes the names of units obscure the fact that they are derived units. For example, a newton (N) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). The newton is defined as 1 N = 1 kg⋅m⋅s−2.


=== Percentages and derivatives ===
Percentages are dimensionless quantities, since they are ratios of two quantities with the same dimensions. In other words, the % sign can be read as ""hundredths"", since 1% = 1/100.
Taking a derivative with respect to a quantity adds the dimension of the variable one is differentiating with respect to, in the denominator. Thus:

position (x) has the dimension L (length);
derivative of position with respect to time (dx/dt, velocity) has dimension LT−1—length from position, time due to the gradient;
the second derivative (d2x/dt2 = d(dx/dt) / dt, acceleration) has dimension LT−2.In economics, one distinguishes between stocks and flows: a stock has units of ""units"" (say, widgets or dollars), while a flow is a derivative of a stock, and has units of ""units/time"" (say, dollars/year).
In some contexts, dimensional quantities are expressed as dimensionless quantities or percentages by omitting some dimensions. For example, debt-to-GDP ratios are generally expressed as percentages: total debt outstanding (dimension of currency) divided by annual GDP (dimension of currency)—but one may argue that, in comparing a stock to a flow, annual GDP should have dimensions of currency/time (dollars/year, for instance) and thus Debt-to-GDP should have units of years, which indicates that Debt-to-GDP is the number of years needed for a constant GDP to pay the debt, if all GDP is spent on the debt and the debt is otherwise unchanged.


== Conversion factor ==

In dimensional analysis, a ratio which converts one unit of measure into another without changing the quantity is called a conversion factor. For example, kPa and bar are both units of pressure, and 100 kPa = 1 bar. The rules of algebra allow both sides of an equation to be divided by the same expression, so this is equivalent to 100 kPa / 1 bar = 1.  Since any quantity can be multiplied by 1 without changing it, the expression ""100 kPa / 1 bar"" can be used to convert from bars to kPa by multiplying it with the quantity to be converted, including units. For example, 5 bar × 100 kPa / 1 bar = 500 kPa because 5 × 100 / 1 = 500, and bar/bar cancels out, so 5 bar = 500 kPa.


== Dimensional homogeneity ==

The most basic rule of dimensional analysis is that of dimensional homogeneity.
Only commensurable quantities (physical quantities having the same dimension) may be compared, equated, added, or subtracted.However, the dimensions form an abelian group under multiplication, so:

One may take ratios of incommensurable quantities (quantities with different dimensions), and multiply or divide them.For example, it makes no sense to ask whether 1 hour is more, the same, or less than 1 kilometre, as these have different dimensions, nor to add 1 hour to 1 kilometre. However, it makes perfect sense to ask whether 1 mile is more, the same, or less than 1 kilometre being the same dimension of physical quantity even though the units are different. On the other hand, if an object travels 100 km in 2 hours, one may divide these and conclude that the object's average speed was 50 km/h.
The rule implies that in a physically meaningful expression only quantities of the same dimension can be added, subtracted, or compared. For example, if mman, mrat and Lman denote, respectively, the mass of some man, the mass of a rat and the length of that man, the dimensionally homogeneous expression mman + mrat is meaningful, but the heterogeneous expression mman + Lman is meaningless. However, mman/L2man is fine. Thus, dimensional analysis may be used as a sanity check of physical equations: the two sides of any equation must be commensurable or have the same dimensions.
This has the implication that most mathematical functions, particularly the transcendental functions, must have a dimensionless quantity, a pure number, as the argument and must return a dimensionless number as a result. This is clear because many transcendental functions can be expressed as an infinite power series with dimensionless coefficients.

  
    
      
        f
        (
        x
        )
        =
        
          ∑
          
            n
            =
            0
          
          
            ∞
          
        
        
          a
          
            n
          
        
        
          x
          
            n
          
        
        =
        
          a
          
            0
          
        
        +
        
          a
          
            1
          
        
        x
        +
        
          a
          
            2
          
        
        
          x
          
            2
          
        
        +
        
          a
          
            3
          
        
        
          x
          
            3
          
        
        +
        ⋯
      
    
    {\displaystyle f(x)=\sum _{n=0}^{\infty }a_{n}x^{n}=a_{0}+a_{1}x+a_{2}x^{2}+a_{3}x^{3}+\cdots }
  All powers of x must have the same dimension for the terms to be commensurable. But if x is not dimensionless, then the different powers of x will have different, incommensurable dimensions. However, power functions including root functions may have a dimensional argument and will return a result having dimension that is the same power applied to the argument dimension. This is because power functions and root functions are, loosely, just an expression of multiplication of quantities.
Even when two physical quantities have identical dimensions, it may nevertheless be meaningless to compare or add them. For example, although torque and energy share the dimension L2MT−2, they are fundamentally different physical quantities.
To compare, add, or subtract quantities with the same dimensions but expressed in different units, the standard procedure is first to convert them all to the same units. For example, to compare 32 metres with 35 yards, use 1 yard = 0.9144 m to convert 35 yards to 32.004 m.
A related principle is that any physical law that accurately describes the real world must be independent of the units used to measure the physical variables. For example, Newton's laws of motion must hold true whether distance is measured in miles or kilometres. This principle gives rise to the form that conversion factors must take between units that measure the same dimension: multiplication by a simple constant. It also ensures equivalence; for example, if two buildings are the same height in feet, then they must be the same height in metres.


== The factor-label method for converting units ==
The factor-label method is the sequential application of conversion factors expressed as fractions and arranged so that any dimensional unit appearing in both the numerator and denominator of any of the fractions can be cancelled out until only the desired set of dimensional units is obtained. For example, 10 miles per hour can be converted to meters per second by using a sequence of conversion factors as shown below:

  
    
      
        
          
            
              10
               
              
                
                  mile
                
              
            
            
              1
               
              
                
                  hour
                
              
            
          
        
        ×
        
          
            
              1609.344
              
                 meter
              
            
            
              1
               
              
                
                  mile
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  hour
                
              
            
            
              3600
              
                 second
              
            
          
        
        =
        4.4704
         
        
          
            meter
            second
          
        
        .
      
    
    {\displaystyle {\frac {10\ {\cancel {\text{mile}}}}{1\ {\cancel {\text{hour}}}}}\times {\frac {1609.344{\text{ meter}}}{1\ {\cancel {\text{mile}}}}}\times {\frac {1\ {\cancel {\text{hour}}}}{3600{\text{ second}}}}=4.4704\ {\frac {\text{meter}}{\text{second}}}.}
  Each conversion factor is chosen based on the relationship between one of the original units and one of the desired units (or some intermediary unit), before being re-arranged to create a factor that cancels out the original unit. For example, as ""mile"" is the numerator in the original fraction and 
  
    
      
        1
         
        
          mile
        
        =
        1609.344
         
        
          meter
        
      
    
    {\displaystyle 1\ {\text{mile}}=1609.344\ {\text{meter}}}
  , ""mile"" will need to be the denominator in the conversion factor. Dividing both sides of the equation by 1 mile yields 
  
    
      
        
          
            
              1
               
              
                mile
              
            
            
              1
               
              
                mile
              
            
          
        
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle {\frac {1\ {\text{mile}}}{1\ {\text{mile}}}}={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  , which when simplified results in the dimensionless 
  
    
      
        1
        =
        
          
            
              1609.344
               
              
                meter
              
            
            
              1
               
              
                mile
              
            
          
        
      
    
    {\displaystyle 1={\frac {1609.344\ {\text{meter}}}{1\ {\text{mile}}}}}
  . Multiplying any quantity (physical quantity or not) by the dimensionless 1 does not change that quantity. Once this and the conversion factor for seconds per hour have been multiplied by the original fraction to cancel out the units mile and hour, 10 miles per hour converts to 4.4704 meters per second.
As a more complex example, the concentration of nitrogen oxides (i.e., 
  
    
      
        
          
            
              NO
            
            
              x
            
          
        
      
    
    {\displaystyle \color {Blue}{\ce {NO}}_{x}}
  ) in the flue gas from an industrial furnace can be converted to a mass flow rate expressed in grams per hour (i.e., g/h) of 
  
    
      
        
          
            NO
          
          
            x
          
        
      
    
    {\displaystyle {\ce {NO}}_{x}}
   by using the following information as shown below:

NOx concentration
= 10 parts per million by volume = 10 ppmv = 10 volumes/106 volumes
NOx molar mass
= 46 kg/kmol = 46 g/mol
Flow rate of flue gas
= 20 cubic meters per minute = 20 m3/min
The flue gas exits the furnace at 0 °C temperature and 101.325 kPa absolute pressure.
The molar volume of a gas at 0 °C temperature and 101.325 kPa is 22.414 m3/kmol.
  
    
      
        
          
            
              1000
               
              
                
                  g
                   
                  NO
                
                
                  x
                
              
            
            
              1
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              46
               
              
                
                  
                    
                      kg
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              1
               
              
                
                  
                    
                      kmol
                       
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              22.414
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
          
        
        ×
        
          
            
              10
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    
                      NO
                    
                    
                      x
                    
                  
                
              
            
            
              
                10
                
                  6
                
              
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
          
        
        ×
        
          
            
              20
               
              
                
                  
                    
                      m
                    
                    
                      3
                    
                  
                   
                  
                    gas
                  
                
              
            
            
              1
               
              
                
                  minute
                
              
            
          
        
        ×
        
          
            
              60
               
              
                
                  minute
                
              
            
            
              1
               
              
                hour
              
            
          
        
        =
        24.63
         
        
          
            
              
                g
                 
                NO
              
              
                x
              
            
            hour
          
        
      
    
    {\displaystyle {\frac {1000\ {\ce {g\ NO}}_{x}}{1{\cancel {{\ce {kg\ NO}}_{x}}}}}\times {\frac {46\ {\cancel {{\ce {kg\ NO}}_{x}}}}{1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}}\times {\frac {1\ {\cancel {{\ce {kmol\ NO}}_{x}}}}{22.414\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}}\times {\frac {10\ {\cancel {{\ce {m}}^{3}\ {\ce {NO}}_{x}}}}{10^{6}\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}}\times {\frac {20\ {\cancel {{\ce {m}}^{3}\ {\ce {gas}}}}}{1\ {\cancel {\ce {minute}}}}}\times {\frac {60\ {\cancel {\ce {minute}}}}{1\ {\ce {hour}}}}=24.63\ {\frac {{\ce {g\ NO}}_{x}}{\ce {hour}}}}
  After canceling out any dimensional units that appear both in the numerators and denominators of the fractions in the above equation, the NOx concentration of 10 ppmv converts to mass flow rate of 24.63 grams per hour.


=== Checking equations that involve dimensions ===
The factor-label method can also be used on any mathematical equation to check whether or not the dimensional units on the left hand side of the equation are the same as the dimensional units on the right hand side of the equation. Having the same units on both sides of an equation does not ensure that the equation is correct, but having different units on the two sides (when expressed in terms of base units) of an equation implies that the equation is wrong.
For example, check the Universal Gas Law equation of PV = nRT, when:

the pressure P is in pascals (Pa)
the volume V is in cubic meters (m3)
the amount of substance n is in moles (mol)
the universal gas law constant R is 8.3145 Pa⋅m3/(mol⋅K)
the temperature T is in kelvins (K)
  
    
      
        
          Pa
          ⋅
          
            m
            
              3
            
          
        
        =
        
          
            
              
                mol
              
            
            1
          
        
        ×
        
          
            
              Pa
              ⋅
              
                m
                
                  3
                
              
            
            
              
                
                  
                    mol
                  
                
              
               
              
                
                  
                    K
                  
                
              
            
          
        
        ×
        
          
            
              
                K
              
            
            1
          
        
      
    
    {\displaystyle {\ce {Pa.m^3}}={\frac {\cancel {{\ce {mol}}}}{1}}\times {\frac {{\ce {Pa.m^3}}}{{\cancel {{\ce {mol}}}}\ {\cancel {{\ce {K}}}}}}\times {\frac {\cancel {{\ce {K}}}}{1}}}
  As can be seen, when the dimensional units appearing in the numerator and denominator of the equation's right hand side are cancelled out, both sides of the equation have the same dimensional units. Dimensional analysis can be used as a tool to construct equations that relate non-associated physico-chemical properties. The equations may reveal hitherto unknown or overlooked properties of matter, in the form of left-over dimensions — dimensional  adjusters — that can then be assigned physical significance. It is important to point out that such ‘mathematical manipulation’ is   neither without prior precedent, nor without considerable scientific significance. Indeed, the Planck's constant, a fundamental constant of the universe, was ‘discovered’ as a purely mathematical abstraction or representation that built on the Rayleigh-Jeans Equation for preventing the ultraviolet catastrophe. It was assigned and ascended to its quantum physical significance either in tandem or post mathematical dimensional adjustment – not earlier.


=== Limitations ===
The factor-label method can convert only unit quantities for which the units are in a linear relationship intersecting at 0. (Ratio scale in Stevens's typology) Most units fit this paradigm. An example for which it cannot be used is the conversion between degrees Celsius and kelvins (or degrees Fahrenheit). Between degrees Celsius and kelvins, there is a constant difference rather than a constant ratio, while between degrees Celsius and degrees Fahrenheit there is neither a constant difference nor a constant ratio. There is, however, an affine transform (
  
    
      
        x
        ↦
        a
        x
        +
        b
      
    
    {\displaystyle x\mapsto ax+b}
  , rather than a linear transform 
  
    
      
        x
        ↦
        a
        x
      
    
    {\displaystyle x\mapsto ax}
  ) between them.
For example, the freezing point of water is 0 °C and 32 °F, and a 5 °C change is the same as a 9 °F change. Thus, to convert from units of Fahrenheit to units of Celsius, one subtracts 32 °F (the offset from the point of reference), divides by 9 °F and multiplies by 5 °C (scales by the ratio of units), and adds 0 °C (the offset from the point of reference). Reversing this yields the formula for obtaining a quantity in units of Celsius from units of Fahrenheit; one could have started with the equivalence between 100 °C and 212 °F, though this would yield the same formula at the end.
Hence, to convert the numerical quantity value of a temperature T[F] in degrees Fahrenheit to a numerical quantity value T[C] in degrees Celsius, this formula may be used:

T[C] = (T[F] − 32) × 5/9.To convert T[C] in degrees Celsius to T[F] in degrees Fahrenheit, this formula may be used:

T[F] = (T[C] × 9/5) + 32.


== Applications ==
Dimensional analysis is most often used in physics and chemistry – and in the mathematics thereof – but finds some applications outside of those fields as well.


=== Mathematics ===
A simple application of dimensional analysis to mathematics is in computing the form of the volume of an n-ball (the solid ball in n dimensions), or the area of its surface, the n-sphere: being an n-dimensional figure, the volume scales as 
  
    
      
        
          x
          
            n
          
        
        ,
      
    
    {\displaystyle x^{n},}
   while the surface area, being 
  
    
      
        (
        n
        −
        1
        )
      
    
    {\displaystyle (n-1)}
  -dimensional, scales as 
  
    
      
        
          x
          
            n
            −
            1
          
        
        .
      
    
    {\displaystyle x^{n-1}.}
   Thus the volume of the n-ball in terms of the radius is 
  
    
      
        
          C
          
            n
          
        
        
          r
          
            n
          
        
        ,
      
    
    {\displaystyle C_{n}r^{n},}
   for some constant 
  
    
      
        
          C
          
            n
          
        
        .
      
    
    {\displaystyle C_{n}.}
   Determining the constant takes more involved mathematics, but the form can be deduced and checked by dimensional analysis alone.


=== Finance, economics, and accounting ===
In finance, economics, and accounting, dimensional analysis is most commonly referred to in terms of the distinction between stocks and flows. More generally, dimensional analysis is used in interpreting various financial ratios, economics ratios, and accounting ratios.

For example, the P/E ratio has dimensions of time (units of years), and can be interpreted as ""years of earnings to earn the price paid"".
In economics, debt-to-GDP ratio also has units of years (debt has units of currency, GDP has units of currency/year).
In financial analysis, some bond duration types also have dimension of time (unit of years) and can be interpreted as ”years to balance point between interest payments and nominal repayment”.
Velocity of money has units of 1/years (GDP/money supply has units of currency/year over currency): how often a unit of currency circulates per year.
Interest rates are often expressed as a percentage, but more properly percent per annum, which has dimensions of 1/years.


=== Fluid mechanics ===
In fluid mechanics, dimensional analysis is performed in order to obtain dimensionless pi terms or groups. According to the principles of dimensional analysis, any prototype can be described by a series of these terms or groups that describe the behaviour of the system. Using suitable pi terms or groups, it is possible to develop a similar set of pi terms for a model that has the same dimensional relationships. In other words, pi terms provide a shortcut to developing a model representing a certain prototype. Common dimensionless groups in fluid mechanics include:

Reynolds number (Re), generally important in all types of fluid problems:

  
    
      
        
          R
          e
        
        =
        
          
            
              ρ
              
              u
              d
            
            μ
          
        
      
    
    {\displaystyle \mathrm {Re} ={\frac {\rho \,ud}{\mu }}}
  .
Froude number (Fr), modeling flow with a free surface:

  
    
      
        
          F
          r
        
        =
        
          
            u
            
              g
              
              L
            
          
        
        .
      
    
    {\displaystyle \mathrm {Fr} ={\frac {u}{\sqrt {g\,L}}}.}
  
Euler number (Eu), used in problems in which pressure is of interest:

  
    
      
        
          E
          u
        
        =
        
          
            
              Δ
              p
            
            
              ρ
              
                u
                
                  2
                
              
            
          
        
        .
      
    
    {\displaystyle \mathrm {Eu} ={\frac {\Delta p}{\rho u^{2}}}.}
  
Mach number (Ma), important in high speed flows where the velocity approaches or exceeds the local speed of sound:

  
    
      
        
          M
          a
        
        =
        
          
            u
            c
          
        
        ,
      
    
    {\displaystyle \mathrm {Ma} ={\frac {u}{c}},}
   where: c is the local speed of sound.


== History ==
The origins of dimensional analysis have been disputed by historians.The first written application of dimensional analysis has been credited to an article of François Daviet at the Turin Academy of Science. Daviet had the master Lagrange as teacher. 
His fundamental works are contained in acta of the Academy dated 1799.This led to the conclusion that meaningful laws must be homogeneous equations in their various units of measurement, a result which was eventually later formalized in the Buckingham π theorem.
Simeon Poisson also treated the same problem of the parallelogram law by Daviet, in his treatise of 1811 and 1833 (vol I, p.39). In the second edition of 1833, Poisson explicitly introduces the term dimension instead of the Daviet homogeneity.
In 1822, the important Napoleonic scientist Joseph Fourier made the first credited important contributions based on the idea that physical laws like F = ma should be independent of the units employed to measure the physical variables.
Maxwell played a major role in establishing modern use of dimensional analysis by distinguishing mass, length, and time as fundamental units, while referring to other units as derived. Although Maxwell defined length, time and mass to be ""the three fundamental units"", he also noted that gravitational mass can be derived from length and time by assuming a form of Newton's law of universal gravitation in which the gravitational constant G is taken as unity, thereby defining M = L3T−2. By assuming a form of Coulomb's law in which Coulomb's constant ke is taken as unity, Maxwell then determined that the dimensions of an electrostatic unit of charge were Q = L3/2M1/2T−1, which, after substituting his M = L3T−2 equation for mass, results in charge having the same dimensions as mass, viz. Q = L3T−2.
Dimensional analysis is also used to derive relationships between the physical quantities that are involved in a particular phenomenon that one wishes to understand and characterize.  It was used for the first time (Pesic 2005) in this way in 1872 by Lord Rayleigh, who was trying to understand why the sky is blue.  Rayleigh first published the technique in his 1877 book The Theory of Sound.The original meaning of the word dimension, in Fourier's Theorie de la Chaleur, was the numerical value of the exponents of the base units. For example, acceleration was considered to have the dimension 1 with respect to the unit of length, and the dimension −2 with respect to the unit of time. This was slightly changed by Maxwell, who said the dimensions of acceleration are LT−2, instead of just the exponents.


== Mathematical formulation ==
The Buckingham π theorem describes how every physically meaningful equation involving n variables can be equivalently rewritten as an equation of n − m dimensionless parameters, where m is the rank of the dimensional matrix. Furthermore, and most importantly, it provides a method for computing these dimensionless parameters from the given variables.
A dimensional equation can have the dimensions reduced or eliminated through nondimensionalization, which begins with dimensional analysis, and involves scaling quantities by characteristic units of a system or natural units of nature. This gives insight into the fundamental properties of the system, as illustrated in the examples below.


=== Definition ===
The dimension of a physical quantity can be expressed as a product of the basic physical dimensions such as length, mass and time, each raised to a rational power. The dimension of a physical quantity is more fundamental than some scale unit used to express the amount of that physical quantity.  For example, mass is a dimension, while the kilogram is a particular scale unit chosen to express a quantity of mass. Except for natural units, the choice of scale is cultural and arbitrary.
There are many possible choices of basic physical dimensions. The SI standard recommends the usage of the following dimensions and corresponding symbols: length (L), mass (M), time (T), electric current (I), absolute temperature (Θ), amount of substance (N) and luminous intensity (J). The symbols are by convention usually written in roman sans serif typeface. Mathematically, the dimension of the quantity Q is given by 

  
    
      
        
          dim
        
         
        
          Q
        
        =
        
          
            
              L
            
          
          
            a
          
        
        
          
            
              M
            
          
          
            b
          
        
        
          
            
              T
            
          
          
            c
          
        
        
          
            
              I
            
          
          
            d
          
        
        
          
            
              Θ
            
          
          
            e
          
        
        
          
            
              N
            
          
          
            f
          
        
        
          
            
              J
            
          
          
            g
          
        
      
    
    {\displaystyle {\text{dim}}~{Q}={\mathsf {L}}^{a}{\mathsf {M}}^{b}{\mathsf {T}}^{c}{\mathsf {I}}^{d}{\mathsf {\Theta }}^{e}{\mathsf {N}}^{f}{\mathsf {J}}^{g}}
  where a, b, c, d, e, f, g are the dimensional exponents. Other physical quantities could be defined as the base quantities, as long as they form a linearly independent basis. For instance, one could replace the dimension of electric current (I) of the SI basis with a dimension of electric charge (Q), since Q = IT.
As examples, the dimension of the physical quantity speed v is

  
    
      
        
          dim
        
         
        v
        =
        
          
            length
            time
          
        
        =
        
          
            
              L
            
            
              T
            
          
        
        =
        
          
            
              L
              T
            
          
          
            −
            1
          
        
      
    
    {\displaystyle {\text{dim}}~v={\frac {\text{length}}{\text{time}}}={\frac {\mathsf {L}}{\mathsf {T}}}={\mathsf {LT}}^{-1}}
  and the dimension of the physical quantity force F is

  
    
      
        
          dim
        
         
        F
        =
        
          mass
        
        ×
        
          acceleration
        
        =
        
          mass
        
        ×
        
          
            length
            
              
                time
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
            
            
              
                
                  T
                
              
              
                2
              
            
          
        
        =
        
          
            
              M
              L
              T
            
          
          
            −
            2
          
        
      
    
    {\displaystyle {\text{dim}}~F={\text{mass}}\times {\text{acceleration}}={\text{mass}}\times {\frac {\text{length}}{{\text{time}}^{2}}}={\frac {\mathsf {ML}}{{\mathsf {T}}^{2}}}={\mathsf {MLT}}^{-2}}
  The unit chosen to express a physical quantity and its dimension are related, but not identical concepts. The units of a physical quantity are defined by convention and related to some standard; e.g., length may have units of metres, feet, inches, miles or micrometres; but any length always has a dimension of L, no matter what units of length are chosen to express it. Two different units of the same physical quantity have conversion factors that relate them. For example, 1 in = 2.54 cm; in this case (2.54 cm/in) is the conversion factor, which is itself dimensionless. Therefore, multiplying by that conversion factor does not change the dimensions of a physical quantity.
There are also physicists that have cast doubt on the very existence of incompatible fundamental dimensions of physical quantity, although this does not invalidate the usefulness of dimensional analysis.


=== Mathematical properties ===

The dimensions that can be formed from a given collection of basic physical dimensions, such as M, L, and T, form an abelian group: The identity is written as 1; L0 = 1, and the inverse to L is 1/L or L−1. L raised to any rational power p is a member of the group, having an inverse of L−p or 1/Lp.  The operation of the group is multiplication, having the usual rules for handling exponents (Ln × Lm = Ln+m).
This group can be described as a vector space over the rational numbers, with for example dimensional symbol MiLjTk corresponding to the vector (i, j, k). When physical measured quantities (be they like-dimensioned or unlike-dimensioned) are multiplied or divided by one other, their dimensional units are likewise multiplied or divided; this corresponds to addition or subtraction in the vector space. When measurable quantities are raised to a rational power, the same is done to the dimensional symbols attached to those quantities; this corresponds to scalar multiplication in the vector space.
A basis for such a vector space of dimensional symbols is called a set of base quantities, and all other vectors are called derived units. As in any vector space, one may choose different bases, which yields different systems of units (e.g., choosing whether the unit for charge is derived from the unit for current, or vice versa).
The group identity 1, the dimension of dimensionless quantities, corresponds to the origin in this vector space.
The set of units of the physical quantities involved in a problem correspond to a set of vectors (or a matrix). The nullity describes some number (e.g., m) of ways in which these vectors can be combined to produce a zero vector. These correspond to producing (from the measurements) a number of dimensionless quantities, {π1, ..., πm}. (In fact these ways completely span the null subspace of another different space, of powers of the measurements.) Every possible way of multiplying (and exponentiating) together the measured quantities to produce something with the same units as some derived quantity X can be expressed in the general form

  
    
      
        X
        =
        
          ∏
          
            i
            =
            1
          
          
            m
          
        
        (
        
          π
          
            i
          
        
        
          )
          
            
              k
              
                i
              
            
          
        
        
        .
      
    
    {\displaystyle X=\prod _{i=1}^{m}(\pi _{i})^{k_{i}}\,.}
  Consequently, every possible commensurate equation for the physics of the system can be rewritten in the form

  
    
      
        f
        (
        
          π
          
            1
          
        
        ,
        
          π
          
            2
          
        
        ,
        .
        .
        .
        ,
        
          π
          
            m
          
        
        )
        =
        0
        
        .
      
    
    {\displaystyle f(\pi _{1},\pi _{2},...,\pi _{m})=0\,.}
  Knowing this restriction can be a powerful tool for obtaining new insight into the system.


=== Mechanics ===
The dimension of physical quantities of interest in mechanics can be expressed in terms of base dimensions M, L, and T – these form a 3-dimensional vector space. This is not the only valid choice of base dimensions, but it is the one most commonly used. For example, one might choose force, length and mass as the base dimensions (as some have done), with associated dimensions F, L, M; this corresponds to a different basis, and one may convert between these representations by a change of basis. The choice of the base set of dimensions is thus a convention, with the benefit of increased utility and familiarity. The choice of base dimensions is not entirely arbitrary, because they must form a basis: they must span the space, and be linearly independent.
For example, F, L, M form a set of fundamental dimensions because they form a basis that is equivalent to M, L, T: the former can be expressed as [F = ML/T2], L, M, while the latter can be expressed as M, L, [T = (ML/F)1/2].
On the other hand, length, velocity and time (L, V, T) do not form a set of base dimensions for mechanics, for two reasons:

There is no way to obtain mass – or anything derived from it, such as force – without introducing another base dimension (thus, they do not span the space).
Velocity, being expressible in terms of length and time (V = L/T), is redundant (the set is not linearly independent).


=== Other fields of physics and chemistry ===
Depending on the field of physics, it may be advantageous to choose one or another extended set of dimensional symbols. In electromagnetism, for example, it may be useful to use dimensions of M, L, T, and Q, where Q represents the dimension of electric charge.  In thermodynamics, the base set of dimensions is often extended to include a dimension for temperature, Θ.  In chemistry, the amount of substance (the number of molecules divided by the Avogadro constant, ≈ 6.02×1023 mol−1) is defined as a base dimension, N, as well.
In the interaction of relativistic plasma with strong laser pulses, a dimensionless relativistic similarity parameter, connected with the symmetry properties of the collisionless Vlasov equation, is constructed from the plasma-, electron- and critical-densities in addition to the electromagnetic vector potential. The choice of the dimensions or even the number of dimensions to be used in different fields of physics is to some extent arbitrary, but consistency in use and ease of communications are common and necessary features.


=== Polynomials and transcendental functions ===
Scalar arguments to transcendental functions such as exponential, trigonometric and logarithmic functions, or to inhomogeneous polynomials, must be dimensionless quantities.  (Note: this requirement is somewhat relaxed in Siano's orientational analysis described below, in which the square of certain dimensioned quantities are dimensionless.)
While most mathematical identities about dimensionless numbers translate in a straightforward manner to dimensional quantities, care must be taken with logarithms of ratios: the identity log(a/b) = log a − log b, where the logarithm is taken in any base, holds for dimensionless numbers a and b, but it does not hold if a and b are dimensional, because in this case the left-hand side is well-defined but the right-hand side is not.
Similarly, while one can evaluate monomials (xn) of dimensional quantities, one cannot evaluate polynomials of mixed degree with dimensionless coefficients on dimensional quantities: for x2, the expression (3 m)2 = 9 m2 makes sense (as an area), while for x2 + x, the expression (3 m)2 + 3 m = 9 m2 + 3 m does not make sense.
However, polynomials of mixed degree can make sense if the coefficients are suitably chosen physical quantities that are not dimensionless.  For example,

  
    
      
        
          
            1
            2
          
        
        ⋅
        
          (
          
            −
            9.8
             
            
              
                meter
                
                  
                    second
                  
                  
                    2
                  
                
              
            
          
          )
        
        ⋅
        
          t
          
            2
          
        
        +
        
          (
          
            500
             
            
              
                meter
                second
              
            
          
          )
        
        ⋅
        t
        .
      
    
    {\displaystyle {\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot t^{2}+\left(500\ {\frac {\text{meter}}{\text{second}}}\right)\cdot t.}
  This is the height to which an object rises in time t if the acceleration of gravity is 9.8 meter per second per second and the initial upward speed is 500 meter per second.  It is not necessary for t to be in seconds.  For example, suppose t = 0.01 minutes.  Then the first term would be

  
    
      
        
          
            
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                
                  (
                  
                    −
                    9.8
                     
                    
                      
                        meter
                        
                          
                            second
                          
                          
                            2
                          
                        
                      
                    
                  
                  )
                
                ⋅
                (
                0.01
                
                   minute
                
                
                  )
                  
                    2
                  
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                
                  
                    (
                    
                      
                        minute
                        second
                      
                    
                    )
                  
                  
                    2
                  
                
                ⋅
                
                  meter
                
              
            
            
              
                =
                

                
              
              
                
                  
                    1
                    2
                  
                
                ⋅
                −
                9.8
                ⋅
                
                  (
                  
                    0.01
                    
                      2
                    
                  
                  )
                
                ⋅
                
                  60
                  
                    2
                  
                
                ⋅
                
                  meter
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}&{\frac {1}{2}}\cdot \left(-9.8\ {\frac {\text{meter}}{{\text{second}}^{2}}}\right)\cdot (0.01{\text{ minute}})^{2}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\left({\frac {\text{minute}}{\text{second}}}\right)^{2}\cdot {\text{meter}}\\[10pt]={}&{\frac {1}{2}}\cdot -9.8\cdot \left(0.01^{2}\right)\cdot 60^{2}\cdot {\text{meter}}.\end{aligned}}}
  


=== Incorporating units ===
The value of a dimensional physical quantity Z is written as the product of a unit [Z] within the dimension and a dimensionless numerical factor, n.

  
    
      
        Z
        =
        n
        ×
        [
        Z
        ]
        =
        n
        [
        Z
        ]
      
    
    {\displaystyle Z=n\times [Z]=n[Z]}
  When like-dimensioned quantities are added or subtracted or compared, it is convenient to express them in consistent units so that the numerical values of these quantities may be directly added or subtracted. But, in concept, there is no problem adding quantities of the same dimension expressed in different units. For example, 1 meter added to 1 foot is a length, but one cannot derive that length by simply adding 1 and 1. A conversion factor, which is a ratio of like-dimensioned quantities and is equal to the dimensionless unity, is needed:

  
    
      
        1
         
        
          
            ft
          
        
        =
        0.3048
         
        
          
            m
          
        
         
      
    
    {\displaystyle 1\ {\mbox{ft}}=0.3048\ {\mbox{m}}\ }
    is identical to 
  
    
      
        1
        =
        
          
            
              0.3048
               
              
                
                  m
                
              
            
            
              1
               
              
                
                  ft
                
              
            
          
        
        .
         
      
    
    {\displaystyle 1={\frac {0.3048\ {\mbox{m}}}{1\ {\mbox{ft}}}}.\ }
  The factor 
  
    
      
        0.3048
         
        
          
            
              m
            
            
              ft
            
          
        
      
    
    {\displaystyle 0.3048\ {\frac {\mbox{m}}{\mbox{ft}}}}
   is identical to the dimensionless 1, so multiplying by this conversion factor changes nothing. Then when adding two quantities of like dimension, but expressed in different units, the appropriate conversion factor, which is essentially the dimensionless 1, is used to convert the quantities to identical units so that their numerical values can be added or subtracted.
Only in this manner is it meaningful to speak of adding like-dimensioned quantities of differing units.


=== Position vs displacement ===

Some discussions of dimensional analysis implicitly describe all quantities as mathematical vectors. (In mathematics scalars are considered a special case of vectors; vectors can be added to or subtracted from other vectors, and, inter alia, multiplied or divided by scalars. If a vector is used to define a position, this assumes an implicit point of reference: an origin. While this is useful and often perfectly adequate, allowing many important errors to be caught, it can fail to model certain aspects of physics. A more rigorous approach requires distinguishing between position and displacement (or moment in time versus duration, or absolute temperature versus temperature change).
Consider points on a line, each with a position with respect to a given origin, and distances among them. Positions and displacements all have units of length, but their meaning is not interchangeable:

adding two displacements should yield a new displacement (walking ten paces then twenty paces gets you thirty paces forward),
adding a displacement to a position should yield a new position (walking one block down the street from an intersection gets you to the next intersection),
subtracting two positions should yield a displacement,
but one may not add two positions.This illustrates the subtle distinction between affine quantities (ones modeled by an affine space, such as position) and vector quantities (ones modeled by a vector space, such as displacement).

Vector quantities may be added to each other, yielding a new vector quantity, and a vector quantity may be added to a suitable affine quantity (a vector space acts on an affine space), yielding a new affine quantity.
Affine quantities cannot be added, but may be subtracted, yielding relative quantities which are vectors, and these relative differences may then be added to each other or to an affine quantity.Properly then, positions have dimension of affine length, while displacements have dimension of vector length. To assign a number to an affine unit, one must not only choose a unit of measurement, but also a point of reference, while to assign a number to a vector unit only requires a unit of measurement.
Thus some physical quantities are better modeled by vectorial quantities while others tend to require affine representation, and the distinction is reflected in their dimensional analysis.
This distinction is particularly important in the case of temperature, for which the numeric value of absolute zero is not the origin 0 in some scales. For absolute zero,

−273.15 °C ≘ 0 K = 0 °R ≘ −459.67 °F,where the symbol ≘ means corresponds to, since although these values on the respective temperature scales correspond, they represent distinct quantities in the same way that the distances from distinct starting points to the same end point are distinct quantities, and cannot in general be equated.
For temperature differences,

1 K = 1 °C ≠ 1 °F = 1 °R.(Here °R refers to the Rankine scale, not the Réaumur scale).
Unit conversion for temperature differences is simply a matter of multiplying by, e.g., 1 °F / 1 K (although the ratio is not a constant value). But because some of these scales have origins that do not correspond to absolute zero, conversion from one temperature scale to another requires accounting for that. As a result, simple dimensional analysis can lead to errors if it is ambiguous whether 1 K means the absolute temperature equal to −272.15 °C, or the temperature difference equal to 1 °C.


=== Orientation and frame of reference ===
Similar to the issue of a point of reference is the issue of orientation: a displacement in 2 or 3 dimensions is not just a length, but is a length together with a direction. (This issue does not arise in 1 dimension, or rather is equivalent to the distinction between positive and negative.) Thus, to compare or combine two dimensional quantities in a multi-dimensional space, one also needs an orientation: they need to be compared to a frame of reference.
This leads to the extensions discussed below, namely Huntley's directed dimensions and Siano's orientational analysis.


== Examples ==


=== A simple example: period of a harmonic oscillator ===
What is the period of oscillation T of a mass m attached to an ideal linear spring with spring constant k suspended in gravity of strength g? That period is the solution for T of some dimensionless equation in the variables T, m, k, and g.
The four quantities have the following dimensions:  T  [T];  m  [M]; k [M/T2]; and  g [L/T2]. From these we can form only one dimensionless product of powers of our chosen variables, 
  
    
      
        
          G
          
            1
          
        
      
    
    {\displaystyle G_{1}}
   = 
  
    
      
        
          T
          
            2
          
        
        k
        
          /
        
        m
      
    
    {\displaystyle T^{2}k/m}
   [T2 · M/T2 / M = 1], and putting 
  
    
      
        
          G
          
            1
          
        
        =
        C
      
    
    {\displaystyle G_{1}=C}
   for some dimensionless constant C gives the dimensionless equation sought.  The dimensionless product of powers of variables is sometimes referred to as a dimensionless group of variables; here the term ""group"" means ""collection"" rather than mathematical group.  They are often called dimensionless numbers as well.
Note that the variable g does not occur in the group. It is easy to see that it is impossible to form a dimensionless product of powers that combines g with k, m, and T, because g is the only quantity that involves the dimension L. This implies that in this problem the g is irrelevant. Dimensional analysis can sometimes yield strong statements about the irrelevance of some quantities in a problem, or the need for additional parameters. If we have chosen enough variables to properly describe the problem, then from this argument we can conclude that the period of the mass on the spring is independent of g: it is the same on the earth or the moon. The equation demonstrating the existence of a product of powers for our problem can be written in an entirely equivalent way:  
  
    
      
        T
        =
        κ
        
          
            
              
                m
                k
              
            
          
        
      
    
    {\displaystyle T=\kappa {\sqrt {\tfrac {m}{k}}}}
  , for some dimensionless constant κ (equal to 
  
    
      
        
          
            C
          
        
      
    
    {\displaystyle {\sqrt {C}}}
   from the original dimensionless equation).
When faced with a case where dimensional analysis rejects a variable (g, here) that one intuitively expects to belong in a physical description of the situation, another possibility is that the rejected variable is in fact relevant, but that some other relevant variable has been omitted, which might combine with the rejected variable to form a dimensionless quantity. That is, however, not the case here.
When dimensional analysis yields only one dimensionless group, as here, there are no unknown functions, and the solution is said to be ""complete"" – although it still may involve unknown dimensionless constants, such as κ.


=== A more complex example: energy of a vibrating wire ===
Consider the case of a vibrating wire of length ℓ (L) vibrating with an amplitude A (L).  The wire has a linear density ρ (M/L) and is under tension s (ML/T2), and we want to know the energy E (ML2/T2) in the wire.  Let π1 and π2 be two dimensionless products of powers of the variables chosen, given by

  
    
      
        
          
            
              
                
                  π
                  
                    1
                  
                
              
              
                
                =
                
                  
                    E
                    
                      A
                      s
                    
                  
                
              
            
            
              
                
                  π
                  
                    2
                  
                
              
              
                
                =
                
                  
                    ℓ
                    A
                  
                
                .
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\pi _{1}&={\frac {E}{As}}\\\pi _{2}&={\frac {\ell }{A}}.\end{aligned}}}
  The linear density of the wire is not involved. The two groups found can be combined into an equivalent form as an equation

  
    
      
        F
        
          (
          
            
              
                E
                
                  A
                  s
                
              
            
            ,
            
              
                ℓ
                A
              
            
          
          )
        
        =
        0
        ,
      
    
    {\displaystyle F\left({\frac {E}{As}},{\frac {\ell }{A}}\right)=0,}
  where F is some unknown function, or, equivalently as

  
    
      
        E
        =
        A
        s
        f
        
          (
          
            
              ℓ
              A
            
          
          )
        
        ,
      
    
    {\displaystyle E=Asf\left({\frac {\ell }{A}}\right),}
  where f is some other unknown function.  Here the unknown function implies that our solution is now incomplete, but dimensional analysis has given us something that may not have been obvious: the energy is proportional to the first power of the tension.  Barring further analytical analysis, we might proceed to experiments to discover the form for the unknown function f.  But our experiments are simpler than in the absence of dimensional analysis.  We'd perform none to verify that the energy is proportional to the tension.  Or perhaps we might guess that the energy is proportional to ℓ, and so infer that E = ℓs.  The power of dimensional analysis as an aid to experiment and forming hypotheses becomes evident.
The power of dimensional analysis really becomes apparent when it is applied to situations, unlike those given above, that are more complicated, the set of variables involved are not apparent, and the underlying equations hopelessly complex.  Consider, for example, a small pebble sitting on the bed of a river.  If the river flows fast enough, it will actually raise the pebble and cause it to flow along with the water.  At what critical velocity will this occur?  Sorting out the guessed variables is not so easy as before.  But dimensional analysis can be a powerful aid in understanding problems like this, and is usually the very first tool to be applied to complex problems where the underlying equations and constraints are poorly understood. In such cases, the answer may depend on a dimensionless number such as the Reynolds number, which may be interpreted by dimensional analysis.


=== A third example: demand versus capacity for a rotating disc ===

Consider the case of a thin, solid, parallel-sided rotating disc of axial thickness t (L) and radius R (L).  The disc has a density ρ (M/L3), rotates at an angular velocity ω (T−1) and this leads to a stress S (ML−1T−2) in the material.  There is a theoretical linear elastic solution, given by Lame, to this problem when the disc is thin relative to its radius, the faces of the disc are free to move axially, and the plane stress constitutive relations can be assumed to be valid.  As the disc becomes thicker relative to the radius then the plane stress solution breaks down.  If the disc is restrained axially on its free faces then a state of plane strain will occur.  However, if this is not the case then the state of stress may only be determined though consideration of three-dimensional elasticity and there is no known theoretical solution for this case.  An engineer might, therefore, be interested in establishing a relationship between the five variables.  Dimensional analysis for this case leads to the following (5 − 3 = 2) non-dimensional groups:

demand/capacity = ρR2ω2/S
thickness/radius or aspect ratio = t/RThrough the use of numerical experiments using, for example, the finite element method, the nature of the relationship between the two non-dimensional groups can be obtained as shown in the figure.  As this problem only involves two non-dimensional groups, the complete picture is provided in a single plot and this can be used as a design/assessment chart for rotating discs


== Extensions ==


=== Huntley's extension: directed dimensions and quantity of matter ===
Huntley (Huntley 1967) has pointed out that a dimensional analysis can become more powerful by discovering new independent dimensions in the quantities under consideration, thus increasing the rank 
  
    
      
        m
      
    
    {\displaystyle m}
   of the dimensional matrix. He introduced two approaches to doing so:

The magnitudes of the components of a vector are to be considered dimensionally independent. For example, rather than an undifferentiated length dimension L, we may have Lx represent dimension in the x-direction, and so forth. This requirement stems ultimately from the requirement that each component of a physically meaningful equation (scalar, vector, or tensor) must be dimensionally consistent.
Mass as a measure of the quantity of matter is to be considered dimensionally independent from mass as a measure of inertia.As an example of the usefulness of the first approach, suppose we wish to calculate the distance a cannonball travels when fired with a vertical velocity component 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   and a horizontal velocity component 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , assuming it is fired on a flat surface. Assuming no use of directed lengths, the quantities of interest are then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
  , 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
  , both dimensioned as LT−1, R, the distance travelled, having dimension L, and g the downward acceleration of gravity, with dimension LT−2.
With these four quantities, we may conclude that the equation for the range R may be written:

  
    
      
        R
        ∝
        
          V
          
            x
          
          
            a
          
        
        
        
          V
          
            y
          
          
            b
          
        
        
        
          g
          
            c
          
        
        .
        
      
    
    {\displaystyle R\propto V_{\text{x}}^{a}\,V_{\text{y}}^{b}\,g^{c}.\,}
  Or dimensionally

  
    
      
        
          
            L
          
        
        =
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            a
            +
            b
          
        
        
          
            (
            
              
                
                  L
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
        
      
    
    {\displaystyle {\mathsf {L}}=\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{a+b}\left({\frac {\mathsf {L}}{{\mathsf {T}}^{2}}}\right)^{c}\,}
  from which we may deduce that 
  
    
      
        a
        +
        b
        +
        c
        =
        1
      
    
    {\displaystyle a+b+c=1}
   and 
  
    
      
        a
        +
        b
        +
        2
        c
        =
        0
      
    
    {\displaystyle a+b+2c=0}
  , which leaves one exponent undetermined. This is to be expected since we have two fundamental dimensions L and T, and four parameters, with one equation.
If, however, we use directed length dimensions, then 
  
    
      
        
          V
          
            
              x
            
          
        
      
    
    {\displaystyle V_{\mathrm {x} }}
   will be dimensioned as LxT−1, 
  
    
      
        
          V
          
            
              y
            
          
        
      
    
    {\displaystyle V_{\mathrm {y} }}
   as LyT−1, R as Lx and g as LyT−2. The dimensional equation becomes:

  
    
      
        
          
            
              L
            
          
          
            
              x
            
          
        
        =
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      x
                    
                  
                
                
                  T
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            c
          
        
      
    
    {\displaystyle {\mathsf {L}}_{\mathrm {x} }=\left({\frac {{\mathsf {L}}_{\mathrm {x} }}{\mathsf {T}}}\right)^{a}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{\mathsf {T}}}\right)^{b}\left({\frac {{\mathsf {L}}_{\mathrm {y} }}{{\mathsf {T}}^{2}}}\right)^{c}}
  and we may solve completely as 
  
    
      
        a
        =
        1
      
    
    {\displaystyle a=1}
  , 
  
    
      
        b
        =
        1
      
    
    {\displaystyle b=1}
   and 
  
    
      
        c
        =
        −
        1
      
    
    {\displaystyle c=-1}
  . The increase in deductive power gained by the use of directed length dimensions is apparent.
In his second approach, Huntley holds that it is sometimes useful (e.g., in fluid mechanics and thermodynamics) to distinguish between mass as a measure of inertia (inertial mass), and mass as a measure of the quantity of matter. Quantity of matter is defined by Huntley as a quantity (a) proportional to inertial mass, but (b) not implicating inertial properties. No further restrictions are added to its definition.
For example, consider the derivation of Poiseuille's Law. We wish to find the rate of mass flow of a viscous fluid through a circular pipe. Without drawing distinctions between inertial and substantial mass we may choose as the relevant variables

  
    
      
        
          
            
              m
              ˙
            
          
        
      
    
    {\displaystyle {\dot {m}}}
   the mass flow rate with dimension MT−1

  
    
      
        
          p
          
            x
          
        
      
    
    {\displaystyle p_{\text{x}}}
   the pressure gradient along the pipe with dimension ML−2T−2
ρ the density with dimension ML−3
η the dynamic fluid viscosity with dimension ML−1T−1
r the radius of the pipe with dimension LThere are three fundamental variables so the above five equations will yield two dimensionless variables which we may take to be 
  
    
      
        
          π
          
            1
          
        
        =
        
          
            
              m
              ˙
            
          
        
        
          /
        
        η
        r
      
    
    {\displaystyle \pi _{1}={\dot {m}}/\eta r}
   and 
  
    
      
        
          π
          
            2
          
        
        =
        
          p
          
            
              x
            
          
        
        ρ
        
          r
          
            5
          
        
        
          /
        
        
          
            
              
                m
                ˙
              
            
          
          
            2
          
        
      
    
    {\displaystyle \pi _{2}=p_{\mathrm {x} }\rho r^{5}/{\dot {m}}^{2}}
   and we may express the dimensional equation as

  
    
      
        C
        =
        
          π
          
            1
          
        
        
          π
          
            2
          
          
            a
          
        
        =
        
          (
          
            
              
                
                  m
                  ˙
                
              
              
                η
                r
              
            
          
          )
        
        
          
            (
            
              
                
                  
                    p
                    
                      
                        x
                      
                    
                  
                  ρ
                  
                    r
                    
                      5
                    
                  
                
                
                  
                    
                      
                        m
                        ˙
                      
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
      
    
    {\displaystyle C=\pi _{1}\pi _{2}^{a}=\left({\frac {\dot {m}}{\eta r}}\right)\left({\frac {p_{\mathrm {x} }\rho r^{5}}{{\dot {m}}^{2}}}\right)^{a}}
  where C and a are undetermined constants. If we draw a distinction between inertial mass with dimension 
  
    
      
        
          M
          
            i
          
        
      
    
    {\displaystyle M_{\text{i}}}
   and quantity of matter with dimension 
  
    
      
        
          M
          
            m
          
        
      
    
    {\displaystyle M_{\text{m}}}
  , then mass flow rate and density will use quantity of matter as the mass parameter, while the pressure gradient and coefficient of viscosity will use inertial mass. We now have four fundamental parameters, and one dimensionless constant, so that the dimensional equation may be written:

  
    
      
        C
        =
        
          
            
              
                p
                
                  
                    x
                  
                
              
              ρ
              
                r
                
                  4
                
              
            
            
              η
              
                
                  
                    m
                    ˙
                  
                
              
            
          
        
      
    
    {\displaystyle C={\frac {p_{\mathrm {x} }\rho r^{4}}{\eta {\dot {m}}}}}
  where now only C is an undetermined constant (found to be equal to 
  
    
      
        π
        
          /
        
        8
      
    
    {\displaystyle \pi /8}
   by methods outside of dimensional analysis). This equation may be solved for the mass flow rate to yield Poiseuille's law.
Huntley's recognition of quantity of matter as an independent quantity dimension is evidently successful in the problems where it is applicable, but his definition of quantity of matter is open to interpretation, as it lacks specificity beyond the two requirements (a) and (b) he postulated for it. For a given substance, the SI dimension amount of substance, with unit mole, does satisfy Huntley's two requirements as a measure of quantity of matter, and could be used as a quantity of matter in any problem of dimensional analysis where Huntley's concept is applicable.
Huntley's concept of directed length dimensions however has some serious limitations:

It does not deal well with vector equations involving the cross product,
nor does it handle well the use of angles as physical variables.It also is often quite difficult to assign the L, Lx, Ly, Lz, symbols to the physical variables involved in the problem of interest. He invokes a procedure that involves the ""symmetry"" of the physical problem. This is often very difficult to apply reliably: It is unclear as to what parts of the problem that the notion of ""symmetry"" is being invoked. Is it the symmetry of the physical body that forces are acting upon, or to the points, lines or areas at which forces are being applied? What if more than one body is involved with different symmetries?
Consider the spherical bubble attached to a cylindrical tube, where one wants the flow rate of air as a function of the pressure difference in the two parts. What are the Huntley extended dimensions of the viscosity of the air contained in the connected parts? What are the extended dimensions of the pressure of the two parts? Are they the same or different? These difficulties are responsible for the limited application of Huntley's directed length dimensions to real problems.


=== Siano's extension: orientational analysis ===
Angles are, by convention, considered to be dimensionless quantities. As an example, consider again the projectile problem in which a point mass is launched from the origin (x, y) = (0, 0) at a speed v and angle θ above the x-axis, with the force of gravity directed along the negative y-axis. It is desired to find the range R, at which point the mass returns to the x-axis. Conventional analysis will yield the dimensionless variable π = R g/v2, but offers no insight into the relationship between R and θ.
Siano (1985-I, 1985-II) has suggested that the directed dimensions of Huntley be replaced by using orientational symbols 1x 1y 1z to denote vector directions, and an orientationless symbol 10. Thus, Huntley's Lx becomes L 1x with L specifying the dimension of length, and 1x specifying the orientation. Siano further shows that the orientational symbols have an algebra of their own.  Along with the requirement that 1i−1 = 1i, the following multiplication table for the orientation symbols results:

  
    
      
        
          
            
              
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  
                    1
                    
                      z
                    
                  
                
              
            
            
              
                
                  
                    1
                    
                      0
                    
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
            
            
              
                
                  
                    1
                    
                      x
                    
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
            
            
              
                
                  
                    1
                    
                      y
                    
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
            
            
              
                
                  
                    1
                    
                      z
                    
                  
                
              
              
                
                  1
                  
                    z
                  
                
              
              
                
                  1
                  
                    y
                  
                
              
              
                
                  1
                  
                    x
                  
                
              
              
                
                  1
                  
                    0
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{array}{c|cccc}&\mathbf {1_{0}} &\mathbf {1_{\text{x}}} &\mathbf {1_{\text{y}}} &\mathbf {1_{\text{z}}} \\\hline \mathbf {1_{0}} &1_{0}&1_{\text{x}}&1_{\text{y}}&1_{\text{z}}\\\mathbf {1_{\text{x}}} &1_{\text{x}}&1_{0}&1_{\text{z}}&1_{\text{y}}\\\mathbf {1_{\text{y}}} &1_{\text{y}}&1_{\text{z}}&1_{0}&1_{\text{x}}\\\mathbf {1_{\text{z}}} &1_{\text{z}}&1_{\text{y}}&1_{\text{x}}&1_{0}\end{array}}}
  Note that the orientational symbols form a group (the Klein four-group or ""Viergruppe""). In this system, scalars always have the same orientation as the identity element, independent of the ""symmetry of the problem"".  Physical quantities that are vectors have the orientation expected:  a force or a velocity in the z-direction has the orientation of 1z.  For angles, consider an angle θ that lies in the z-plane.  Form a right triangle in the z-plane with θ being one of the acute angles.  The side of the right triangle adjacent to the angle then has an orientation 1x and the side opposite has an orientation 1y.  Since (using ~ to indicate orientational equivalence) tan(θ) = θ + ... ~ 1y/1x we conclude that an angle in the xy-plane must have an orientation 1y/1x = 1z, which is not unreasonable.  Analogous reasoning forces the conclusion that sin(θ) has orientation 1z while cos(θ) has orientation 10.  These are different, so one concludes (correctly), for example, that there are no solutions of physical equations that are of the form a cos(θ) + b sin(θ), where a and b are real scalars. Note that an expression such as 
  
    
      
        sin
        ⁡
        (
        θ
        +
        π
        
          /
        
        2
        )
        =
        cos
        ⁡
        (
        θ
        )
      
    
    {\displaystyle \sin(\theta +\pi /2)=\cos(\theta )}
   is not dimensionally inconsistent since it is a special case of the sum of angles formula and should properly be written:

  
    
      
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            +
            b
            
            
              1
              
                z
              
            
          
          )
        
        =
        sin
        ⁡
        
          (
          
            a
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            b
            
            
              1
              
                z
              
            
          
          )
        
        +
        sin
        ⁡
        
          (
          
            b
            
            
              1
              
                z
              
            
            )
            cos
            ⁡
            (
            a
            
            
              1
              
                z
              
            
          
          )
        
        ,
      
    
    {\displaystyle \sin \left(a\,1_{\text{z}}+b\,1_{\text{z}}\right)=\sin \left(a\,1_{\text{z}})\cos(b\,1_{\text{z}}\right)+\sin \left(b\,1_{\text{z}})\cos(a\,1_{\text{z}}\right),}
  which for 
  
    
      
        a
        =
        θ
      
    
    {\displaystyle a=\theta }
   and 
  
    
      
        b
        =
        π
        
          /
        
        2
      
    
    {\displaystyle b=\pi /2}
   yields 
  
    
      
        sin
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        +
        [
        π
        
          /
        
        2
        ]
        
        
          1
          
            z
          
        
        )
        =
        
          1
          
            z
          
        
        cos
        ⁡
        (
        θ
        
        
          1
          
            z
          
        
        )
      
    
    {\displaystyle \sin(\theta \,1_{\text{z}}+[\pi /2]\,1_{\text{z}})=1_{\text{z}}\cos(\theta \,1_{\text{z}})}
  . Siano distinguishes between geometric angles, which have an orientation in 3-dimensional space, and phase angles associated with time-based oscillations, which have no spatial orientation, i.e. the orientation of a phase angle is 
  
    
      
        
          1
          
            0
          
        
      
    
    {\displaystyle 1_{0}}
  .
The assignment of orientational symbols to physical quantities and the requirement that physical equations be orientationally homogeneous can actually be used in a way that is similar to dimensional analysis to derive a little more information about acceptable solutions of physical problems.  In this approach one sets up the dimensional equation and solves it as far as one can.  If the lowest power of a physical variable is fractional, both sides of the solution is raised to a power such that all powers are integral.  This puts it into ""normal form"".  The orientational equation is then solved to give a more restrictive condition on the unknown powers of the orientational symbols, arriving at a solution that is more complete than the one that dimensional analysis alone gives. Often the added information is that one of the powers of a certain variable is even or odd.
As an example, for the projectile problem, using orientational symbols, θ, being in the xy-plane will thus have dimension 1z and the range of the projectile R will be of the form:

  
    
      
        R
        =
        
          g
          
            a
          
        
        
        
          v
          
            b
          
        
        
        
          θ
          
            c
          
        
        
           which means 
        
        
          
            L
          
        
        
        
          1
          
            
              x
            
          
        
        ∼
        
          
            (
            
              
                
                  
                    
                      L
                    
                  
                  
                  
                    1
                    
                      y
                    
                  
                
                
                  
                    
                      T
                    
                  
                  
                    2
                  
                
              
            
            )
          
          
            a
          
        
        
          
            (
            
              
                
                  L
                
                
                  T
                
              
            
            )
          
          
            b
          
        
        
        
          1
          
            
              z
            
          
          
            c
          
        
        .
        
      
    
    {\displaystyle R=g^{a}\,v^{b}\,\theta ^{c}{\text{ which means }}{\mathsf {L}}\,1_{\mathrm {x} }\sim \left({\frac {{\mathsf {L}}\,1_{\text{y}}}{{\mathsf {T}}^{2}}}\right)^{a}\left({\frac {\mathsf {L}}{\mathsf {T}}}\right)^{b}\,1_{\mathsf {z}}^{c}.\,}
  Dimensional homogeneity will now correctly yield a = −1 and b = 2, and orientational homogeneity requires that 
  
    
      
        
          1
          
            x
          
        
        
          /
        
        (
        
          1
          
            y
          
          
            a
          
        
        
          1
          
            z
          
          
            c
          
        
        )
        =
        
          1
          
            z
          
          
            c
            +
            1
          
        
        =
        1
      
    
    {\displaystyle 1_{x}/(1_{y}^{a}1_{z}^{c})=1_{z}^{c+1}=1}
  . In other words, that c must be an odd integer. In fact the required function of theta will be sin(θ)cos(θ) which is a series consisting of odd powers of θ.
It is seen that the Taylor series of sin(θ) and cos(θ) are orientationally homogeneous using the above multiplication table, while expressions like cos(θ) + sin(θ) and exp(θ) are not, and are (correctly) deemed unphysical.
Siano's orientational analysis is compatible with the conventional conception of angular quantities as being dimensionless, and within orientational analysis, the radian may still be considered a dimensionless unit. The orientational analysis of a quantity equation is carried out separately from the ordinary dimensional analysis, yielding information that supplements the dimensional analysis.


== Dimensionless concepts ==


=== Constants ===

The dimensionless constants that arise in the results obtained, such as the C in the Poiseuille's Law problem and the 
  
    
      
        κ
      
    
    {\displaystyle \kappa }
   in the spring problems discussed above, come from a more detailed analysis of the underlying physics and often arise from integrating some differential equation.  Dimensional analysis itself has little to say about these constants, but it is useful to know that they very often have a magnitude of order unity.  This observation can allow one to sometimes make ""back of the envelope"" calculations about the phenomenon of interest, and therefore be able to more efficiently design experiments to measure it, or to judge whether it is important, etc.


=== Formalisms ===
Paradoxically, dimensional analysis can be a useful tool even if all the parameters in the underlying theory are dimensionless, e.g., lattice models such as the Ising model can be used to study phase transitions and critical phenomena. Such models can be formulated in a purely dimensionless way. As we approach the critical point closer and closer, the distance over which the variables in the lattice model are correlated (the so-called correlation length, 
  
    
      
        ξ
      
    
    {\displaystyle \xi }
   ) becomes larger and larger. Now, the correlation length is the relevant length scale related to critical phenomena, so one can, e.g., surmise on ""dimensional grounds"" that the non-analytical part of the free energy per lattice site should be 
  
    
      
        ∼
        1
        
          /
        
        
          ξ
          
            d
          
        
      
    
    {\displaystyle \sim 1/\xi ^{d}}
   where 
  
    
      
        d
      
    
    {\displaystyle d}
   is the dimension of the lattice.
It has been argued by some physicists, e.g., M. J. Duff, that the laws of physics are inherently dimensionless. The fact that we have assigned incompatible dimensions to Length, Time and Mass is, according to this point of view, just a matter of convention, borne out of the fact that before the advent of modern physics, there was no way to relate mass, length, and time to each other. The three independent dimensionful constants: c, ħ, and G, in the fundamental equations of physics must then be seen as mere conversion factors to convert Mass, Time and Length into each other.
Just as in the case of critical properties of lattice models, one can recover the results of dimensional analysis in the appropriate scaling limit; e.g., dimensional analysis in mechanics can be derived by reinserting the constants ħ, c, and G (but we can now consider them to be dimensionless) and demanding that a nonsingular relation between quantities exists in the limit 
  
    
      
        c
        →
        ∞
      
    
    {\displaystyle c\rightarrow \infty }
  ,  
  
    
      
        ℏ
        →
        0
      
    
    {\displaystyle \hbar \rightarrow 0}
   and 
  
    
      
        G
        →
        0
      
    
    {\displaystyle G\rightarrow 0}
  . In problems involving a gravitational field the latter limit should be taken such that the field stays finite.


== Dimensional equivalences ==
Following are tables of commonly occurring expressions in physics, related to the dimensions of energy, momentum, and force.


=== SI units ===


=== Natural units ===

If c = ħ = 1, where c is the speed of light and ħ is the reduced Planck constant, and a suitable fixed unit of energy is chosen, then all quantities of length L, mass M and time T can be expressed (dimensionally) as a power of energy E, because length, mass and time can be expressed using speed v, action S, and energy E:

  
    
      
        M
        =
        
          
            E
            
              v
              
                2
              
            
          
        
        ,
        
        L
        =
        
          
            
              S
              v
            
            E
          
        
        ,
        
        t
        =
        
          
            S
            E
          
        
      
    
    {\displaystyle M={\frac {E}{v^{2}}},\quad L={\frac {Sv}{E}},\quad t={\frac {S}{E}}}
  though speed and action are dimensionless (v = c = 1 and S = ħ = 1) – so the only remaining quantity with dimension is energy. In terms of powers of dimensions:

  
    
      
        
          
            
              E
            
          
          
            n
          
        
        =
        
          
            
              M
            
          
          
            p
          
        
        
          
            
              L
            
          
          
            q
          
        
        
          
            
              T
            
          
          
            r
          
        
        =
        
          
            
              E
            
          
          
            p
            −
            q
            −
            r
          
        
      
    
    {\displaystyle {\mathsf {E}}^{n}={\mathsf {M}}^{p}{\mathsf {L}}^{q}{\mathsf {T}}^{r}={\mathsf {E}}^{p-q-r}}
  This is particularly useful in particle physics and high energy physics, in which case the energy unit is the electron volt (eV). Dimensional checks and estimates become very simple in this system.
However, if electric charges and currents are involved, another unit to be fixed is for electric charge, normally the electron charge e though other choices are possible.


== See also ==
Buckingham π theorem
Dimensionless numbers in fluid mechanics
Fermi estimate — used to teach dimensional analysis
Rayleigh's method of dimensional analysis
Similitude (model) — an application of dimensional analysis
System of measurement


=== Related areas of math ===
Covariance and contravariance of vectors
Exterior algebra
Geometric algebra
Quantity calculus


=== Programming languages ===
Dimensional correctness as part of type checking has been studied since 1977.
Implementations for Ada and C++ were described in 1985 and 1988.
Kennedy's 1996 thesis describes an implementation in Standard ML,  and later in F#. There are implementations for Haskell, OCaml, and Rust, Python, and a code checker for Fortran.
Griffioen's 2019 thesis extended Kennedy's Hindley–Milner type system to support Hart's matrices.


== Notes ==


== References ==
Barenblatt, G. I. (1996), Scaling, Self-Similarity, and Intermediate Asymptotics, Cambridge, UK: Cambridge University Press, ISBN 978-0-521-43522-2
Bhaskar, R.; Nigam, Anil (1990), ""Qualitative Physics Using Dimensional Analysis"", Artificial Intelligence, 45 (1–2): 73–111, doi:10.1016/0004-3702(90)90038-2
Bhaskar, R.; Nigam, Anil (1991), ""Qualitative Explanations of Red Giant Formation"", The Astrophysical Journal, 372: 592–6, Bibcode:1991ApJ...372..592B, doi:10.1086/170003
Boucher; Alves (1960), ""Dimensionless Numbers"", Chemical Engineering Progress, 55: 55–64
Bridgman, P. W. (1922), Dimensional Analysis, Yale University Press, ISBN 978-0-548-91029-0
Buckingham, Edgar (1914), ""On Physically Similar Systems: Illustrations of the Use of Dimensional Analysis"", Physical Review, 4 (4): 345–376, Bibcode:1914PhRv....4..345B, doi:10.1103/PhysRev.4.345, hdl:10338.dmlcz/101743
Drobot, S. (1953–1954), ""On the foundations of dimensional analysis"" (PDF), Studia Mathematica, 14: 84–99, doi:10.4064/sm-14-1-84-99
Gibbings, J.C. (2011), Dimensional Analysis, Springer, ISBN 978-1-84996-316-9
Hart, George W. (1994), ""The theory of dimensioned matrices"",  in Lewis, John G. (ed.), Proceedings of the Fifth SIAM Conference on Applied Linear Algebra, SIAM, pp. 186–190, ISBN 978-0-89871-336-7 As postscript
Hart, George W. (1995), Multidimensional Analysis: Algebras and Systems for Science and Engineering, Springer-Verlag, ISBN 978-0-387-94417-3
Huntley, H. E. (1967), Dimensional Analysis, Dover, LOC 67-17978
Klinkenberg, A. (1955), ""Dimensional systems and systems of units in physics with special reference to chemical engineering: Part I. The principles according to which dimensional systems and systems of units are constructed"", Chemical Engineering Science, 4 (3): 130–140, 167–177, doi:10.1016/0009-2509(55)80004-8
Langhaar, Henry L. (1951), Dimensional Analysis and Theory of Models, Wiley, ISBN 978-0-88275-682-0
Mendez, P.F.; Ordóñez, F. (September 2005), ""Scaling Laws From Statistical Data and Dimensional Analysis"", Journal of Applied Mechanics, 72 (5): 648–657, Bibcode:2005JAM....72..648M, CiteSeerX 10.1.1.422.610, doi:10.1115/1.1943434
Moody, L. F. (1944), ""Friction Factors for Pipe Flow"", Transactions of the American Society of Mechanical Engineers, 66 (671)
Murphy, N. F. (1949), ""Dimensional Analysis"", Bulletin of the Virginia Polytechnic Institute, 42 (6)
Perry, J. H.;  et al. (1944), ""Standard System of Nomenclature for Chemical Engineering Unit Operations"", Transactions of the American Institute of Chemical Engineers, 40 (251)
Pesic, Peter (2005), Sky in a Bottle, MIT Press, pp. 227–8, ISBN 978-0-262-16234-0
Petty, G. W. (2001), ""Automated computation and consistency checking of physical dimensions and units in scientific programs"", Software – Practice and Experience, 31 (11): 1067–76, doi:10.1002/spe.401, S2CID 206506776
Porter, Alfred W. (1933), The Method of Dimensions (3rd ed.), Methuen
J. W. Strutt (3rd Baron Rayleigh) (1915), ""The Principle of Similitude"", Nature, 95 (2368): 66–8, Bibcode:1915Natur..95...66R, doi:10.1038/095066c0
Siano, Donald (1985), ""Orientational Analysis – A Supplement to Dimensional Analysis – I"", Journal of the Franklin Institute, 320 (6): 267–283, doi:10.1016/0016-0032(85)90031-6
Siano, Donald (1985), ""Orientational Analysis, Tensor Analysis and The Group Properties of the SI Supplementary Units – II"", Journal of the Franklin Institute, 320 (6): 285–302, doi:10.1016/0016-0032(85)90032-8
Silberberg, I. H.; McKetta, J. J. Jr. (1953), ""Learning How to Use Dimensional Analysis"", Petroleum Refiner, 32 (4): 5, (5): 147, (6): 101, (7): 129
Van Driest, E. R. (March 1946), ""On Dimensional Analysis and the Presentation of Data in Fluid Flow Problems"", Journal of Applied Mechanics, 68 (A–34)
Whitney, H. (1968), ""The Mathematics of Physical Quantities, Parts I and II"", American Mathematical Monthly, 75 (2): 115–138, 227–256, doi:10.2307/2315883, JSTOR 2315883
Vignaux, GA (1992),  Erickson, Gary J.; Neudorfer, Paul O. (eds.), Dimensional Analysis in Data Modelling, Kluwer Academic, ISBN 978-0-7923-2031-9
Kasprzak, Wacław; Lysik, Bertold; Rybaczuk, Marek (1990), Dimensional Analysis in the Identification of Mathematical Models, World Scientific, ISBN 978-981-02-0304-7


== External links ==
List of dimensions for variety of physical quantities
Unicalc Live web calculator doing units conversion by dimensional analysis
A C++ implementation of compile-time dimensional analysis in the Boost open-source libraries
Buckingham’s pi-theorem
Quantity System calculator for units conversion based on dimensional approach
Units, quantities, and fundamental constants project dimensional analysis maps
Bowley, Roger (2009). ""[ ] Dimensional Analysis"". Sixty Symbols. Brady Haran for the University of Nottingham.
Dureisseix, David (2019). An introduction to dimensional analysis (lecture). INSA Lyon.


=== Converting units ===
Unicalc Live web calculator doing units conversion by dimensional analysis
Math Skills Review
U.S. EPA tutorial
A Discussion of Units
Short Guide to Unit Conversions
Canceling Units Lesson
Chapter 11: Behavior of Gases Chemistry: Concepts and Applications, Denton Independent School District
Air Dispersion Modeling Conversions and Formulas
www.gnu.org/software/units  free program, very practical","pandas(index=124, _1=124, text='in engineering and science, dimensional analysis is the analysis of the relationships between different physical quantities by identifying their base quantities (such as length, mass, time, and electric charge) and units of measure (such as miles vs. kilometres, or pounds vs. kilograms) and tracking these dimensions as calculations or comparisons are performed. the conversion of units from one dimensional unit to another is often easier within the metric or si system than in others, due to the regular 10-base in all units. dimensional analysis, or more specifically the factor-label method, also known as the unit-factor method, is a widely used technique for such conversions using the rules of algebra.commensurable physical quantities are of the same kind and have the same dimension, and can be directly compared to each other, even if they are originally expressed in differing units of measure, e.g. yards and metres, pounds(mass) and kilograms, seconds and years. incommensurable physical quantities are of different kinds and have different dimensions, and can not be directly compared to each other, no matter what units they are originally expressed in, e.g. meters and kilograms, seconds and kilograms,  meters and seconds. for example, asking whether a kilogram is larger than an hour is meaningless. any physically meaningful equation, or inequality, must have the same dimensions on its left and right sides, a property known as dimensional homogeneity. checking for dimensional homogeneity is a common application of dimensional analysis, serving as a plausibility check on derived equations and computations. it also serves as a guide and constraint in deriving equations that may describe a physical system in the absence of a more rigorous derivation. the concept of physical dimension, and of dimensional analysis, was introduced by joseph fourier in 1822.   == concrete numbers and base units == many parameters and measurements in the physical sciences and engineering are expressed as a concrete number—a numerical quantity and a corresponding dimensional unit. often a quantity is expressed in terms of several other quantities; for example, speed is a combination of length and time, e.g. 60 kilometres per hour or 1.4 kilometres per second. compound relations with ""per"" are expressed with division, e.g. 60 km/1 h.  other relations can involve multiplication (often shown with a centered dot or juxtaposition), powers (like m2 for square metres), or combinations thereof. a set of base units for a system of measurement is a conventionally chosen set of units, none of which can be expressed as a combination of the others and in terms of which all the remaining units of the system can be expressed. for example, units for length and time are normally chosen as base units. units for volume, however, can be factored into the base units of length (m3), thus they are considered derived or compound units. sometimes the names of units obscure the fact that they are derived units. for example, a newton (n) is a unit of force, which has units of mass (kg) times units of acceleration (m⋅s−2). the newton is defined as 1 n = 1 kg⋅m⋅s−2. unicalc live web calculator doing units conversion by dimensional analysis math skills review u.s. epa tutorial a discussion of units short guide to unit conversions canceling units lesson chapter 11: behavior of gases chemistry: concepts and applications, denton independent school district air dispersion modeling conversions and formulas www.gnu.org/software/units  free program, very practical')"
125,"Thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. The behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. Thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology.
Historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of French physicist Nicolas Léonard Sadi Carnot (1824) who believed that engine efficiency was the key that could help France win the Napoleonic Wars. Scots-Irish physicist Lord Kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, ""Thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency.""
The initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. Chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. Other formulations of thermodynamics emerged. Statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. In 1909, Constantin Carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.


== Introduction ==
A description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. The first law specifies that energy can be exchanged between physical systems as heat and work. The second law defines the existence of a quantity called entropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.In thermodynamics, interactions between large ensembles of objects are studied and categorized. Central to this are the concepts of the thermodynamic system and its surroundings. A system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. Properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes.
With these tools, thermodynamics can be used to describe how systems respond to changes in their environment. This can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. The results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.This article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. Non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.


== History ==
The history of thermodynamics as a scientific discipline generally begins with Otto von Guericke who, in 1650, built and designed the world's first vacuum pump and demonstrated a vacuum using his Magdeburg hemispheres. Guericke was driven to make a vacuum in order to disprove Aristotle's long-held supposition that 'nature abhors a vacuum'. Shortly after Guericke, the Anglo-Irish physicist and chemist Robert Boyle had learned of Guericke's designs and, in 1656, in coordination with English scientist Robert Hooke, built an air pump. Using this pump, Boyle and Hooke noticed a correlation between pressure, temperature, and volume. In time, Boyle's Law was formulated, which states that pressure and volume are inversely proportional. Then, in 1679, based on these concepts, an associate of Boyle's named Denis Papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated.
Later designs implemented a steam release valve that kept the machine from exploding. By watching the valve rhythmically move up and down, Papin conceived of the idea of a piston and a cylinder engine. He did not, however, follow through with his design. Nevertheless, in 1697, based on Papin's designs, engineer Thomas Savery built the first engine, followed by Thomas Newcomen in 1712. Although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time.
The fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by Professor Joseph Black at the University of Glasgow, where James Watt was employed as an instrument maker. Black and Watt performed experiments together, but it was Watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. Drawing on all the previous work led Sadi Carnot, the ""father of thermodynamics"", to publish Reflections on the Motive Power of Fire (1824), a discourse on heat, power, energy and engine efficiency. The book outlined the basic energetic relations between the Carnot engine, the Carnot cycle, and motive power. It marked the start of thermodynamics as a modern science.The first thermodynamic textbook was written in 1859 by William Rankine, originally trained as a physicist and a civil and mechanical engineering professor at the University of Glasgow. The first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of William Rankine, Rudolf Clausius, and William Thomson (Lord Kelvin).The foundations of statistical thermodynamics were set out by physicists such as James Clerk Maxwell, Ludwig Boltzmann, Max Planck, Rudolf Clausius and J. Willard Gibbs.
During the years 1873–76 the American mathematical physicist Josiah Willard Gibbs published a series of three papers, the most famous being On the Equilibrium of Heterogeneous Substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. Also Pierre Duhem in the 19th century wrote about chemical thermodynamics. During the early 20th century, chemists such as Gilbert N. Lewis, Merle Randall, and E. A. Guggenheim applied the mathematical methods of Gibbs to the analysis of chemical processes.


== Etymology ==
The etymology of thermodynamics has an intricate history. It was first spelled in a hyphenated form as an adjective (thermo-dynamic) and from 1854 to 1868 as the noun thermo-dynamics to represent the science of generalized heat engines.American biophysicist Donald Haynie claims that thermodynamics was coined in 1840 from the Greek root θέρμη therme, meaning “heat”, and δύναμις dynamis, meaning “power”.Pierre Perrot claims that the term thermodynamics was coined by James Joule in 1858 to designate the science of relations between heat and power, however, Joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to Thomson's 1849 phraseology.By 1858, thermo-dynamics, as a functional term, was used in William Thomson's paper ""An Account of Carnot's Theory of the Motive Power of Heat.""


== Branches of thermodynamics ==
The study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems.


=== Classical thermodynamics ===
Classical thermodynamics is the description of the states of thermodynamic systems at near-equilibrium, that uses macroscopic, measurable properties. It is used to model exchanges of energy, work and heat based on the laws of thermodynamics. The qualifier classical reflects the fact that it represents the first level of understanding of the subject as it developed in the 19th century and describes the changes of a system in terms of macroscopic empirical (large scale, and measurable) parameters. A microscopic interpretation of these concepts was later provided by the development of statistical mechanics.


=== Statistical mechanics ===
Statistical mechanics, also called statistical thermodynamics, emerged with the development of atomic and molecular theories in the late 19th century and early 20th century, and supplemented classical thermodynamics with an interpretation of the microscopic interactions between individual particles or quantum-mechanical states. This field relates the microscopic properties of individual atoms and molecules to the macroscopic, bulk properties of materials that can be observed on the human scale, thereby explaining classical thermodynamics as a natural result of statistics, classical mechanics, and quantum theory at the microscopic level.


=== Chemical thermodynamics ===
Chemical thermodynamics is the study of the interrelation of energy with chemical reactions or with a physical change of state within the confines of the laws of thermodynamics.


=== Equilibrium thermodynamics ===
Equilibrium thermodynamics is the study of transfers of matter and energy in systems or bodies that, by agencies in their surroundings, can be driven from one state of thermodynamic equilibrium to another. The term 'thermodynamic equilibrium' indicates a state of balance, in which all macroscopic flows are zero; in the case of the simplest systems or bodies, their intensive properties are homogeneous, and their pressures are perpendicular to their boundaries. In an equilibrium state there are no unbalanced potentials, or driving forces, between macroscopically distinct parts of the system. A central aim in equilibrium thermodynamics is: given a system in a well-defined initial equilibrium state, and given its surroundings, and given its constitutive walls, to calculate what will be the final equilibrium state of the system after a specified thermodynamic operation has changed its walls or surroundings.
Non-equilibrium thermodynamics is a branch of thermodynamics that deals with systems that are not in thermodynamic equilibrium. Most systems found in nature are not in thermodynamic equilibrium because they are not in stationary states, and are continuously and discontinuously subject to flux of matter and energy to and from other systems. The thermodynamic study of non-equilibrium systems requires more general concepts than are dealt with by equilibrium thermodynamics. Many natural systems still today remain beyond the scope of currently known macroscopic thermodynamic methods.


== Laws of thermodynamics ==

Thermodynamics is principally based on a set of four laws which are universally valid when applied to systems that fall within the constraints implied by each. In the various theoretical descriptions of thermodynamics these laws may be expressed in seemingly differing forms, but the most prominent formulations are the following.


=== Zeroth Law ===
The zeroth law of thermodynamics states: If two systems are each in thermal equilibrium with a third, they are also in thermal equilibrium with each other.
This statement implies that thermal equilibrium is an equivalence relation on the set of thermodynamic systems under consideration. Systems are said to be in equilibrium if the small, random exchanges between them (e.g. Brownian motion) do not lead to a net change in energy. This law is tacitly assumed in every measurement of temperature. Thus, if one seeks to decide whether two bodies are at the same temperature, it is not necessary to bring them into contact and measure any changes of their observable properties in time. The law provides an empirical definition of temperature, and justification for the construction of practical thermometers.
The zeroth law was not initially recognized as a separate law of thermodynamics, as its basis in thermodynamical equilibrium was implied in the other laws. The first, second, and third laws had been explicitly stated already, and found common acceptance in the physics community before the importance of the zeroth law for the definition of temperature was realized. As it was impractical to renumber the other laws, it was named the zeroth law.


=== First Law ===
The first law of thermodynamics states: In a process without transfer of matter, the change in internal energy, ΔU, of a thermodynamic system is equal to the energy gained as heat, Q, less the thermodynamic work, W, done by the system on its surroundings.

  
    
      
        Δ
        U
        =
        Q
        −
        W
      
    
    {\displaystyle \Delta U=Q-W}
  .For processes that include transfer of matter, a further statement is needed: With due account of the respective fiducial reference states of the systems, when two systems, which may be of different chemical compositions, initially separated only by an impermeable wall, and otherwise isolated, are combined into a new system by the thermodynamic operation of removal of the wall, then

  
    
      
        
          U
          
            0
          
        
        =
        
          U
          
            1
          
        
        +
        
          U
          
            2
          
        
      
    
    {\displaystyle U_{0}=U_{1}+U_{2}}
  ,where U0 denotes the internal energy of the combined system, and U1 and U2 denote the internal energies of the respective separated systems.
Adapted for thermodynamics, this law is an expression of the principle of conservation of energy, which states that energy can be transformed (changed from one form to another), but cannot be created or destroyed.Internal energy is a principal property of the thermodynamic state, while heat and work are modes of energy transfer by which a process may change this state. A change of internal energy of a system may be achieved by any combination of heat added or removed and work performed on or by the system. As a function of state, the internal energy does not depend on the manner, or on the path through intermediate steps, by which the system arrived at its state.


=== Second Law ===
A traditional version of the second law of thermodynamics states: Heat does not spontaneously flow from a colder body to a hotter.The second law is an observation of the fact that over time, inhomogeneities in temperature, pressure, and chemical potential tend to even out in a physical system that is isolated from the outside world. Entropy is a measure of how much this process has progressed. The entropy of an isolated system, that is internally constrained away from equilibrium, will increase when its internal constraints are removed, reaching a maximum value at thermodynamic equilibrium. Though several such have been proposed, there is known no general thermodynamic principle that guides the rates of changes in unconstrained systems that are far from thermodynamic equilibrium.
In classical thermodynamics, the second law is a basic postulate applicable to any actual thermodynamic process; in statistical thermodynamics, the second law is a consequence of molecular chaos. There are many versions of the second law, but they all have the same effect, which is to express the phenomenon of [irreversibility] in nature.


=== Third Law ===
The third law of thermodynamics states: As the temperature of a system approaches absolute zero, all processes cease and the entropy of the system approaches a minimum value.
This law of thermodynamics is a statistical law of nature regarding entropy and the impossibility of reaching absolute zero of temperature. This law provides an absolute reference point for the determination of entropy. The entropy determined relative to this point is the absolute entropy. Alternate definitions include ""the entropy of all systems and of all states of a system is smallest at absolute zero,"" or equivalently ""it is impossible to reach the absolute zero of temperature by any finite number of processes"".
Absolute zero, at which all activity would stop if it were possible to achieve, is −273.15 °C (degrees Celsius), or −459.67 °F (degrees Fahrenheit), or 0 K (kelvin), or 0° R (degrees Rankine).


== System models ==

An important concept in thermodynamics is the thermodynamic system, which is a precisely defined region of the universe under study. Everything in the universe except the system is called the surroundings. A system is separated from the remainder of the universe by a boundary which may be a physical or notional, but serve to confine the system to a finite volume. Segments of the boundary are often described as walls; they have respective defined 'permeabilities'. Transfers of energy as work, or as heat, or of matter, between the system and the surroundings, take place through the walls, according to their respective permeabilities.
Matter or energy that pass across the boundary so as to effect a change in the internal energy of the system need to be accounted for in the energy balance equation. The volume contained by the walls can be the region surrounding a single atom resonating energy, such as Max Planck defined in 1900; it can be a body of steam or air in a steam engine, such as Sadi Carnot defined in 1824. The system could also be just one nuclide (i.e. a system of quarks) as hypothesized in quantum thermodynamics. When a looser viewpoint is adopted, and the requirement of thermodynamic equilibrium is dropped, the system can be the body of a tropical cyclone, such as Kerry Emanuel theorized in 1986 in the field of atmospheric thermodynamics, or the event horizon of a black hole.
Boundaries are of four types: fixed, movable, real, and imaginary. For example, in an engine, a fixed boundary means the piston is locked at its position, within which a constant volume process might occur. If the piston is allowed to move that boundary is movable while the cylinder and cylinder head boundaries are fixed. For closed systems, boundaries are real while for open systems boundaries are often imaginary. In the case of a jet engine, a fixed imaginary boundary might be assumed at the intake of the engine, fixed boundaries along the surface of the case and a second fixed imaginary boundary across the exhaust nozzle.
Generally, thermodynamics distinguishes three classes of systems, defined in terms of what is allowed to cross their boundaries:

As time passes in an isolated system, internal differences of pressures, densities, and temperatures tend to even out. A system in which all equalizing processes have gone to completion is said to be in a state of thermodynamic equilibrium.
Once in thermodynamic equilibrium, a system's properties are, by definition, unchanging in time. Systems in equilibrium are much simpler and easier to understand than are systems which are not in equilibrium. Often, when analysing a dynamic thermodynamic process, the simplifying assumption is made that each intermediate state in the process is at equilibrium, producing thermodynamic processes which develop so slowly as to allow each intermediate step to be an equilibrium state and are said to be reversible processes.


== States and processes ==
When a system is at equilibrium under a given set of conditions, it is said to be in a definite thermodynamic state. The state of the system can be described by a number of state quantities that do not depend on the process by which the system arrived at its state. They are called intensive variables or extensive variables according to how they change when the size of the system changes. The properties of the system can be described by an equation of state which specifies the relationship between these variables. State may be thought of as the instantaneous quantitative description of a system with a set number of variables held constant.
A thermodynamic process may be defined as the energetic evolution of a thermodynamic system proceeding from an initial state to a final state. It can be described by process quantities. Typically, each thermodynamic process is distinguished from other processes in energetic character according to what parameters, such as temperature, pressure, or volume, etc., are held fixed; Furthermore, it is useful to group these processes into pairs, in which each variable held constant is one member of a conjugate pair.
Several commonly studied thermodynamic processes are:

Adiabatic process: occurs without loss or gain of energy by heat
Isenthalpic process: occurs at a constant enthalpy
Isentropic process: a reversible adiabatic process, occurs at a constant entropy
Isobaric process: occurs at constant pressure
Isochoric process: occurs at constant volume (also called isometric/isovolumetric)
Isothermal process: occurs at a constant temperature
Steady state process: occurs without a change in the internal energy


== Instrumentation ==
There are two types of thermodynamic instruments, the meter and the reservoir. A thermodynamic meter is any device which measures any parameter of a thermodynamic system. In some cases, the thermodynamic parameter is actually defined in terms of an idealized measuring instrument. For example, the zeroth law states that if two bodies are in thermal equilibrium with a third body, they are also in thermal equilibrium with each other. This principle, as noted by James Maxwell in 1872, asserts that it is possible to measure temperature. An idealized thermometer is a sample of an ideal gas at constant pressure. From the ideal gas law pV=nRT, the volume of such a sample can be used as an indicator of temperature; in this manner it defines temperature. Although pressure is defined mechanically, a pressure-measuring device, called a barometer may also be constructed from a sample of an ideal gas held at a constant temperature. A calorimeter is a device which is used to measure and define the internal energy of a system.
A thermodynamic reservoir is a system which is so large that its state parameters are not appreciably altered when it is brought into contact with the system of interest. When the reservoir is brought into contact with the system, the system is brought into equilibrium with the reservoir. For example, a pressure reservoir is a system at a particular pressure, which imposes that pressure upon the system to which it is mechanically connected. The Earth's atmosphere is often used as a pressure reservoir. The ocean can act as temperature reservoir when used to cool power plants.


== Conjugate variables ==

The central concept of thermodynamics is that of energy, the ability to do work. By the First Law, the total energy of a system and its surroundings is conserved. Energy may be transferred into a system by heating, compression, or addition of matter, and extracted from a system by cooling, expansion, or extraction of matter. In mechanics, for example, energy transfer equals the product of the force applied to a body and the resulting displacement.
Conjugate variables are pairs of thermodynamic concepts, with the first being akin to a ""force"" applied to some thermodynamic system, the second being akin to the resulting ""displacement,"" and the product of the two equaling the amount of energy transferred. The common conjugate variables are:

Pressure-volume (the mechanical parameters);
Temperature-entropy (thermal parameters);
Chemical potential-particle number (material parameters).


== Potentials ==
Thermodynamic potentials are different quantitative measures of the stored energy in a system. Potentials are used to measure the energy changes in systems as they evolve from an initial state to a final state. The potential used depends on the constraints of the system, such as constant temperature or pressure. For example, the Helmholtz and Gibbs energies are the energies available in a system to do useful work when the temperature and volume or the pressure and temperature are fixed, respectively.
The five most well known potentials are:

where 
  
    
      
        T
      
    
    {\displaystyle T}
   is the temperature, 
  
    
      
        S
      
    
    {\displaystyle S}
   the entropy, 
  
    
      
        p
      
    
    {\displaystyle p}
   the pressure, 
  
    
      
        V
      
    
    {\displaystyle V}
   the volume, 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   the chemical potential, 
  
    
      
        N
      
    
    {\displaystyle N}
   the number of particles in the system, and 
  
    
      
        i
      
    
    {\displaystyle i}
   is the count of particles types in the system.
Thermodynamic potentials can be derived from the energy balance equation applied to a thermodynamic system. Other thermodynamic potentials can also be obtained through Legendre transformation.


== Applied fields ==


== See also ==

Thermodynamic process path


=== Lists and timelines ===
List of important publications in thermodynamics
List of textbooks on thermodynamics and statistical mechanics
List of thermal conductivities
List of thermodynamic properties
Table of thermodynamic equations
Timeline of thermodynamics


== Notes ==


== References ==


== Further reading ==
Goldstein, Martin & Inge F. (1993). The Refrigerator and the Universe. Harvard University Press. ISBN 978-0-674-75325-9. OCLC 32826343. A nontechnical introduction, good on historical and interpretive matters.
Kazakov, Andrei; Muzny, Chris D.; Chirico, Robert D.; Diky, Vladimir V.; Frenkel, Michael (2008). ""Web Thermo Tables – an On-Line Version of the TRC Thermodynamic Tables"". Journal of Research of the National Institute of Standards and Technology. 113 (4): 209–220. doi:10.6028/jres.113.016. ISSN 1044-677X. PMC 4651616. PMID 27096122.
Gibbs J.W. (1928). The Collected Works of J. Willard Gibbs Thermodynamics. New York: Longmans, Green and Co. Vol. 1, pp. 55–349.
Guggenheim E.A. (1933). Modern thermodynamics by the methods of Willard Gibbs. London: Methuen & co. ltd.
Denbigh K. (1981). The Principles of Chemical Equilibrium: With Applications in Chemistry and Chemical Engineering. London: Cambridge University Press.
Stull, D.R., Westrum Jr., E.F. and Sinke, G.C. (1969). The Chemical Thermodynamics of Organic Compounds. London: John Wiley and Sons, Inc.CS1 maint: multiple names: authors list (link)
Bazarov I.P. (2010). Thermodynamics: Textbook. St. Petersburg: Lan publishing house. p. 384. ISBN 978-5-8114-1003-3. 5th ed. (in Russian)
Bawendi Moungi G., Alberty Robert A. and Silbey Robert J. (2004). Physical Chemistry. J. Wiley & Sons, Incorporated.
Alberty Robert A. (2003). Thermodynamics of Biochemical Reactions. Wiley-Interscience.
Alberty Robert A. (2006). Biochemical Thermodynamics: Applications of Mathematica. John Wiley & Sons, Inc. ISBN 978-0-471-75798-6. PMID 16878778.
Dill Ken A., Bromberg Sarina (2011). Molecular Driving Forces: Statistical Thermodynamics in Biology, Chemistry, Physics, and Nanoscience. Garland Science. ISBN 978-0-8153-4430-8.
M. Scott Shell (2015). Thermodynamics and Statistical Mechanics: An Integrated Approach. Cambridge University Press. ISBN 978-1107656789.
Douglas E. Barrick (2018). Biomolecular Thermodynamics: From Theory to Applications. CRC Press. ISBN 978-1-4398-0019-5.The following titles are more technical:

Bejan, Adrian (2016). Advanced Engineering Thermodynamics (4 ed.). Wiley. ISBN 978-1-119-05209-8.
Cengel, Yunus A., & Boles, Michael A. (2002). Thermodynamics – an Engineering Approach. McGraw Hill. ISBN 978-0-07-238332-4. OCLC 45791449.CS1 maint: multiple names: authors list (link)
Dunning-Davies, Jeremy (1997). Concise Thermodynamics: Principles and Applications. Horwood Publishing. ISBN 978-1-8985-6315-0. OCLC 36025958.
Kroemer, Herbert & Kittel, Charles (1980). Thermal Physics. W.H. Freeman Company. ISBN 978-0-7167-1088-2. OCLC 32932988.


== External links ==
 Media related to Thermodynamics at Wikimedia CommonsCallendar, Hugh Longbourne (1911). ""Thermodynamics"" . Encyclopædia Britannica. 26 (11th ed.). pp. 808–814.
Thermodynamics Data & Property Calculation Websites
Thermodynamics Educational Websites
Thermodynamics at ScienceWorld
Biochemistry Thermodynamics
Thermodynamics and Statistical Mechanics
Engineering Thermodynamics – A Graphical Approach
Thermodynamics and Statistical Mechanics by Richard Fitzpatrick","pandas(index=125, _1=125, text='thermodynamics is a branch of physics that deals with heat, work, and temperature, and their relation to energy, radiation, and physical properties of matter. the behavior of these quantities is governed by the four laws of thermodynamics which convey a quantitative description using measurable macroscopic physical quantities, but may be explained in terms of microscopic constituents by statistical mechanics. thermodynamics applies to a wide variety of topics in science and engineering, especially physical chemistry, biochemistry, chemical engineering and mechanical engineering, but also in other complex fields such as meteorology. historically, thermodynamics developed out of a desire to increase the efficiency of early steam engines, particularly through the work of french physicist nicolas léonard sadi carnot (1824) who believed that engine efficiency was the key that could help france win the napoleonic wars. scots-irish physicist lord kelvin was the first to formulate a concise definition of thermodynamics in 1854 which stated, ""thermo-dynamics is the subject of the relation of heat to forces acting between contiguous parts of bodies, and the relation of heat to electrical agency."" the initial application of thermodynamics to mechanical heat engines was quickly extended to the study of chemical compounds and chemical reactions. chemical thermodynamics studies the nature of the role of entropy in the process of chemical reactions and has provided the bulk of expansion and knowledge of the field. other formulations of thermodynamics emerged. statistical thermodynamics, or statistical mechanics, concerns itself with statistical predictions of the collective motion of particles from their microscopic behavior. in 1909, constantin carathéodory presented a purely mathematical approach in an axiomatic formulation, a description often referred to as geometrical thermodynamics.   == introduction == a description of any thermodynamic system employs the four laws of thermodynamics that form an axiomatic basis. the first law specifies that energy can be exchanged between physical systems as heat and work. the second law defines the existence of a quantity called entropy, that describes the direction, thermodynamically, that a system can evolve and quantifies the state of order of a system and that can be used to quantify the useful work that can be extracted from the system.in thermodynamics, interactions between large ensembles of objects are studied and categorized. central to this are the concepts of the thermodynamic system and its surroundings. a system is composed of particles, whose average motions define its properties, and those properties are in turn related to one another through equations of state. properties can be combined to express internal energy and thermodynamic potentials, which are useful for determining conditions for equilibrium and spontaneous processes. with these tools, thermodynamics can be used to describe how systems respond to changes in their environment. this can be applied to a wide variety of topics in science and engineering, such as engines, phase transitions, chemical reactions, transport phenomena, and even black holes. the results of thermodynamics are essential for other fields of physics and for chemistry, chemical engineering, corrosion engineering, aerospace engineering, mechanical engineering, cell biology, biomedical engineering, materials science, and economics, to name a few.this article is focused mainly on classical thermodynamics which primarily studies systems in thermodynamic equilibrium. non-equilibrium thermodynamics is often treated as an extension of the classical treatment, but statistical mechanics has brought many advances to that field.   == history == the history of thermodynamics as a scientific discipline generally begins with otto von guericke who, in 1650, built and designed the world\'s first vacuum pump and demonstrated a vacuum using his magdeburg hemispheres. guericke was driven to make a vacuum in order to disprove aristotle\'s long-held supposition that \'nature abhors a vacuum\'. shortly after guericke, the anglo-irish physicist and chemist robert boyle had learned of guericke\'s designs and, in 1656, in coordination with english scientist robert hooke, built an air pump. using this pump, boyle and hooke noticed a correlation between pressure, temperature, and volume. in time, boyle\'s law was formulated, which states that pressure and volume are inversely proportional. then, in 1679, based on these concepts, an associate of boyle\'s named denis papin built a steam digester, which was a closed vessel with a tightly fitting lid that confined steam until a high pressure was generated. later designs implemented a steam release valve that kept the machine from exploding. by watching the valve rhythmically move up and down, papin conceived of the idea of a piston and a cylinder engine. he did not, however, follow through with his design. nevertheless, in 1697, based on papin\'s designs, engineer thomas savery built the first engine, followed by thomas newcomen in 1712. although these early engines were crude and inefficient, they attracted the attention of the leading scientists of the time. the fundamental concepts of heat capacity and latent heat, which were necessary for the development of thermodynamics, were developed by professor joseph black at the university of glasgow, where james watt was employed as an instrument maker. black and watt performed experiments together, but it was watt who conceived the idea of the external condenser which resulted in a large increase in steam engine efficiency. drawing on all the previous work led sadi carnot, the ""father of thermodynamics"", to publish reflections on the motive power of fire (1824), a discourse on heat, power, energy and engine efficiency. the book outlined the basic energetic relations between the carnot engine, the carnot cycle, and motive power. it marked the start of thermodynamics as a modern science.the first thermodynamic textbook was written in 1859 by william rankine, originally trained as a physicist and a civil and mechanical engineering professor at the university of glasgow. the first and second laws of thermodynamics emerged simultaneously in the 1850s, primarily out of the works of william rankine, rudolf clausius, and william thomson (lord kelvin).the foundations of statistical thermodynamics were set out by physicists such as james clerk maxwell, ludwig boltzmann, max planck, rudolf clausius and j. willard gibbs. during the years 1873–76 the american mathematical physicist josiah willard gibbs published a series of three papers, the most famous being on the equilibrium of heterogeneous substances, in which he showed how thermodynamic processes, including chemical reactions, could be graphically analyzed, by studying the energy, entropy, volume, temperature and pressure of the thermodynamic system in such a manner, one can determine if a process would occur spontaneously. also pierre duhem in the 19th century wrote about chemical thermodynamics. during the early 20th century, chemists such as gilbert n. lewis, merle randall, and e. a. guggenheim applied the mathematical methods of gibbs to the analysis of chemical processes.   == etymology == the etymology of thermodynamics has an intricate history. it was first spelled in a hyphenated form as an adjective (thermo-dynamic) and from 1854 to 1868 as the noun thermo-dynamics to represent the science of generalized heat engines.american biophysicist donald haynie claims that thermodynamics was coined in 1840 from the greek root θέρμη therme, meaning “heat”, and δύναμις dynamis, meaning “power”.pierre perrot claims that the term thermodynamics was coined by james joule in 1858 to designate the science of relations between heat and power, however, joule never used that term, but used instead the term perfect thermo-dynamic engine in reference to thomson\'s 1849 phraseology.by 1858, thermo-dynamics, as a functional term, was used in william thomson\'s paper ""an account of carnot\'s theory of the motive power of heat.""   == branches of thermodynamics == the study of thermodynamical systems has developed into several related branches, each using a different fundamental model as a theoretical or experimental basis, or applying the principles to varying types of systems. list of important publications in thermodynamics list of textbooks on thermodynamics and statistical mechanics list of thermal conductivities list of thermodynamic properties table of thermodynamic equations timeline of thermodynamics   == notes ==   == references ==   == further reading == goldstein, martin & inge f. (1993). the refrigerator and the universe. harvard university press. isbn 978-0-674-75325-9. oclc 32826343. a nontechnical introduction, good on historical and interpretive matters. kazakov, andrei; muzny, chris d.; chirico, robert d.; diky, vladimir v.; frenkel, michael (2008). ""web thermo tables – an on-line version of the trc thermodynamic tables"". journal of research of the national institute of standards and technology. 113 (4): 209–220. doi:10.6028/jres.113.016. issn 1044-677x. pmc 4651616. pmid 27096122. gibbs j.w. (1928). the collected works of j. willard gibbs thermodynamics. new york: longmans, green and co. vol. 1, pp. 55–349. guggenheim e.a. (1933). modern thermodynamics by the methods of willard gibbs. london: methuen & co. ltd. denbigh k. (1981). the principles of chemical equilibrium: with applications in chemistry and chemical engineering. london: cambridge university press. stull, d.r., westrum jr., e.f. and sinke, g.c. (1969). the chemical thermodynamics of organic compounds. london: john wiley and sons, inc.cs1 maint: multiple names: authors list (link) bazarov i.p. (2010). thermodynamics: textbook. st. petersburg: lan publishing house. p. 384. isbn 978-5-8114-1003-3. 5th ed. (in russian) bawendi moungi g., alberty robert a. and silbey robert j. (2004). physical chemistry. j. wiley & sons, incorporated. alberty robert a. (2003). thermodynamics of biochemical reactions. wiley-interscience. alberty robert a. (2006). biochemical thermodynamics: applications of mathematica. john wiley & sons, inc. isbn 978-0-471-75798-6. pmid 16878778. dill ken a., bromberg sarina (2011). molecular driving forces: statistical thermodynamics in biology, chemistry, physics, and nanoscience. garland science. isbn 978-0-8153-4430-8. m. scott shell (2015). thermodynamics and statistical mechanics: an integrated approach. cambridge university press. isbn 978-1107656789. douglas e. barrick (2018). biomolecular thermodynamics: from theory to applications. crc press. isbn 978-1-4398-0019-5.the following titles are more technical:  bejan, adrian (2016). advanced engineering thermodynamics (4 ed.). wiley. isbn 978-1-119-05209-8. cengel, yunus a., & boles, michael a. (2002). thermodynamics – an engineering approach. mcgraw hill. isbn 978-0-07-238332-4. oclc 45791449.cs1 maint: multiple names: authors list (link) dunning-davies, jeremy (1997). concise thermodynamics: principles and applications. horwood publishing. isbn 978-1-8985-6315-0. oclc 36025958. kroemer, herbert & kittel, charles (1980). thermal physics. w.h. freeman company. isbn 978-0-7167-1088-2. oclc 32932988.   == external links == media related to thermodynamics at wikimedia commonscallendar, hugh longbourne (1911). ""thermodynamics"" . encyclopædia britannica. 26 (11th ed.). pp. 808–814. thermodynamics data & property calculation websites thermodynamics educational websites thermodynamics at scienceworld biochemistry thermodynamics thermodynamics and statistical mechanics engineering thermodynamics – a graphical approach thermodynamics and statistical mechanics by richard fitzpatrick')"
126,"Civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewerage systems, pipelines, structural components of buildings, and railways.Civil engineering is traditionally broken into a number of sub-disciplines. It is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. Civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global Fortune 500 companies.


== History ==


=== Civil engineering as a discipline ===
Civil engineering is the application of physical and scientific principles for solving the problems of society, and its history is intricately linked to advances in the understanding of physics and mathematics throughout history. Because civil engineering is a broad profession, including several specialized sub-disciplines, its history is linked to knowledge of structures, materials science, geography, geology, soils, hydrology, environmental science, mechanics, project management, and other fields.Throughout ancient and medieval history most architectural design and construction was carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. Knowledge was retained in guilds and seldom supplanted by advances. Structures, roads, and infrastructure that existed were repetitive, and increases in scale were incremental.One of the earliest examples of a scientific approach to physical and mathematical problems applicable to civil engineering is the work of Archimedes in the 3rd century BC, including Archimedes Principle, which underpins our understanding of buoyancy, and practical solutions such as Archimedes' screw. Brahmagupta, an Indian mathematician, used arithmetic in the 7th century AD, based on Hindu-Arabic numerals, for excavation (volume) computations.


=== Civil engineering profession ===

Engineering has been an aspect of life since the beginnings of human existence. The earliest practice of civil engineering may have commenced between 4000 and 2000 BC in ancient Egypt, the Indus Valley Civilization, and Mesopotamia (ancient Iraq) when humans started to abandon a nomadic existence, creating a need for the construction of shelter. During this time, transportation became increasingly important leading to the development of the wheel and sailing.
Until modern times there was no clear distinction between civil engineering and architecture, and the term engineer and architect were mainly geographical variations referring to the same occupation, and often used interchangeably. The construction of pyramids in Egypt (circa 2700–2500 BC) were some of the first instances of large structure constructions. Other ancient historic civil engineering constructions include the Qanat water management system (the oldest is older than 3000 years and longer than 71 km,) the Parthenon by Iktinos in Ancient Greece (447–438 BC), the Appian Way by Roman engineers (c. 312 BC), the Great Wall of China by General Meng T'ien under orders from Ch'in Emperor Shih Huang Ti (c. 220 BC) and the stupas constructed in ancient Sri Lanka like the Jetavanaramaya and the extensive irrigation works in Anuradhapura. The Romans developed civil structures throughout their empire, including especially aqueducts, insulae, harbors, bridges, dams and roads.

In the 18th century, the term civil engineering was coined to incorporate all things civilian as opposed to military engineering. In 1747, the first institution for the teaching of civil engineering, the École Nationale des Ponts et Chaussées was established in France; and more examples followed in other European countries, like Spain. The first self-proclaimed civil engineer was John Smeaton, who constructed the Eddystone Lighthouse. In 1771 Smeaton and some of his colleagues formed the Smeatonian Society of Civil Engineers, a group of leaders of the profession who met informally over dinner. Though there was evidence of some technical meetings, it was little more than a social society.

In 1818 the Institution of Civil Engineers was founded in London, and in 1820 the eminent engineer Thomas Telford became its first president. The institution received a Royal Charter in 1828, formally recognising civil engineering as a profession. Its charter defined civil engineering as:the art of directing the great sources of power in nature for the use and convenience of man, as the means of production and of traffic in states, both for external and internal trade, as applied in the construction of roads, bridges, aqueducts, canals, river navigation and docks for internal intercourse and exchange, and in the construction of ports, harbours, moles, breakwaters and lighthouses, and in the art of navigation by artificial power for the purposes of commerce, and in the construction and application of machinery, and in the drainage of cities and towns.


=== Civil engineering education ===
The first private college to teach civil engineering in the United States was Norwich University, founded in 1819 by Captain Alden Partridge. The first degree in civil engineering in the United States was awarded by Rensselaer Polytechnic Institute in 1835. The first such degree to be awarded to a woman was granted by Cornell University to Nora Stanton Blatch in 1905.In the UK during the early 19th century, the division between civil engineering and military engineering (served by the Royal Military Academy, Woolwich), coupled with the demands of the Industrial Revolution, spawned new engineering education initiatives: the Class of Civil Engineering and Mining was founded at King's College London in 1838, mainly as a response to the growth of the railway system and the need for more qualified engineers, the private College for Civil Engineers in Putney was established in 1839, and the UK's first Chair of Engineering was established at the University of Glasgow in 1840.


== Education ==
Civil engineers typically possess an academic degree in civil engineering. The length of study is three to five years, and the completed degree is designated as a bachelor of technology, or a bachelor of engineering. The curriculum generally includes classes in physics, mathematics, project management, design and specific topics in civil engineering. After taking basic courses in most sub-disciplines of civil engineering, they move on to specialize in one or more sub-disciplines at advanced levels. While an undergraduate degree (BEng/BSc) normally provides successful students with industry-accredited qualification, some academic institutions offer post-graduate degrees (MEng/MSc), which allow students to further specialize in their particular area of interest.


== Practicing engineers ==
In most countries, a bachelor's degree in engineering represents the first step towards professional certification, and a professional body certifies the degree program. After completing a certified degree program, the engineer must satisfy a range of requirements including work experience and exam requirements before being certified. Once certified, the engineer is designated as a professional engineer (in the United States, Canada and South Africa), a chartered engineer (in most Commonwealth countries), a chartered professional engineer (in Australia and New Zealand), or a European engineer (in most countries of the European Union). There are international agreements between relevant professional bodies to allow engineers to practice across national borders.
The benefits of certification vary depending upon location. For example, in the United States and Canada, ""only a licensed professional engineer may prepare, sign and seal, and submit engineering plans and drawings to a public authority for approval, or seal engineering work for public and private clients."" This requirement is enforced under provincial law such as the Engineers Act in Quebec. No such legislation has been enacted in other countries including the United Kingdom. In Australia, state licensing of engineers is limited to the state of Queensland. Almost all certifying bodies maintain a code of ethics which all members must abide by.Engineers must obey contract law in their contractual relationships with other parties. In cases where an engineer's work fails, they may be subject to the law of tort of negligence, and in extreme cases, criminal charges. An engineer's work must also comply with numerous other rules and regulations such as building codes and environmental law.


== Sub-disciplines ==

There are a number of sub-disciplines within the broad field of civil engineering. General civil engineers work closely with surveyors and specialized civil engineers to design grading, drainage, pavement, water supply, sewer service, dams, electric and communications supply. General civil engineering is also referred to as site engineering, a branch of civil engineering that primarily focuses on converting a tract of land from one usage to another. Site engineers spend time visiting project sites, meeting with stakeholders, and preparing construction plans. Civil engineers apply the principles of geotechnical engineering, structural engineering, environmental engineering, transportation engineering and construction engineering to residential, commercial, industrial and public works projects of all sizes and levels of construction.


=== Coastal engineering ===

Coastal engineering is concerned with managing coastal areas. In some jurisdictions, the terms sea defense and coastal protection mean defense against flooding and erosion, respectively. The term coastal defense is the more traditional term, but coastal management has become more popular as the field has expanded to techniques that allow erosion to claim land.


=== Construction engineering ===

Construction engineering involves planning and execution, transportation of materials, site development based on hydraulic, environmental, structural and geotechnical engineering. As construction firms tend to have higher business risk than other types of civil engineering firms do, construction engineers often engage in more business-like transactions, for example, drafting and reviewing contracts, evaluating logistical operations, and monitoring prices of supplies.


=== Earthquake engineering ===

Earthquake engineering involves designing structures to withstand hazardous earthquake exposures. Earthquake engineering is a sub-discipline of structural engineering. The main objectives of earthquake engineering are to understand interaction of structures on the shaky ground; foresee the consequences of possible earthquakes; and design, construct and maintain structures to perform at earthquake in compliance with building codes.


=== Environmental engineering ===

Environmental engineering is the contemporary term for sanitary engineering, though sanitary engineering traditionally had not included much of the hazardous waste management and environmental remediation work covered by environmental engineering. Public health engineering and environmental health engineering are other terms being used.
Environmental engineering deals with treatment of chemical, biological, or thermal wastes, purification of water and air, and remediation of contaminated sites after waste disposal or accidental contamination. Among the topics covered by environmental engineering are pollutant transport, water purification, waste water treatment, air pollution, solid waste treatment, recycling, and hazardous waste management. Environmental engineers administer pollution reduction, green engineering, and industrial ecology. Environmental engineers also compile information on environmental consequences of proposed actions.


=== Forensic engineering ===

Forensic engineering is the investigation of materials, products, structures or components that fail or do not operate or function as intended, causing personal injury or damage to property. The consequences of failure are dealt with by the law of product liability. The field also deals with retracing processes and procedures leading to accidents in operation of vehicles or machinery. The subject is applied most commonly in civil law cases, although it may be of use in criminal law cases. Generally the purpose of a Forensic engineering investigation is to locate cause or causes of failure with a view to improve performance or life of a component, or to assist a court in determining the facts of an accident. It can also involve investigation of intellectual property claims, especially patents.


=== Geotechnical engineering ===

Geotechnical engineering studies rock and soil supporting civil engineering systems. Knowledge from the field of soil science, materials science, mechanics, and hydraulics is applied to safely and economically design foundations, retaining walls, and other structures. Environmental efforts to protect groundwater and safely maintain landfills have spawned a new area of research called geo-environmental engineering.Identification of soil properties presents challenges to geotechnical engineers. Boundary conditions are often well defined in other branches of civil engineering, but unlike steel or concrete, the material properties and behavior of soil are difficult to predict due to its variability and limitation on investigation. Furthermore, soil exhibits nonlinear (stress-dependent) strength, stiffness, and dilatancy (volume change associated with application of shear stress), making studying soil mechanics all the more difficult. Geotechnical engineers frequently work with professional geologists and soil scientists.


=== Materials science and engineering ===

Materials science is closely related to civil engineering. It studies fundamental characteristics of materials, and deals with ceramics such as concrete and mix asphalt concrete, strong metals such as aluminum and steel, and thermosetting polymers including polymethylmethacrylate (PMMA) and carbon fibers.
Materials engineering involves protection and prevention (paints and finishes). Alloying combines two types of metals to produce another metal with desired properties. It incorporates elements of applied physics and chemistry. With recent media attention on nanoscience and nanotechnology, materials engineering has been at the forefront of academic research. It is also an important part of forensic engineering and failure analysis.


=== Site development and planning ===

Site development, also known as site planning, is focused on the planning and development potential of a site as well as addressing possible impacts from permitting issues and environmental challenges.


=== Structural engineering ===

Structural engineering is concerned with the structural design and structural analysis of buildings, bridges, towers, flyovers (overpasses), tunnels, off shore structures like oil and gas fields in the sea, aerostructure and other structures. This involves identifying the loads which act upon a structure and the forces and stresses which arise within that structure due to those loads, and then designing the structure to successfully support and resist those loads. The loads can be self weight of the structures, other dead load, live loads, moving (wheel) load, wind load, earthquake load, load from temperature change etc. The structural engineer must design structures to be safe for their users and to successfully fulfill the function they are designed for (to be serviceable). Due to the nature of some loading conditions, sub-disciplines within structural engineering have emerged, including wind engineering and earthquake engineering.Design considerations will include strength, stiffness, and stability of the structure when subjected to loads which may be static, such as furniture or self-weight, or dynamic, such as wind, seismic, crowd or vehicle loads, or transitory, such as temporary construction loads or impact. Other considerations include cost, constructability, safety, aesthetics and sustainability.


=== Surveying ===

Surveying is the process by which a surveyor measures certain dimensions that occur on or near the surface of the Earth. Surveying equipment such as levels and theodolites are used for accurate measurement of angular deviation, horizontal, vertical and slope distances. With computerisation, electronic distance measurement (EDM), total stations, GPS surveying and laser scanning have to a large extent supplanted traditional instruments. Data collected by survey measurement is converted into a graphical representation of the Earth's surface in the form of a map. This information is then used by civil engineers, contractors and realtors to design from, build on, and trade, respectively. Elements of a structure must be sized and positioned in relation to each other and to site boundaries and adjacent structures.
Although surveying is a distinct profession with separate qualifications and licensing arrangements, civil engineers are trained in the basics of surveying and mapping, as well as geographic information systems. Surveyors also lay out the routes of railways, tramway tracks, highways, roads, pipelines and streets as well as position other infrastructure, such as harbors, before construction.

Land surveying
In the United States, Canada, the United Kingdom and most Commonwealth countries land surveying is considered to be a separate and distinct profession. Land surveyors are not considered to be engineers, and have their own professional associations and licensing requirements. The services of a licensed land surveyor are generally required for boundary surveys (to establish the boundaries of a parcel using its legal description) and subdivision plans (a plot or map based on a survey of a parcel of land, with boundary lines drawn inside the larger parcel to indicate the creation of new boundary lines and roads), both of which are generally referred to as Cadastral surveying.
Construction surveyingConstruction surveying is generally performed by specialized technicians. Unlike land surveyors, the resulting plan does not have legal status. Construction surveyors perform the following tasks:

Surveying existing conditions of the future work site, including topography, existing buildings and infrastructure, and underground infrastructure when possible;
""lay-out"" or ""setting-out"": placing reference points and markers that will guide the construction of new structures such as roads or buildings;
Verifying the location of structures during construction;
As-Built surveying: a survey conducted at the end of the construction project to verify that the work authorized was completed to the specifications set on plans.


=== Transportation engineering ===
Transportation engineering is concerned with moving people and goods efficiently, safely, and in a manner conducive to a vibrant community. This involves specifying, designing, constructing, and maintaining transportation infrastructure which includes streets, canals, highways, rail systems, airports, ports, and mass transit. It includes areas such as transportation design, transportation planning, traffic engineering, some aspects of urban engineering, queueing theory, pavement engineering, Intelligent Transportation System (ITS), and infrastructure management.


=== Municipal or urban engineering ===

Municipal engineering is concerned with municipal infrastructure. This involves specifying, designing, constructing, and maintaining streets, sidewalks, water supply networks, sewers, street lighting, municipal solid waste management and disposal, storage depots for various bulk materials used for maintenance and public works (salt, sand, etc.), public parks and cycling infrastructure. In the case of underground utility networks, it may also include the civil portion (conduits and access chambers) of the local distribution networks of electrical and telecommunications services. It can also include the optimizing of waste collection and bus service networks. Some of these disciplines overlap with other civil engineering specialties, however municipal engineering focuses on the coordination of these infrastructure networks and services, as they are often built simultaneously, and managed by the same municipal authority.  Municipal engineers may also design the site civil works for large buildings, industrial plants or campuses (i.e. access roads, parking lots, potable water supply, treatment or pretreatment of waste water, site drainage, etc.)


=== Water resources engineering ===

Water resources engineering is concerned with the collection and management of water (as a natural resource). As a discipline it therefore combines elements of hydrology, environmental science, meteorology, conservation, and resource management. This area of civil engineering relates to the prediction and management of both the quality and the quantity of water in both underground (aquifers) and above ground (lakes, rivers, and streams) resources. Water resource engineers analyze and model very small to very large areas of the earth to predict the amount and content of water as it flows into, through, or out of a facility. Although the actual design of the facility may be left to other engineers.

Hydraulic engineering is concerned with the flow and conveyance of fluids, principally water. This area of civil engineering is intimately related to the design of pipelines, water supply network, drainage facilities (including bridges, dams, channels, culverts, levees, storm sewers), and canals. Hydraulic engineers design these facilities using the concepts of fluid pressure, fluid statics, fluid dynamics, and hydraulics, among others.


=== Civil engineering systems ===
Civil engineering systems is a discipline that promotes the use of systems thinking to manage complexity and change in civil engineering within its wider public context. It posits that the proper development of civil engineering infrastructure requires a holistic, coherent understanding of the relationships between all of the important factors that contribute to successful projects while at the same time emphasizing the importance of attention to technical detail. Its purpose is to help integrate the entire civil engineering project life cycle from conception, through planning, designing, making, operating to decommissioning.


== See also ==


=== Associations ===


== References ==


== Further reading ==
W.F. Chen; J.Y. Richard Liew, eds. (2002). The Civil Engineering Handbook. CRC Press. ISBN 978-0-8493-0958-8.
Jonathan T. Ricketts; M. Kent Loftin; Frederick S. Merritt, eds. (2004). Standard handbook for civil engineers (5 ed.). McGraw Hill. ISBN 978-0-07-136473-7.
Muir Wood, David (2012). Civil Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-957863-4.
Blockley, David (2014). Structural Engineering: a very short introduction. New York: Oxford University Press. ISBN 978-0-19-967193-9.


== External links ==
The Institution of Civil Engineers
Civil Engineering Software Database
The Institution of Civil Engineering Surveyors
Civil engineering classes, from MIT OpenCourseWare","pandas(index=126, _1=126, text='civil engineering is a professional engineering discipline that deals with the design, construction, and maintenance of the physical and naturally built environment, including public works such as roads, bridges, canals, dams, airports, sewerage systems, pipelines, structural components of buildings, and railways.civil engineering is traditionally broken into a number of sub-disciplines. it is considered the second-oldest engineering discipline after military engineering, and it is defined to distinguish non-military engineering from military engineering. civil engineering can take place in the public sector from municipal public works departments through to federal government agencies, and in the private sector from locally based firms to global fortune 500 companies.   == history == == references ==   == further reading == w.f. chen; j.y. richard liew, eds. (2002). the civil engineering handbook. crc press. isbn 978-0-8493-0958-8. jonathan t. ricketts; m. kent loftin; frederick s. merritt, eds. (2004). standard handbook for civil engineers (5 ed.). mcgraw hill. isbn 978-0-07-136473-7. muir wood, david (2012). civil engineering: a very short introduction. new york: oxford university press. isbn 978-0-19-957863-4. blockley, david (2014). structural engineering: a very short introduction. new york: oxford university press. isbn 978-0-19-967193-9.   == external links == the institution of civil engineers civil engineering software database the institution of civil engineering surveyors civil engineering classes, from mit opencourseware')"
127,"Structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the 'bones and muscles' that create the form and shape of man-made structures. Structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. The structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. They can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  See glossary of structural engineering.
Structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. Structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. Structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.


== History ==

Structural engineering dates back to 2700 B.C.E. when the step pyramid for Pharaoh Djoser was built by Imhotep, the first engineer in history known by name. Pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).The structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. The limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 MPa (MPa = Pa × 106). Therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid's geometry.
Throughout ancient and medieval history most architectural design and construction were carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. No theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of 'what had worked before'. Knowledge was retained by guilds and seldom supplanted by advances. Structures were repetitive, and increases in scale were incremental.No record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of a structural engineer only really took shape with the Industrial Revolution and the re-invention of concrete (see History of Concrete. The physical sciences underlying structural engineering began to be understood in the Renaissance and have since developed into computer-based applications pioneered in the 1970s.


=== Timeline ===

1452–1519 Leonardo da Vinci made many contributions
1638: Galileo Galilei published the book Two New Sciences in which he examined the failure of simple
1660: Hooke's law by Robert Hooke
1687: Isaac Newton published Philosophiæ Naturalis Principia Mathematica which contains the Newton's laws of motion
1750: Euler–Bernoulli beam equation
1700–1782: Daniel Bernoulli introduced the principle of virtual work
1707–1783: Leonhard Euler developed the theory of buckling of columns
1826: Claude-Louis Navier published a treatise on the elastic behaviors of structures
1873: Carlo Alberto Castigliano presented his dissertation ""Intorno ai sistemi elastici"", which contains his theorem for computing displacement as the partial derivative of the strain energy. This theorem includes the method of ""least work"" as a special case
1874: Otto Mohr formalized the idea of a statically indeterminate structure.
1922: Timoshenko corrects the Euler-Bernoulli beam equation
1936: Hardy Cross' publication of the moment distribution method, an important innovation in the design of continuous frames.
1941: Alexander Hrennikoff solved the discretization of plane elasticity problems using a lattice framework
1942: R. Courant divided a domain into finite subregions
1956: J. Turner, R. W. Clough, H. C. Martin, and L. J. Topp's paper on the ""Stiffness and Deflection of Complex Structures"" introduces the name ""finite-element method"" and is widely recognized as the first comprehensive treatment of the method as it is known today


=== Structural failure ===

The history of structural engineering contains many collapses and failures.  Sometimes this is due to obvious negligence, as in the case of the Pétion-Ville school collapse, in which Rev. Fortin Augustin "" constructed the building all by himself, saying he didn't need an engineer as he had good knowledge of construction"" following a partial collapse of the three-story schoolhouse that sent neighbors fleeing. The final collapse killed 94 people, mostly children.
In other cases structural failures require careful study, and the results of these inquiries have resulted in improved practices and a greater understanding of the science of structural engineering.  Some such studies are the result of forensic engineering investigations where the original engineer seems to have done everything in accordance with the state of the profession and acceptable practice yet a failure still eventuated.  A famous case of structural knowledge and practice being advanced in this manner can be found in a series of failures involving box girders which collapsed in Australia during the 1970s.


== Theory ==

Structural engineering depends upon a detailed knowledge of applied mechanics, materials science, and applied mathematics to understand and predict how structures support and resist self-weight and imposed loads. To apply the knowledge successfully a structural engineer generally requires detailed knowledge of relevant empirical and theoretical design codes, the techniques of structural analysis, as well as some knowledge of the corrosion resistance of the materials and structures, especially when those structures are exposed to the external environment. Since the 1990s, specialist software has become available to aid in the design of structures, with the functionality to assist in the drawing, analyzing and designing of structures with maximum precision; examples include AutoCAD, StaadPro, ETABS, Prokon, Revit Structure, Inducta RCB, etc. Such software may also take into consideration environmental loads, such as earthquakes and winds.


== Profession ==

Structural engineers are responsible for engineering design and structural analysis. Entry-level structural engineers may design the individual structural elements of a structure, such as the beams and columns of a building. More experienced engineers may be responsible for the structural design and integrity of an entire system, such as a building.
Structural engineers often specialize in particular types of structures, such as buildings, bridges, pipelines, industrial, tunnels, vehicles, ships, aircraft, and spacecraft.  Structural engineers who specialize in buildings often specialize in particular construction materials such as concrete, steel, wood, masonry, alloys, and composites, and may focus on particular types of buildings such as offices, schools, hospitals, residential, and so forth.
Structural engineering has existed since humans first started to construct their structures. It became a more defined and formalized profession with the emergence of architecture as a distinct profession from engineering during the industrial revolution in the late 19th century. Until then, the architect and the structural engineer were usually one and the same thing – the master builder. Only with the development of specialized knowledge of structural theories that emerged during the 19th and early 20th centuries, did the professional structural engineers come into existence.
The role of a structural engineer today involves a significant understanding of both static and dynamic loading and the structures that are available to resist them. The complexity of modern structures often requires a great deal of creativity from the engineer in order to ensure the structures support and resist the loads they are subjected to. A structural engineer will typically have a four or five-year undergraduate degree, followed by a minimum of three years of professional practice before being considered fully qualified.
Structural engineers are licensed or accredited by different learned societies and regulatory bodies around the world (for example, the Institution of Structural Engineers in the UK). Depending on the degree course they have studied and/or the jurisdiction they are seeking licensure in, they may be accredited (or licensed) as just structural engineers, or as civil engineers, or as both civil and structural engineers.
Another international organisation is IABSE(International Association for Bridge and Structural Engineering). The aim of that association is to exchange knowledge and to advance the practice of structural engineering worldwide in the service of the profession and society.


== Specializations ==


=== Building structures ===

Structural building engineering includes all structural engineering related to the design of buildings. It is a branch of structural engineering closely affiliated with architecture.
Structural building engineering is primarily driven by the creative manipulation of materials and forms and the underlying mathematical and scientific ideas to achieve an end that fulfills its functional requirements and is structurally safe when subjected to all the loads it could reasonably be expected to experience. This is subtly different from architectural design, which is driven by the creative manipulation of materials and forms, mass, space, volume, texture, and light to achieve an end which is aesthetic, functional, and often artistic.
The architect is usually the lead designer on buildings, with a structural engineer employed as a sub-consultant. The degree to which each discipline leads the design depends heavily on the type of structure. Many structures are structurally simple and led by architecture, such as multi-story office buildings and housing, while other structures, such as tensile structures, shells and gridshells are heavily dependent on their form for their strength, and the engineer may have a more significant influence on the form, and hence much of the aesthetic, than the architect.
The structural design for a building must ensure that the building can stand up safely, able to function without excessive deflections or movements which may cause fatigue of structural elements, cracking or failure of fixtures, fittings or partitions, or discomfort for occupants. It must account for movements and forces due to temperature, creep, cracking, and imposed loads. It must also ensure that the design is practically buildable within acceptable manufacturing tolerances of the materials. It must allow the architecture to work, and the building services to fit within the building and function (air conditioning, ventilation, smoke extract, electrics, lighting, etc.). The structural design of a modern building can be extremely complex and often requires a large team to complete.
Structural engineering specialties for buildings include:

Earthquake engineering
Façade engineering
Fire engineering
Roof engineering
Tower engineering
Wind engineering


=== Earthquake engineering structures ===

Earthquake engineering structures are those engineered to withstand earthquakes.
The main objectives of earthquake engineering are to understand the interaction of structures with the shaking ground, foresee the consequences of possible earthquakes, and design and construct the structures to perform during an earthquake.
Earthquake-proof structures are not necessarily extremely strong like the El Castillo pyramid at Chichen Itza shown above.
One important tool of earthquake engineering is base isolation, which allows the base of a structure to move freely with the ground.


=== Civil engineering structures ===
Civil structural engineering includes all structural engineering related to the built environment. It includes:

The structural engineer is the lead designer on these structures, and often the sole designer. In the design of structures such as these, structural safety is of paramount importance (in the UK, designs for dams, nuclear power stations and bridges must be signed off by a chartered engineer).
Civil engineering structures are often subjected to very extreme forces, such as large variations in temperature, dynamic loads such as waves or traffic, or high pressures from water or compressed gases. They are also often constructed in corrosive environments, such as at sea, in industrial facilities, or below ground.


=== Mechanical structures ===
The principles of structural engineering apply to a variety of mechanical (moveable) structures. The design of static structures assumes they always have the same geometry (in fact, so-called static structures can move significantly, and structural engineering design must take this into account where necessary), but the design of moveable or moving structures must account for fatigue, variation in the method in which load is resisted and significant deflections of structures.
The forces which parts of a machine are subjected to can vary significantly and can do so at a great rate. The forces which a boat or aircraft are subjected to vary enormously and will do so thousands of times over the structure's lifetime. The structural design must ensure that such structures can endure such loading for their entire design life without failing.
These works can require mechanical structural engineering:

Boilers and pressure vessels
Coachworks and carriages
Cranes
Elevators
Escalators
Marine vessels and hulls


=== Aerospace structures ===

Aerospace structure types include launch vehicles, (Atlas, Delta, Titan), missiles (ALCM, Harpoon), Hypersonic vehicles (Space Shuttle), military aircraft (F-16, F-18) and commercial aircraft (Boeing 777, MD-11). Aerospace structures typically consist of thin plates with stiffeners for the external surfaces, bulkheads, and frames to support the shape and fasteners such as welds, rivets, screws, and bolts to hold the components together.


=== Nanoscale structures ===
A nanostructure is an object of intermediate size between molecular and microscopic (micrometer-sized) structures. In describing nanostructures it is necessary to differentiate between the number of dimensions on the nanoscale. Nanotextured surfaces have one dimension on the nanoscale, i.e., only the thickness of the surface of an object is between 0.1 and 100 nm. Nanotubes have two dimensions on the nanoscale, i.e., the diameter of the tube is between 0.1 and 100 nm; its length could be much greater. Finally, spherical nanoparticles have three dimensions on the nanoscale, i.e., the particle is between 0.1 and 100 nm in each spatial dimension. The terms nanoparticles and ultrafine particles (UFP) often are used synonymously although UFP can reach into the micrometer range. The term 'nanostructure' is often used when referring to magnetic technology.


=== Structural engineering for medical science ===

Medical equipment (also known as armamentarium) is designed to aid in the diagnosis, monitoring or treatment of medical conditions. There are several basic types: diagnostic equipment includes medical imaging machines, used to aid in diagnosis; equipment includes infusion pumps, medical lasers, and LASIK surgical machines; medical monitors allow medical staff to measure a patient's medical state. Monitors may measure patient vital signs and other parameters including ECG, EEG, blood pressure, and dissolved gases in the blood; diagnostic medical equipment may also be used in the home for certain purposes, e.g. for the control of diabetes mellitus. A biomedical equipment technician (BMET) is a vital component of the healthcare delivery system. Employed primarily by hospitals, BMETs are the people responsible for maintaining a facility's medical equipment.


== Structural elements ==

Any structure is essentially made up of only a small number of different types of elements:

Columns
Beams
Plates
Arches
Shells
CatenariesMany of these elements can be classified according to form (straight, plane / curve) and dimensionality (one-dimensional / two-dimensional):


=== Columns ===

Columns are elements that carry only axial force (compression) or both axial force and bending (which is technically called a beam-column but practically, just a column). The design of a column must check the axial capacity of the element and the buckling capacity.
The buckling capacity is the capacity of the element to withstand the propensity to buckle. Its capacity depends upon its geometry, material, and the effective length of the column, which depends upon the restraint conditions at the top and bottom of the column. The effective length is 
  
    
      
        K
        ∗
        l
      
    
    {\displaystyle K*l}
   where 
  
    
      
        l
      
    
    {\displaystyle l}
   is the real length of the column and K is the factor dependent on the restraint conditions.
The capacity of a column to carry axial load depends on the degree of bending it is subjected to, and vice versa. This is represented on an interaction chart and is a complex non-linear relationship.


=== Beams ===

A beam may be defined as an element in which one dimension is much greater than the other two and the applied loads are usually normal to the main axis of the element. Beams and columns are called line elements and are often represented by simple lines in structural modeling.

cantilevered (supported at one end only with a fixed connection)
simply supported (fixed against vertical translation at each end and horizontal translation at one end only,  and able to rotate at the supports)
fixed (supported in all directions for translation and rotation at each end)
continuous (supported by three or more supports)
a combination of the above (ex. supported at one end and in the middle)Beams are elements that carry pure bending only. Bending causes one part of the section of a beam (divided along its length) to go into compression and the other part into tension. The compression part must be designed to resist buckling and crushing, while the tension part must be able to adequately resist the tension.


=== Trusses ===

A truss is a structure comprising members and connection points or nodes. When members are connected at nodes and forces are applied at nodes members can act in tension or compression. Members acting in compression are referred to as compression members or struts while members acting in tension are referred to as tension members or ties. Most trusses use gusset plates to connect intersecting elements.  Gusset plates are relatively flexible and unable to transfer bending moments. The connection is usually arranged so that the lines of force in the members are coincident at the joint thus allowing the truss members to act in pure tension or compression.
Trusses are usually used in large-span structures, where it would be uneconomical to use solid beams.


=== Plates ===

Plates carry bending in two directions. A concrete flat slab is an example of a plate. Plates are understood by using continuum mechanics, but due to the complexity involved they are most often designed using a codified empirical approach, or computer analysis.
They can also be designed with yield line theory, where an assumed collapse mechanism is analyzed to give an upper bound on the collapse load. This technique is used in practice  but because the method provides an upper-bound, i.e. an unsafe prediction of the collapse load, for poorly conceived collapse mechanisms great care is needed to ensure that the assumed collapse mechanism is realistic.


=== Shells ===

Shells derive their strength from their form and carry forces in compression in two directions. A dome is an example of a shell. They can be designed by making a hanging-chain model, which will act as a catenary in pure tension and inverting the form to achieve pure compression.


=== Arches ===

Arches carry forces in compression in one direction only, which is why it is appropriate to build arches out of masonry. They are designed by ensuring that the line of thrust of the force remains within the depth of the arch. It is mainly used to increase the bountifulness of any structure.


=== Catenaries ===

Catenaries derive their strength from their form and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). They are almost always cable or fabric structures. A fabric structure acts as a catenary in two directions.


== Materials ==

Structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads. It also involves a knowledge of Corrosion engineering to avoid for example galvanic coupling of dissimilar materials. 
Common structural materials are:

Iron: wrought iron, cast iron
Concrete: reinforced concrete, prestressed concrete
Alloy: steel, stainless steel
Masonry
Timber: hardwood, softwood
Aluminium
Composite materials: plywood
Other structural materials: adobe, bamboo, carbon fibre, fiber reinforced plastic, mudbrick, roofing materials


== See also ==


== Notes ==


== References ==
Hibbeler, R. C. (2010). Structural Analysis. Prentice-Hall.
Blank, Alan; McEvoy, Michael; Plank, Roger (1993). Architecture and Construction in Steel. Taylor & Francis. ISBN 0-419-17660-8.
Hewson, Nigel R. (2003). Prestressed Concrete Bridges: Design and Construction. Thomas Telford. ISBN 0-7277-2774-5.
Heyman, Jacques (1999). The Science of Structural Engineering. Imperial College Press. ISBN 1-86094-189-3.
Hosford, William F. (2005). Mechanical Behavior of Materials. Cambridge University Press. ISBN 0-521-84670-6.


== Further reading ==
Blockley, David (2014). A Very Short Introduction to Structural Engineering. Oxford University Press ISBN 978-0-19967193-9.
Bradley, Robert E.; Sandifer, Charles Edward (2007). Leonhard Euler: Life, Work, and Legacy. Elsevier. ISBN 0-444-52728-1.
Chapman, Allan. (2005). England's Leornardo: Robert Hooke and the Seventeenth Century's Scientific Revolution. CRC Press. ISBN 0-7503-0987-3.
Dugas, René (1988). A History of Mechanics. Courier Dover Publications. ISBN 0-486-65632-2.
Feld, Jacob; Carper, Kenneth L. (1997). Construction Failure. John Wiley & Sons. ISBN 0-471-57477-5.
Galilei, Galileo. (translators: Crew, Henry; de Salvio, Alfonso) (1954). Dialogues Concerning Two New Sciences. Courier Dover Publications. ISBN 0-486-60099-8
Kirby, Richard Shelton (1990). Engineering in History. Courier Dover Publications. ISBN 0-486-26412-2.
Heyman, Jacques (1998). Structural Analysis: A Historical Approach. Cambridge University Press. ISBN 0-521-62249-2.
Labrum, E.A. (1994). Civil Engineering Heritage. Thomas Telford. ISBN 0-7277-1970-X.
Lewis, Peter R. (2004). Beautiful Bridge of the Silvery Tay. Tempus.
Mir, Ali (2001). Art of the Skyscraper: the Genius of Fazlur Khan. Rizzoli International Publications. ISBN 0-8478-2370-9.
Rozhanskaya, Mariam; Levinova, I. S. (1996). ""Statics"" in Morelon, Régis & Rashed, Roshdi (1996). Encyclopedia of the History of Arabic Science, vol. 2–3, Routledge. ISBN 0-415-02063-8
Whitbeck, Caroline (1998). Ethics in Engineering Practice and Research. Cambridge University Press. ISBN 0-521-47944-4.
Hoogenboom P.C.J. (1998). ""Discrete Elements and Nonlinearity in Design of Structural Concrete Walls"", Section 1.3 Historical Overview of Structural Concrete Modelling, ISBN 90-901184-3-8.
Nedwell, P.J.; Swamy, R.N.(ed) (1994). Ferrocement:Proceedings of the Fifth International Symposium. Taylor & Francis. ISBN 0-419-19700-1.


== External links ==

Structural Engineering Association – International
National Council of Structural Engineers Associations
Structural Engineering Institute, an institute of the American Society of Civil Engineers
Structurae database of structures
Structuremag The Definition of Structural Engineering
The EN Eurocodes are a series of 10 European Standards, EN 1990 – EN 1999, providing a common approach for the design of buildings and other civil engineering works and construction products","pandas(index=127, _1=127, text='structural engineering is a sub-discipline of civil engineering in which structural engineers are trained to design the \'bones and muscles\' that create the form and shape of man-made structures. structural engineers also must understand and calculate the stability, strength, rigidity and earthquake-susceptibility of built structures for buildings and nonbuilding structures. the structural designs are integrated with those of other designers such as architects and building services engineer and often supervise the construction of projects by contractors on site. they can also be involved in the design of machinery, medical equipment, and vehicles where structural integrity affects functioning and safety.  see glossary of structural engineering. structural engineering theory is based upon applied physical laws and empirical knowledge of the structural performance of different materials and geometries. structural engineering design uses a number of relatively simple structural concepts to build complex structural systems. structural engineers are responsible for making creative and efficient use of funds, structural elements and materials to achieve these goals.   == history ==  structural engineering dates back to 2700 b.c.e. when the step pyramid for pharaoh djoser was built by imhotep, the first engineer in history known by name. pyramids were the most common major structures built by ancient civilizations because the structural form of a pyramid is inherently stable and can be almost infinitely scaled (as opposed to most other structural forms, which cannot be linearly increased in size in proportion to increased loads).the structural stability of the pyramid, whilst primarily gained from its shape, relies also on the strength of the stone from which it is constructed, and its ability to support the weight of the stone above it. the limestone blocks were often taken from a quarry near the building site and have a compressive strength from 30 to 250 mpa (mpa = pa × 106). therefore, the structural strength of the pyramid stems from the material properties of the stones from which it was built rather than the pyramid\'s geometry. throughout ancient and medieval history most architectural design and construction were carried out by artisans, such as stonemasons and carpenters, rising to the role of master builder. no theory of structures existed, and understanding of how structures stood up was extremely limited, and based almost entirely on empirical evidence of \'what had worked before\'. knowledge was retained by guilds and seldom supplanted by advances. structures were repetitive, and increases in scale were incremental.no record exists of the first calculations of the strength of structural members or the behavior of structural material, but the profession of a structural engineer only really took shape with the industrial revolution and the re-invention of concrete (see history of concrete. the physical sciences underlying structural engineering began to be understood in the renaissance and have since developed into computer-based applications pioneered in the 1970s. catenaries derive their strength from their form and carry transverse forces in pure tension by deflecting (just as a tightrope will sag when someone walks on it). they are almost always cable or fabric structures. a fabric structure acts as a catenary in two directions.   == materials ==  structural engineering depends on the knowledge of materials and their properties, in order to understand how different materials support and resist loads. it also involves a knowledge of corrosion engineering to avoid for example galvanic coupling of dissimilar materials. common structural materials are:  iron: wrought iron, cast iron concrete: reinforced concrete, prestressed concrete alloy: steel, stainless steel masonry timber: hardwood, softwood aluminium composite materials: plywood other structural materials: adobe, bamboo, carbon fibre, fiber reinforced plastic, mudbrick, roofing materials   == see also ==   == notes ==   == references == hibbeler, r. c. (2010). structural analysis. prentice-hall. blank, alan; mcevoy, michael; plank, roger (1993). architecture and construction in steel. taylor & francis. isbn 0-419-17660-8. hewson, nigel r. (2003). prestressed concrete bridges: design and construction. thomas telford. isbn 0-7277-2774-5. heyman, jacques (1999). the science of structural engineering. imperial college press. isbn 1-86094-189-3. hosford, william f. (2005). mechanical behavior of materials. cambridge university press. isbn 0-521-84670-6.   == further reading == blockley, david (2014). a very short introduction to structural engineering. oxford university press isbn 978-0-19967193-9. bradley, robert e.; sandifer, charles edward (2007). leonhard euler: life, work, and legacy. elsevier. isbn 0-444-52728-1. chapman, allan. (2005). england\'s leornardo: robert hooke and the seventeenth century\'s scientific revolution. crc press. isbn 0-7503-0987-3. dugas, rené (1988). a history of mechanics. courier dover publications. isbn 0-486-65632-2. feld, jacob; carper, kenneth l. (1997). construction failure. john wiley & sons. isbn 0-471-57477-5. galilei, galileo. (translators: crew, henry; de salvio, alfonso) (1954). dialogues concerning two new sciences. courier dover publications. isbn 0-486-60099-8 kirby, richard shelton (1990). engineering in history. courier dover publications. isbn 0-486-26412-2. heyman, jacques (1998). structural analysis: a historical approach. cambridge university press. isbn 0-521-62249-2. labrum, e.a. (1994). civil engineering heritage. thomas telford. isbn 0-7277-1970-x. lewis, peter r. (2004). beautiful bridge of the silvery tay. tempus. mir, ali (2001). art of the skyscraper: the genius of fazlur khan. rizzoli international publications. isbn 0-8478-2370-9. rozhanskaya, mariam; levinova, i. s. (1996). ""statics"" in morelon, régis & rashed, roshdi (1996). encyclopedia of the history of arabic science, vol. 2–3, routledge. isbn 0-415-02063-8 whitbeck, caroline (1998). ethics in engineering practice and research. cambridge university press. isbn 0-521-47944-4. hoogenboom p.c.j. (1998). ""discrete elements and nonlinearity in design of structural concrete walls"", section 1.3 historical overview of structural concrete modelling, isbn 90-901184-3-8. nedwell, p.j.; swamy, r.n.(ed) (1994). ferrocement:proceedings of the fifth international symposium. taylor & francis. isbn 0-419-19700-1.   == external links ==  structural engineering association – international national council of structural engineers associations structural engineering institute, an institute of the american society of civil engineers structurae database of structures structuremag the definition of structural engineering the en eurocodes are a series of 10 european standards, en 1990 – en 1999, providing a common approach for the design of buildings and other civil engineering works and construction products')"
128,"Transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport.  
The planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  Technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  More sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). Passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system.
A review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. The National Council of Examiners for Engineering and Surveying (NCEES) list online the safety protocols, geometric design requirements, and signal timing.
Transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. The facilities support air, highway, railroad, pipeline, water, and even space transportation. The design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track).
Before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place. This inventory or database must include information on population, land use, economic activity, transportation facilities and services, travel patterns and volumes, laws and ordinances, regional financial resources, and community values and expectations. These inventories help the engineer create business models to complete accurate forecasts of the future conditions of the system.
Operations and management involve traffic engineering, so that vehicles move smoothly on the road or track.  Older techniques include signs, signals, markings, and tolling.  Newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. Human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.


== Highway engineering ==

Engineers in this specialization:

Handle the planning, design, construction, and operation of highways, roads, and other vehicular facilities as well as their related bicycle and pedestrian realms
Estimate the transportation needs of the public and then secure the funding for projects
Analyze locations of high traffic volumes and high collisions for safety and capacity
Use engineering principles to improve the transportation system
Utilize the three design controls, which are the drivers, the vehicles, and the roadways themselves


== Railroad engineering ==

Railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or monorails).  Typical tasks include determining horizontal and vertical alignment design, station location and design, and construction cost estimating.  Railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control.
Railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands.  In the United States, railway engineers work with elected officials in Washington, D.C. on rail transportation issues to make sure that the rail system meets the country's transportation needs.


== Port and harbor engineering ==

Port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities.


== Airport engineering ==
Airport engineers design and construct airports.  Airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. These engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.


== See also ==


== References ==


== External links ==
 Media related to Transport engineering at Wikimedia Commons
http://www.ite.org Institute of Transportation Engineers, a professional society for transportation engineers
http://www.itsa.org ITS America
http://www.asce.org ASCE","pandas(index=128, _1=128, text=""transportation engineering or transport engineering is the application of technology and scientific principles to the planning, functional design, operation and management of facilities for any mode of transportation in order to provide for the safe, efficient, rapid, comfortable, convenient, economical, and environmentally compatible movement of people and goods transport. the planning aspects of transportation engineering relate to elements of urban planning, and involve technical forecasting decisions and political factors.  technical forecasting of passenger travel usually involves an urban transportation planning model, requiring the estimation of trip generation (number of purposeful trips), trip distribution (destination choice, where the traveler is going), mode choice (mode that is being taken), and route assignment (the streets or routes that are being used).  more sophisticated forecasting can include other aspects of traveler decisions, including auto ownership, trip chaining (the decision to link individual trips together in a tour) and the choice of residential or business location (known as land use forecasting). passenger trips are the focus of transportation engineering because they often represent the peak of demand on any transportation system. a review of descriptions of the scope of various committees indicates that while facility planning and design continue to be the core of the transportation engineering field, such areas as operations planning, logistics, network analysis, financing, and policy analysis are also important, particularly to those working in highway and urban transportation. the national council of examiners for engineering and surveying (ncees) list online the safety protocols, geometric design requirements, and signal timing. transportation engineering, primarily involves planning, design, construction, maintenance, and operation of transportation facilities. the facilities support air, highway, railroad, pipeline, water, and even space transportation. the design aspects of transportation engineering include the sizing of transportation facilities (how many lanes or how much capacity the facility has), determining the materials and thickness used in pavement designing the geometry (vertical and horizontal alignment) of the roadway (or track). before any planning occurs an engineer must take what is known as an inventory of the area or, if it is appropriate, the previous system in place. this inventory or database must include information on population, land use, economic activity, transportation facilities and services, travel patterns and volumes, laws and ordinances, regional financial resources, and community values and expectations. these inventories help the engineer create business models to complete accurate forecasts of the future conditions of the system. operations and management involve traffic engineering, so that vehicles move smoothly on the road or track.  older techniques include signs, signals, markings, and tolling.  newer technologies involve intelligent transportation systems, including advanced traveler information systems (such as variable message signs), advanced traffic control systems (such as ramp meters), and vehicle infrastructure integration. human factors are an aspect of transportation engineering, particularly concerning driver-vehicle interface and user interface of road signs, signals, and markings.   == highway engineering ==  engineers in this specialization:  handle the planning, design, construction, and operation of highways, roads, and other vehicular facilities as well as their related bicycle and pedestrian realms estimate the transportation needs of the public and then secure the funding for projects analyze locations of high traffic volumes and high collisions for safety and capacity use engineering principles to improve the transportation system utilize the three design controls, which are the drivers, the vehicles, and the roadways themselves   == railroad engineering ==  railway engineers handle the design, construction, and operation of railroads and mass transit systems that use a fixed guideway (such as light rail or monorails).  typical tasks include determining horizontal and vertical alignment design, station location and design, and construction cost estimating.  railroad engineers can also move into the specialized field of train dispatching which focuses on train movement control. railway engineers also work to build a cleaner and safer transportation network by reinvesting and revitalizing the rail system to meet future demands.  in the united states, railway engineers work with elected officials in washington, d.c. on rail transportation issues to make sure that the rail system meets the country's transportation needs.   == port and harbor engineering ==  port and harbor engineers handle the design, construction, and operation of ports, harbors, canals, and other maritime facilities.   == airport engineering == airport engineers design and construct airports.  airport engineers must account for the impacts and demands of aircraft in their design of airport facilities. these engineers must use the analysis of predominant wind direction to determine runway orientation, determine the size of runway border and safety areas, different wing tip to wing tip clearances for all gates and must designate the clear zones in the entire port.   == see also ==   == references ==   == external links == media related to transport engineering at wikimedia commons http://www.ite.org institute of transportation engineers, a professional society for transportation engineers http://www.itsa.org its america http://www.asce.org asce"")"
